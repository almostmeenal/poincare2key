{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import spacy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.spatial as spatial\n",
    "import re\n",
    "import json\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "stop.append(\"'s\")\n",
    "with open('mj.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "H = nx.read_gexf(\"graph.gexf\")\n",
    "pos = nx.spring_layout(H)\n",
    "val = list(pos.values())\n",
    "x = []\n",
    "y = []\n",
    "for i in val:\n",
    "    x.append(i[0])\n",
    "    y.append(i[1])\n",
    "# plt.scatter(x,y)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UTILITY FUNCTIONS\n",
    "\n",
    "def get_pos_indexes(lst):\n",
    "\n",
    "    index = []\n",
    "\n",
    "    #Iterate over the list using indexes\n",
    "    for i in range(len(lst)-1):\n",
    "\n",
    "        #If first element was positive, add 0 as index\n",
    "        if i == 0:\n",
    "            if lst[i] > 0:\n",
    "                index.append(0)\n",
    "        #If successive values are negative and positive, i.e indexes switch over, collect the positive index\n",
    "        if lst[i] < 0 and lst[i+1] > 0:\n",
    "            index.append(i+1)\n",
    "\n",
    "    #If index list was empty, all negative characters were encountered, hence add -1 to index\n",
    "    if len(index) == 0:\n",
    "        index = [-1]\n",
    "\n",
    "    return index\n",
    "\n",
    "def preprocessor(text):\n",
    "    if type(text)!=str:\n",
    "        text = str(text)\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    doc = nlp(text)\n",
    "    lemma_list = []\n",
    "    for token in doc:\n",
    "        if token.is_alpha is True and token.is_stop is False and token not in stop:\n",
    "            lemma_list.append(token.lemma_)\n",
    "    return_list=[]\n",
    "    for i in range(len(lemma_list)):\n",
    "        temp_text = re.sub('<[^>]*>', '', lemma_list[i])\n",
    "        temp_text = re.sub('[\\W]+', '', temp_text.lower())\n",
    "        return_list.append(temp_text)\n",
    "    return(return_list)\n",
    "\n",
    "def evaluate_keywords(proposed,groundtruth):\n",
    "    \n",
    "    proposed_set = set(proposed)\n",
    "    true_set = set(groundtruth)\n",
    "    \n",
    "    true_positives = len(proposed_set.intersection(true_set))\n",
    "    \n",
    "    if len(proposed_set)==0:\n",
    "        precision = 0\n",
    "    \n",
    "    else:\n",
    "        precision = true_positives/float(len(proposed)) \n",
    "    \n",
    "    if len(true_set)==0:\n",
    "        recall = 0\n",
    "    \n",
    "    else:\n",
    "        recall = true_positives/float(len(true_set))\n",
    "        \n",
    "    if precision + recall > 0:\n",
    "        f1 = 2*precision*recall/float(precision + recall)\n",
    "    \n",
    "    else:\n",
    "        f1 = 0\n",
    "    \n",
    "    return (precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Radius which covers 10 percent of total nodes from center\n",
    "\n",
    "meh = []\n",
    "for i in range(0, 100, 5):\n",
    "    meh.append(len(point_tree.query_ball_point([0.0, 0.0], i/100))- len(H.nodes())/10)\n",
    "index = get_pos_indexes(meh)\n",
    "r = (index[0]*5)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find nodes which are in this Radius\n",
    "\n",
    "coordinate = np.column_stack((x, y))\n",
    "point_tree = spatial.cKDTree(coordinate)\n",
    "\n",
    "index = point_tree.query_ball_point([0.0, 0.0], r)\n",
    "wordlist= list(pos.keys())\n",
    "\n",
    "#Candidate Keywords\n",
    "candidate_key = []\n",
    "for i in index:\n",
    "    candidate_key.append(wordlist[i].split('.',1)[0])\n",
    "\n",
    "#Ground Truth\n",
    "final_res = data[0]['keywords'].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.15384615384615385, 0.4782608695652174, 0.23280423280423282)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_keywords(candidate_key, final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
