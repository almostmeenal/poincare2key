[{"id": "S221266781400080X", "text": "Several inorganic flocculating agents, including FeSO4, Al2(SO4)3, FeCl3 and an organic coagulant aid PAM, were used to treat the wastewater from domestic anima and poultry breeding in this paper. The ideal operating conditions were attained by single factor experiment and orthogonal design experiment. And the ideal operating conditions are follows: the dose of FeSO4 and PAM is 135.2mg/L and 0.384mg/L respectively when keeping the pH 10; and the corresponding removal rate is 55% and 60% for COD and turbidity. Based on the experimental results, this paper analyzes the main factors that affect wastewater flocculation treatment.", "keywords": "Al2(SO4)3,\nanalyzes the main factors\nCOD\nFeCl3\nFeSO4\ninorganic flocculating agents\norganic coagulant aid\northogonal design experiment\nPAM\nremoval rate\nsingle factor experiment\ntreat the wastewater\nwastewater flocculation\nwastewater flocculation treatment\n"}, {"id": "S221266781400080X", "text": "Several inorganic flocculating agents, including FeSO4, Al2(SO4)3, FeCl3 and an organic coagulant aid PAM, were used to treat the wastewater from domestic anima and poultry breeding in this paper. The ideal operating conditions were attained by single factor experiment and orthogonal design experiment. And the ideal operating conditions are follows: the dose of FeSO4 and PAM is 135.2mg/L and 0.384mg/L respectively when keeping the pH 10; and the corresponding removal rate is 55% and 60% for COD and turbidity. Based on the experimental results, this paper analyzes the main factors that affect wastewater flocculation treatment.", "keywords": "Al2(SO4)3,\nanalyzes the main factors\nCOD\nFeCl3\nFeSO4\ninorganic flocculating agents\norganic coagulant aid\northogonal design experiment\nPAM\nremoval rate\nsingle factor experiment\ntreat the wastewater\nwastewater flocculation\nwastewater flocculation treatment\n"}, {"id": "S2212671612000613", "text": "Video-oculography (VOG) is one of eye movement measurement methods. A key problem of VOG is to accurately estimate the pupil center. Then a pupil location method based on morphology and Canny algorithm was proposed for a WIFI-based VOG system which was developed our latest work. Moreover, a healthy volunteer was introduced to do sinusoidal tracking test to evaluate the pupil location method. Experimental results showed that the method could well trace eye movement and meet the anticipated results with stimulation.", "keywords": "accurately estimate the pupil center\nCanny algorithm\nevaluate the pupil location method\neye movement measurement\neye movement measurement methods\nhealthy volunteer\nmorphology\npupil location method\nsinusoidal tracking test\nstimulation\ntrace eye movement\nVideo-oculography\nVOG\nWIFI-based VOG system\n"}, {"id": "S2212671612000613", "text": "Video-oculography (VOG) is one of eye movement measurement methods. A key problem of VOG is to accurately estimate the pupil center. Then a pupil location method based on morphology and Canny algorithm was proposed for a WIFI-based VOG system which was developed our latest work. Moreover, a healthy volunteer was introduced to do sinusoidal tracking test to evaluate the pupil location method. Experimental results showed that the method could well trace eye movement and meet the anticipated results with stimulation.", "keywords": "accurately estimate the pupil center\nCanny algorithm\nevaluate the pupil location method\neye movement measurement\neye movement measurement methods\nhealthy volunteer\nmorphology\npupil location method\nsinusoidal tracking test\nstimulation\ntrace eye movement\nVideo-oculography\nVOG\nWIFI-based VOG system\n"}, {"id": "S1359646214000165", "text": "The first-principles calculations are performed using the Cambridge Serial Total Energy Package (CASTEP) [21] which implements the plane-wave pseudopotential DFT method. The exchange correlation functional is approximated using the generalized gradient approximation (PBE-GGA) [22], and the electron\u2013ion interactions are described by Vanderbilt-type ultrasoft pseudopotentials [23]. The plane wave basis set is truncated at a cutoff of 400eV, and the Brillouin-zone sampling was performed using the Monkhorst-Pack scheme with a k-point spacing in reciprocal space of 0.04\u00c5\u22121. Tests show that these computational parameters give results that are sufficiently accurate for present purposes. The ferromagnetism of nickel is accounted for by performing all calculations using spin polarization, starting at a ferromagnetic initial configuration and relaxing towards its ground state. However, for all compositions considered, the ground state electronic structure of each alloy is found to exhibit only very weak ferromagnetism, and the effect is not thought to influence their phase stability. Table 1 shows the calculated equilibrium lattice constants of the \u03b7 phase at various Ti concentrations, using partially ordered \u03b7P structures. The change in lattice constant upon Ti alloying is relatively small, but can be related to the \u223c10% larger covalent radius of Ti. The calculated lattice constants are in good agreement with the experimental values, which relate to an alloy with a Al/Ti ratio of \u223c2.75.\n", "keywords": "Al\nalloy\nBrillouin-zone sampling\nCambridge Serial Total Energy Package\nCASTEP\nelectron\nelectron\u2013ion interactions\nexchange correlation functional\nferromagnetism\nfirst-principles calculations\ngeneralized gradient approximation\nion\nk-point spacing\nMonkhorst-Pack scheme\nnickel\nPBE-GGA\nplane wave\nplane-wave pseudopotential DFT method\nspin polarization\nTi\nTi alloying\nVanderbilt-type ultrasoft pseudopotentials\n\u03b7P structures\n"}, {"id": "S1359646214000165", "text": "The first-principles calculations are performed using the Cambridge Serial Total Energy Package (CASTEP) [21] which implements the plane-wave pseudopotential DFT method. The exchange correlation functional is approximated using the generalized gradient approximation (PBE-GGA) [22], and the electron\u2013ion interactions are described by Vanderbilt-type ultrasoft pseudopotentials [23]. The plane wave basis set is truncated at a cutoff of 400eV, and the Brillouin-zone sampling was performed using the Monkhorst-Pack scheme with a k-point spacing in reciprocal space of 0.04\u00c5\u22121. Tests show that these computational parameters give results that are sufficiently accurate for present purposes. The ferromagnetism of nickel is accounted for by performing all calculations using spin polarization, starting at a ferromagnetic initial configuration and relaxing towards its ground state. However, for all compositions considered, the ground state electronic structure of each alloy is found to exhibit only very weak ferromagnetism, and the effect is not thought to influence their phase stability. Table 1 shows the calculated equilibrium lattice constants of the \u03b7 phase at various Ti concentrations, using partially ordered \u03b7P structures. The change in lattice constant upon Ti alloying is relatively small, but can be related to the \u223c10% larger covalent radius of Ti. The calculated lattice constants are in good agreement with the experimental values, which relate to an alloy with a Al/Ti ratio of \u223c2.75.\n", "keywords": "Al\nalloy\nBrillouin-zone sampling\nCambridge Serial Total Energy Package\nCASTEP\nelectron\nelectron\u2013ion interactions\nexchange correlation functional\nferromagnetism\nfirst-principles calculations\ngeneralized gradient approximation\nion\nk-point spacing\nMonkhorst-Pack scheme\nnickel\nPBE-GGA\nplane wave\nplane-wave pseudopotential DFT method\nspin polarization\nTi\nTi alloying\nVanderbilt-type ultrasoft pseudopotentials\n\u03b7P structures\n"}, {"id": "S0032386108010392", "text": "Microhardness can be related to other macroscopic mechanical properties such as yield stress, \u03c3, and elastic modulus, E, both derived from compression testing. For work-hardened metals, Tabor derived a direct proportionality between hardness and compressive yield stress: H\u22483\u03c3 [20]. However, it was soon realized that Tabor's relationship only applies to materials that exhibit full plasticity [9,10]. Deviations from this relationship have been reported for a number of metals, glasses and polymers where the elastic strains are non-negligible [9]. Hence, the different expressions describing the correlation of hardness with conventional macroscopic mechanical properties rely on the validity of the above-mentioned elasto-plastic models. In this way, hardness and yield stress no longer hold direct proportionality but their relationship depends on the specific material properties, such as Poisson's ratio and elastic modulus [9,11\u201313]. It has been shown that these elasto-plastic models not only satisfactorily explain an H/\u03c3 ratio of \u22482 for a number of polyethylene materials of different nature, but also theoretically account for the range of H/E ratios experimentally determined [21].\n", "keywords": "compression testing\nderived a direct proportionality\nDeviations from this relationship\nglasses\nhold direct proportionality but their relationship\nmaterials that exhibit full plasticity\nMicrohardness\npolyethylene materials\npolymers where the elastic strains\nspecific material properties,\nthe different expressions describing the correlation of hardness with conventional macroscopic mechanical properties\nwork-hardened metals\n"}, {"id": "S0370269304009232", "text": "We consider finite-time, future (sudden or Big Rip type) singularities which may occur even when strong energy condition is not violated but equation of state parameter is time-dependent. Recently, example of such singularity has been presented by Barrow, we found another example of it. Taking into account back reaction of conformal quantum fields near singularity, it is shown explicitly that quantum effects may delay (or make milder) the singularity. It is argued that if the evolution to singularity is realistic, due to quantum effects the universe may end up in de Sitter phase before scale factor blows up. This picture is generalized for braneworld where sudden singularity may occur on the brane with qualitatively similar conclusions.", "keywords": "back reaction of conformal quantum fields near singularity\nbrane\nbraneworld\nevolution to singularity\nexample of such singularity\nfinite-time, future (sudden or Big Rip type) singularities\nqualitatively similar conclusions\nquantum effects\nscale factor blows up\nsingularity\nstrong energy condition\nsudden singularity\nthe universe may end up in de Sitter phase\nwe found another example of it\n"}, {"id": "S0370269304009232", "text": "We consider finite-time, future (sudden or Big Rip type) singularities which may occur even when strong energy condition is not violated but equation of state parameter is time-dependent. Recently, example of such singularity has been presented by Barrow, we found another example of it. Taking into account back reaction of conformal quantum fields near singularity, it is shown explicitly that quantum effects may delay (or make milder) the singularity. It is argued that if the evolution to singularity is realistic, due to quantum effects the universe may end up in de Sitter phase before scale factor blows up. This picture is generalized for braneworld where sudden singularity may occur on the brane with qualitatively similar conclusions.", "keywords": "back reaction of conformal quantum fields near singularity\nbrane\nbraneworld\nevolution to singularity\nexample of such singularity\nfinite-time, future (sudden or Big Rip type) singularities\nqualitatively similar conclusions\nquantum effects\nscale factor blows up\nsingularity\nstrong energy condition\nsudden singularity\nthe universe may end up in de Sitter phase\nwe found another example of it\n"}, {"id": "S0370269304009232", "text": "We consider finite-time, future (sudden or Big Rip type) singularities which may occur even when strong energy condition is not violated but equation of state parameter is time-dependent. Recently, example of such singularity has been presented by Barrow, we found another example of it. Taking into account back reaction of conformal quantum fields near singularity, it is shown explicitly that quantum effects may delay (or make milder) the singularity. It is argued that if the evolution to singularity is realistic, due to quantum effects the universe may end up in de Sitter phase before scale factor blows up. This picture is generalized for braneworld where sudden singularity may occur on the brane with qualitatively similar conclusions.", "keywords": "back reaction of conformal quantum fields near singularity\nbrane\nbraneworld\nevolution to singularity\nexample of such singularity\nfinite-time, future (sudden or Big Rip type) singularities\nqualitatively similar conclusions\nquantum effects\nscale factor blows up\nsingularity\nstrong energy condition\nsudden singularity\nthe universe may end up in de Sitter phase\nwe found another example of it\n"}, {"id": "S0003491615000433", "text": "Nuclear theory devoted major efforts since 4 decades to describe thermalization in nuclear reactions, predominantly using semi-classical methods\u00a0 [13,14,10], in line with similar problems in quantum liquids\u00a0 [15,16]. There were attempts to develop improved molecular dynamics methods combining quantum features with a semi classical treatment of dynamical correlations\u00a0 [17,18]. Still, no clear-cut quantum approach is readily available yet, in spite of numerous formal attempts [19,20,10]. The field of clusters and nano structures is far younger but fast developing in relation to the ongoing developments of lasers and imaging techniques. Semiclassical approaches were also considered in the field to include some dynamical corrections\u00a0 [21,22] and could qualitatively describe dynamical processes. But such approaches are bound to simple metals with sufficiently delocalized wave functions, and thus smooth potentials justifying semiclassical approximations. The case of organic systems, in particular the much celebrated C60 \u00a0 [4,23], cannot be treated this way. Semi classical, and even classical approaches, can be used at very high excitations such as delivered by very intense laser pulses\u00a0 [2]. In such cases the system is blown up and details of its quantum mechanical features do not matter anymore. But for less violent scenarios, quantum shell effects cannot be ignored.\n", "keywords": "C60\ncombining quantum features\nfield of clusters and nano structures\nimaging techniques\nimproved molecular dynamics methods\nlasers\nnuclear reactions\nNuclear theory\norganic systems\nqualitatively describe dynamical processes.\nquantum approach\nquantum liquids\nquantum mechanical features\nquantum shell effects\nsemi-classical methods\nsemi classical treatment of dynamical correlations\nsimple metals\nsimple metals with sufficiently delocalized wave functions\nthermalization\nvery intense laser pulses\n"}, {"id": "S0009261408017028", "text": "We use open and close aperture Z-scan experiments, in analogy to the saturation absorption work discussed earlier in water [8], to respectively measure the \u03b2 and n2 for a series of primary alcohols with the help of 1560nm femtosecond laser pulses, however, with the important inclusion of an optical-chopper. The vibrational combination states of the alcohols are coupled by the femtosecond laser pulses at 1560nm. These couplings result in the absorption of 1560nm and the excited molecules undergo relaxation through non-radiative processes, which gives rise to transient thermal effects. These transient thermal effects are related to the pure optical nonlinearity of the samples and can be measured as a change in their n2 values [14]. The transient thermal effects of individual pulses accumulate in case of high repetition-rate lasers to produce a cumulative thermal effect at longer timescales. We measure this cumulative thermal effect with the mode-mismatched two-color pump\u2013probe experiment.\n", "keywords": "1560nm femtosecond laser pulses\nabsorption\nalcohols\naperture Z-scan experiments\ncumulative thermal effect\nfemtosecond laser pulses at 1560nm\nhigh repetition-rate lasers\nmeasure this cumulative thermal effect\nmode-mismatched two-color pump\u2013probe experiment\nmolecules\nnon-radiative processes\noptical-chopper\nprimary alcohols\npure optical nonlinearity\nsaturation absorption\ntransient thermal effects\nvibrational combination states\nwater\n"}, {"id": "S0009261408017028", "text": "We use open and close aperture Z-scan experiments, in analogy to the saturation absorption work discussed earlier in water [8], to respectively measure the \u03b2 and n2 for a series of primary alcohols with the help of 1560nm femtosecond laser pulses, however, with the important inclusion of an optical-chopper. The vibrational combination states of the alcohols are coupled by the femtosecond laser pulses at 1560nm. These couplings result in the absorption of 1560nm and the excited molecules undergo relaxation through non-radiative processes, which gives rise to transient thermal effects. These transient thermal effects are related to the pure optical nonlinearity of the samples and can be measured as a change in their n2 values [14]. The transient thermal effects of individual pulses accumulate in case of high repetition-rate lasers to produce a cumulative thermal effect at longer timescales. We measure this cumulative thermal effect with the mode-mismatched two-color pump\u2013probe experiment.\n", "keywords": "1560nm femtosecond laser pulses\nabsorption\nalcohols\naperture Z-scan experiments\ncumulative thermal effect\nfemtosecond laser pulses at 1560nm\nhigh repetition-rate lasers\nmeasure this cumulative thermal effect\nmode-mismatched two-color pump\u2013probe experiment\nmolecules\nnon-radiative processes\noptical-chopper\nprimary alcohols\npure optical nonlinearity\nsaturation absorption\ntransient thermal effects\nvibrational combination states\nwater\n"}, {"id": "S0009261408017028", "text": "We use open and close aperture Z-scan experiments, in analogy to the saturation absorption work discussed earlier in water [8], to respectively measure the \u03b2 and n2 for a series of primary alcohols with the help of 1560nm femtosecond laser pulses, however, with the important inclusion of an optical-chopper. The vibrational combination states of the alcohols are coupled by the femtosecond laser pulses at 1560nm. These couplings result in the absorption of 1560nm and the excited molecules undergo relaxation through non-radiative processes, which gives rise to transient thermal effects. These transient thermal effects are related to the pure optical nonlinearity of the samples and can be measured as a change in their n2 values [14]. The transient thermal effects of individual pulses accumulate in case of high repetition-rate lasers to produce a cumulative thermal effect at longer timescales. We measure this cumulative thermal effect with the mode-mismatched two-color pump\u2013probe experiment.\n", "keywords": "1560nm femtosecond laser pulses\nabsorption\nalcohols\naperture Z-scan experiments\ncumulative thermal effect\nfemtosecond laser pulses at 1560nm\nhigh repetition-rate lasers\nmeasure this cumulative thermal effect\nmode-mismatched two-color pump\u2013probe experiment\nmolecules\nnon-radiative processes\noptical-chopper\nprimary alcohols\npure optical nonlinearity\nsaturation absorption\ntransient thermal effects\nvibrational combination states\nwater\n"}, {"id": "S0375960115005630", "text": "The systems in which the Stern\u2013Gerlach force is most prominent are those with a high electromagnetic field gradient. Section 2 considers the implications of the coupling between the spin of a classical electron and the rapidly varying electromagnetic field produced by a laser-driven plasma wave. Sufficiently short, high-intensity laser pulses can form longitudinal waves within the electron density of a plasma. These density waves propagate with speed comparable to the group speed of the laser pulse. Not all plasma electrons form this wave, however; some of the electrons are caught up in the wave and accelerated by its high fields. The wave eventually collapses as these electrons damp the wave (the wave \u2018breaks\u2019). The extremely high electric field gradient of a plasma wave near wavebreaking provides an excellent theoretical testing ground for the effects of Stern\u2013Gerlach-type contributions to the trajectory of a test electron.\n", "keywords": "density waves\nelectromagnetic field\nelectromagnetic field gradient\nelectron\nelectrons\nelectrons damp the wave\nhigh electric field gradient of a plasma wave\nhigh-intensity laser pulses\nlaser\nlaser-driven plasma wave\nlaser pulse\nlongitudinal waves\nplasma\nplasma electrons\nStern\u2013Gerlach force\nStern\u2013Gerlach-type contributions\nwave \u2018breaks\u2019\n"}, {"id": "S0167931712003905", "text": "In summary, we have developed a technique for site-specific nanowire size reduction by FIB thinning. Transmission electron microscope images of a thinned tungsten composite nanowire with width reduced from 80 to 20nm show uniform shrinking along the length of the wire and high resolution images show no obvious changes of the morphology after thinning. The critical current density of the as-deposited wire and one thinned to a width of 50nm is 1.7\u00d7105 and 1.4\u00d7105A/cm2 at 4.26K, respectively, suggesting insignificant modulation of the electrical properties during thinning. These results suggest that FIB-milling is a potential approach for controllable size reduction with high resolution towards the observation of size- and quantum effects, as well as for construction of 3D superconducting nanodevices.\n", "keywords": "3D superconducting nanodevices\nas-deposited wire\nconstruction of 3D superconducting nanodevices\ncontrollable size reduction with high resolution towards the observation of size- and quantum effects\nFIB-milling\nFIB thinning\nhigh resolution images\nmodulation of the electrical properties\nsite-specific nanowire size reduction\nthinned tungsten composite nanowire\nthinning\nTransmission electron microscope images\nuniform shrinking\nwire\n"}, {"id": "S0009261412012365", "text": "Under these experimental conditions, the observed dynamics has to occur where the probe laser induces the reactions resulting in further ionization [30]. The two-step decay model [26] was applied to explain the above-mentioned fragmentation of DCPD to CPD, shown in Figure 8a. The fitting of the rise and decay components of the transients were done by Matlab\u00ae programming using the curve fitting Levenberg\u2013Marquardt algorithm. The best fit decay constants for the biexponential decay components of C10H12+ ion signal is \u03c41=35fs and \u03c42=240fs, while that for C5H6+ ion signal is \u03c41=36fs and \u03c42=280fs, respectively. These decay constants conform to the previously reported time constants of norbornene and norbornadiene [22,23]. The transients of the reaction fragment C5H6+ are sufficiently different from that of the parent ion C10H12+ indicating that we are studying the distinct dynamics of the neutrals and not that of the parent ion fragmentation [24]. Applying laser control principles under such experimental circumstances also confirms that we are controlling the product yield of C5H6+, resulting from the photochemical reaction of DCPD.\n", "keywords": "Applying laser control principles\nbest fit decay constants\nbiexponential decay components\nC10H12+\nC10H12+ ion signal\nC5H6+\nC5H6+ ion signal\ncontrolling the product yield of C5H6+\nCPD\ncurve fitting Levenberg\u2013Marquardt algorithm\nDCPD\ndecay constants\ndistinct dynamics of the neutrals\nfragmentation of DCPD to CPD\nfurther ionization\nlaser control principles\nMatlab\u00ae programming\nnorbornadiene\nnorbornene\nobserved dynamics\nparent ion\nparent ion fragmentation\nphotochemical reaction of DCPD\nprobe laser\nreactions\nrise and decay components\ntransients\ntwo-step decay model\n\u03c41=35fs\n\u03c41=36fs\n\u03c42=240fs\n\u03c42=280fs\n"}, {"id": "S0009261413004612", "text": "The control of the RP re-encounter probability finds a direct application to improve the performance of chemical devices. Here, we show how a simple-to-implement control scheme highly enhances the sensitivity of a model chemical magnetometer by up to two orders of magnitude. The basic idea behind a chemical magnetometer is that, since a change in the magnetic field modifies the amount of singlet products, one can reverse the reasoning and measure the chemical yield to estimate B. Intuitively, the magnetic sensitivity is high when a small change in the magnetic field intensity produces large effects on the singlet yield. Formally, it is defined as:(2)\u039bs(B)\u2261\u2202\u03a6s(B)\u2202B=\u222b0\u221epre(t)gs(B,t)dt,with gs(B,t)\u2261\u2202fs(B,t)\u2202B being the instantaneous magnetic sensitivity. The functional form of fs(B,t)=S\u03c1el(t)S strongly depends on the specific realization of the radical pair, in particular on the number of the surrounding nuclear spins. Here, we consider a radical pair in which the first electron spin is devoid of hyperfine interactions, while the second electron spin interacts isotropically with one spin-1 nucleus, e.g. nitrogen. In the context of the chemical compass (i.e. when the task is determining the magnetic field direction through anisotropic hyperfine interactions), an analogous configuration (with only one spin-1/2 nucleus) has been proposed [3], and numerically characterized [8], as being optimal: Additional nuclear spins would perturb the intuitive \u2018reference and probe\u2019 picture. The Hamiltonian then simplifies to H=-\u03b3eB(S1(z)+S2(z))+|\u03b3e|\u03b1S\u21922\u00b7I\u2192, where \u03b1 is the isotropic hyperfine coupling.\n", "keywords": "(2)\u039bs(B)\u2261\u2202\u03a6s(B)\u2202B=\u222b0\u221epre(t)gs(B,t)dt\nanalogous configuration\nanisotropic hyperfine interactions\nB\nchemical compass\nchemical devices\nchemical magnetometer\nchemical yield\ncontrol scheme\ndetermining the magnetic field direction through anisotropic hyperfine interactions\nelectron spin\ngs(B,t)\u2261\u2202fs(B,t)\u2202B\nhyperfine interactions\nH=-\u03b3eB(S1(z)+S2(z))+|\u03b3e|\u03b1S\u21922\u00b7I\u2192\nimprove the performance of chemical devices\ninstantaneous magnetic sensitivity\nisotropic hyperfine coupling\nmagnetic field\nmagnetic field direction\nmagnetic field intensity\nmagnetic sensitivity\nmodel chemical magnetometer\nnitrogen\nnuclear spins\nrealization of the radical pair\nRP re-encounter probability\nsinglet products\nsinglet yield\nspin-1/2 nucleus\nspin-1 nucleus\n\u03b1\n"}, {"id": "S0375960112002885", "text": "First-principles calculations have clarified the electronic structure and stability of the W@Si12 cluster under O2 molecule adsorption and reaction. Our results show that the W-encapsulated Si12 hexagonal prism cage is very inert to oxidation. The O2 molecule only weakly adsorbs onto the cluster at relatively low temperatures, in the range of several tens meV. However, significant reaction barriers (0.593\u20131.118 eV) for the O2 molecule on the cluster are identified on different adsorption sites, nevertheless, these reaction paths are spin forbidden reactions according to Winger\u02bcs spin selection rule. These results imply that O2 readily desorb from the cluster surface rather than dissociate and oxide the W@Si12 cluster upon excitations. In high temperature and high pressure conditions, the O2 molecules may dissociate on the preferential edge site by overcoming a significantly large energy barrier.\n", "keywords": "0.593\u20131.118 eV\nadsorption\ncluster\nelectronic structure and stability\nFirst-principles calculations\nhigh temperature and high pressure conditions\nlow temperatures\nO2\nO2 molecule adsorption and reaction\novercoming a significantly large energy barrier\noxidation\nreaction\nsignificant reaction barriers\nspin forbidden reactions\nW-encapsulated Si12 hexagonal prism cage\nWinger\u02bcs spin selection rule\nW@Si12 cluster\n"}, {"id": "S0021999115008153", "text": "As discussed above, proper inclusion of these interactions requires segment synchronization after every iteration. In order to minimize simulation errors due to incorrect values of the interactions potential, segments are synchronized after every iteration. Although relatively long communication times between remote processors may hinder this process in typical parallel computers, this is not the case for GPGPU architectures. Still, full recalculation of the interaction potential after each iteration is time consuming. Instead, the algorithm corrects the current potential by adding dipole contributions for every nearby charge that hopped during the previous iteration. Full updates of the interaction potential are only required for the grid points that are related to charges that hopped during the last iteration. Accumulative rounding errors that arise due to repetitive addition and subtraction are solve this by rounding all interaction potentials to a uniformly spaced range of floating point numbers.\n", "keywords": "adding dipole contributions\nalgorithm\ncorrects the current potential\nGPGPU architectures\ninteraction potential\nminimize simulation errors\nparallel computers\nprocessors\nproper inclusion of these interactions\nrecalculation of the interaction potential\nsegments are synchronized after every iteration\nsegment synchronization after every iteration\n"}, {"id": "S0021999115008153", "text": "As discussed above, proper inclusion of these interactions requires segment synchronization after every iteration. In order to minimize simulation errors due to incorrect values of the interactions potential, segments are synchronized after every iteration. Although relatively long communication times between remote processors may hinder this process in typical parallel computers, this is not the case for GPGPU architectures. Still, full recalculation of the interaction potential after each iteration is time consuming. Instead, the algorithm corrects the current potential by adding dipole contributions for every nearby charge that hopped during the previous iteration. Full updates of the interaction potential are only required for the grid points that are related to charges that hopped during the last iteration. Accumulative rounding errors that arise due to repetitive addition and subtraction are solve this by rounding all interaction potentials to a uniformly spaced range of floating point numbers.\n", "keywords": "adding dipole contributions\nalgorithm\ncorrects the current potential\nGPGPU architectures\ninteraction potential\nminimize simulation errors\nparallel computers\nprocessors\nproper inclusion of these interactions\nrecalculation of the interaction potential\nsegments are synchronized after every iteration\nsegment synchronization after every iteration\n"}, {"id": "S0010938X15002085", "text": "The oxide thickness is calculated using the weight gain and surface area. Artificially changing the surface profile will modify the surface area and calculated oxide thickness. SEM images of samples removed after 111 days oxidation were used to define the change in surface profile length with variation in applied roughness. The profile lengths extracted from the images were then used to modify the length of the sample and therefore the surface area. Table 1 shows the original oxide thicknesses after 111 days oxidation, the modified oxide thicknesses based on the surface profile length and the percentage difference. Results show a maximum decrease in the oxide thickness of 4% when using a surface which accounts for roughness. Comparing the change in oxide thickness between different surface finishes indicates a variation of less than 1%. As such, the impact of the variation in the profile length on the calculated oxide thickness is considered to be insignificant. In addition, if the differences in weight gain were only due to differences in surface area, rougher samples would be expected to demonstrate thicker oxides at the earliest stages of oxidation.\n", "keywords": "images\noxidation\noxide\noxides\nprofile\nsample\nsamples\nSEM images\nsurface\nsurface area\nsurface profile\n"}, {"id": "S0010938X15002085", "text": "The oxide thickness is calculated using the weight gain and surface area. Artificially changing the surface profile will modify the surface area and calculated oxide thickness. SEM images of samples removed after 111 days oxidation were used to define the change in surface profile length with variation in applied roughness. The profile lengths extracted from the images were then used to modify the length of the sample and therefore the surface area. Table 1 shows the original oxide thicknesses after 111 days oxidation, the modified oxide thicknesses based on the surface profile length and the percentage difference. Results show a maximum decrease in the oxide thickness of 4% when using a surface which accounts for roughness. Comparing the change in oxide thickness between different surface finishes indicates a variation of less than 1%. As such, the impact of the variation in the profile length on the calculated oxide thickness is considered to be insignificant. In addition, if the differences in weight gain were only due to differences in surface area, rougher samples would be expected to demonstrate thicker oxides at the earliest stages of oxidation.\n", "keywords": "images\noxidation\noxide\noxides\nprofile\nsample\nsamples\nSEM images\nsurface\nsurface area\nsurface profile\n"}, {"id": "S0010938X15002085", "text": "The oxide thickness is calculated using the weight gain and surface area. Artificially changing the surface profile will modify the surface area and calculated oxide thickness. SEM images of samples removed after 111 days oxidation were used to define the change in surface profile length with variation in applied roughness. The profile lengths extracted from the images were then used to modify the length of the sample and therefore the surface area. Table 1 shows the original oxide thicknesses after 111 days oxidation, the modified oxide thicknesses based on the surface profile length and the percentage difference. Results show a maximum decrease in the oxide thickness of 4% when using a surface which accounts for roughness. Comparing the change in oxide thickness between different surface finishes indicates a variation of less than 1%. As such, the impact of the variation in the profile length on the calculated oxide thickness is considered to be insignificant. In addition, if the differences in weight gain were only due to differences in surface area, rougher samples would be expected to demonstrate thicker oxides at the earliest stages of oxidation.\n", "keywords": "images\noxidation\noxide\noxides\nprofile\nsample\nsamples\nSEM images\nsurface\nsurface area\nsurface profile\n"}, {"id": "S0010938X15002085", "text": "The oxide thickness is calculated using the weight gain and surface area. Artificially changing the surface profile will modify the surface area and calculated oxide thickness. SEM images of samples removed after 111 days oxidation were used to define the change in surface profile length with variation in applied roughness. The profile lengths extracted from the images were then used to modify the length of the sample and therefore the surface area. Table 1 shows the original oxide thicknesses after 111 days oxidation, the modified oxide thicknesses based on the surface profile length and the percentage difference. Results show a maximum decrease in the oxide thickness of 4% when using a surface which accounts for roughness. Comparing the change in oxide thickness between different surface finishes indicates a variation of less than 1%. As such, the impact of the variation in the profile length on the calculated oxide thickness is considered to be insignificant. In addition, if the differences in weight gain were only due to differences in surface area, rougher samples would be expected to demonstrate thicker oxides at the earliest stages of oxidation.\n", "keywords": "images\noxidation\noxide\noxides\nprofile\nsample\nsamples\nSEM images\nsurface\nsurface area\nsurface profile\n"}, {"id": "S0010938X15002085", "text": "The oxide thickness is calculated using the weight gain and surface area. Artificially changing the surface profile will modify the surface area and calculated oxide thickness. SEM images of samples removed after 111 days oxidation were used to define the change in surface profile length with variation in applied roughness. The profile lengths extracted from the images were then used to modify the length of the sample and therefore the surface area. Table 1 shows the original oxide thicknesses after 111 days oxidation, the modified oxide thicknesses based on the surface profile length and the percentage difference. Results show a maximum decrease in the oxide thickness of 4% when using a surface which accounts for roughness. Comparing the change in oxide thickness between different surface finishes indicates a variation of less than 1%. As such, the impact of the variation in the profile length on the calculated oxide thickness is considered to be insignificant. In addition, if the differences in weight gain were only due to differences in surface area, rougher samples would be expected to demonstrate thicker oxides at the earliest stages of oxidation.\n", "keywords": "images\noxidation\noxide\noxides\nprofile\nsample\nsamples\nSEM images\nsurface\nsurface area\nsurface profile\n"}, {"id": "S0010938X15002085", "text": "The oxide thickness is calculated using the weight gain and surface area. Artificially changing the surface profile will modify the surface area and calculated oxide thickness. SEM images of samples removed after 111 days oxidation were used to define the change in surface profile length with variation in applied roughness. The profile lengths extracted from the images were then used to modify the length of the sample and therefore the surface area. Table 1 shows the original oxide thicknesses after 111 days oxidation, the modified oxide thicknesses based on the surface profile length and the percentage difference. Results show a maximum decrease in the oxide thickness of 4% when using a surface which accounts for roughness. Comparing the change in oxide thickness between different surface finishes indicates a variation of less than 1%. As such, the impact of the variation in the profile length on the calculated oxide thickness is considered to be insignificant. In addition, if the differences in weight gain were only due to differences in surface area, rougher samples would be expected to demonstrate thicker oxides at the earliest stages of oxidation.\n", "keywords": "images\noxidation\noxide\noxides\nprofile\nsample\nsamples\nSEM images\nsurface\nsurface area\nsurface profile\n"}, {"id": "S0010938X15002085", "text": "The oxide thickness is calculated using the weight gain and surface area. Artificially changing the surface profile will modify the surface area and calculated oxide thickness. SEM images of samples removed after 111 days oxidation were used to define the change in surface profile length with variation in applied roughness. The profile lengths extracted from the images were then used to modify the length of the sample and therefore the surface area. Table 1 shows the original oxide thicknesses after 111 days oxidation, the modified oxide thicknesses based on the surface profile length and the percentage difference. Results show a maximum decrease in the oxide thickness of 4% when using a surface which accounts for roughness. Comparing the change in oxide thickness between different surface finishes indicates a variation of less than 1%. As such, the impact of the variation in the profile length on the calculated oxide thickness is considered to be insignificant. In addition, if the differences in weight gain were only due to differences in surface area, rougher samples would be expected to demonstrate thicker oxides at the earliest stages of oxidation.\n", "keywords": "images\noxidation\noxide\noxides\nprofile\nsample\nsamples\nSEM images\nsurface\nsurface area\nsurface profile\n"}, {"id": "S0010938X15002085", "text": "The oxide thickness is calculated using the weight gain and surface area. Artificially changing the surface profile will modify the surface area and calculated oxide thickness. SEM images of samples removed after 111 days oxidation were used to define the change in surface profile length with variation in applied roughness. The profile lengths extracted from the images were then used to modify the length of the sample and therefore the surface area. Table 1 shows the original oxide thicknesses after 111 days oxidation, the modified oxide thicknesses based on the surface profile length and the percentage difference. Results show a maximum decrease in the oxide thickness of 4% when using a surface which accounts for roughness. Comparing the change in oxide thickness between different surface finishes indicates a variation of less than 1%. As such, the impact of the variation in the profile length on the calculated oxide thickness is considered to be insignificant. In addition, if the differences in weight gain were only due to differences in surface area, rougher samples would be expected to demonstrate thicker oxides at the earliest stages of oxidation.\n", "keywords": "images\noxidation\noxide\noxides\nprofile\nsample\nsamples\nSEM images\nsurface\nsurface area\nsurface profile\n"}, {"id": "S1877750315000460", "text": "FabHemeLB is a Python tool which helps automate the construction and management of ensemble simulation workflows. FabHemeLB is an extended version of FabSim [27] configured to handle HemeLB operations. Both FabSim and FabHemeLB help to automate application deployment, execution and data analysis on remote resources. FabHemeLB can be used to compile and build HemeLB on any remote resource, to reuse machine-specific configurations, and to organize and curate simulation data. It can also submit HemeLB jobs to a remote resource specifying the number of cores and the wall clock time limit for completing a simulation. The tool is also able to monitor the queue status on remote resources, fetch results of completed jobs, and can conveniently combine functionalities into single one-line commands. In general, the FabHemeLB commands have the following structure:\n", "keywords": "automate application deployment, execution and data analysis on remote resources\nautomate the construction and management of ensemble simulation workflows\ncombine functionalities into single one-line commands\ndata analysis\ndeployment\nexecution\nFabHemeLB\nFabSim\nfetch results of completed jobs\nHemeLB\nmonitor the queue status on remote resources\norganize and curate simulation data\nPython tool\nsimulation\nsimulation data\n"}, {"id": "S0022311515002354", "text": "Hydrides, once precipitated in zirconium, degrade the mechanical properties of a component, leading to reductions in tensile strength, ductility and fracture toughness [35\u201340]. These changes can ultimately compromise the integrity of cladding during normal operating life , accident conditions and fuel storage [13]. As well as the degradation of mechanical properties, the presence of hydrides can also affect phenomena like pellet cladding mechanical interaction (PCMI); or introduce mechanisms for failure, such as delayed hydride cracking (DHC). The former mechanism is the product of thermal expansion in fuel pellets introducing stresses into the cladding, which may then lead to the formation of cracks in areas made brittle by large hydride concentrations [13]. The latter mechanism, DHC, is a sub-critical, time dependent cracking phenomenon that requires long range hydrogen diffusion for repeated local hydride growth and fracture at a hydrostatic tensile stress raiser [5,41,42]. The process occurs over an extended period of time under a continuously applied load that is below the yield stress of the material [5,41,42].\n", "keywords": "cladding\ncracking phenomenon\ndelayed hydride cracking\nDHC\nfuel\nfuel pellets\nhydride\nhydrides\nHydrides\nhydrogen\nhydrogen diffusion\nhydrostatic tensile stress raiser\nlocal hydride growth\nPCMI\npellet cladding mechanical interaction\nthermal expansion\nzirconium\n"}, {"id": "S037596011300741X", "text": "In exploring the WKB limit of quantum theory, Bohm [2] was the first to notice that although one starts with all the ambiguities about the nature of a quantum system, the first order approximation fits the ordinary classical ontology. By that we mean that the real part of the Schr\u00f6dinger equation under polar decomposition of the wave function becomes the classical Hamilton\u2013Jacobi equation in the limit where terms involving \u210f are neglected. In contrast to this approach, in this Letter we show that the classical trajectories arise from a short-time quantum propagator when terms of O(\u0394t2) can be neglected. This fact was actually already observed by Holland some twenty years ago: In page 269 of his book [6] infinitesimal time intervals are considered whose sequence constructs a finite path. It is shown that along each segment the motion is classical (negligible quantum potential), and that it follows that the quantum path may be decomposed into a sequence of segments along each of which the classical action is a minimum. The novel contribution of the present Letter is an improved proof of Holland\u02bcs result using an improved version of the propagator due to Makri and Miller [9,10]. (See also de Gosson [3] for a further discussion.)\n", "keywords": "Hamilton\u2013Jacobi equation\ninfinitesimal time intervals\npropagator\nSchr\u00f6dinger equation under polar decomposition of the wave function\nshort-time quantum propagator\nWKB limit of quantum theory\n"}, {"id": "S037596011300741X", "text": "In exploring the WKB limit of quantum theory, Bohm [2] was the first to notice that although one starts with all the ambiguities about the nature of a quantum system, the first order approximation fits the ordinary classical ontology. By that we mean that the real part of the Schr\u00f6dinger equation under polar decomposition of the wave function becomes the classical Hamilton\u2013Jacobi equation in the limit where terms involving \u210f are neglected. In contrast to this approach, in this Letter we show that the classical trajectories arise from a short-time quantum propagator when terms of O(\u0394t2) can be neglected. This fact was actually already observed by Holland some twenty years ago: In page 269 of his book [6] infinitesimal time intervals are considered whose sequence constructs a finite path. It is shown that along each segment the motion is classical (negligible quantum potential), and that it follows that the quantum path may be decomposed into a sequence of segments along each of which the classical action is a minimum. The novel contribution of the present Letter is an improved proof of Holland\u02bcs result using an improved version of the propagator due to Makri and Miller [9,10]. (See also de Gosson [3] for a further discussion.)\n", "keywords": "Hamilton\u2013Jacobi equation\ninfinitesimal time intervals\npropagator\nSchr\u00f6dinger equation under polar decomposition of the wave function\nshort-time quantum propagator\nWKB limit of quantum theory\n"}, {"id": "S037596011300741X", "text": "In exploring the WKB limit of quantum theory, Bohm [2] was the first to notice that although one starts with all the ambiguities about the nature of a quantum system, the first order approximation fits the ordinary classical ontology. By that we mean that the real part of the Schr\u00f6dinger equation under polar decomposition of the wave function becomes the classical Hamilton\u2013Jacobi equation in the limit where terms involving \u210f are neglected. In contrast to this approach, in this Letter we show that the classical trajectories arise from a short-time quantum propagator when terms of O(\u0394t2) can be neglected. This fact was actually already observed by Holland some twenty years ago: In page 269 of his book [6] infinitesimal time intervals are considered whose sequence constructs a finite path. It is shown that along each segment the motion is classical (negligible quantum potential), and that it follows that the quantum path may be decomposed into a sequence of segments along each of which the classical action is a minimum. The novel contribution of the present Letter is an improved proof of Holland\u02bcs result using an improved version of the propagator due to Makri and Miller [9,10]. (See also de Gosson [3] for a further discussion.)\n", "keywords": "Hamilton\u2013Jacobi equation\ninfinitesimal time intervals\npropagator\nSchr\u00f6dinger equation under polar decomposition of the wave function\nshort-time quantum propagator\nWKB limit of quantum theory\n"}, {"id": "S0963869514001066", "text": "In the Total Focusing Method (TFM) the beam is synthetically focused at every point in the target region [7] as follows. After obtaining the FMC data, the target region, which is in the x\u2013z plane in 2D (Fig. 1), is discretized into a grid. The signals from all elements in the array are then summed to synthesize a focus at every point in this grid. Linear interpolation of the time domain signals is necessary since they are discretely sampled. The intensity of the TFM image ITFM at any point (x,z) is given by:(10)ITFM(x,z)=|\u2211HTR(1c((xT\u2212x)2+z2+(xR\u2212x)2+z2))|forallT,Rwhere HTR(t) is the Hilbert transform of a signal uTR(t) in the FMC data, xT is the x-position of the transmitting element (T) and xR is the x-position of the receiving element (R). Note that the z-position of all elements is zero (Fig. 3a). The summation is carried out for all possible transmitter\u2013receiver pairs and therefore uses all the information captured with FMC. This algorithm is referred to as \u2018conventional TFM\u2019 in this paper.\n", "keywords": "conventional TFM\ndiscretely sampled\ndiscretized into a grid\nFMC\nFMC data\ngrid\nHilbert transform\nLinear interpolation\nR\nreceiving element\nsummation\nsummed\nsynthesize a focus at every point in this grid\nT\nTFM\nTFM image\nTFM image ITFM\nTotal Focusing Method\ntransmitter\u2013receiver pairs\ntransmitting element\nx\u2013z plane\n"}, {"id": "S0927025612000249", "text": "The need for power generation industry to improve the thermal efficiency of power plant has led to the development of 9\u201312% Cr martensitic steels. The development of and research on P91 steels started since late 1970s and early 1990s, respectively [1]. The work has focussed on their creep strengths due to its intended application at high temperature. Recently, the introduction of more cyclic operation of power plant has introduced the possibility of fatigue problems. Bore cracking due to the effects of varying steam warming has been reported [2]. The temperature cycling causes thermal gradients between the inside and outside of components and this can cause cyclic stress levels to be of concerns. Recently, research on thermal\u2013mechanical analysis of P91 has been carried out including the characterisation of the cyclic behaviour of the material using the two-layer and unified visco-plasticity models [3,4].\n", "keywords": "9\u201312% Cr martensitic steels\ncharacterisation of the cyclic behaviour of the material\ndevelopment of and research on P91 steels\nfatigue problems\nimprove the thermal efficiency of power plant\nP91\nP91 steels\npower generation\npower plant\nsteam warming\ntemperature cycling\nthe material\nthermal\u2013mechanical analysis of P91\ntwo-layer and unified visco-plasticity models\n"}, {"id": "S0370269304009347", "text": "The presence of chaotic motion in nuclear systems has been firmly related with the statistics of high-lying energy levels\u00a0[8,9]. Poisson distributions of normalized spacings of successive nuclear or atomic excited levels with the same spin and parity correspond to integrable classical dynamics, while Wigner's statistics signal chaotic motion in the corresponding classical regime\u00a0[10]. Intermediate situations are more difficult to assess. Very recently a proposal has been made to treat the spectral fluctuations \u03b4n as discrete time series [11]. Defining (1)\u03b4n=\u222b\u2212\u221eEn+1\u03c1\u02dc(E)dE\u2212n, with \u03c1\u02dc(E) the mean level density which allows the mapping to dimensionless levels with unitary average level density, and analyzing the energy fluctuations as a discrete time series, they found that nuclear power spectra behave like 1f noise, postulating that this might be a characteristic signature of generic quantum chaotic systems. In the present work we implement this idea, using the 1f spectral behavior as a test for the presence of chaos in nuclear mass errors.\n", "keywords": "1f spectral behavior\nanalyzing the energy fluctuations\nchaos in nuclear mass errors\nchaotic motion\nchaotic motion in nuclear systems\nclassical regime\ndiscrete time series\nexcited levels\nintegrable classical dynamics\nmapping to dimensionless levels\nnuclear power spectra\nparity\nPoisson distributions\nquantum chaotic systems\nspectral fluctuations\nspin\nstatistics of high-lying energy levels\nunitary average level density\nWigner's statistics\n\u03b4n\n"}, {"id": "S0021999115001412", "text": "Inspired by energy-fueled phenomena such as cortical cytoskeleton flows [46,45,32] during biological morphogenesis, the theory of active polar viscous gels has been developed [37,33]. The theory models the continuum, macroscopic mechanics of a collection of uniaxial active agents, embedded in a viscous bulk medium, in which internal stresses are induced due to dissipation of energy [41,58]. The energy-consuming uniaxial polar agents constituting the gel are modeled as unit vectors. The average of unit vectors in a small local volume at each point defines the macroscopic directionality of the agents and is described by a polarization field. The polarization field is governed by an equation of motion accounting for energy consumption and for the strain rate in the fluid. The relationship between the strain rate and the stress in the fluid is provided by a constitutive equation that accounts for anisotropic, polar agents and consumption of energy. These equations, along with conservation of momentum, provide a continuum hydrodynamic description modeling active polar viscous gels as an energy consuming, anisotropic, non-Newtonian fluid [37,33,32,41]. The resulting partial differential equations governing the hydrodynamics of active polar viscous gels are, however, in general analytically intractable.\n", "keywords": "active polar viscous gels\nbiological morphogenesis\nconstitutive equation\ncontinuum hydrodynamic description\ncortical cytoskeleton flows\nenergy consuming, anisotropic, non-Newtonian fluid\nequation of motion\nfluid\ngel\nmodels the continuum, macroscopic mechanics\npolarization field\npolar viscous gels\ntheory of active polar viscous gels\nThese equations, along with conservation of momentum\nuniaxial active agents\nuniaxial polar agents\nviscous bulk medium\n"}, {"id": "S1875952116300209", "text": "In order to test whether haptic patterns can convey or enhance the mood music of a movie, an affective movie clip corpus was required consisting of clips labeled according to the emotion conveyed in the mood music. The following database collections were examined as possible sources for the corpus: the Emotional Movie Database (EMDB) [19], and Film Stim [20]. However, these were discarded after review as unsuitable. The aim of this study is to enhance the mood in the film score, and in the case of the clips in the EMDB, no audio is provided which deemed the clips unsuitable. In the case of the Film Stim database, the clips are in French rather than English, and with no subtitles which where also deemed unsuitable since the studies are carried out with English speaking participants. Furthermore, the Film Stim selection is based on the affective content of the narrative as in most of them there is no music which is also unsuitable as discussed. From our review of available database collections, it was found that at present there is no standard corpus of affective movie clips where the affective indexing referred to the musical score of the clip.\n", "keywords": "affective movie clip corpus\ndatabase collections\nEMDB\nEmotional Movie Database\nEnglish speaking participants\nenhance the mood\nFilm Stim\nFilm Stim database\nFilm Stim selection\nreview\nreview of available database collections\ntest whether haptic patterns can convey or enhance the mood music\n"}, {"id": "S0021999115007895", "text": "The dynamics of various physical phenomena, such as the movement of pendulums, planets, or water waves can be described in a variational framework. The development of variational principles for classical mechanics traces back to Euler, Lagrange, and Hamilton; an overview of this history can be found in [1,19]. This approach allows to express all the dynamics of a system in a single functional \u2013 the Lagrangian \u2013 which is an action integral. Hamiltonian mechanics is a reformulation of Lagrangian mechanics which provides a convenient framework to study the symmetry properties of a system. This is expressed by Noether's theorem which establishes the direct connection between the symmetry properties of Hamiltonian systems and conservation laws. When one approximates the system numerically, it is advantageous to preserve the Hamiltonian structure also at the discrete level. Given that Hamiltonian systems are abundant in nature, their numerical approximation is therefore a topic of significant relevance.\n", "keywords": "action integral\napproach\ndynamics of a system\nexpress all the dynamics of a system\nframework\nHamiltonian mechanics\nHamiltonian systems\nLagrangian\nmovement of pendulums, planets, or water waves\nnumerical approximation\npendulums\nphysical phenomena\nplanets\nreformulation of Lagrangian mechanics\nsingle functional\nstudy the symmetry properties of a system\nsystem\nThe dynamics of various physical phenomena\nvariational framework\nvariational principles\nwater waves\n"}, {"id": "S0021999115007895", "text": "The dynamics of various physical phenomena, such as the movement of pendulums, planets, or water waves can be described in a variational framework. The development of variational principles for classical mechanics traces back to Euler, Lagrange, and Hamilton; an overview of this history can be found in [1,19]. This approach allows to express all the dynamics of a system in a single functional \u2013 the Lagrangian \u2013 which is an action integral. Hamiltonian mechanics is a reformulation of Lagrangian mechanics which provides a convenient framework to study the symmetry properties of a system. This is expressed by Noether's theorem which establishes the direct connection between the symmetry properties of Hamiltonian systems and conservation laws. When one approximates the system numerically, it is advantageous to preserve the Hamiltonian structure also at the discrete level. Given that Hamiltonian systems are abundant in nature, their numerical approximation is therefore a topic of significant relevance.\n", "keywords": "action integral\napproach\ndynamics of a system\nexpress all the dynamics of a system\nframework\nHamiltonian mechanics\nHamiltonian systems\nLagrangian\nmovement of pendulums, planets, or water waves\nnumerical approximation\npendulums\nphysical phenomena\nplanets\nreformulation of Lagrangian mechanics\nsingle functional\nstudy the symmetry properties of a system\nsystem\nThe dynamics of various physical phenomena\nvariational framework\nvariational principles\nwater waves\n"}, {"id": "S0021999115007895", "text": "The dynamics of various physical phenomena, such as the movement of pendulums, planets, or water waves can be described in a variational framework. The development of variational principles for classical mechanics traces back to Euler, Lagrange, and Hamilton; an overview of this history can be found in [1,19]. This approach allows to express all the dynamics of a system in a single functional \u2013 the Lagrangian \u2013 which is an action integral. Hamiltonian mechanics is a reformulation of Lagrangian mechanics which provides a convenient framework to study the symmetry properties of a system. This is expressed by Noether's theorem which establishes the direct connection between the symmetry properties of Hamiltonian systems and conservation laws. When one approximates the system numerically, it is advantageous to preserve the Hamiltonian structure also at the discrete level. Given that Hamiltonian systems are abundant in nature, their numerical approximation is therefore a topic of significant relevance.\n", "keywords": "action integral\napproach\ndynamics of a system\nexpress all the dynamics of a system\nframework\nHamiltonian mechanics\nHamiltonian systems\nLagrangian\nmovement of pendulums, planets, or water waves\nnumerical approximation\npendulums\nphysical phenomena\nplanets\nreformulation of Lagrangian mechanics\nsingle functional\nstudy the symmetry properties of a system\nsystem\nThe dynamics of various physical phenomena\nvariational framework\nvariational principles\nwater waves\n"}, {"id": "S0021999115007895", "text": "The dynamics of various physical phenomena, such as the movement of pendulums, planets, or water waves can be described in a variational framework. The development of variational principles for classical mechanics traces back to Euler, Lagrange, and Hamilton; an overview of this history can be found in [1,19]. This approach allows to express all the dynamics of a system in a single functional \u2013 the Lagrangian \u2013 which is an action integral. Hamiltonian mechanics is a reformulation of Lagrangian mechanics which provides a convenient framework to study the symmetry properties of a system. This is expressed by Noether's theorem which establishes the direct connection between the symmetry properties of Hamiltonian systems and conservation laws. When one approximates the system numerically, it is advantageous to preserve the Hamiltonian structure also at the discrete level. Given that Hamiltonian systems are abundant in nature, their numerical approximation is therefore a topic of significant relevance.\n", "keywords": "action integral\napproach\ndynamics of a system\nexpress all the dynamics of a system\nframework\nHamiltonian mechanics\nHamiltonian systems\nLagrangian\nmovement of pendulums, planets, or water waves\nnumerical approximation\npendulums\nphysical phenomena\nplanets\nreformulation of Lagrangian mechanics\nsingle functional\nstudy the symmetry properties of a system\nsystem\nThe dynamics of various physical phenomena\nvariational framework\nvariational principles\nwater waves\n"}, {"id": "S0167931713004991", "text": "The number of experiments conducted was reduced by selecting the four most important parameters for variation, Table 1 while the remaining parameters were kept constant. The O2 flow rate (QO2) was keep constant at 99sccm, while the SF6 flow rate (QSF6) was varied between 0 and 20sccm. The pressure in the etch chamber was controlled to keep the gas density stable. Since the pressure has a pronounced effect on etch characteristics, the pressure (p) was varied between 20 and 40mTorr. It should be noted that the system was run in automatic pressure control mode, which continuously adjusts the throttle valve to keep a constant pressure during etch. The coil power (PC) was fixed at 1000W, while the bias power (PB) was varied between 0 and 30W. Finally, the substrate chuck temperature (T) was controlled between 10 and 50\u00b0C. This design resulted in a full factorial screening in four parameters, where three center points were used to check for quadratic curvature, where the quadratic term of a parameter is needed to generate a valid model. The total number of experiments in this setup is 19, which were processed for 20min each. The experiments in the design were carried out in random order.\n", "keywords": "adjusts the throttle valve\nautomatic pressure control\ncheck for quadratic curvature\netch chamber\nfactorial screening\ngas\nkeep the gas density stable\npressure in the etch chamber\nthrottle valve\n"}, {"id": "S0167931713004991", "text": "The number of experiments conducted was reduced by selecting the four most important parameters for variation, Table 1 while the remaining parameters were kept constant. The O2 flow rate (QO2) was keep constant at 99sccm, while the SF6 flow rate (QSF6) was varied between 0 and 20sccm. The pressure in the etch chamber was controlled to keep the gas density stable. Since the pressure has a pronounced effect on etch characteristics, the pressure (p) was varied between 20 and 40mTorr. It should be noted that the system was run in automatic pressure control mode, which continuously adjusts the throttle valve to keep a constant pressure during etch. The coil power (PC) was fixed at 1000W, while the bias power (PB) was varied between 0 and 30W. Finally, the substrate chuck temperature (T) was controlled between 10 and 50\u00b0C. This design resulted in a full factorial screening in four parameters, where three center points were used to check for quadratic curvature, where the quadratic term of a parameter is needed to generate a valid model. The total number of experiments in this setup is 19, which were processed for 20min each. The experiments in the design were carried out in random order.\n", "keywords": "adjusts the throttle valve\nautomatic pressure control\ncheck for quadratic curvature\netch chamber\nfactorial screening\ngas\nkeep the gas density stable\npressure in the etch chamber\nthrottle valve\n"}, {"id": "S2212667814001294", "text": "Knowledge Management (KM) is one of the hotspots for research in the past decade. In most cases, the number of users in a Knowledge Management System (KMS) is very large, and they are from varied departments, even other companies. In this paper, some defects when existing methods about access control and recommendation are deployed in KMS are analyzed to show that these widely-used approaches need to be extended. To overcome the deficiencies of previous work, this paper proposes an extended Role-Based Access Control (RBAC) method and a hybrid recommendation approach for Knowledge Management System. Also, a real-life system is presented to verify the proposed methodology.", "keywords": "defects when existing methods about access control and recommendation are deployed in KMS are analyzed\nextended\nhybrid recommendation approach\nKM\nKMS\nKnowledge Management\nKnowledge Management System\nRBAC\nreal-life system\nRole-Based Access Contro\nverify the proposed methodology\n"}, {"id": "S0301010415300355", "text": "Alternatively to H-atom photodetachment from the intermediate radicals, the latter may serve as reducing agents. Evidence has been reported in recent years that the pyridinyl radical (PyH) is an exceptionally strong reducing agent which can even reduce CO2 to formaldehyde, formic acid or methanol with suitable catalyzers [27\u201329], albeit the mechanisms of these reactions are currently poorly understood [30\u201332]. The theoretically predicted dissociation thresholds of the AcH, AOH and BAH radicals are about 2.7eV, 2.5eV and 3.0eV, respectively (see Fig. 4), while the predicted dissociation threshold of the pyridinyl radical is much lower, about 1.7eV [1]. Pyridinyl is thus a significantly stronger reductant than acridinyl and related radicals. It is therefore not expected that the latter will be able to reduce carbon dioxide in dark reactions.\n", "keywords": "AcH, AOH and BAH radicals\nacridinyl\ncarbon dioxide\ncatalyzers\nCO2\nformaldehyde\nformic acid\nH-atom photodetachment\nintermediate radicals\nmethanol\nPyH\nPyridinyl\npyridinyl radical\nradicals\nreduce carbon dioxide in dark reactions\nreducing agent\nreducing agents\nreductant\n"}, {"id": "S0038092X15000559", "text": "Progressive photon mapping was first proposed by Hachisuka et al. (2008) as an iterative extension of the standard static photon mapping approach as implemented in the Radiance extension. It combines multiple smaller photon maps to approximate a much larger one which may not fit into memory using the traditional approach. Through iteration, the process mitigates the noise inherent in Monte Carlo raytracing by combining successive results and averaging them. At the same time, the density estimate bandwidth1Bandwidth describes the support, or area of influence, of a filter used to weight the photons retrieved from the photon map during a nearest neighbour lookup on a surface (Jensen, 2001). The resulting irradiance is proportional to the photon density, and the bandwidth is defined by the distance (radius) to the furthest photon found. In this paper, we generalise the term to describe either the radius or the number of nearest neighbours for a density estimate, depending on the implementation.1 (radius or number of nearest photons) is gradually reduced to mitigate bias. As Hachisuka points out, the accumulated density estimates converge to an unbiased solution in the limit.\n", "keywords": "combining successive results and averaging\ndensity estimate bandwidth1Bandwidth\ndescribe either the radius or the number of nearest neighbours for a density estimate\nirradiance\niterative extension of the standard static photon mapping\nmitigate bias\nmitigates the noise\nMonte Carlo raytracing\nnearest neighbour lookup\nphoton\nphoton map\nphoton maps\nphotons\nProgressive photon mapping\nRadiance extension\nsupport, or area of influence, of a filter\nsurface\ntraditional approach\nweight the photons\n"}, {"id": "S0038092X15000559", "text": "Progressive photon mapping was first proposed by Hachisuka et al. (2008) as an iterative extension of the standard static photon mapping approach as implemented in the Radiance extension. It combines multiple smaller photon maps to approximate a much larger one which may not fit into memory using the traditional approach. Through iteration, the process mitigates the noise inherent in Monte Carlo raytracing by combining successive results and averaging them. At the same time, the density estimate bandwidth1Bandwidth describes the support, or area of influence, of a filter used to weight the photons retrieved from the photon map during a nearest neighbour lookup on a surface (Jensen, 2001). The resulting irradiance is proportional to the photon density, and the bandwidth is defined by the distance (radius) to the furthest photon found. In this paper, we generalise the term to describe either the radius or the number of nearest neighbours for a density estimate, depending on the implementation.1 (radius or number of nearest photons) is gradually reduced to mitigate bias. As Hachisuka points out, the accumulated density estimates converge to an unbiased solution in the limit.\n", "keywords": "combining successive results and averaging\ndensity estimate bandwidth1Bandwidth\ndescribe either the radius or the number of nearest neighbours for a density estimate\nirradiance\niterative extension of the standard static photon mapping\nmitigate bias\nmitigates the noise\nMonte Carlo raytracing\nnearest neighbour lookup\nphoton\nphoton map\nphoton maps\nphotons\nProgressive photon mapping\nRadiance extension\nsupport, or area of influence, of a filter\nsurface\ntraditional approach\nweight the photons\n"}, {"id": "S0165212511000874", "text": "The propagation of unsteady disturbances in ducts of slowly-varying geometry, such as those typical of an aeroengine, can be successfully modelled using a multiple scales approach. From the first application\u00a0[1] of multiple-scales analysis to sound propagation in ducts of rectangular and circular cross section without mean flow, more recent developments have extended the method to cases with uniform mean flow\u00a0[2], mean swirling flow\u00a0[3], ducts of arbitrary cross section\u00a0[4] (with uniform mean flow) and strongly curved ducts\u00a0[5]. The multiple-scales approach has a number of distinct advantages over full numerical methods as it is ideally suited to handle higher frequencies and the computational complexity is only marginally more than calculating the eigenmodes inside a straight parallel duct. The accuracy and usefulness of the multiple scales approach has been validated against finite-element methods\u00a0[6] for realistic aeroengine configurations and acoustic frequencies\u00a0[7,8].\n", "keywords": "acoustic frequencies\naeroengine\naeroengine configurations\ncalculating the eigenmodes\nducts\nfinite-element methods\nhigher frequencies\nmean flow\nmean swirling flow\nmultiple-scales analysis\nmultiple scales approach\nmultiple-scales approach\nnumerical methods\npropagation of unsteady disturbances\nsound propagation\nstraight parallel duct\nstrongly curved ducts\nuniform mean flow\n"}, {"id": "S0165212511000874", "text": "The propagation of unsteady disturbances in ducts of slowly-varying geometry, such as those typical of an aeroengine, can be successfully modelled using a multiple scales approach. From the first application\u00a0[1] of multiple-scales analysis to sound propagation in ducts of rectangular and circular cross section without mean flow, more recent developments have extended the method to cases with uniform mean flow\u00a0[2], mean swirling flow\u00a0[3], ducts of arbitrary cross section\u00a0[4] (with uniform mean flow) and strongly curved ducts\u00a0[5]. The multiple-scales approach has a number of distinct advantages over full numerical methods as it is ideally suited to handle higher frequencies and the computational complexity is only marginally more than calculating the eigenmodes inside a straight parallel duct. The accuracy and usefulness of the multiple scales approach has been validated against finite-element methods\u00a0[6] for realistic aeroengine configurations and acoustic frequencies\u00a0[7,8].\n", "keywords": "acoustic frequencies\naeroengine\naeroengine configurations\ncalculating the eigenmodes\nducts\nfinite-element methods\nhigher frequencies\nmean flow\nmean swirling flow\nmultiple-scales analysis\nmultiple scales approach\nmultiple-scales approach\nnumerical methods\npropagation of unsteady disturbances\nsound propagation\nstraight parallel duct\nstrongly curved ducts\nuniform mean flow\n"}, {"id": "S0370269304009025", "text": "A central question from the point of view of nuclear physics involves the changes to the quark and antiquark distributions of a bound proton. Since one must develop a reliable model of both the free proton and the binding of nucleons starting from the quark level\u00a0[8], this problem is rather complicated. We intend to report on our investigation of that problem in future work. For the present, we have chosen to illustrate the formal ideas developed here by applying them to a toy model, namely the quark distributions of isospin symmetric quark matter in which each quark feels a scalar potential, \u2212Vsq, and a vector potential, Vvq. This is the premise of the Quark\u2013Meson Coupling (QMC) model\u00a0[9] which has been used successfully to calculate the properties of nuclear matter as well as finite nuclei\u00a0[10,11]. Most recently it has also been used to derive an effective nuclear force which is very close to the widely used Skyrme III force\u00a0[12]. (Except that in QMC the quarks are confined by the MIT bag, as well as feeling the mean-field scalar and vector potentials generated by the surrounding nucleons.) In the mean field approximation, the Dirac equation for the quark in infinite quark matter is written as: (30)i\u03b3\u00b7\u2202\u2212m\u2212Vqs\u2212\u03b30Vqv\u03c8QMq(x)=0.\n", "keywords": "(30)i\u03b3\u00b7\u2202\u2212m\u2212Vqs\u2212\u03b30Vqv\u03c8QMq(x)=0\ncalculate the properties of nuclear matter as well as finite nuclei\nchanges to the quark and antiquark distributions of a bound proton\ndevelop a reliable model of both the free proton and the binding of nucleons starting from the quark level\nDirac equation for the quark in infinite quark matter\neach quark feels a scalar potential, \u2212Vsq, and a vector potential, Vvq\neffective nuclear force\nillustrate the formal ideas developed here by applying them to a toy model\nmean field approximation\nQMC\nquark distributions of isospin symmetric quark matter\nQuark\u2013Meson Coupling\n"}, {"id": "S0370269304009025", "text": "A central question from the point of view of nuclear physics involves the changes to the quark and antiquark distributions of a bound proton. Since one must develop a reliable model of both the free proton and the binding of nucleons starting from the quark level\u00a0[8], this problem is rather complicated. We intend to report on our investigation of that problem in future work. For the present, we have chosen to illustrate the formal ideas developed here by applying them to a toy model, namely the quark distributions of isospin symmetric quark matter in which each quark feels a scalar potential, \u2212Vsq, and a vector potential, Vvq. This is the premise of the Quark\u2013Meson Coupling (QMC) model\u00a0[9] which has been used successfully to calculate the properties of nuclear matter as well as finite nuclei\u00a0[10,11]. Most recently it has also been used to derive an effective nuclear force which is very close to the widely used Skyrme III force\u00a0[12]. (Except that in QMC the quarks are confined by the MIT bag, as well as feeling the mean-field scalar and vector potentials generated by the surrounding nucleons.) In the mean field approximation, the Dirac equation for the quark in infinite quark matter is written as: (30)i\u03b3\u00b7\u2202\u2212m\u2212Vqs\u2212\u03b30Vqv\u03c8QMq(x)=0.\n", "keywords": "(30)i\u03b3\u00b7\u2202\u2212m\u2212Vqs\u2212\u03b30Vqv\u03c8QMq(x)=0\ncalculate the properties of nuclear matter as well as finite nuclei\nchanges to the quark and antiquark distributions of a bound proton\ndevelop a reliable model of both the free proton and the binding of nucleons starting from the quark level\nDirac equation for the quark in infinite quark matter\neach quark feels a scalar potential, \u2212Vsq, and a vector potential, Vvq\neffective nuclear force\nillustrate the formal ideas developed here by applying them to a toy model\nmean field approximation\nQMC\nquark distributions of isospin symmetric quark matter\nQuark\u2013Meson Coupling\n"}, {"id": "S0370269304009025", "text": "A central question from the point of view of nuclear physics involves the changes to the quark and antiquark distributions of a bound proton. Since one must develop a reliable model of both the free proton and the binding of nucleons starting from the quark level\u00a0[8], this problem is rather complicated. We intend to report on our investigation of that problem in future work. For the present, we have chosen to illustrate the formal ideas developed here by applying them to a toy model, namely the quark distributions of isospin symmetric quark matter in which each quark feels a scalar potential, \u2212Vsq, and a vector potential, Vvq. This is the premise of the Quark\u2013Meson Coupling (QMC) model\u00a0[9] which has been used successfully to calculate the properties of nuclear matter as well as finite nuclei\u00a0[10,11]. Most recently it has also been used to derive an effective nuclear force which is very close to the widely used Skyrme III force\u00a0[12]. (Except that in QMC the quarks are confined by the MIT bag, as well as feeling the mean-field scalar and vector potentials generated by the surrounding nucleons.) In the mean field approximation, the Dirac equation for the quark in infinite quark matter is written as: (30)i\u03b3\u00b7\u2202\u2212m\u2212Vqs\u2212\u03b30Vqv\u03c8QMq(x)=0.\n", "keywords": "(30)i\u03b3\u00b7\u2202\u2212m\u2212Vqs\u2212\u03b30Vqv\u03c8QMq(x)=0\ncalculate the properties of nuclear matter as well as finite nuclei\nchanges to the quark and antiquark distributions of a bound proton\ndevelop a reliable model of both the free proton and the binding of nucleons starting from the quark level\nDirac equation for the quark in infinite quark matter\neach quark feels a scalar potential, \u2212Vsq, and a vector potential, Vvq\neffective nuclear force\nillustrate the formal ideas developed here by applying them to a toy model\nmean field approximation\nQMC\nquark distributions of isospin symmetric quark matter\nQuark\u2013Meson Coupling\n"}, {"id": "S0165212511000862", "text": "In this paper we construct such a physical model with a continuous distribution of relaxations. It is based on the phenomenological theory of relaxation processes which have a long history in physics literature and was recently summarized in a monograph in which references to other relevant publications can be found,\u00a0[24]; also see\u00a0[25]. The present work is confined to relaxation mechanisms which result from changes in normal stresses. More specifically, we are interested in the local mechanisms of irreversible energy loss caused by uniform compression or expansion of a medium for which all components remain unchanged, rather than the losses caused by friction between different layers of a medium which move with different velocities (for a more detailed discussion of this issue see\u00a0[26]). No attempt is made to model effects of shear viscosity and heat conduction beyond the conventional Navier\u2013Stokes approach, since this topic goes far beyond the scope of this paper.\n", "keywords": "compression or expansion of a medium\ncontinuous distribution of relaxations\nfriction between different layers of a medium\nheat conduction\nirreversible energy loss\nlocal mechanisms of irreversible energy loss\nNavier\u2013Stokes approach\nphenomenological theory of relaxation processes\nphysical model\nrelaxation mechanisms\nrelaxation processes\nrelaxations\nshear viscosity\n"}, {"id": "S0370269304008305", "text": "Correlation of charm-quark\u2013charm-antiquark in \u03b3p scattering are calculated in the kt-factorization approach. We apply different unintegrated gluon distributions (uGDF) used in the literature. The results of our calculations are compared with very recent experimental results from the FOCUS Collaboration. The CCFM uGDF developed recently by Kwieci\u0144ski et al. gives a good description of the data. New observables are suggested for future studies. Predictions and perspectives for the HERA energies are presented.", "keywords": "antiquark\nCCFM uGDF\ncharm\nCorrelation of charm-quark\u2013charm-antiquark in \u03b3p scattering\nHERA energies\nkt-factorization approach\nquark\nuGDF\nunintegrated gluon distributions\n\u03b3p scattering\n"}, {"id": "S0370269304008305", "text": "Correlation of charm-quark\u2013charm-antiquark in \u03b3p scattering are calculated in the kt-factorization approach. We apply different unintegrated gluon distributions (uGDF) used in the literature. The results of our calculations are compared with very recent experimental results from the FOCUS Collaboration. The CCFM uGDF developed recently by Kwieci\u0144ski et al. gives a good description of the data. New observables are suggested for future studies. Predictions and perspectives for the HERA energies are presented.", "keywords": "antiquark\nCCFM uGDF\ncharm\nCorrelation of charm-quark\u2013charm-antiquark in \u03b3p scattering\nHERA energies\nkt-factorization approach\nquark\nuGDF\nunintegrated gluon distributions\n\u03b3p scattering\n"}, {"id": "S0370269304008305", "text": "Correlation of charm-quark\u2013charm-antiquark in \u03b3p scattering are calculated in the kt-factorization approach. We apply different unintegrated gluon distributions (uGDF) used in the literature. The results of our calculations are compared with very recent experimental results from the FOCUS Collaboration. The CCFM uGDF developed recently by Kwieci\u0144ski et al. gives a good description of the data. New observables are suggested for future studies. Predictions and perspectives for the HERA energies are presented.", "keywords": "antiquark\nCCFM uGDF\ncharm\nCorrelation of charm-quark\u2013charm-antiquark in \u03b3p scattering\nHERA energies\nkt-factorization approach\nquark\nuGDF\nunintegrated gluon distributions\n\u03b3p scattering\n"}, {"id": "S0370269304008305", "text": "Correlation of charm-quark\u2013charm-antiquark in \u03b3p scattering are calculated in the kt-factorization approach. We apply different unintegrated gluon distributions (uGDF) used in the literature. The results of our calculations are compared with very recent experimental results from the FOCUS Collaboration. The CCFM uGDF developed recently by Kwieci\u0144ski et al. gives a good description of the data. New observables are suggested for future studies. Predictions and perspectives for the HERA energies are presented.", "keywords": "antiquark\nCCFM uGDF\ncharm\nCorrelation of charm-quark\u2013charm-antiquark in \u03b3p scattering\nHERA energies\nkt-factorization approach\nquark\nuGDF\nunintegrated gluon distributions\n\u03b3p scattering\n"}, {"id": "S0305440314001927", "text": "Traditionally, archaeologists have recorded sites and artefacts via a combination of ordinary still photographs, 2D line drawings and occasional cross-sections. Given these constraints, the attractions of 3D models have been obvious for some time, with digital photogrammetry and laser scanners offering two well-known methods for data capture at close range (e.g. Bates et\u00a0al., 2010; Hess and Robson, 2010). The highest specification laser scanners still boast better positional accuracy and greater true colour fidelity than SfM\u2013MVS methods (James and Robson, 2012), but the latter produce very good quality models nonetheless and have many unique selling points. Unlike traditional digital photogrammetry, little or no prior control of camera position is necessary, and unlike laser scanning, no major equipment costs or setup are involved. However, the key attraction of SfM\u2013MVS is that the required input can be taken by anyone with a digital camera and modest prior training about the required number and overlap of photographs. A whole series of traditional bottlenecks are thereby removed from the recording process and large numbers of archaeological landscapes, sites or artefacts can now be captured rapidly, in the field, in the laboratory or in the museum. Fig.\u00a02a\u2013c shows examples of terracotta warrior models for which the level of surface detail is considerable.\n", "keywords": "2D line drawings\n3D models\narchaeological landscapes\nartefacts\ncontrol of camera position\ndata capture at close range\ndigital camera\ndigital photogrammetry\nlaser scanners\nlaser scanning\noccasional cross-sections\nordinary still photographs\nprior training about the required number and overlap of photographs\nrecorded sites and artefacts\nrecording process\nSfM\u2013MVS\nSfM\u2013MVS methods\nsites\nterracotta warrior models\ntraditional digital photogrammetry\n"}, {"id": "S0370269304007208", "text": "It is well known that one of the long standing problems in physics is understanding the confinement physics from first principles. Hence the challenge is to develop analytical approaches which provide valuable insight and theoretical guidance. According to this viewpoint, an effective theory in which confining potentials are obtained as a consequence of spontaneous symmetry breaking of scale invariance has been developed\u00a0[1]. In particular, it was shown that a such theory relies on a scale-invariant Lagrangian of the type\u00a0[2] (1)L=14w2\u221212w\u2212F\u03bc\u03bdaFa\u03bc\u03bd, where F\u03bc\u03bda=\u2202\u03bcA\u03bda\u2212\u2202\u03bdA\u03bca+gfabcA\u03bcbA\u03bdc, and w is not a fundamental field but rather is a function of 4-index field strength, that is, (2)w=\u03b5\u03bc\u03bd\u03b1\u03b2\u2202\u03bcA\u03bd\u03b1\u03b2. The A\u03bd\u03b1\u03b2 equation of motion leads to (3)\u03b5\u03bc\u03bd\u03b1\u03b2\u2202\u03b2w\u2212\u2212F\u03b3\u03b4aFa\u03b3\u03b4=0, which is then integrated to (4)w=\u2212F\u03bc\u03bdaFa\u03bc\u03bd+M. It is easy to verify that the Aa\u03bc equation of motion leads us to (5)\u2207\u03bcFa\u03bc\u03bd+MFa\u03bc\u03bd\u2212F\u03b1\u03b2bFb\u03b1\u03b2=0. It is worth stressing at this stage that the above equation can be obtained from the effective Lagrangian (6)Leff=\u221214F\u03bc\u03bdaFa\u03bc\u03bd+M2\u2212F\u03bc\u03bdaFa\u03bc\u03bd. Spherically symmetric solutions of Eq.\u00a0(5) display, even in the Abelian case, a Coulomb piece and a confining part. Also, the quantum theory calculation of the static energy between two charges displays the same behavior\u00a0[1]. It is well known that the square root part describes string like solutions\u00a0[3,4].\n", "keywords": "(2)w=\u03b5\u03bc\u03bd\u03b1\u03b2\u2202\u03bcA\u03bd\u03b1\u03b2\nAa\u03bc equation of motion\nA\u03bd\u03b1\u03b2 equation of motion leads\nconfining part\nCoulomb piece\ndevelop analytical approaches\nfunction of 4-index field strength\nintegrated to (4)w=\u2212F\u03bc\u03bdaFa\u03bc\u03bd+M\nLeff=\u221214F\u03bc\u03bdaFa\u03bc\u03bd+M2\u2212F\u03bc\u03bdaFa\u03bc\u03bd\nquantum theory calculation of the static energy between two charges\nscale-invariant Lagrangian\nSpherically symmetric solutions\nspontaneous symmetry breaking of scale invariance\nstring like solutions\nthe effective Lagrangian\nunderstanding the confinement physics from first principles\n\u03b5\u03bc\u03bd\u03b1\u03b2\u2202\u03b2w\u2212\u2212F\u03b3\u03b4aFa\u03b3\u03b4=0\n"}, {"id": "S0370269304007208", "text": "It is well known that one of the long standing problems in physics is understanding the confinement physics from first principles. Hence the challenge is to develop analytical approaches which provide valuable insight and theoretical guidance. According to this viewpoint, an effective theory in which confining potentials are obtained as a consequence of spontaneous symmetry breaking of scale invariance has been developed\u00a0[1]. In particular, it was shown that a such theory relies on a scale-invariant Lagrangian of the type\u00a0[2] (1)L=14w2\u221212w\u2212F\u03bc\u03bdaFa\u03bc\u03bd, where F\u03bc\u03bda=\u2202\u03bcA\u03bda\u2212\u2202\u03bdA\u03bca+gfabcA\u03bcbA\u03bdc, and w is not a fundamental field but rather is a function of 4-index field strength, that is, (2)w=\u03b5\u03bc\u03bd\u03b1\u03b2\u2202\u03bcA\u03bd\u03b1\u03b2. The A\u03bd\u03b1\u03b2 equation of motion leads to (3)\u03b5\u03bc\u03bd\u03b1\u03b2\u2202\u03b2w\u2212\u2212F\u03b3\u03b4aFa\u03b3\u03b4=0, which is then integrated to (4)w=\u2212F\u03bc\u03bdaFa\u03bc\u03bd+M. It is easy to verify that the Aa\u03bc equation of motion leads us to (5)\u2207\u03bcFa\u03bc\u03bd+MFa\u03bc\u03bd\u2212F\u03b1\u03b2bFb\u03b1\u03b2=0. It is worth stressing at this stage that the above equation can be obtained from the effective Lagrangian (6)Leff=\u221214F\u03bc\u03bdaFa\u03bc\u03bd+M2\u2212F\u03bc\u03bdaFa\u03bc\u03bd. Spherically symmetric solutions of Eq.\u00a0(5) display, even in the Abelian case, a Coulomb piece and a confining part. Also, the quantum theory calculation of the static energy between two charges displays the same behavior\u00a0[1]. It is well known that the square root part describes string like solutions\u00a0[3,4].\n", "keywords": "(2)w=\u03b5\u03bc\u03bd\u03b1\u03b2\u2202\u03bcA\u03bd\u03b1\u03b2\nAa\u03bc equation of motion\nA\u03bd\u03b1\u03b2 equation of motion leads\nconfining part\nCoulomb piece\ndevelop analytical approaches\nfunction of 4-index field strength\nintegrated to (4)w=\u2212F\u03bc\u03bdaFa\u03bc\u03bd+M\nLeff=\u221214F\u03bc\u03bdaFa\u03bc\u03bd+M2\u2212F\u03bc\u03bdaFa\u03bc\u03bd\nquantum theory calculation of the static energy between two charges\nscale-invariant Lagrangian\nSpherically symmetric solutions\nspontaneous symmetry breaking of scale invariance\nstring like solutions\nthe effective Lagrangian\nunderstanding the confinement physics from first principles\n\u03b5\u03bc\u03bd\u03b1\u03b2\u2202\u03b2w\u2212\u2212F\u03b3\u03b4aFa\u03b3\u03b4=0\n"}, {"id": "S0370269304007208", "text": "It is well known that one of the long standing problems in physics is understanding the confinement physics from first principles. Hence the challenge is to develop analytical approaches which provide valuable insight and theoretical guidance. According to this viewpoint, an effective theory in which confining potentials are obtained as a consequence of spontaneous symmetry breaking of scale invariance has been developed\u00a0[1]. In particular, it was shown that a such theory relies on a scale-invariant Lagrangian of the type\u00a0[2] (1)L=14w2\u221212w\u2212F\u03bc\u03bdaFa\u03bc\u03bd, where F\u03bc\u03bda=\u2202\u03bcA\u03bda\u2212\u2202\u03bdA\u03bca+gfabcA\u03bcbA\u03bdc, and w is not a fundamental field but rather is a function of 4-index field strength, that is, (2)w=\u03b5\u03bc\u03bd\u03b1\u03b2\u2202\u03bcA\u03bd\u03b1\u03b2. The A\u03bd\u03b1\u03b2 equation of motion leads to (3)\u03b5\u03bc\u03bd\u03b1\u03b2\u2202\u03b2w\u2212\u2212F\u03b3\u03b4aFa\u03b3\u03b4=0, which is then integrated to (4)w=\u2212F\u03bc\u03bdaFa\u03bc\u03bd+M. It is easy to verify that the Aa\u03bc equation of motion leads us to (5)\u2207\u03bcFa\u03bc\u03bd+MFa\u03bc\u03bd\u2212F\u03b1\u03b2bFb\u03b1\u03b2=0. It is worth stressing at this stage that the above equation can be obtained from the effective Lagrangian (6)Leff=\u221214F\u03bc\u03bdaFa\u03bc\u03bd+M2\u2212F\u03bc\u03bdaFa\u03bc\u03bd. Spherically symmetric solutions of Eq.\u00a0(5) display, even in the Abelian case, a Coulomb piece and a confining part. Also, the quantum theory calculation of the static energy between two charges displays the same behavior\u00a0[1]. It is well known that the square root part describes string like solutions\u00a0[3,4].\n", "keywords": "(2)w=\u03b5\u03bc\u03bd\u03b1\u03b2\u2202\u03bcA\u03bd\u03b1\u03b2\nAa\u03bc equation of motion\nA\u03bd\u03b1\u03b2 equation of motion leads\nconfining part\nCoulomb piece\ndevelop analytical approaches\nfunction of 4-index field strength\nintegrated to (4)w=\u2212F\u03bc\u03bdaFa\u03bc\u03bd+M\nLeff=\u221214F\u03bc\u03bdaFa\u03bc\u03bd+M2\u2212F\u03bc\u03bdaFa\u03bc\u03bd\nquantum theory calculation of the static energy between two charges\nscale-invariant Lagrangian\nSpherically symmetric solutions\nspontaneous symmetry breaking of scale invariance\nstring like solutions\nthe effective Lagrangian\nunderstanding the confinement physics from first principles\n\u03b5\u03bc\u03bd\u03b1\u03b2\u2202\u03b2w\u2212\u2212F\u03b3\u03b4aFa\u03b3\u03b4=0\n"}, {"id": "S0370269304007208", "text": "It is well known that one of the long standing problems in physics is understanding the confinement physics from first principles. Hence the challenge is to develop analytical approaches which provide valuable insight and theoretical guidance. According to this viewpoint, an effective theory in which confining potentials are obtained as a consequence of spontaneous symmetry breaking of scale invariance has been developed\u00a0[1]. In particular, it was shown that a such theory relies on a scale-invariant Lagrangian of the type\u00a0[2] (1)L=14w2\u221212w\u2212F\u03bc\u03bdaFa\u03bc\u03bd, where F\u03bc\u03bda=\u2202\u03bcA\u03bda\u2212\u2202\u03bdA\u03bca+gfabcA\u03bcbA\u03bdc, and w is not a fundamental field but rather is a function of 4-index field strength, that is, (2)w=\u03b5\u03bc\u03bd\u03b1\u03b2\u2202\u03bcA\u03bd\u03b1\u03b2. The A\u03bd\u03b1\u03b2 equation of motion leads to (3)\u03b5\u03bc\u03bd\u03b1\u03b2\u2202\u03b2w\u2212\u2212F\u03b3\u03b4aFa\u03b3\u03b4=0, which is then integrated to (4)w=\u2212F\u03bc\u03bdaFa\u03bc\u03bd+M. It is easy to verify that the Aa\u03bc equation of motion leads us to (5)\u2207\u03bcFa\u03bc\u03bd+MFa\u03bc\u03bd\u2212F\u03b1\u03b2bFb\u03b1\u03b2=0. It is worth stressing at this stage that the above equation can be obtained from the effective Lagrangian (6)Leff=\u221214F\u03bc\u03bdaFa\u03bc\u03bd+M2\u2212F\u03bc\u03bdaFa\u03bc\u03bd. Spherically symmetric solutions of Eq.\u00a0(5) display, even in the Abelian case, a Coulomb piece and a confining part. Also, the quantum theory calculation of the static energy between two charges displays the same behavior\u00a0[1]. It is well known that the square root part describes string like solutions\u00a0[3,4].\n", "keywords": "(2)w=\u03b5\u03bc\u03bd\u03b1\u03b2\u2202\u03bcA\u03bd\u03b1\u03b2\nAa\u03bc equation of motion\nA\u03bd\u03b1\u03b2 equation of motion leads\nconfining part\nCoulomb piece\ndevelop analytical approaches\nfunction of 4-index field strength\nintegrated to (4)w=\u2212F\u03bc\u03bdaFa\u03bc\u03bd+M\nLeff=\u221214F\u03bc\u03bdaFa\u03bc\u03bd+M2\u2212F\u03bc\u03bdaFa\u03bc\u03bd\nquantum theory calculation of the static energy between two charges\nscale-invariant Lagrangian\nSpherically symmetric solutions\nspontaneous symmetry breaking of scale invariance\nstring like solutions\nthe effective Lagrangian\nunderstanding the confinement physics from first principles\n\u03b5\u03bc\u03bd\u03b1\u03b2\u2202\u03b2w\u2212\u2212F\u03b3\u03b4aFa\u03b3\u03b4=0\n"}, {"id": "S0022311515301963", "text": "Following fission, noble gas atoms will be distributed in the fuel matrix initially accommodated at point defects trap sites, generally thought to be Schottky trivacancy defects [4,5,31]. Diffusion to either bubbles or grain boundaries is then facilitated by associating a further uranium vacancy defect for the gas atom to \u2018hop\u2019 into, with the original vacancy then able to loop around to ensure continued diffusion. The rate determining step in the process is not the migration of the Xe itself but rather the rearrangement of the VU defect to facilitate net Xe diffusion [6\u20138]. Activation energies for the overall process depend on the availability of the defect trap sites, which in turn depends on the crystal stoichiometry. For Xe diffusion in UO2\u2212x, UO2 and UO2+x the activation energies calculated using DFT are 7.04\u201312.92\u00a0eV, 4.15\u20137.88\u00a0eV and 1.38\u20134.07\u00a0eV with the ranges reflecting the way the calculations were performed depending on the charge states of the defects involved and the presence of a Jahn\u2013Teller distortion [7]. Activation energies calculated using empirical pair potentials can vary strongly depending on the choice of potential. Govers et\u00a0al. examined three different potentials for UO2 (those of Basak [9], Jackson [10] and Morelon [11]) coupled with different parameterisations for the U\u2013Xe and O\u2013Xe interactions from Geng [12] and Nicoll [13] and recommend values of 6.5\u00a0eV, 4.5\u00a0eV and 2.4\u00a0eV [6] for the different stoichiometric regimes in very good agreement with the experimental values of 6.0\u00a0eV, 3.9\u00a0eV and 1.7\u00a0eV respectively [14].\n", "keywords": "Activation\nbubbles\ncharge\ncrystal\ncrystal stoichiometry\ndefect trap sites\nDFT\ndiffusion\nDiffusion\nempirical pair potentials\nfission\nfuel matrix\ngas atom\ngrain boundaries\n\u2018hop\u2019 into\nJahn\u2013Teller distortion\nloop around\nmigration\nnoble gas atoms\nO\u2013Xe\npoint defects trap sites\npotential\npotentials\nrearrangement\nSchottky trivacancy defects\nUO2\nUO2+x\nUO2\u2212x\nuranium\nU\u2013Xe\nVU\nVU defect\nXe\nXe diffusion\n"}, {"id": "S0022311515301963", "text": "Following fission, noble gas atoms will be distributed in the fuel matrix initially accommodated at point defects trap sites, generally thought to be Schottky trivacancy defects [4,5,31]. Diffusion to either bubbles or grain boundaries is then facilitated by associating a further uranium vacancy defect for the gas atom to \u2018hop\u2019 into, with the original vacancy then able to loop around to ensure continued diffusion. The rate determining step in the process is not the migration of the Xe itself but rather the rearrangement of the VU defect to facilitate net Xe diffusion [6\u20138]. Activation energies for the overall process depend on the availability of the defect trap sites, which in turn depends on the crystal stoichiometry. For Xe diffusion in UO2\u2212x, UO2 and UO2+x the activation energies calculated using DFT are 7.04\u201312.92\u00a0eV, 4.15\u20137.88\u00a0eV and 1.38\u20134.07\u00a0eV with the ranges reflecting the way the calculations were performed depending on the charge states of the defects involved and the presence of a Jahn\u2013Teller distortion [7]. Activation energies calculated using empirical pair potentials can vary strongly depending on the choice of potential. Govers et\u00a0al. examined three different potentials for UO2 (those of Basak [9], Jackson [10] and Morelon [11]) coupled with different parameterisations for the U\u2013Xe and O\u2013Xe interactions from Geng [12] and Nicoll [13] and recommend values of 6.5\u00a0eV, 4.5\u00a0eV and 2.4\u00a0eV [6] for the different stoichiometric regimes in very good agreement with the experimental values of 6.0\u00a0eV, 3.9\u00a0eV and 1.7\u00a0eV respectively [14].\n", "keywords": "Activation\nbubbles\ncharge\ncrystal\ncrystal stoichiometry\ndefect trap sites\nDFT\ndiffusion\nDiffusion\nempirical pair potentials\nfission\nfuel matrix\ngas atom\ngrain boundaries\n\u2018hop\u2019 into\nJahn\u2013Teller distortion\nloop around\nmigration\nnoble gas atoms\nO\u2013Xe\npoint defects trap sites\npotential\npotentials\nrearrangement\nSchottky trivacancy defects\nUO2\nUO2+x\nUO2\u2212x\nuranium\nU\u2013Xe\nVU\nVU defect\nXe\nXe diffusion\n"}, {"id": "S0022311515301963", "text": "Following fission, noble gas atoms will be distributed in the fuel matrix initially accommodated at point defects trap sites, generally thought to be Schottky trivacancy defects [4,5,31]. Diffusion to either bubbles or grain boundaries is then facilitated by associating a further uranium vacancy defect for the gas atom to \u2018hop\u2019 into, with the original vacancy then able to loop around to ensure continued diffusion. The rate determining step in the process is not the migration of the Xe itself but rather the rearrangement of the VU defect to facilitate net Xe diffusion [6\u20138]. Activation energies for the overall process depend on the availability of the defect trap sites, which in turn depends on the crystal stoichiometry. For Xe diffusion in UO2\u2212x, UO2 and UO2+x the activation energies calculated using DFT are 7.04\u201312.92\u00a0eV, 4.15\u20137.88\u00a0eV and 1.38\u20134.07\u00a0eV with the ranges reflecting the way the calculations were performed depending on the charge states of the defects involved and the presence of a Jahn\u2013Teller distortion [7]. Activation energies calculated using empirical pair potentials can vary strongly depending on the choice of potential. Govers et\u00a0al. examined three different potentials for UO2 (those of Basak [9], Jackson [10] and Morelon [11]) coupled with different parameterisations for the U\u2013Xe and O\u2013Xe interactions from Geng [12] and Nicoll [13] and recommend values of 6.5\u00a0eV, 4.5\u00a0eV and 2.4\u00a0eV [6] for the different stoichiometric regimes in very good agreement with the experimental values of 6.0\u00a0eV, 3.9\u00a0eV and 1.7\u00a0eV respectively [14].\n", "keywords": "Activation\nbubbles\ncharge\ncrystal\ncrystal stoichiometry\ndefect trap sites\nDFT\ndiffusion\nDiffusion\nempirical pair potentials\nfission\nfuel matrix\ngas atom\ngrain boundaries\n\u2018hop\u2019 into\nJahn\u2013Teller distortion\nloop around\nmigration\nnoble gas atoms\nO\u2013Xe\npoint defects trap sites\npotential\npotentials\nrearrangement\nSchottky trivacancy defects\nUO2\nUO2+x\nUO2\u2212x\nuranium\nU\u2013Xe\nVU\nVU defect\nXe\nXe diffusion\n"}, {"id": "S0032386109003991", "text": "A living polymerization is a reaction without transfer and termination reactions that can proceed up to complete monomer conversion. In addition, when initiation is quantitative and fast compared to the propagation reaction, polymers with precisely controlled chain length and narrow molar mass distribution can be obtained. In the case of an industrial styrene polymerization this would permit to avoid any specific washing or degassing steps, which are necessary in the radical process to remove residual monomer and low molar mass oligomers. Since head-to-head defects along the chains are absent, anionic polystyrene would exhibit also a better thermal stability than radical one. Therefore, production of anionic polystyrene (PS) would be of interest if the conditions required to control the polymerization could be adapted to the market and be able to compete economically with industrial radical processes. The use of organic solvents and of expensive alkyllithium initiators, as well as the relatively low reaction temperatures required, was some important limitation to overcome. The possibilities to achieve a quantitative living-like anionic polymerization of styrene in the absence of solvent and at elevated temperature, using inexpensive initiating systems, were the main targets identified to tremendously decrease the cost of the anionic process. This implied at first to control the reactivity and stability of initiating and propagating active species in such unusual operating conditions.\n", "keywords": "alkyllithium initiators\nanionic polymerization of styrene\nanionic polystyrene\ndegassing\nindustrial styrene polymerization\nlow molar mass oligomers\nmonomer conversion\norganic solvents\npolymerization\npolymers\nproduction of anionic polystyrene (PS)\npropagating active species\npropagation\n(PS\nreaction\nsolvent\ntermination\nwashing\n"}, {"id": "S0032386109003991", "text": "A living polymerization is a reaction without transfer and termination reactions that can proceed up to complete monomer conversion. In addition, when initiation is quantitative and fast compared to the propagation reaction, polymers with precisely controlled chain length and narrow molar mass distribution can be obtained. In the case of an industrial styrene polymerization this would permit to avoid any specific washing or degassing steps, which are necessary in the radical process to remove residual monomer and low molar mass oligomers. Since head-to-head defects along the chains are absent, anionic polystyrene would exhibit also a better thermal stability than radical one. Therefore, production of anionic polystyrene (PS) would be of interest if the conditions required to control the polymerization could be adapted to the market and be able to compete economically with industrial radical processes. The use of organic solvents and of expensive alkyllithium initiators, as well as the relatively low reaction temperatures required, was some important limitation to overcome. The possibilities to achieve a quantitative living-like anionic polymerization of styrene in the absence of solvent and at elevated temperature, using inexpensive initiating systems, were the main targets identified to tremendously decrease the cost of the anionic process. This implied at first to control the reactivity and stability of initiating and propagating active species in such unusual operating conditions.\n", "keywords": "alkyllithium initiators\nanionic polymerization of styrene\nanionic polystyrene\ndegassing\nindustrial styrene polymerization\nlow molar mass oligomers\nmonomer conversion\norganic solvents\npolymerization\npolymers\nproduction of anionic polystyrene (PS)\npropagating active species\npropagation\n(PS\nreaction\nsolvent\ntermination\nwashing\n"}, {"id": "S0032386109003991", "text": "A living polymerization is a reaction without transfer and termination reactions that can proceed up to complete monomer conversion. In addition, when initiation is quantitative and fast compared to the propagation reaction, polymers with precisely controlled chain length and narrow molar mass distribution can be obtained. In the case of an industrial styrene polymerization this would permit to avoid any specific washing or degassing steps, which are necessary in the radical process to remove residual monomer and low molar mass oligomers. Since head-to-head defects along the chains are absent, anionic polystyrene would exhibit also a better thermal stability than radical one. Therefore, production of anionic polystyrene (PS) would be of interest if the conditions required to control the polymerization could be adapted to the market and be able to compete economically with industrial radical processes. The use of organic solvents and of expensive alkyllithium initiators, as well as the relatively low reaction temperatures required, was some important limitation to overcome. The possibilities to achieve a quantitative living-like anionic polymerization of styrene in the absence of solvent and at elevated temperature, using inexpensive initiating systems, were the main targets identified to tremendously decrease the cost of the anionic process. This implied at first to control the reactivity and stability of initiating and propagating active species in such unusual operating conditions.\n", "keywords": "alkyllithium initiators\nanionic polymerization of styrene\nanionic polystyrene\ndegassing\nindustrial styrene polymerization\nlow molar mass oligomers\nmonomer conversion\norganic solvents\npolymerization\npolymers\nproduction of anionic polystyrene (PS)\npropagating active species\npropagation\n(PS\nreaction\nsolvent\ntermination\nwashing\n"}, {"id": "S0032386109003991", "text": "A living polymerization is a reaction without transfer and termination reactions that can proceed up to complete monomer conversion. In addition, when initiation is quantitative and fast compared to the propagation reaction, polymers with precisely controlled chain length and narrow molar mass distribution can be obtained. In the case of an industrial styrene polymerization this would permit to avoid any specific washing or degassing steps, which are necessary in the radical process to remove residual monomer and low molar mass oligomers. Since head-to-head defects along the chains are absent, anionic polystyrene would exhibit also a better thermal stability than radical one. Therefore, production of anionic polystyrene (PS) would be of interest if the conditions required to control the polymerization could be adapted to the market and be able to compete economically with industrial radical processes. The use of organic solvents and of expensive alkyllithium initiators, as well as the relatively low reaction temperatures required, was some important limitation to overcome. The possibilities to achieve a quantitative living-like anionic polymerization of styrene in the absence of solvent and at elevated temperature, using inexpensive initiating systems, were the main targets identified to tremendously decrease the cost of the anionic process. This implied at first to control the reactivity and stability of initiating and propagating active species in such unusual operating conditions.\n", "keywords": "alkyllithium initiators\nanionic polymerization of styrene\nanionic polystyrene\ndegassing\nindustrial styrene polymerization\nlow molar mass oligomers\nmonomer conversion\norganic solvents\npolymerization\npolymers\nproduction of anionic polystyrene (PS)\npropagating active species\npropagation\n(PS\nreaction\nsolvent\ntermination\nwashing\n"}, {"id": "S0032386109003991", "text": "A living polymerization is a reaction without transfer and termination reactions that can proceed up to complete monomer conversion. In addition, when initiation is quantitative and fast compared to the propagation reaction, polymers with precisely controlled chain length and narrow molar mass distribution can be obtained. In the case of an industrial styrene polymerization this would permit to avoid any specific washing or degassing steps, which are necessary in the radical process to remove residual monomer and low molar mass oligomers. Since head-to-head defects along the chains are absent, anionic polystyrene would exhibit also a better thermal stability than radical one. Therefore, production of anionic polystyrene (PS) would be of interest if the conditions required to control the polymerization could be adapted to the market and be able to compete economically with industrial radical processes. The use of organic solvents and of expensive alkyllithium initiators, as well as the relatively low reaction temperatures required, was some important limitation to overcome. The possibilities to achieve a quantitative living-like anionic polymerization of styrene in the absence of solvent and at elevated temperature, using inexpensive initiating systems, were the main targets identified to tremendously decrease the cost of the anionic process. This implied at first to control the reactivity and stability of initiating and propagating active species in such unusual operating conditions.\n", "keywords": "alkyllithium initiators\nanionic polymerization of styrene\nanionic polystyrene\ndegassing\nindustrial styrene polymerization\nlow molar mass oligomers\nmonomer conversion\norganic solvents\npolymerization\npolymers\nproduction of anionic polystyrene (PS)\npropagating active species\npropagation\n(PS\nreaction\nsolvent\ntermination\nwashing\n"}, {"id": "S0032386109003991", "text": "A living polymerization is a reaction without transfer and termination reactions that can proceed up to complete monomer conversion. In addition, when initiation is quantitative and fast compared to the propagation reaction, polymers with precisely controlled chain length and narrow molar mass distribution can be obtained. In the case of an industrial styrene polymerization this would permit to avoid any specific washing or degassing steps, which are necessary in the radical process to remove residual monomer and low molar mass oligomers. Since head-to-head defects along the chains are absent, anionic polystyrene would exhibit also a better thermal stability than radical one. Therefore, production of anionic polystyrene (PS) would be of interest if the conditions required to control the polymerization could be adapted to the market and be able to compete economically with industrial radical processes. The use of organic solvents and of expensive alkyllithium initiators, as well as the relatively low reaction temperatures required, was some important limitation to overcome. The possibilities to achieve a quantitative living-like anionic polymerization of styrene in the absence of solvent and at elevated temperature, using inexpensive initiating systems, were the main targets identified to tremendously decrease the cost of the anionic process. This implied at first to control the reactivity and stability of initiating and propagating active species in such unusual operating conditions.\n", "keywords": "alkyllithium initiators\nanionic polymerization of styrene\nanionic polystyrene\ndegassing\nindustrial styrene polymerization\nlow molar mass oligomers\nmonomer conversion\norganic solvents\npolymerization\npolymers\nproduction of anionic polystyrene (PS)\npropagating active species\npropagation\n(PS\nreaction\nsolvent\ntermination\nwashing\n"}, {"id": "S0021961415003821", "text": "The homologous series of n-alkanes are represented here as homonuclear chains of tangent Mie spherical CG segments. The development of CG models for long n-alkanes such as n-decane (n-C10H22) and n-eicosane (n-C20H42) has already been successfully demonstrated using the SAFT-\u03b3 Mie formalism [118]. The n-decane molecule was represented by chains of three and n-eicosane chains of six fully flexible tangentially bonded Mie segments. A certain degree of parameter degeneracy in terms of overall performance is expected as a consequence of the conformal nature of the EOS description [132]. In our current work, we use an alternative CG mapping for n-alkanes developed in reference [122], where each segment was taken to represent three alkyl carbon backbone atoms and their corresponding hydrogen atoms. By applying this mapping, n-alkanes chains containing multiples of three carbon units can be represented directly: n-C6H14, n-C9H20, n-C12H26, n-C15H32, n-C18H38, etc. A good description of the thermodynamic properties of these alkanes is found to be provided with CG alkyl beads characterised by the Mie (15\u20136) potential. For convenience, the exponent pair (15\u20136) is also used to represent the interactions between the CG beads of the intervening alkanes considered here; the number of segments m is taken to be the nearest integer of the division of the carbon number C by three. The size \u03c3 and energy \u220a parameters are then estimated from the experimental saturated-liquid density and vapour pressure of the individual alkanes following the usual SAFT-\u03b3 Mie procedure. The chosen mapping is by no means unique, as one can postulate parameter sets that fulfil other requisites, such as being \u201cuniversal\u201d across the entire homologous series [119] or correlated to the critical properties [125].\n", "keywords": "alkanes\nalkyl carbon backbone atoms\nalternative CG mapping\ncarbon\ncarbon units\nCG alkyl beads\nCG beads\ndescription of the thermodynamic properties\ndevelopment of CG models for long n-alkanes\nhomologous series of n-alkanes\nhomonuclear chains of tangent Mie spherical CG segments\nhydrogen atoms\nMie (15\u20136) potential\nMie segments\nn-alkanes\nn-alkanes chains\nn-C10H22\nn-C12H26\nn-C15H32\nn-C18H38\nn-C20H42\nn-C6H14\nn-C9H20\nn-decane\nn-decane molecule\nn-eicosane\nSAFT-\u03b3 Mie formalism\nSAFT-\u03b3 Mie procedure\nsaturated-liquid density\nvapour pressure\n"}, {"id": "S0045782513000479", "text": "Algorithms regarding distance fields go back to the level set equation. The level set method was presented by Osher and Sethian [20] who described the temporal propagation of moving interfaces by numerical methods solving the Hamilton\u2013Jacobi equation. This is performed by a finite difference scheme working on a rectangular grid in two or three dimensions. Information on normal vectors and curvature can be obtained. The fast marching method [21] provides an efficient numerical scheme of complexity nlogn to compute the support values on the grid. It is a reinterpretation of the propagation process, i.e. the time where the interface passes a certain grid point is influenced only by those neighboring grid points which are previously passed by the interface. An overview on the theory of level set and fast marching methods and their applications to problems of various areas are given in [22,23], for example shape offsetting, computing distances, photolithography development, seismic travel times, etc. Distance fields are a special case of the level set equation where the absolute value of the advection velocity is 1.\n", "keywords": "Algorithms regarding distance fields\nareas\ncomplexity nlogn\ncomputing distances\nDistance fields\nfast marching\nfast marching method\nfinite difference scheme\nHamilton\u2013Jacobi equation\nlevel set\nlevel set equation\nlevel set method\nnumerical methods\nnumerical scheme of complexity nlogn to compute the support values on the grid\nphotolithography development\npropagation process\nrectangular grid in two or three dimensions\nseismic travel times\nshape offsetting\ntemporal propagation\ntheory of level set\nthe time where the interface passes a certain grid point is influenced only by those neighboring grid points which are previously passed by the interface\n"}, {"id": "S0045782515002686", "text": "The immersed boundary method (IBM), proposed by Peskin for studying flow patterns around heart valves\u00a0 [3], has been applied to a wide range of problems including arterial blood flow\u00a0 [4], modelling of the cochlea\u00a0 [5], modelling of red blood cells in Poiseuille flow\u00a0 [6] and flows involving suspended particles\u00a0 [7]. A comprehensive list of applications can be found in\u00a0 [8]. The IBM is both a mathematical formulation and a numerical scheme for fluid\u2013structure interaction problems. As mentioned above, in a classical fluid\u2013structure interaction problem, the fluid and the structure are considered separately and then coupled together via some suitable jump conditions. In the IBM however, the structure\u2013which is usually immersed in a Newtonian fluid\u2013is viewed as being part of the surrounding fluid. This means that only a single equation of motion needs to be solved (i.e.\u00a0a one-phase formulation). Additionally, the IBM allows the immersed structure to move freely over the underlying fluid mesh, alleviating the need for the remeshing required in a classical formulation.\n", "keywords": "arterial blood flow\ncoupled together\nflows involving suspended particles\nfluid\nfluid\u2013structure interaction problem\nfluid\u2013structure interaction problems\nIBM\nimmersed boundary method\nmodelling of red blood cells in Poiseuille flow\u00a0\nmodelling of the cochlea\nmotion\nNewtonian fluid\nremeshing\nstructure\nstudying flow patterns around heart valves\nsurrounding fluid\nunderlying fluid mesh\n"}, {"id": "S0045782515002686", "text": "The immersed boundary method (IBM), proposed by Peskin for studying flow patterns around heart valves\u00a0 [3], has been applied to a wide range of problems including arterial blood flow\u00a0 [4], modelling of the cochlea\u00a0 [5], modelling of red blood cells in Poiseuille flow\u00a0 [6] and flows involving suspended particles\u00a0 [7]. A comprehensive list of applications can be found in\u00a0 [8]. The IBM is both a mathematical formulation and a numerical scheme for fluid\u2013structure interaction problems. As mentioned above, in a classical fluid\u2013structure interaction problem, the fluid and the structure are considered separately and then coupled together via some suitable jump conditions. In the IBM however, the structure\u2013which is usually immersed in a Newtonian fluid\u2013is viewed as being part of the surrounding fluid. This means that only a single equation of motion needs to be solved (i.e.\u00a0a one-phase formulation). Additionally, the IBM allows the immersed structure to move freely over the underlying fluid mesh, alleviating the need for the remeshing required in a classical formulation.\n", "keywords": "arterial blood flow\ncoupled together\nflows involving suspended particles\nfluid\nfluid\u2013structure interaction problem\nfluid\u2013structure interaction problems\nIBM\nimmersed boundary method\nmodelling of red blood cells in Poiseuille flow\u00a0\nmodelling of the cochlea\nmotion\nNewtonian fluid\nremeshing\nstructure\nstudying flow patterns around heart valves\nsurrounding fluid\nunderlying fluid mesh\n"}, {"id": "S0045782515002686", "text": "The immersed boundary method (IBM), proposed by Peskin for studying flow patterns around heart valves\u00a0 [3], has been applied to a wide range of problems including arterial blood flow\u00a0 [4], modelling of the cochlea\u00a0 [5], modelling of red blood cells in Poiseuille flow\u00a0 [6] and flows involving suspended particles\u00a0 [7]. A comprehensive list of applications can be found in\u00a0 [8]. The IBM is both a mathematical formulation and a numerical scheme for fluid\u2013structure interaction problems. As mentioned above, in a classical fluid\u2013structure interaction problem, the fluid and the structure are considered separately and then coupled together via some suitable jump conditions. In the IBM however, the structure\u2013which is usually immersed in a Newtonian fluid\u2013is viewed as being part of the surrounding fluid. This means that only a single equation of motion needs to be solved (i.e.\u00a0a one-phase formulation). Additionally, the IBM allows the immersed structure to move freely over the underlying fluid mesh, alleviating the need for the remeshing required in a classical formulation.\n", "keywords": "arterial blood flow\ncoupled together\nflows involving suspended particles\nfluid\nfluid\u2013structure interaction problem\nfluid\u2013structure interaction problems\nIBM\nimmersed boundary method\nmodelling of red blood cells in Poiseuille flow\u00a0\nmodelling of the cochlea\nmotion\nNewtonian fluid\nremeshing\nstructure\nstudying flow patterns around heart valves\nsurrounding fluid\nunderlying fluid mesh\n"}, {"id": "S0045782515002686", "text": "The immersed boundary method (IBM), proposed by Peskin for studying flow patterns around heart valves\u00a0 [3], has been applied to a wide range of problems including arterial blood flow\u00a0 [4], modelling of the cochlea\u00a0 [5], modelling of red blood cells in Poiseuille flow\u00a0 [6] and flows involving suspended particles\u00a0 [7]. A comprehensive list of applications can be found in\u00a0 [8]. The IBM is both a mathematical formulation and a numerical scheme for fluid\u2013structure interaction problems. As mentioned above, in a classical fluid\u2013structure interaction problem, the fluid and the structure are considered separately and then coupled together via some suitable jump conditions. In the IBM however, the structure\u2013which is usually immersed in a Newtonian fluid\u2013is viewed as being part of the surrounding fluid. This means that only a single equation of motion needs to be solved (i.e.\u00a0a one-phase formulation). Additionally, the IBM allows the immersed structure to move freely over the underlying fluid mesh, alleviating the need for the remeshing required in a classical formulation.\n", "keywords": "arterial blood flow\ncoupled together\nflows involving suspended particles\nfluid\nfluid\u2013structure interaction problem\nfluid\u2013structure interaction problems\nIBM\nimmersed boundary method\nmodelling of red blood cells in Poiseuille flow\u00a0\nmodelling of the cochlea\nmotion\nNewtonian fluid\nremeshing\nstructure\nstudying flow patterns around heart valves\nsurrounding fluid\nunderlying fluid mesh\n"}, {"id": "S0045782515002686", "text": "The immersed boundary method (IBM), proposed by Peskin for studying flow patterns around heart valves\u00a0 [3], has been applied to a wide range of problems including arterial blood flow\u00a0 [4], modelling of the cochlea\u00a0 [5], modelling of red blood cells in Poiseuille flow\u00a0 [6] and flows involving suspended particles\u00a0 [7]. A comprehensive list of applications can be found in\u00a0 [8]. The IBM is both a mathematical formulation and a numerical scheme for fluid\u2013structure interaction problems. As mentioned above, in a classical fluid\u2013structure interaction problem, the fluid and the structure are considered separately and then coupled together via some suitable jump conditions. In the IBM however, the structure\u2013which is usually immersed in a Newtonian fluid\u2013is viewed as being part of the surrounding fluid. This means that only a single equation of motion needs to be solved (i.e.\u00a0a one-phase formulation). Additionally, the IBM allows the immersed structure to move freely over the underlying fluid mesh, alleviating the need for the remeshing required in a classical formulation.\n", "keywords": "arterial blood flow\ncoupled together\nflows involving suspended particles\nfluid\nfluid\u2013structure interaction problem\nfluid\u2013structure interaction problems\nIBM\nimmersed boundary method\nmodelling of red blood cells in Poiseuille flow\u00a0\nmodelling of the cochlea\nmotion\nNewtonian fluid\nremeshing\nstructure\nstudying flow patterns around heart valves\nsurrounding fluid\nunderlying fluid mesh\n"}, {"id": "S0377025714001682", "text": "In order for DLS based micro-rheology to be successful, there must be sufficient scattering contrast between the sample and the tracer particles. In order to achieve this, the maximum possible concentration of tracer particles was added such that single scattering events still dominated (as determined by measurements of diffusion coefficients in water at different concentrations). In order to determine whether or not the background scattering from the sample was sufficiently low compared to that of the tracer particles, we also compared the scattering intensities obtained from samples with and without tracer particles as a function of time. The results of this exercise are shown in Fig. 6. From this figure it can be seen that although initially the scattering from the sample without tracer particles is low compared to those containing tracer particles, as gelation proceeds this eventually ceases to be the case. This is presumably because of the development of supra-molecular structures, such as those seen previously (Fig. 2B). Based on the results in Fig. 6 it was decided to only use data collected in the first 240min of the experiment, after which point the scattering from the gel network became rather too large to ignore.\n", "keywords": "DLS based micro-rheology\ngelation\nsample and the tracer particles\nscattering\nscattering from the gel network\nscattering intensities\nsupra-molecular structures\ntracer particles\nwater\n"}, {"id": "S0377025714001682", "text": "In order for DLS based micro-rheology to be successful, there must be sufficient scattering contrast between the sample and the tracer particles. In order to achieve this, the maximum possible concentration of tracer particles was added such that single scattering events still dominated (as determined by measurements of diffusion coefficients in water at different concentrations). In order to determine whether or not the background scattering from the sample was sufficiently low compared to that of the tracer particles, we also compared the scattering intensities obtained from samples with and without tracer particles as a function of time. The results of this exercise are shown in Fig. 6. From this figure it can be seen that although initially the scattering from the sample without tracer particles is low compared to those containing tracer particles, as gelation proceeds this eventually ceases to be the case. This is presumably because of the development of supra-molecular structures, such as those seen previously (Fig. 2B). Based on the results in Fig. 6 it was decided to only use data collected in the first 240min of the experiment, after which point the scattering from the gel network became rather too large to ignore.\n", "keywords": "DLS based micro-rheology\ngelation\nsample and the tracer particles\nscattering\nscattering from the gel network\nscattering intensities\nsupra-molecular structures\ntracer particles\nwater\n"}, {"id": "S0895611116300684", "text": "In the case of PSR applied to vessels, preservation of high curvature and branches (concavities) demands a high value of the d parameter, resulting in models with high number of polygons. To cope with this problem, Wu et al. (2013) evaluates a variant of PSR (in that work referred to as scale-adaptive [SA]), which includes curvature-dependent polygonization (e.g. increasing/decreasing the size of triangles according to the local curvature) (Wu et al., 2010). In Wu et al. (2013), other methods including MC (without smoothing and decimation) are evaluated with application to vessel modeling. The authors, point at SA as a suitable method for reconstruction of vessels with applications to surgery planning. The methods evaluated by Wu et al. (2013) could be also compared with another set of techniques (known as model-based methods) (Preim and Oeltze, 2008), widely used in the context of vessel modeling for surgery planning.\n", "keywords": "curvature-dependent polygonization\nincreasing/decreasing the size of triangles according to the local curvature\nMC\nmodel-based methods\nPSR\nPSR applied to vessels\nreconstruction of vessels\nSA\nscale-adaptive\nsurgery planning\nvessel modeling\nvessel modeling for surgery planning\n"}, {"id": "S0888613X16300767", "text": "However this is not just a useful depiction of an apposite well-supported statistical model. If we are prepared to allow that the process is driven by a CRG and that the MAP model that we have discovered is indeed generating the idle process, then identifying the disconnected components of the system allows us to immediately make assertions about the impact of various controls we might apply to this regulatory process \u2013 just as we can were we to believe the model was a causal extension of a BN. In the context of microarrays, the objective of clustering is to identify patterns among the data and decide which genes to focus on in further, more gene-specific, experiments. It is therefore necessary for the scientist to make such causal conjectures about the effect of controls available to her on the expressions reflecting the underlying regulatory process she studies. These conjectures can be universal or nuanced by evoking ideas of parsimony.\n", "keywords": "an apposite well-supported statistical model\nBN\nclustering\nconjectures\nCRG\nevoking ideas of parsimony\nidentifying the disconnected components of the system\nidentify patterns among the data\nidle process\nMAP model\nthe effect of controls\nthe impact of various controls\n"}, {"id": "S0888613X16300767", "text": "However this is not just a useful depiction of an apposite well-supported statistical model. If we are prepared to allow that the process is driven by a CRG and that the MAP model that we have discovered is indeed generating the idle process, then identifying the disconnected components of the system allows us to immediately make assertions about the impact of various controls we might apply to this regulatory process \u2013 just as we can were we to believe the model was a causal extension of a BN. In the context of microarrays, the objective of clustering is to identify patterns among the data and decide which genes to focus on in further, more gene-specific, experiments. It is therefore necessary for the scientist to make such causal conjectures about the effect of controls available to her on the expressions reflecting the underlying regulatory process she studies. These conjectures can be universal or nuanced by evoking ideas of parsimony.\n", "keywords": "an apposite well-supported statistical model\nBN\nclustering\nconjectures\nCRG\nevoking ideas of parsimony\nidentifying the disconnected components of the system\nidentify patterns among the data\nidle process\nMAP model\nthe effect of controls\nthe impact of various controls\n"}, {"id": "S0888613X16300767", "text": "However this is not just a useful depiction of an apposite well-supported statistical model. If we are prepared to allow that the process is driven by a CRG and that the MAP model that we have discovered is indeed generating the idle process, then identifying the disconnected components of the system allows us to immediately make assertions about the impact of various controls we might apply to this regulatory process \u2013 just as we can were we to believe the model was a causal extension of a BN. In the context of microarrays, the objective of clustering is to identify patterns among the data and decide which genes to focus on in further, more gene-specific, experiments. It is therefore necessary for the scientist to make such causal conjectures about the effect of controls available to her on the expressions reflecting the underlying regulatory process she studies. These conjectures can be universal or nuanced by evoking ideas of parsimony.\n", "keywords": "an apposite well-supported statistical model\nBN\nclustering\nconjectures\nCRG\nevoking ideas of parsimony\nidentifying the disconnected components of the system\nidentify patterns among the data\nidle process\nMAP model\nthe effect of controls\nthe impact of various controls\n"}, {"id": "S0888613X16300767", "text": "However this is not just a useful depiction of an apposite well-supported statistical model. If we are prepared to allow that the process is driven by a CRG and that the MAP model that we have discovered is indeed generating the idle process, then identifying the disconnected components of the system allows us to immediately make assertions about the impact of various controls we might apply to this regulatory process \u2013 just as we can were we to believe the model was a causal extension of a BN. In the context of microarrays, the objective of clustering is to identify patterns among the data and decide which genes to focus on in further, more gene-specific, experiments. It is therefore necessary for the scientist to make such causal conjectures about the effect of controls available to her on the expressions reflecting the underlying regulatory process she studies. These conjectures can be universal or nuanced by evoking ideas of parsimony.\n", "keywords": "an apposite well-supported statistical model\nBN\nclustering\nconjectures\nCRG\nevoking ideas of parsimony\nidentifying the disconnected components of the system\nidentify patterns among the data\nidle process\nMAP model\nthe effect of controls\nthe impact of various controls\n"}, {"id": "S0021999114008523", "text": "A multi-physics description of a multiscale system is often referred to as a \u2018hybrid\u2019 model. In fluid dynamics, a typical hybrid combines a molecular treatment (a \u2018micro\u2019 model) with a continuum-fluid one (a \u2018macro\u2019 model), with the aim of obtaining the accuracy of the former with the efficiency of the latter [1\u20134]. The micro and macro models generally have characteristic timescales that are very different, which means that time-accurate simulations can be extremely challenging: the size of the timestep required to make the micro model stable and accurate is so small that simulations over significant macro-scale time periods are intractable. If the system is \u2018scale-separated\u2019, a physical (as distinct from numerical) approximation can be made that enables the coupled models to advance at different rates (asynchronously) with negligible penalty on macro-scale accuracy. E et al. [5] were the first to introduce and implement this concept in a time-stepping method for coupled systems, referred to in the classification of Lockerby et al. [6] as a continuous asynchronous (CA) scheme (\u2018continuous\u2019 since the micro and macro models advance without interruption [5]). In this paper we extend this idea to multiscale systems comprising an arbitrary number of coupled models.\n", "keywords": "CA\ncontinuous asynchronous\ncontinuum-fluid one\ncoupled models\nfluid\nfluid dynamics\nhybrid\n\u2018hybrid\u2019 model\nmacro\u2019 model\nmicro and macro models\n\u2018micro\u2019 model\nmolecular treatment\nmulti-physics description of a multiscale system\nmultiscale systems comprising an arbitrary number of coupled models\nobtaining the accuracy of the former with the efficiency of the latter\nphysical (as distinct from numerical) approximation\nscale-separated\ntime-stepping method for coupled systems\n"}, {"id": "S0021999114008523", "text": "A multi-physics description of a multiscale system is often referred to as a \u2018hybrid\u2019 model. In fluid dynamics, a typical hybrid combines a molecular treatment (a \u2018micro\u2019 model) with a continuum-fluid one (a \u2018macro\u2019 model), with the aim of obtaining the accuracy of the former with the efficiency of the latter [1\u20134]. The micro and macro models generally have characteristic timescales that are very different, which means that time-accurate simulations can be extremely challenging: the size of the timestep required to make the micro model stable and accurate is so small that simulations over significant macro-scale time periods are intractable. If the system is \u2018scale-separated\u2019, a physical (as distinct from numerical) approximation can be made that enables the coupled models to advance at different rates (asynchronously) with negligible penalty on macro-scale accuracy. E et al. [5] were the first to introduce and implement this concept in a time-stepping method for coupled systems, referred to in the classification of Lockerby et al. [6] as a continuous asynchronous (CA) scheme (\u2018continuous\u2019 since the micro and macro models advance without interruption [5]). In this paper we extend this idea to multiscale systems comprising an arbitrary number of coupled models.\n", "keywords": "CA\ncontinuous asynchronous\ncontinuum-fluid one\ncoupled models\nfluid\nfluid dynamics\nhybrid\n\u2018hybrid\u2019 model\nmacro\u2019 model\nmicro and macro models\n\u2018micro\u2019 model\nmolecular treatment\nmulti-physics description of a multiscale system\nmultiscale systems comprising an arbitrary number of coupled models\nobtaining the accuracy of the former with the efficiency of the latter\nphysical (as distinct from numerical) approximation\nscale-separated\ntime-stepping method for coupled systems\n"}, {"id": "S0021999114008523", "text": "A multi-physics description of a multiscale system is often referred to as a \u2018hybrid\u2019 model. In fluid dynamics, a typical hybrid combines a molecular treatment (a \u2018micro\u2019 model) with a continuum-fluid one (a \u2018macro\u2019 model), with the aim of obtaining the accuracy of the former with the efficiency of the latter [1\u20134]. The micro and macro models generally have characteristic timescales that are very different, which means that time-accurate simulations can be extremely challenging: the size of the timestep required to make the micro model stable and accurate is so small that simulations over significant macro-scale time periods are intractable. If the system is \u2018scale-separated\u2019, a physical (as distinct from numerical) approximation can be made that enables the coupled models to advance at different rates (asynchronously) with negligible penalty on macro-scale accuracy. E et al. [5] were the first to introduce and implement this concept in a time-stepping method for coupled systems, referred to in the classification of Lockerby et al. [6] as a continuous asynchronous (CA) scheme (\u2018continuous\u2019 since the micro and macro models advance without interruption [5]). In this paper we extend this idea to multiscale systems comprising an arbitrary number of coupled models.\n", "keywords": "CA\ncontinuous asynchronous\ncontinuum-fluid one\ncoupled models\nfluid\nfluid dynamics\nhybrid\n\u2018hybrid\u2019 model\nmacro\u2019 model\nmicro and macro models\n\u2018micro\u2019 model\nmolecular treatment\nmulti-physics description of a multiscale system\nmultiscale systems comprising an arbitrary number of coupled models\nobtaining the accuracy of the former with the efficiency of the latter\nphysical (as distinct from numerical) approximation\nscale-separated\ntime-stepping method for coupled systems\n"}, {"id": "S0021999114008523", "text": "A multi-physics description of a multiscale system is often referred to as a \u2018hybrid\u2019 model. In fluid dynamics, a typical hybrid combines a molecular treatment (a \u2018micro\u2019 model) with a continuum-fluid one (a \u2018macro\u2019 model), with the aim of obtaining the accuracy of the former with the efficiency of the latter [1\u20134]. The micro and macro models generally have characteristic timescales that are very different, which means that time-accurate simulations can be extremely challenging: the size of the timestep required to make the micro model stable and accurate is so small that simulations over significant macro-scale time periods are intractable. If the system is \u2018scale-separated\u2019, a physical (as distinct from numerical) approximation can be made that enables the coupled models to advance at different rates (asynchronously) with negligible penalty on macro-scale accuracy. E et al. [5] were the first to introduce and implement this concept in a time-stepping method for coupled systems, referred to in the classification of Lockerby et al. [6] as a continuous asynchronous (CA) scheme (\u2018continuous\u2019 since the micro and macro models advance without interruption [5]). In this paper we extend this idea to multiscale systems comprising an arbitrary number of coupled models.\n", "keywords": "CA\ncontinuous asynchronous\ncontinuum-fluid one\ncoupled models\nfluid\nfluid dynamics\nhybrid\n\u2018hybrid\u2019 model\nmacro\u2019 model\nmicro and macro models\n\u2018micro\u2019 model\nmolecular treatment\nmulti-physics description of a multiscale system\nmultiscale systems comprising an arbitrary number of coupled models\nobtaining the accuracy of the former with the efficiency of the latter\nphysical (as distinct from numerical) approximation\nscale-separated\ntime-stepping method for coupled systems\n"}, {"id": "S0021999114008523", "text": "A multi-physics description of a multiscale system is often referred to as a \u2018hybrid\u2019 model. In fluid dynamics, a typical hybrid combines a molecular treatment (a \u2018micro\u2019 model) with a continuum-fluid one (a \u2018macro\u2019 model), with the aim of obtaining the accuracy of the former with the efficiency of the latter [1\u20134]. The micro and macro models generally have characteristic timescales that are very different, which means that time-accurate simulations can be extremely challenging: the size of the timestep required to make the micro model stable and accurate is so small that simulations over significant macro-scale time periods are intractable. If the system is \u2018scale-separated\u2019, a physical (as distinct from numerical) approximation can be made that enables the coupled models to advance at different rates (asynchronously) with negligible penalty on macro-scale accuracy. E et al. [5] were the first to introduce and implement this concept in a time-stepping method for coupled systems, referred to in the classification of Lockerby et al. [6] as a continuous asynchronous (CA) scheme (\u2018continuous\u2019 since the micro and macro models advance without interruption [5]). In this paper we extend this idea to multiscale systems comprising an arbitrary number of coupled models.\n", "keywords": "CA\ncontinuous asynchronous\ncontinuum-fluid one\ncoupled models\nfluid\nfluid dynamics\nhybrid\n\u2018hybrid\u2019 model\nmacro\u2019 model\nmicro and macro models\n\u2018micro\u2019 model\nmolecular treatment\nmulti-physics description of a multiscale system\nmultiscale systems comprising an arbitrary number of coupled models\nobtaining the accuracy of the former with the efficiency of the latter\nphysical (as distinct from numerical) approximation\nscale-separated\ntime-stepping method for coupled systems\n"}, {"id": "S1524070312000380", "text": "Isogeometric analysis (IGA) is a numerical simulation method which is directly based on the NURBS-based representation of CAD models. It exploits the tensor-product structure of 2- or 3-dimensional NURBS objects to parameterize the physical domain. Hence the physical domain is parameterized with respect to a rectangle or to a cube. Consequently, singularly parameterized NURBS surfaces and NURBS volumes are needed in order to represent non-quadrangular or non-hexahedral domains without splitting, thereby producing a very compact and convenient representation.The Galerkin projection introduces finite-dimensional spaces of test functions in the weak formulation of partial differential equations. In particular, the test functions used in isogeometric analysis are obtained by composing the inverse of the domain parameterization with the NURBS basis functions. In the case of singular parameterizations, however, some of the resulting test functions do not necessarily fulfill the required regularity properties. Consequently, numerical methods for the solution of partial differential equations cannot be applied properly.We discuss the regularity properties of the test functions. For one- and two-dimensional domains we consider several important classes of singularities of NURBS parameterizations. For specific cases we derive additional conditions which guarantee the regularity of the test functions. In addition we present a modification scheme for the discretized function space in case of insufficient regularity. It is also shown how these results can be applied for computational domains in higher dimensions that can be parameterized via sweeping.\n", "keywords": "CAD models\nconsider several important classes of singularities of NURBS parameterizations\nderive additional conditions which guarantee the regularity of the test functions\ndiscretized function space\ndiscuss the regularity properties of the test functions\nformulation of partial differential equations\nGalerkin projection\nIGA\nisogeometric analysis\nIsogeometric analysis\nmodification scheme\nnumerical methods\nnumerical simulation method\nNURBS\nNURBS-based representation\nNURBS-based representation of CAD models\nNURBS basis functions\nNURBS objects\nNURBS surfaces\nNURBS volumes\nparameterized\nphysical domain is parameterized\npresent a modification scheme\nsplitting\nsweeping\n"}, {"id": "S1524070312000380", "text": "Isogeometric analysis (IGA) is a numerical simulation method which is directly based on the NURBS-based representation of CAD models. It exploits the tensor-product structure of 2- or 3-dimensional NURBS objects to parameterize the physical domain. Hence the physical domain is parameterized with respect to a rectangle or to a cube. Consequently, singularly parameterized NURBS surfaces and NURBS volumes are needed in order to represent non-quadrangular or non-hexahedral domains without splitting, thereby producing a very compact and convenient representation.The Galerkin projection introduces finite-dimensional spaces of test functions in the weak formulation of partial differential equations. In particular, the test functions used in isogeometric analysis are obtained by composing the inverse of the domain parameterization with the NURBS basis functions. In the case of singular parameterizations, however, some of the resulting test functions do not necessarily fulfill the required regularity properties. Consequently, numerical methods for the solution of partial differential equations cannot be applied properly.We discuss the regularity properties of the test functions. For one- and two-dimensional domains we consider several important classes of singularities of NURBS parameterizations. For specific cases we derive additional conditions which guarantee the regularity of the test functions. In addition we present a modification scheme for the discretized function space in case of insufficient regularity. It is also shown how these results can be applied for computational domains in higher dimensions that can be parameterized via sweeping.\n", "keywords": "CAD models\nconsider several important classes of singularities of NURBS parameterizations\nderive additional conditions which guarantee the regularity of the test functions\ndiscretized function space\ndiscuss the regularity properties of the test functions\nformulation of partial differential equations\nGalerkin projection\nIGA\nisogeometric analysis\nIsogeometric analysis\nmodification scheme\nnumerical methods\nnumerical simulation method\nNURBS\nNURBS-based representation\nNURBS-based representation of CAD models\nNURBS basis functions\nNURBS objects\nNURBS surfaces\nNURBS volumes\nparameterized\nphysical domain is parameterized\npresent a modification scheme\nsplitting\nsweeping\n"}, {"id": "S0370269304007816", "text": "We define a new multispecies model of Calogero type in D dimensions with harmonic, two-body and three-body interactions. Using the underlying conformal SU(1,1) algebra, we indicate how to find the complete set of the states in Bargmann\u2013Fock space. There are towers of states, with equidistant energy spectra in each tower. We explicitely construct all polynomial eigenstates, namely the center-of-mass states and global dilatation modes, and find their corresponding eigenenergies. We also construct ladder operators for these global collective states. Analysing corresponding Fock space, we detect the universal critical point at which the model exhibits singular behavior. The above results are universal for all systems with underlying conformal SU(1,1) symmetry.We define a new multispecies model of Calogero type in D dimensions with harmonic, two-body and three-body interactions. Using the underlying conformal SU(1,1) algebra, we indicate how to find the complete set of the states in Bargmann\u2013Fock space. There are towers of states, with equidistant energy spectra in each tower. We explicitely construct all polynomial eigenstates, namely the center-of-mass states and global dilatation modes, and find their corresponding eigenenergies. We also construct ladder operators for these global collective states. Analysing corresponding Fock space, we detect the universal critical point at which the model exhibits singular behavior. The above results are universal for all systems with underlying conformal SU(1,1) symmetry.\n", "keywords": "Analysing corresponding Fock space\nconformal SU(1,1) algebra\nconstruct all polynomial eigenstates\nconstruct ladder operators\ndefine a new multispecies model of Calogero type in D dimensions\nequidistant energy spectra\nfind their corresponding eigenenergies\nharmonic, two-body and three-body interactions\nmultispecies model\n"}, {"id": "S0370269304009608", "text": "It should be noted that BEBC\u00a0[21] and NOMAD\u00a0[20] observed a discrepancy between experimental \u03c1 rates and those estimated with JETSET\u00a0[16]. NOMAD\u00a0[20] proposed to retune some of the parameters used within JETSET to obtain better agreement. Therefore, for the purpose of this analysis events were simulated both with the default setting and with the setting proposed by NOMAD of key JETSET parameters, taking an average between them as a result and half a difference as a systematic error. We used experimental rates of light neutral mesons and resonances where available (Table\u00a01) for normalization purposes. The uncertainty introduced by the JETSET parameter settings (which amounts to 20% at most) affects only the production of the \u03b7\u2032 and \u03d5 for which no experimental data are available. This uncertainty is reflected in the error quoted in the table. However, since the contribution from \u03b7\u2032 and \u03d5 is small, the overall effect is less important.\n", "keywords": "analysis\nBEBC\ndefault setting\nevents were simulated\nexperimental data\nhalf a difference as a systematic error\nJETSET\nlight neutral mesons\nNOMAD\nnormalization\nobserved a discrepancy\nobtain better agreement\nparameter settings\nresonances\nretune some of the parameters\nsetting proposed by NOMAD\ntaking an average between them\n"}, {"id": "S0379711215000223", "text": "The mentioned difficulties associated with the calibration process inspired the concept of inverse modelling. In this case, the experimental data become entirely integrated in the calibration process and an optimization routine is used to quantify the best set of parameters which explain the observed pyrolysis behaviour (i.e. multivariable curve fitting). The most used experimental data for model calibration have been the mass loss rate and the surface temperature [10\u201312]. The optimization technique used is function of the number of variables and their interactions. In the past, only the few most uncertain parameters (i.e. the kinetics parameters) were generally used as potentiometers [13]. However, sophisticated mathematical procedures have been developed to increase the number of parameters optimized simultaneously (e.g. Genetic Algorithm (GA) [10,14] or Shuffled Complex Evolution (SCE) [11]). Lautenberger and Fernandez-Pello [12] have recently investigated the influence that the choice of algorithm can have on the optimized parameters. They generated using their code GPYRO a set of synthetic data (mass loss rate and surfaces temperature) and tried with different algorithms to find back the set of input parameters. The four optimization algorithms provided results with an absolute average error between 1% and 25%. SCE was the most suitable algorithm. The use of synthetic data conveniently avoids the problem of agreement between the actual physical phenomena and any modelling assumption.\n", "keywords": "algorithm\nalgorithms\ncalibration process\nchoice of algorithm\nexperimental data\nGA\nGenetic Algorithm\nGPYRO\ninverse modelling\nkinetics parameters\nmass loss rate\nmathematical procedures\nmodel calibration\nmultivariable curve fitting\noptimization algorithms\noptimization routine\noptimization technique\nparameters\npotentiometers\npyrolysis behaviour\nquantify the best set of parameters\nSCE\nShuffled Complex Evolution\nsurface temperature [\nsynthetic data\n"}, {"id": "S2352179114200032", "text": "Although the presented model is developed and tested with a-C:H layers in mind, it is not necessarily limited to them. Moreover, the only assumptions are chemical reactions between the gas and the solid forming volatiles, the loss of these volatiles from the material and the two stated boundary conditions of gas influx at a single outer surface and the possibility of reactions throughout the bulk. Porosity and significant gas inventories were observed not only for carbon [12] but, e.g. also for beryllium co-deposits [25] and can be expected for other co-deposits formed in plasma devices [1]. Thus, TCR and its description by the presented model may be applicable to all deposits. If a layer has constituents that are not forming volatiles with the reactive gas, e.g. W and Be with O2, these constituents cannot be removed by TCR, as they will not be removed from the deposit. This can influence the removal of other deposit constituents and the time evolution of the process can change. The new understanding of TCR may, for the first time, allow applying the method in a controlled way to nuclear fusion devices, possibly solving the tritium retention issue especially related to carbon based materials.\n", "keywords": "a-C:H layers\nberyllium co-deposits\ncarbon\ncarbon based materials\nchemical reactions\nconstituents that are not forming volatiles with the reactive gas\ndeposit constituents\ngas\ngas influx\ngas inventories\nnuclear fusion devices\nplasma devices\nPorosity\nreactions\nreactive gas\nsingle outer surface\nsolid forming volatiles\nTCR\ntritium\ntritium retention\ntritium retention issue\nW and Be with O2\n"}, {"id": "S0079642515000705", "text": "When dominated by surface shadowing mechanisms, the aggregation of vapor particles onto a surface is a complex, non-local phenomenon. In the literature, there have been many attempts to analyze the growth mechanism by means of pure geometrical considerations; i.e., by assuming that vapor particles arrive at the film surface along a single angular direction [38,41]. Continuum approaches, which are based on the fact that the geometrical features of the film (i.e., the nanocolumns) are much larger than the typical size of an atom [42,266,267], have been also explored. For instance, Poxson et al. [228] developed an analytic model that takes into account geometrical factors as well as surface diffusion. This model accurately predicted the porosity and deposition rate of thin films using a single input parameter related to the cross-sectional area of the nanocolumns, the volume of material and the thickness of the film. Moreover, in Ref. [39], an analytical semi-empirical model was presented to quantitatively describe the aggregation of columnar structures by means of a single parameter dubbed the fan angle. This material-dependent quantity can be experimentally obtained by performing deposition at normal incidence on an imprinted groove seeded substrate, and then measuring the increase in column diameter with film thickness. This model was tested under various conditions [40], which returned good results and an accurate prediction of the relation between the incident angle of the deposition flux and the tilt angle of the columns for several materials.\n", "keywords": "aggregation of vapor particles onto a surface\nanalytical semi-empirical model\nanalytic model\nanalyze the growth mechanism\natom\nContinuum approaches\ndeposition flux\nfilm\nfilm surface\ngeometrical considerations\ngeometrical factors\ngeometrical features\ngroove seeded substrate\nnanocolumns\nprediction of the relation between the incident angle of the deposition flux and the tilt angle of the columns\nquantitatively describe the aggregation of columnar structures\nsingle angular direction\nsurface diffusion\nsurface shadowing mechanisms\nthin films\nvapor particles\n"}, {"id": "S0079642515000705", "text": "When dominated by surface shadowing mechanisms, the aggregation of vapor particles onto a surface is a complex, non-local phenomenon. In the literature, there have been many attempts to analyze the growth mechanism by means of pure geometrical considerations; i.e., by assuming that vapor particles arrive at the film surface along a single angular direction [38,41]. Continuum approaches, which are based on the fact that the geometrical features of the film (i.e., the nanocolumns) are much larger than the typical size of an atom [42,266,267], have been also explored. For instance, Poxson et al. [228] developed an analytic model that takes into account geometrical factors as well as surface diffusion. This model accurately predicted the porosity and deposition rate of thin films using a single input parameter related to the cross-sectional area of the nanocolumns, the volume of material and the thickness of the film. Moreover, in Ref. [39], an analytical semi-empirical model was presented to quantitatively describe the aggregation of columnar structures by means of a single parameter dubbed the fan angle. This material-dependent quantity can be experimentally obtained by performing deposition at normal incidence on an imprinted groove seeded substrate, and then measuring the increase in column diameter with film thickness. This model was tested under various conditions [40], which returned good results and an accurate prediction of the relation between the incident angle of the deposition flux and the tilt angle of the columns for several materials.\n", "keywords": "aggregation of vapor particles onto a surface\nanalytical semi-empirical model\nanalytic model\nanalyze the growth mechanism\natom\nContinuum approaches\ndeposition flux\nfilm\nfilm surface\ngeometrical considerations\ngeometrical factors\ngeometrical features\ngroove seeded substrate\nnanocolumns\nprediction of the relation between the incident angle of the deposition flux and the tilt angle of the columns\nquantitatively describe the aggregation of columnar structures\nsingle angular direction\nsurface diffusion\nsurface shadowing mechanisms\nthin films\nvapor particles\n"}, {"id": "S0003491613001516", "text": "Complex Langevin (CL) dynamics\u00a0 [1,2] provides an approach to circumvent the sign problem in numerical simulations of lattice field theories with a complex Boltzmann weight, since it does not rely on importance sampling. In recent years a number of stimulating results has been obtained in the context of nonzero chemical potential, in both lower and four-dimensional field theories with a severe sign problem in the thermodynamic limit\u00a0 [3\u20138] (for two recent reviews, see e.g.\u00a0Refs.\u00a0 [9,10]). However, as has been known since shortly after its inception, correct results are not guaranteed\u00a0 [11\u201316]. This calls for an improved understanding, relying on the combination of analytical and numerical insight. In the recent past, the important role played by the properties of the real and positive probability distribution in the complexified configuration space, which is effectively sampled during the Langevin process, has been clarified\u00a0 [17,18]. An important conclusion was that this distribution should be sufficiently localised in order for CL to yield valid results. Importantly, this insight has recently also led to promising results in nonabelian gauge theories, with the implementation of SL(N,C) gauge cooling\u00a0 [8,10].\n", "keywords": "CL\ncomplexified configuration space\nComplex Langevin\ndistribution\nimproved understanding, relying on the combination of analytical and numerical insight\nLangevin process\nlower and four-dimensional field theories\nnonabelian gauge theories\nnonzero chemical potential\nnumerical simulations of lattice field theories\nprobability distribution\nsign problem\nsign problem in the thermodynamic limit\nSL(N,C) gauge cooling\n"}, {"id": "S0003491613001516", "text": "Complex Langevin (CL) dynamics\u00a0 [1,2] provides an approach to circumvent the sign problem in numerical simulations of lattice field theories with a complex Boltzmann weight, since it does not rely on importance sampling. In recent years a number of stimulating results has been obtained in the context of nonzero chemical potential, in both lower and four-dimensional field theories with a severe sign problem in the thermodynamic limit\u00a0 [3\u20138] (for two recent reviews, see e.g.\u00a0Refs.\u00a0 [9,10]). However, as has been known since shortly after its inception, correct results are not guaranteed\u00a0 [11\u201316]. This calls for an improved understanding, relying on the combination of analytical and numerical insight. In the recent past, the important role played by the properties of the real and positive probability distribution in the complexified configuration space, which is effectively sampled during the Langevin process, has been clarified\u00a0 [17,18]. An important conclusion was that this distribution should be sufficiently localised in order for CL to yield valid results. Importantly, this insight has recently also led to promising results in nonabelian gauge theories, with the implementation of SL(N,C) gauge cooling\u00a0 [8,10].\n", "keywords": "CL\ncomplexified configuration space\nComplex Langevin\ndistribution\nimproved understanding, relying on the combination of analytical and numerical insight\nLangevin process\nlower and four-dimensional field theories\nnonabelian gauge theories\nnonzero chemical potential\nnumerical simulations of lattice field theories\nprobability distribution\nsign problem\nsign problem in the thermodynamic limit\nSL(N,C) gauge cooling\n"}, {"id": "S0003491613001516", "text": "Complex Langevin (CL) dynamics\u00a0 [1,2] provides an approach to circumvent the sign problem in numerical simulations of lattice field theories with a complex Boltzmann weight, since it does not rely on importance sampling. In recent years a number of stimulating results has been obtained in the context of nonzero chemical potential, in both lower and four-dimensional field theories with a severe sign problem in the thermodynamic limit\u00a0 [3\u20138] (for two recent reviews, see e.g.\u00a0Refs.\u00a0 [9,10]). However, as has been known since shortly after its inception, correct results are not guaranteed\u00a0 [11\u201316]. This calls for an improved understanding, relying on the combination of analytical and numerical insight. In the recent past, the important role played by the properties of the real and positive probability distribution in the complexified configuration space, which is effectively sampled during the Langevin process, has been clarified\u00a0 [17,18]. An important conclusion was that this distribution should be sufficiently localised in order for CL to yield valid results. Importantly, this insight has recently also led to promising results in nonabelian gauge theories, with the implementation of SL(N,C) gauge cooling\u00a0 [8,10].\n", "keywords": "CL\ncomplexified configuration space\nComplex Langevin\ndistribution\nimproved understanding, relying on the combination of analytical and numerical insight\nLangevin process\nlower and four-dimensional field theories\nnonabelian gauge theories\nnonzero chemical potential\nnumerical simulations of lattice field theories\nprobability distribution\nsign problem\nsign problem in the thermodynamic limit\nSL(N,C) gauge cooling\n"}, {"id": "S2212671612001692", "text": "The key point of robot dynamics is optimal design and control. The efficiency of robot dynamics has been the goal of researchers in recent years. Screws are used to describe dynamic problems in this paper, and an O(N) recursive robot forward dynamic algorithm is given on this. It can be easily extended to tree topology, closed loop and spatial robot systems. And three classic methods of robot dynamics are compared for easy of use. The results show that dynamics described with screws are helpful in high efficient dynamics modelling. The dynamical expressions based on screws are concise and clear. It's efficiency is high of O(N) and is linear to the degree of freedom. With the improvement of computation efficiency, it will make the real-time dynamics control become possible.", "keywords": "describe dynamic problems\ndynamical expressions\ndynamics described with screws\nefficiency of robot dynamics\nhigh efficient dynamics modelling.\nimprovement of computation efficiency\nlinear to the degree of freedom\nO(N) recursive robot\nO(N) recursive robot forward dynamic algorithm\nreal-time dynamics control\nrobot dynamics\nscrews\nScrews\ntree topology, closed loop and spatial robot systems\n"}, {"id": "S2212671612001692", "text": "The key point of robot dynamics is optimal design and control. The efficiency of robot dynamics has been the goal of researchers in recent years. Screws are used to describe dynamic problems in this paper, and an O(N) recursive robot forward dynamic algorithm is given on this. It can be easily extended to tree topology, closed loop and spatial robot systems. And three classic methods of robot dynamics are compared for easy of use. The results show that dynamics described with screws are helpful in high efficient dynamics modelling. The dynamical expressions based on screws are concise and clear. It's efficiency is high of O(N) and is linear to the degree of freedom. With the improvement of computation efficiency, it will make the real-time dynamics control become possible.", "keywords": "describe dynamic problems\ndynamical expressions\ndynamics described with screws\nefficiency of robot dynamics\nhigh efficient dynamics modelling.\nimprovement of computation efficiency\nlinear to the degree of freedom\nO(N) recursive robot\nO(N) recursive robot forward dynamic algorithm\nreal-time dynamics control\nrobot dynamics\nscrews\nScrews\ntree topology, closed loop and spatial robot systems\n"}, {"id": "S2212671612001692", "text": "The key point of robot dynamics is optimal design and control. The efficiency of robot dynamics has been the goal of researchers in recent years. Screws are used to describe dynamic problems in this paper, and an O(N) recursive robot forward dynamic algorithm is given on this. It can be easily extended to tree topology, closed loop and spatial robot systems. And three classic methods of robot dynamics are compared for easy of use. The results show that dynamics described with screws are helpful in high efficient dynamics modelling. The dynamical expressions based on screws are concise and clear. It's efficiency is high of O(N) and is linear to the degree of freedom. With the improvement of computation efficiency, it will make the real-time dynamics control become possible.", "keywords": "describe dynamic problems\ndynamical expressions\ndynamics described with screws\nefficiency of robot dynamics\nhigh efficient dynamics modelling.\nimprovement of computation efficiency\nlinear to the degree of freedom\nO(N) recursive robot\nO(N) recursive robot forward dynamic algorithm\nreal-time dynamics control\nrobot dynamics\nscrews\nScrews\ntree topology, closed loop and spatial robot systems\n"}, {"id": "S2212671612001692", "text": "The key point of robot dynamics is optimal design and control. The efficiency of robot dynamics has been the goal of researchers in recent years. Screws are used to describe dynamic problems in this paper, and an O(N) recursive robot forward dynamic algorithm is given on this. It can be easily extended to tree topology, closed loop and spatial robot systems. And three classic methods of robot dynamics are compared for easy of use. The results show that dynamics described with screws are helpful in high efficient dynamics modelling. The dynamical expressions based on screws are concise and clear. It's efficiency is high of O(N) and is linear to the degree of freedom. With the improvement of computation efficiency, it will make the real-time dynamics control become possible.", "keywords": "describe dynamic problems\ndynamical expressions\ndynamics described with screws\nefficiency of robot dynamics\nhigh efficient dynamics modelling.\nimprovement of computation efficiency\nlinear to the degree of freedom\nO(N) recursive robot\nO(N) recursive robot forward dynamic algorithm\nreal-time dynamics control\nrobot dynamics\nscrews\nScrews\ntree topology, closed loop and spatial robot systems\n"}, {"id": "S2212671612001692", "text": "The key point of robot dynamics is optimal design and control. The efficiency of robot dynamics has been the goal of researchers in recent years. Screws are used to describe dynamic problems in this paper, and an O(N) recursive robot forward dynamic algorithm is given on this. It can be easily extended to tree topology, closed loop and spatial robot systems. And three classic methods of robot dynamics are compared for easy of use. The results show that dynamics described with screws are helpful in high efficient dynamics modelling. The dynamical expressions based on screws are concise and clear. It's efficiency is high of O(N) and is linear to the degree of freedom. With the improvement of computation efficiency, it will make the real-time dynamics control become possible.", "keywords": "describe dynamic problems\ndynamical expressions\ndynamics described with screws\nefficiency of robot dynamics\nhigh efficient dynamics modelling.\nimprovement of computation efficiency\nlinear to the degree of freedom\nO(N) recursive robot\nO(N) recursive robot forward dynamic algorithm\nreal-time dynamics control\nrobot dynamics\nscrews\nScrews\ntree topology, closed loop and spatial robot systems\n"}, {"id": "S0038092X14004824", "text": "Historically, the interest in accurate measurement of DNI started decades ago. Early studies (e.g., Linke, 1931; Linke and Ulmitz, 1940) identified the difficulty of separating the measurement of DNI from that of the diffuse irradiance in the immediate vicinity of the sun, hereafter referred to as circumsolar irradiance. Pastiels (1959) conducted a detailed study of the geometry of pyrheliometers, and how that geometry interacted with circumsolar radiance, using simplified representations of the latter. Various communications were then presented at a WMO Task Group meeting held in Belgium in 1966 (WMO, 1967) to improve the accuracy of pyrheliometric measurements, including estimates of the circumsolar enhancement. \u00c5ngstr\u00f6m (1961) and \u00c5ngstr\u00f6m and Rohde (1966) later contributed to the same topic, followed years later by Major (1973, 1980). The whole issue of instrument geometry vs. circumsolar irradiance was complex and confusing at the time because different makes and models of instruments had differing geometries. This was considerably simplified after WMO issued guidelines about the recommended geometry of pyrheliometers, which led to a relatively \u201cstandard\u201d geometry used in all recent instruments. The experimental issues related to the measurement of DNI are discussed in Section 3.2.\n", "keywords": "improve the accuracy of pyrheliometric measurements, including estimates of the circumsolar enhancement\npyrheliometers\nseparating the measurement of DNI from that of the diffuse irradiance in the immediate vicinity of the sun\nstudy of the geometry of pyrheliometers, and how that geometry interacted with circumsolar radiance\n"}, {"id": "S0370269302014892", "text": "Brodsky and Lepage\u00a0[8] have proposed a formula for meson pair production which looks similar to (25), except for a different charge factor and the appearance of the timelike electromagnetic meson form factor instead of the annihilation form factor R(s). This formula was obtained from the leading-twist result by neglecting part of the amplitudes with opposite photon helicities. As has been pointed out in\u00a0[9], this part is however not approximately independent of the pion distribution amplitude and not generically small. We also remark that the appearance of F\u03c0(s) in the \u03b3\u03b3\u2192\u03c0+\u03c0\u2212 amplitude is no longer observed if corrections from partonic transverse momentum in the hard scattering process are taken into account, and that these corrections are not numerically small for the values of s we are dealing with\u00a0[13]. Notice further that two-photon annihilation produces two pions in a C-even state, whereas the electromagnetic form factor projects on the C-odd state of a pion pair. In contrast, our annihilation form factor R2\u03c0(s) is C-even as discussed after\u00a0(24). Finally, due to a particular charge factor, the Brodsky\u2013Lepage formula leads to a vanishing cross section for \u03b3\u03b3 annihilation into pairs of neutral pseudoscalars.\n", "keywords": "annihilation form factor\nBrodsky\u2013Lepage formula\nelectromagnetic form factor\nelectromagnetic meson\nformula\nformula for meson pair production\nhard scattering process\nmeson pair\nmeson pair production\nneglecting part of the amplitudes with opposite photon helicities\nneutral pseudoscalars\npartonic transverse momentum\nphoton\nphoton helicities\npion pair\npions\nR2\u03c0(s)\ntwo-photon annihilation\nvanishing cross section for \u03b3\u03b3 annihilation\n"}, {"id": "S0370269302014892", "text": "Brodsky and Lepage\u00a0[8] have proposed a formula for meson pair production which looks similar to (25), except for a different charge factor and the appearance of the timelike electromagnetic meson form factor instead of the annihilation form factor R(s). This formula was obtained from the leading-twist result by neglecting part of the amplitudes with opposite photon helicities. As has been pointed out in\u00a0[9], this part is however not approximately independent of the pion distribution amplitude and not generically small. We also remark that the appearance of F\u03c0(s) in the \u03b3\u03b3\u2192\u03c0+\u03c0\u2212 amplitude is no longer observed if corrections from partonic transverse momentum in the hard scattering process are taken into account, and that these corrections are not numerically small for the values of s we are dealing with\u00a0[13]. Notice further that two-photon annihilation produces two pions in a C-even state, whereas the electromagnetic form factor projects on the C-odd state of a pion pair. In contrast, our annihilation form factor R2\u03c0(s) is C-even as discussed after\u00a0(24). Finally, due to a particular charge factor, the Brodsky\u2013Lepage formula leads to a vanishing cross section for \u03b3\u03b3 annihilation into pairs of neutral pseudoscalars.\n", "keywords": "annihilation form factor\nBrodsky\u2013Lepage formula\nelectromagnetic form factor\nelectromagnetic meson\nformula\nformula for meson pair production\nhard scattering process\nmeson pair\nmeson pair production\nneglecting part of the amplitudes with opposite photon helicities\nneutral pseudoscalars\npartonic transverse momentum\nphoton\nphoton helicities\npion pair\npions\nR2\u03c0(s)\ntwo-photon annihilation\nvanishing cross section for \u03b3\u03b3 annihilation\n"}, {"id": "S0370269304012638", "text": "It has recently been demonstrated [15] (see also [13] and references therein) that for a self-dual background the two-loop QED effective action takes a remarkably simple form that is very similar to the one-loop action in the same background. There are expectations that this similarity persists at higher loops, and therefore there should be some remarkable structure encoded in the all-loop effective action for gauge theories. In the supersymmetric case, one has to replace the requirement of self-duality by that of relaxed super self-duality [16] in order to arrive at conclusions similar to those given in [15]. Further progress in this direction may be achieved through the analysis of N=2 covariant supergraphs. Finally, we believe that the results of this Letter may be helpful in the context of the conjectured correspondence [17\u201319] between the D3-brane action in AdS5\u00d7S5 and the low-energy action for N=4 SU(N) SYM on its Coulomb branch, with the gauge group SU(N) spontaneously broken to SU(N\u22121)\u00d7U(1). There have appeared two independent F6 tests of this conjecture [19,20], with conflicting conclusions. The approach advocated here provides the opportunity for a further test.\n", "keywords": "AdS5\u00d7S5\nall-loop effective action\nanalysis of N=2 covariant supergraphs\nCoulomb branch\ncovariant supergraphs\nD3-brane action\nF6 tests of this conjecture\ngauge group\nhigher loops\none-loop action\nSU(N)\nSU(N\u22121)\u00d7U(1)\nSU(N) SYM\ntwo-loop QED\ntwo-loop QED effective action\n"}, {"id": "S0370269304012638", "text": "It has recently been demonstrated [15] (see also [13] and references therein) that for a self-dual background the two-loop QED effective action takes a remarkably simple form that is very similar to the one-loop action in the same background. There are expectations that this similarity persists at higher loops, and therefore there should be some remarkable structure encoded in the all-loop effective action for gauge theories. In the supersymmetric case, one has to replace the requirement of self-duality by that of relaxed super self-duality [16] in order to arrive at conclusions similar to those given in [15]. Further progress in this direction may be achieved through the analysis of N=2 covariant supergraphs. Finally, we believe that the results of this Letter may be helpful in the context of the conjectured correspondence [17\u201319] between the D3-brane action in AdS5\u00d7S5 and the low-energy action for N=4 SU(N) SYM on its Coulomb branch, with the gauge group SU(N) spontaneously broken to SU(N\u22121)\u00d7U(1). There have appeared two independent F6 tests of this conjecture [19,20], with conflicting conclusions. The approach advocated here provides the opportunity for a further test.\n", "keywords": "AdS5\u00d7S5\nall-loop effective action\nanalysis of N=2 covariant supergraphs\nCoulomb branch\ncovariant supergraphs\nD3-brane action\nF6 tests of this conjecture\ngauge group\nhigher loops\none-loop action\nSU(N)\nSU(N\u22121)\u00d7U(1)\nSU(N) SYM\ntwo-loop QED\ntwo-loop QED effective action\n"}, {"id": "S016793171300244X", "text": "Ever since the identification of the paramagnetic E\u2032 centre in SiO2 as an unpaired electron localised in an sp3 hybrid orbital of an Si atom backbonded to three oxygen atoms, a number of attempts has been made at explaining the optical and electronic properties of SiO2 in the presence of E\u2032 centres. The irradiation or hole injection induces trapping of positive charge in thin layers of a-SiO2 grown on silicon surfaces by thermal oxidation. This effect has been correlated with paramagnetic E\u2032 centre signals and led to the initial assignment of the neutral oxygen vacancy as the major hole trap in a-SiO2 [1\u20133]. In this model, originally proposed for E\u2032 centres in \u03b1-quartz, upon trapping a hole, one Si atom from the two Si atoms constituting the vacancy remains neutral and hosts the localised unpaired electron while its counterpart becomes positively charged. Although this model has initially been accepted widely for its simplicity, it fails to account for a number of observations, such as the positive charge trapping without generation of E\u2032 centres [4], the formation of high density of E\u2032 centres without the corresponding density of positive charge [5], and the absence of correlation between the decrease of the E\u2032 centre density and the density of positive charge upon post-irradiation electron injection in SiO2 [6].\n", "keywords": "a-SiO2\nE\u2032 centre\nE\u2032 centres\nE\u2032 centres in \u03b1-quartz\nexplaining the optical and electronic properties of SiO2 in the presence of E\u2032 centres\nhigh density of E\u2032 centres\nhole injection\nidentification of the paramagnetic E\u2032 centre in SiO2\nirradiation\nlocalised unpaired electron\nmajor hole trap\nneutral oxygen vacancy\noxygen atoms\nparamagnetic E\u2032 centre\nparamagnetic E\u2032 centre signals\npositive charge\npositive charge trapping\npost-irradiation electron injection\nSi atom\nSi atoms\nsilicon surfaces\nSiO2\nsp3 hybrid orbital\nthermal oxidation\ntrapping of positive charge in thin layers of a-SiO2 grown on silicon surfaces\nunpaired electron\n"}, {"id": "S016793171300244X", "text": "Ever since the identification of the paramagnetic E\u2032 centre in SiO2 as an unpaired electron localised in an sp3 hybrid orbital of an Si atom backbonded to three oxygen atoms, a number of attempts has been made at explaining the optical and electronic properties of SiO2 in the presence of E\u2032 centres. The irradiation or hole injection induces trapping of positive charge in thin layers of a-SiO2 grown on silicon surfaces by thermal oxidation. This effect has been correlated with paramagnetic E\u2032 centre signals and led to the initial assignment of the neutral oxygen vacancy as the major hole trap in a-SiO2 [1\u20133]. In this model, originally proposed for E\u2032 centres in \u03b1-quartz, upon trapping a hole, one Si atom from the two Si atoms constituting the vacancy remains neutral and hosts the localised unpaired electron while its counterpart becomes positively charged. Although this model has initially been accepted widely for its simplicity, it fails to account for a number of observations, such as the positive charge trapping without generation of E\u2032 centres [4], the formation of high density of E\u2032 centres without the corresponding density of positive charge [5], and the absence of correlation between the decrease of the E\u2032 centre density and the density of positive charge upon post-irradiation electron injection in SiO2 [6].\n", "keywords": "a-SiO2\nE\u2032 centre\nE\u2032 centres\nE\u2032 centres in \u03b1-quartz\nexplaining the optical and electronic properties of SiO2 in the presence of E\u2032 centres\nhigh density of E\u2032 centres\nhole injection\nidentification of the paramagnetic E\u2032 centre in SiO2\nirradiation\nlocalised unpaired electron\nmajor hole trap\nneutral oxygen vacancy\noxygen atoms\nparamagnetic E\u2032 centre\nparamagnetic E\u2032 centre signals\npositive charge\npositive charge trapping\npost-irradiation electron injection\nSi atom\nSi atoms\nsilicon surfaces\nSiO2\nsp3 hybrid orbital\nthermal oxidation\ntrapping of positive charge in thin layers of a-SiO2 grown on silicon surfaces\nunpaired electron\n"}, {"id": "S0003491615001505", "text": "The next important step might be the derivation of the Dirac equation. The Creutz model\u00a0 [32] suggests that we should consider incorporating into the logical inference treatment, the additional knowledge that one has objects hopping on a lattice instead of particles moving in a space-time continuum. Recall that up to Section\u00a0 2.4, the description of the measurement scenario, robustness etc. is explicitly discrete. In Section\u00a0 2.4, the continuum limit was taken only because our aim was to derive the Pauli equation, which is formulated in continuum space-time. Of course, the description of the motion of the particle in Section\u00a0 2.6 is entirely within a continuum description but there is no fundamental obstacle to replace this treatment by a proper treatment of objects hopping on a lattice. Therefore it seems plausible that the logical inference approach can be extended to describe massless spin-1/2 particles moving in continuum space-time by considering the continuum limit of the corresponding lattice model. An in-depth, general treatment of this problem is beyond the scope of the present paper and we therefore leave this interesting problem for future research.\n", "keywords": "continuum space-time\nDirac equation\nfuture research\nlattice\nlattice model\nmassless spin-1/2 particles\nmeasurement scenario\nmotion of the particle\nobjects\nobjects hopping on a lattice instead of particles\nparticles moving in a space-time continuum\nPauli equation\nThe Creutz model\n"}, {"id": "S0301010413002139", "text": "The vibrational spectra of l-cysteine have been recorded and assigned in both solution [8,9] and the solid state [10\u201314]. Spectral assignments have been made using empirical force fields [15], Hartree\u2013Fock calculations [10,16,17] based on the isolated molecule approximation. For systems that exhibit strong intermolecular interactions, this approximation often leads to poor agreement between experiment and theory. A striking example is purine [18], where a study of the solid state vibrational spectra by isolated molecule and periodic calculations, gave almost quantitative agreement between theory and experiment for the latter, whereas the former gave only modest agreement and was unable to distinguish between the tautomers. In the present case, where the structure consists of ions linked by hydrogen bonds, periodic calculations based on the complete primitive cell are essential [19]. The only work [20] that includes some solid state effects used molecular dynamics but from which it is difficult to extract assignments. The aim of this paper is to provide a complete assignment of the vibrational spectra of l-cysteine in both the orthorhombic and monoclinic forms by the use of a combination of computational and experimental methods.\n", "keywords": "complete primitive cell\ncomputational and experimental methods\nempirical force fields\nHartree\u2013Fock calculations\nhydrogen\nions\nisolated molecule and periodic calculations\nisolated molecule approximation\nl-cysteine\nmolecular dynamics\nperiodic calculations\nprovide a complete assignment of the vibrational spectra of l-cysteine\npurine\nSpectral assignments\nstudy of the solid state vibrational spectra\ntautomers\n"}, {"id": "S0301010413002139", "text": "The vibrational spectra of l-cysteine have been recorded and assigned in both solution [8,9] and the solid state [10\u201314]. Spectral assignments have been made using empirical force fields [15], Hartree\u2013Fock calculations [10,16,17] based on the isolated molecule approximation. For systems that exhibit strong intermolecular interactions, this approximation often leads to poor agreement between experiment and theory. A striking example is purine [18], where a study of the solid state vibrational spectra by isolated molecule and periodic calculations, gave almost quantitative agreement between theory and experiment for the latter, whereas the former gave only modest agreement and was unable to distinguish between the tautomers. In the present case, where the structure consists of ions linked by hydrogen bonds, periodic calculations based on the complete primitive cell are essential [19]. The only work [20] that includes some solid state effects used molecular dynamics but from which it is difficult to extract assignments. The aim of this paper is to provide a complete assignment of the vibrational spectra of l-cysteine in both the orthorhombic and monoclinic forms by the use of a combination of computational and experimental methods.\n", "keywords": "complete primitive cell\ncomputational and experimental methods\nempirical force fields\nHartree\u2013Fock calculations\nhydrogen\nions\nisolated molecule and periodic calculations\nisolated molecule approximation\nl-cysteine\nmolecular dynamics\nperiodic calculations\nprovide a complete assignment of the vibrational spectra of l-cysteine\npurine\nSpectral assignments\nstudy of the solid state vibrational spectra\ntautomers\n"}, {"id": "S0301010413002139", "text": "The vibrational spectra of l-cysteine have been recorded and assigned in both solution [8,9] and the solid state [10\u201314]. Spectral assignments have been made using empirical force fields [15], Hartree\u2013Fock calculations [10,16,17] based on the isolated molecule approximation. For systems that exhibit strong intermolecular interactions, this approximation often leads to poor agreement between experiment and theory. A striking example is purine [18], where a study of the solid state vibrational spectra by isolated molecule and periodic calculations, gave almost quantitative agreement between theory and experiment for the latter, whereas the former gave only modest agreement and was unable to distinguish between the tautomers. In the present case, where the structure consists of ions linked by hydrogen bonds, periodic calculations based on the complete primitive cell are essential [19]. The only work [20] that includes some solid state effects used molecular dynamics but from which it is difficult to extract assignments. The aim of this paper is to provide a complete assignment of the vibrational spectra of l-cysteine in both the orthorhombic and monoclinic forms by the use of a combination of computational and experimental methods.\n", "keywords": "complete primitive cell\ncomputational and experimental methods\nempirical force fields\nHartree\u2013Fock calculations\nhydrogen\nions\nisolated molecule and periodic calculations\nisolated molecule approximation\nl-cysteine\nmolecular dynamics\nperiodic calculations\nprovide a complete assignment of the vibrational spectra of l-cysteine\npurine\nSpectral assignments\nstudy of the solid state vibrational spectra\ntautomers\n"}, {"id": "S0301010413002139", "text": "The vibrational spectra of l-cysteine have been recorded and assigned in both solution [8,9] and the solid state [10\u201314]. Spectral assignments have been made using empirical force fields [15], Hartree\u2013Fock calculations [10,16,17] based on the isolated molecule approximation. For systems that exhibit strong intermolecular interactions, this approximation often leads to poor agreement between experiment and theory. A striking example is purine [18], where a study of the solid state vibrational spectra by isolated molecule and periodic calculations, gave almost quantitative agreement between theory and experiment for the latter, whereas the former gave only modest agreement and was unable to distinguish between the tautomers. In the present case, where the structure consists of ions linked by hydrogen bonds, periodic calculations based on the complete primitive cell are essential [19]. The only work [20] that includes some solid state effects used molecular dynamics but from which it is difficult to extract assignments. The aim of this paper is to provide a complete assignment of the vibrational spectra of l-cysteine in both the orthorhombic and monoclinic forms by the use of a combination of computational and experimental methods.\n", "keywords": "complete primitive cell\ncomputational and experimental methods\nempirical force fields\nHartree\u2013Fock calculations\nhydrogen\nions\nisolated molecule and periodic calculations\nisolated molecule approximation\nl-cysteine\nmolecular dynamics\nperiodic calculations\nprovide a complete assignment of the vibrational spectra of l-cysteine\npurine\nSpectral assignments\nstudy of the solid state vibrational spectra\ntautomers\n"}, {"id": "S0301010413002139", "text": "The vibrational spectra of l-cysteine have been recorded and assigned in both solution [8,9] and the solid state [10\u201314]. Spectral assignments have been made using empirical force fields [15], Hartree\u2013Fock calculations [10,16,17] based on the isolated molecule approximation. For systems that exhibit strong intermolecular interactions, this approximation often leads to poor agreement between experiment and theory. A striking example is purine [18], where a study of the solid state vibrational spectra by isolated molecule and periodic calculations, gave almost quantitative agreement between theory and experiment for the latter, whereas the former gave only modest agreement and was unable to distinguish between the tautomers. In the present case, where the structure consists of ions linked by hydrogen bonds, periodic calculations based on the complete primitive cell are essential [19]. The only work [20] that includes some solid state effects used molecular dynamics but from which it is difficult to extract assignments. The aim of this paper is to provide a complete assignment of the vibrational spectra of l-cysteine in both the orthorhombic and monoclinic forms by the use of a combination of computational and experimental methods.\n", "keywords": "complete primitive cell\ncomputational and experimental methods\nempirical force fields\nHartree\u2013Fock calculations\nhydrogen\nions\nisolated molecule and periodic calculations\nisolated molecule approximation\nl-cysteine\nmolecular dynamics\nperiodic calculations\nprovide a complete assignment of the vibrational spectra of l-cysteine\npurine\nSpectral assignments\nstudy of the solid state vibrational spectra\ntautomers\n"}, {"id": "S0257897213003563", "text": "According to the ellipsometric spectra, optical constants and other physical parameters can be extracted by an appropriate fitting model. In order to estimate the optical constants/dielectric functions of Ni-doped TiO2 films, a three-phase layered system (air/film/substrate) [15] was utilized to study the ellipsometric spectra. TiO2 belongs to the wide band gap semiconductors. Considering the contribution of the M0 type critical point with the lowest three dimensions, its dielectric function can be calculated by Adachi's model [15,22,23]: \u03b5(\u0395)=\u03b5\u221e+{A0[2\u2212(1+\u03c70)1/2\u2212(1\u2212\u03c70)1/2]}/(EOBG2/3\u03c702). In the model, E is the incident photon energy, \u03b5\u221e is the high-frequency dielectric constant, \u03c70=(E+i\u0393), EOBG is the optical gap energy, and A0 and \u0393 are the strength and broadening parameters of the EOBG transition, respectively. As an example, the experimental SE of the film TN1 at an incident angle 70\u00b0 by dot scatter is shown in Fig.\u00a04. The Fabry\u2013P\u00e9rot interference oscillations due to multiple reflections within the film have been found in the photon energy from 1.5eV to 3.5eV (354nm\u2013826nm), which indicates that the films are transparent in this region. Note that a good agreement of the experimental and calculated spectra is attained in the whole measured photon energy range. The fitting thickness for film TN2 is 159nm, which is very near to the value obtained by SEM (see Fig.\u00a01(b)).\n", "keywords": "A0 and \u0393\nAdachi's model\nair/film/substrate\nE\nellipsometric spectra\nEOBG\nFabry\u2013P\u00e9rot interference\nhigh-frequency dielectric constant\nincident photon energy\nNi-doped TiO2 films\noptical constants\noptical constants/dielectric\nSE\nSEM\nsemiconductors\nstrength and broadening parameters\nthree-phase layered system\n\u03b5\u221e\n\u03b5(\u0395)=\u03b5\u221e+{A0[2\u2212(1+\u03c70)1/2\u2212(1\u2212\u03c70)1/2]}/(EOBG2/3\u03c702)\n\u03c70=(E+i\u0393)\n"}, {"id": "S0257897213003563", "text": "According to the ellipsometric spectra, optical constants and other physical parameters can be extracted by an appropriate fitting model. In order to estimate the optical constants/dielectric functions of Ni-doped TiO2 films, a three-phase layered system (air/film/substrate) [15] was utilized to study the ellipsometric spectra. TiO2 belongs to the wide band gap semiconductors. Considering the contribution of the M0 type critical point with the lowest three dimensions, its dielectric function can be calculated by Adachi's model [15,22,23]: \u03b5(\u0395)=\u03b5\u221e+{A0[2\u2212(1+\u03c70)1/2\u2212(1\u2212\u03c70)1/2]}/(EOBG2/3\u03c702). In the model, E is the incident photon energy, \u03b5\u221e is the high-frequency dielectric constant, \u03c70=(E+i\u0393), EOBG is the optical gap energy, and A0 and \u0393 are the strength and broadening parameters of the EOBG transition, respectively. As an example, the experimental SE of the film TN1 at an incident angle 70\u00b0 by dot scatter is shown in Fig.\u00a04. The Fabry\u2013P\u00e9rot interference oscillations due to multiple reflections within the film have been found in the photon energy from 1.5eV to 3.5eV (354nm\u2013826nm), which indicates that the films are transparent in this region. Note that a good agreement of the experimental and calculated spectra is attained in the whole measured photon energy range. The fitting thickness for film TN2 is 159nm, which is very near to the value obtained by SEM (see Fig.\u00a01(b)).\n", "keywords": "A0 and \u0393\nAdachi's model\nair/film/substrate\nE\nellipsometric spectra\nEOBG\nFabry\u2013P\u00e9rot interference\nhigh-frequency dielectric constant\nincident photon energy\nNi-doped TiO2 films\noptical constants\noptical constants/dielectric\nSE\nSEM\nsemiconductors\nstrength and broadening parameters\nthree-phase layered system\n\u03b5\u221e\n\u03b5(\u0395)=\u03b5\u221e+{A0[2\u2212(1+\u03c70)1/2\u2212(1\u2212\u03c70)1/2]}/(EOBG2/3\u03c702)\n\u03c70=(E+i\u0393)\n"}, {"id": "S0257897213003563", "text": "According to the ellipsometric spectra, optical constants and other physical parameters can be extracted by an appropriate fitting model. In order to estimate the optical constants/dielectric functions of Ni-doped TiO2 films, a three-phase layered system (air/film/substrate) [15] was utilized to study the ellipsometric spectra. TiO2 belongs to the wide band gap semiconductors. Considering the contribution of the M0 type critical point with the lowest three dimensions, its dielectric function can be calculated by Adachi's model [15,22,23]: \u03b5(\u0395)=\u03b5\u221e+{A0[2\u2212(1+\u03c70)1/2\u2212(1\u2212\u03c70)1/2]}/(EOBG2/3\u03c702). In the model, E is the incident photon energy, \u03b5\u221e is the high-frequency dielectric constant, \u03c70=(E+i\u0393), EOBG is the optical gap energy, and A0 and \u0393 are the strength and broadening parameters of the EOBG transition, respectively. As an example, the experimental SE of the film TN1 at an incident angle 70\u00b0 by dot scatter is shown in Fig.\u00a04. The Fabry\u2013P\u00e9rot interference oscillations due to multiple reflections within the film have been found in the photon energy from 1.5eV to 3.5eV (354nm\u2013826nm), which indicates that the films are transparent in this region. Note that a good agreement of the experimental and calculated spectra is attained in the whole measured photon energy range. The fitting thickness for film TN2 is 159nm, which is very near to the value obtained by SEM (see Fig.\u00a01(b)).\n", "keywords": "A0 and \u0393\nAdachi's model\nair/film/substrate\nE\nellipsometric spectra\nEOBG\nFabry\u2013P\u00e9rot interference\nhigh-frequency dielectric constant\nincident photon energy\nNi-doped TiO2 films\noptical constants\noptical constants/dielectric\nSE\nSEM\nsemiconductors\nstrength and broadening parameters\nthree-phase layered system\n\u03b5\u221e\n\u03b5(\u0395)=\u03b5\u221e+{A0[2\u2212(1+\u03c70)1/2\u2212(1\u2212\u03c70)1/2]}/(EOBG2/3\u03c702)\n\u03c70=(E+i\u0393)\n"}, {"id": "S0257897213003563", "text": "According to the ellipsometric spectra, optical constants and other physical parameters can be extracted by an appropriate fitting model. In order to estimate the optical constants/dielectric functions of Ni-doped TiO2 films, a three-phase layered system (air/film/substrate) [15] was utilized to study the ellipsometric spectra. TiO2 belongs to the wide band gap semiconductors. Considering the contribution of the M0 type critical point with the lowest three dimensions, its dielectric function can be calculated by Adachi's model [15,22,23]: \u03b5(\u0395)=\u03b5\u221e+{A0[2\u2212(1+\u03c70)1/2\u2212(1\u2212\u03c70)1/2]}/(EOBG2/3\u03c702). In the model, E is the incident photon energy, \u03b5\u221e is the high-frequency dielectric constant, \u03c70=(E+i\u0393), EOBG is the optical gap energy, and A0 and \u0393 are the strength and broadening parameters of the EOBG transition, respectively. As an example, the experimental SE of the film TN1 at an incident angle 70\u00b0 by dot scatter is shown in Fig.\u00a04. The Fabry\u2013P\u00e9rot interference oscillations due to multiple reflections within the film have been found in the photon energy from 1.5eV to 3.5eV (354nm\u2013826nm), which indicates that the films are transparent in this region. Note that a good agreement of the experimental and calculated spectra is attained in the whole measured photon energy range. The fitting thickness for film TN2 is 159nm, which is very near to the value obtained by SEM (see Fig.\u00a01(b)).\n", "keywords": "A0 and \u0393\nAdachi's model\nair/film/substrate\nE\nellipsometric spectra\nEOBG\nFabry\u2013P\u00e9rot interference\nhigh-frequency dielectric constant\nincident photon energy\nNi-doped TiO2 films\noptical constants\noptical constants/dielectric\nSE\nSEM\nsemiconductors\nstrength and broadening parameters\nthree-phase layered system\n\u03b5\u221e\n\u03b5(\u0395)=\u03b5\u221e+{A0[2\u2212(1+\u03c70)1/2\u2212(1\u2212\u03c70)1/2]}/(EOBG2/3\u03c702)\n\u03c70=(E+i\u0393)\n"}, {"id": "S0378381215301291", "text": "Recently, fundamental (thermophysical property) research on ionic clathrate hydrates has experienced remarkable growth, particularly over the last ten years [21\u201330]. Previously, beginning with the first paper on unusual hydrates of tetrabutylammonium salts in 1940 [31], a number of studies could be found on ionic clathrate hydrates (hereafter, semiclathrate hydrates) [32\u201335] before the unified terminology semiclathrate hydrate was generally accepted. Semiclathrate hydrates have been attracting increased attention because of their promising applications as phase change materials for refrigeration systems and in gas capture and storage [36\u201341]. In addition, there is interesting speculation that semiclathrate hydrate may be regarded as a representative substance for the study of thermal conductivity in clathrate hydrate in general. This is because: (1) it can reduce characterization problems as a solid sample, since semiclathrate hydrate is formed around ambient temperature under atmospheric pressure and is easy to handle; (2) accurately measuring the thermal conductivity of semiclathrate hydrates, which have many similarities to clathrate hydrates, may make possible a deeper understanding of the unique (anomalous) behavior of the thermal conductivity of clathrate hydrates; and (3) currently, there are no experimental studies on the thermal conductivity of semiclathrate hydrates.\n", "keywords": "clathrate hydrates\ngas capture and storage\nionic clathrate hydrates\nmeasuring the thermal conductivity\nphase change materials for refrigeration systems\nsemiclathrate hydrate\nsemiclathrate hydrates\nSemiclathrate hydrates\nstudy of thermal conductivity\nthermal conductivity\nunusual hydrates of tetrabutylammonium salts\n"}, {"id": "S0022311514009271", "text": "Structural properties are well reproduced by all models (Table 2), but the significant improvement of our potential stands in the elastic constants which relate to how the system responds to stress. Indeed, structure and elasticity are important parameters for elucidating grain boundary stability. All potential models correctly predict the relative stability of the defect energies. The Morelon potential model performed best as it was specifically derived to replicate defect formation energies, but it largely underestimates the bulk modulus. The energies calculated with the Morl and the Arima potential models are overestimated; this is a known disadvantage of using rigid ion models as the ionic polarisability is not taken into account. For completeness, we report two shell models with the best results given by the Catlow potential model. The Morl, along with the Grimes shell potential model, accurately reproduce the activation energy of oxygen migration (the migration path was the lowest energy and most favourable diffusion mechanism observed in bulk UO2 [1]). The major deficiency of the Morl potential is that the cation defect energies are high, and hence the number of cation defects will be underestimated. However, this should not be an issue unless this model was applied to processes such as grain growth where cation mobility will contribute.\n", "keywords": "cation\nCatlow potential model\ndiffusion mechanism\nelucidating grain boundary stability\ngrain\ngrain growth\nmigration path\nMorelon potential model\nMorl, along with the Grimes shell potential model\nMorl and the Arima potential models\nMorl potential\noxygen\npredict the relative stability of the defect energies\nreplicate defect formation energies\nreproduce the activation energy of oxygen migration\nrigid ion models\nshell models\nUO2\n"}, {"id": "S2352179115300041", "text": "The main drawback of thermo-oxidation in most actual devices and ITER is its limitation to maintenance periods, when the vessel walls can be heated up around 300\u2013400\u00a0\u00b0C by hot helium injection through the cooling system [19,20], and also because of the required reconditioning of the walls before plasma operation to remove the absorbed oxygen [10]. However, the temperature achieved is not homogeneous over the vessel, as it is limited to the distance to the cooling tubes, and thus to the device design. The analysis of this study is a continuation of previous works done for the treatment of ITER carbon co-deposits [1\u20133], so the temperatures studied are in the range of 350\u00a0\u00b0C for divertor and 200\u2013275\u00a0\u00b0C for main wall and remote parts. At present, due to budget restrains as well as due to tritium trapped in co-deposited carbon layers, ITER will not use carbon materials at the divertor strike points in spite of their excellent resilience against large heat loads. Nevertheless, many present experimental nuclear fusion devices (DIII-D, TCV, etc.) and new ones (JT-60SA, KSTAR, Wenderstein-7X) use carbon elements, so the removal of carbon co-deposits is still necessary for a better device operation\u2014plasma density control, dust events, etc. The temperatures used in this work are not very different from the ones achievable in present devices, such that the results can be extrapolated to them. Moreover, even for ITER this study could be useful if carbon materials have to be eventually installed in the case that operation with tungsten tiles at the strike points is precluded by unexpected reasons.\n", "keywords": "carbon\ncarbon elements\ncarbon layers\ncarbon materials\ncooling system\ncooling tubes\nDIII-D\ndivertor\ndust\nhelium\nITER\nITER carbon\nJT-60SA\nKSTAR\nmain wall\nnuclear fusion devices\noxygen\nplasma density control\nplasma operation\nreconditioning\nremote parts\nremoval of carbon co-deposits\nTCV\nthermo-oxidation\ntreatment of ITER carbon co-deposits\ntritium\ntungsten tiles\nvessel\nWenderstein-7X\n"}, {"id": "S2352179115300041", "text": "The main drawback of thermo-oxidation in most actual devices and ITER is its limitation to maintenance periods, when the vessel walls can be heated up around 300\u2013400\u00a0\u00b0C by hot helium injection through the cooling system [19,20], and also because of the required reconditioning of the walls before plasma operation to remove the absorbed oxygen [10]. However, the temperature achieved is not homogeneous over the vessel, as it is limited to the distance to the cooling tubes, and thus to the device design. The analysis of this study is a continuation of previous works done for the treatment of ITER carbon co-deposits [1\u20133], so the temperatures studied are in the range of 350\u00a0\u00b0C for divertor and 200\u2013275\u00a0\u00b0C for main wall and remote parts. At present, due to budget restrains as well as due to tritium trapped in co-deposited carbon layers, ITER will not use carbon materials at the divertor strike points in spite of their excellent resilience against large heat loads. Nevertheless, many present experimental nuclear fusion devices (DIII-D, TCV, etc.) and new ones (JT-60SA, KSTAR, Wenderstein-7X) use carbon elements, so the removal of carbon co-deposits is still necessary for a better device operation\u2014plasma density control, dust events, etc. The temperatures used in this work are not very different from the ones achievable in present devices, such that the results can be extrapolated to them. Moreover, even for ITER this study could be useful if carbon materials have to be eventually installed in the case that operation with tungsten tiles at the strike points is precluded by unexpected reasons.\n", "keywords": "carbon\ncarbon elements\ncarbon layers\ncarbon materials\ncooling system\ncooling tubes\nDIII-D\ndivertor\ndust\nhelium\nITER\nITER carbon\nJT-60SA\nKSTAR\nmain wall\nnuclear fusion devices\noxygen\nplasma density control\nplasma operation\nreconditioning\nremote parts\nremoval of carbon co-deposits\nTCV\nthermo-oxidation\ntreatment of ITER carbon co-deposits\ntritium\ntungsten tiles\nvessel\nWenderstein-7X\n"}, {"id": "S037026930400721X", "text": "In the supersymmetric case, such a small coupling for quartic interaction cannot be realized if the potential is lifted by the gauge D-term interactions, since, if so, the coupling constant \u03bb becomes of the order O(g2) where g is the gauge coupling constant in the standard model. Therefore, we focus our attention on the D-flat directions. For D-flat directions, we have to be more careful since behaviors of the potential depend on which flat direction we consider. In the MSSM, Yukawa interactions exist in the superpotential to generate the fermion masses. Such Yukawa interactions lift some of the D-flat directions. In addition, we can also find several D-flat directions which are not affected by the Yukawa interactions associated with the fermion masses; without R-parity violation, such D-flat directions are only lifted by the effects of supersymmetry breaking.33Here, we assume that coefficients of non-renormalizable terms are suppressed enough to be neglected. This may be explained by the R-symmetry, assigning R-charge 23 to each MSSM chiral superfields. (See Ref.\u00a0[6] for the details.)\n", "keywords": "behaviors of the potential\ncoefficients of non-renormalizable terms\nD-flat directions\ngauge coupling constant\ngauge D-term interactions\nMSSM chiral superfields\nR-charge 23\nR-symmetry\nsmall coupling\nsmall coupling for quartic interaction\nstandard model\nthe coupling constant \u03bb\nYukawa interactions\n"}, {"id": "S0370157312000105", "text": "By the early 1970s, and following the \u2018golden age\u2019 of general relativity that took place in the 1960s, there was a wide array of candidate theories of gravity in existence that could rival Einstein\u2019s. A formalism was needed to deal with this great abundance of possibilities, and this was provided in the form of the Parameterised Post-Newtonian (PPN) formalism by Kenneth Nordtvedt, Kip Thorne and Clifford Will. The PPN formalism was built on the earlier work of Eddington and Dicke, and allowed for the numerous theories available at the time to be compared to cutting edge astrophysical observations such as lunar laser ranging, radio echo, and, in 1974, the Hulse\u2013Taylor binary pulsar. The PPN formalism provided a clear structure within which one could compare and assess various theories, and has been the benchmark for how theories of gravity should be evaluated ever since. We will give an outline of the PPN formalism, and the constraints available within it today, in Section\u00a02.\n", "keywords": "benchmark for how theories of gravity should be evaluated\ncompare and assess\ncutting edge astrophysical observations\ngeneral relativity\nHulse\u2013Taylor binary pulsar\nlunar laser ranging\nParameterised Post-Newtonian\nPPN\nPPN formalism\nradio echo\ntheories of gravity\n"}, {"id": "S0022311515300477", "text": "In all these studies, the association between the transition and lateral cracking in the oxide layer depicts some interaction between the mechanical behaviour of the system, and its corrosion kinetics, but does not provide a clear understanding of the morphology of the metal:oxide interface during the corrosion process, at the nanometre level. Understanding why this transition behaviour happens is critical when modelling the rate of growth of oxide, and therefore to the lifetime prediction of Zr clads, and ultimately to the safety of nuclear power reactors. No model will be complete without a nanoscale understanding of what is going on during oxidation. Thus, it is essential that the oxide scale and the top layers of the metal are studied at nanometre resolution to reveal the detailed structural and chemical changes associated with diffusion of oxygen and the resulting oxidation of the metal. Whilst a number of techniques have been employed for this purpose, it is clear that various techniques within transmission electron microscopy (TEM) will be among the most versatile and informative for this purpose, although additional information can be added by techniques such as atom probe tomography.\n", "keywords": "atom probe tomography\ncorrosion process\ndiffusion of oxygen\nmetal:oxide interface\nmodelling the rate of growth of oxide\nnuclear power reactors\noxidation\noxidation of the metal\noxide\noxide layer\noxide scale\noxygen\nstructural and chemical changes\nTEM\nthe metal\ntop layers of the metal\ntransmission electron microscopy\nunderstanding of the morphology of the metal:oxide interface\nunderstanding of what is going on during oxidation\nUnderstanding why this transition behaviour happens\nZr clads\n"}, {"id": "S0370269304009220", "text": "There exist some interesting cases where the deformation structure becomes simple. One is the limit to the N=1/2 superspace [5], where the action should reduce to N=1/2 super-Yang\u2013Mills theory with adjoint matter. Another interesting case is the singlet deformation [10,11], where the deformation parameters belongs to the singlet representation of the R-symmetry group SU(2)R. In this Letter, we will study N=2 supersymmetric U(1) gauge theory in the harmonic superspace with singlet deformation. In this case, the gauge and supersymmetry transformations get correction linear in the deformation parameter. Therefore we can easily perform the field redefinition such that the component fields transform canonically under the gauge transformation. In the case of N=1/2 super-Yang\u2013Mills theory, such field redefinition is also possible [5]. But in this case the component fields do not transform canonically under the deformed supersymmtery transformation. In the singlet case, we will show that there is a field redefinition such that the redefined fields also transform canonically under the deformed supersymmetry. We will construct a deformed Lagrangian which is invariant under both the gauge and supersymmetry transformations. We find that the deformed Lagrangian is characterized by a single function of an antiholomorphic scalar field.\n", "keywords": "antiholomorphic scalar field\ncomponent fields\ncorrection linear\ndeformation parameter\ndeformation parameters\ndeformation structure\ndeformed Lagrangian\ndeformed supersymmetry\ndeformed supersymmtery transformation\nfield redefinition\ngauge and supersymmetry transformations\ngauge transformation\nharmonic superspace\nN=1/2 superspace\nN=1/2 super-Yang\u2013Mills theory\nN=1/2 super-Yang\u2013Mills theory with adjoint matter\nN=2 supersymmetric U(1) gauge theory\nR-symmetry group\nsingle function\nsinglet case\nsinglet deformation\nsinglet representation\nSU(2)R\nthe gauge and supersymmetry transformations\ntransform canonically under the deformed supersymmetry\n"}, {"id": "S0370269304009220", "text": "There exist some interesting cases where the deformation structure becomes simple. One is the limit to the N=1/2 superspace [5], where the action should reduce to N=1/2 super-Yang\u2013Mills theory with adjoint matter. Another interesting case is the singlet deformation [10,11], where the deformation parameters belongs to the singlet representation of the R-symmetry group SU(2)R. In this Letter, we will study N=2 supersymmetric U(1) gauge theory in the harmonic superspace with singlet deformation. In this case, the gauge and supersymmetry transformations get correction linear in the deformation parameter. Therefore we can easily perform the field redefinition such that the component fields transform canonically under the gauge transformation. In the case of N=1/2 super-Yang\u2013Mills theory, such field redefinition is also possible [5]. But in this case the component fields do not transform canonically under the deformed supersymmtery transformation. In the singlet case, we will show that there is a field redefinition such that the redefined fields also transform canonically under the deformed supersymmetry. We will construct a deformed Lagrangian which is invariant under both the gauge and supersymmetry transformations. We find that the deformed Lagrangian is characterized by a single function of an antiholomorphic scalar field.\n", "keywords": "antiholomorphic scalar field\ncomponent fields\ncorrection linear\ndeformation parameter\ndeformation parameters\ndeformation structure\ndeformed Lagrangian\ndeformed supersymmetry\ndeformed supersymmtery transformation\nfield redefinition\ngauge and supersymmetry transformations\ngauge transformation\nharmonic superspace\nN=1/2 superspace\nN=1/2 super-Yang\u2013Mills theory\nN=1/2 super-Yang\u2013Mills theory with adjoint matter\nN=2 supersymmetric U(1) gauge theory\nR-symmetry group\nsingle function\nsinglet case\nsinglet deformation\nsinglet representation\nSU(2)R\nthe gauge and supersymmetry transformations\ntransform canonically under the deformed supersymmetry\n"}, {"id": "S0370269304009220", "text": "There exist some interesting cases where the deformation structure becomes simple. One is the limit to the N=1/2 superspace [5], where the action should reduce to N=1/2 super-Yang\u2013Mills theory with adjoint matter. Another interesting case is the singlet deformation [10,11], where the deformation parameters belongs to the singlet representation of the R-symmetry group SU(2)R. In this Letter, we will study N=2 supersymmetric U(1) gauge theory in the harmonic superspace with singlet deformation. In this case, the gauge and supersymmetry transformations get correction linear in the deformation parameter. Therefore we can easily perform the field redefinition such that the component fields transform canonically under the gauge transformation. In the case of N=1/2 super-Yang\u2013Mills theory, such field redefinition is also possible [5]. But in this case the component fields do not transform canonically under the deformed supersymmtery transformation. In the singlet case, we will show that there is a field redefinition such that the redefined fields also transform canonically under the deformed supersymmetry. We will construct a deformed Lagrangian which is invariant under both the gauge and supersymmetry transformations. We find that the deformed Lagrangian is characterized by a single function of an antiholomorphic scalar field.\n", "keywords": "antiholomorphic scalar field\ncomponent fields\ncorrection linear\ndeformation parameter\ndeformation parameters\ndeformation structure\ndeformed Lagrangian\ndeformed supersymmetry\ndeformed supersymmtery transformation\nfield redefinition\ngauge and supersymmetry transformations\ngauge transformation\nharmonic superspace\nN=1/2 superspace\nN=1/2 super-Yang\u2013Mills theory\nN=1/2 super-Yang\u2013Mills theory with adjoint matter\nN=2 supersymmetric U(1) gauge theory\nR-symmetry group\nsingle function\nsinglet case\nsinglet deformation\nsinglet representation\nSU(2)R\nthe gauge and supersymmetry transformations\ntransform canonically under the deformed supersymmetry\n"}, {"id": "S2212671612001709", "text": "An algorithm of multi-axis NC tool-path generation for subdivision surfaces is proposed. The algorithm includes two steps: model building and tool path generation. In the section of model building, in order to obtain the deformed surface, the deformation vector is computed which is associated with the curvature and the slope of cutter location surface. In the procedure of tool path generation, the slicing procedure is adopted to get the CL points. In addition, the inversely converted method is used. The method is tested by some examples with actual machining. The results show that the method can effectively reduce the error of the scallop height for subdivision surface and obtain the better shape and quality. In addition, the computational complexity and is scalable and robust.", "keywords": "algorithm\nalgorithm of multi-axis NC tool-path generation\nalgorithm of multi-axis NC tool-path generation for subdivision surfaces\nCL points\ncurvature and the slope of cutter location surface\ncutter location surface\ndeformation vector\ndeformation vector is computed\ninversely converted method\nmodel building\nobtain the deformed surface\nscallop height\nslicing procedure\nsome examples with actual machining\nsubdivision surface\nsubdivision surfaces\ntested by some examples with actual machining\ntool path generation\n"}, {"id": "S2212671612001709", "text": "An algorithm of multi-axis NC tool-path generation for subdivision surfaces is proposed. The algorithm includes two steps: model building and tool path generation. In the section of model building, in order to obtain the deformed surface, the deformation vector is computed which is associated with the curvature and the slope of cutter location surface. In the procedure of tool path generation, the slicing procedure is adopted to get the CL points. In addition, the inversely converted method is used. The method is tested by some examples with actual machining. The results show that the method can effectively reduce the error of the scallop height for subdivision surface and obtain the better shape and quality. In addition, the computational complexity and is scalable and robust.", "keywords": "algorithm\nalgorithm of multi-axis NC tool-path generation\nalgorithm of multi-axis NC tool-path generation for subdivision surfaces\nCL points\ncurvature and the slope of cutter location surface\ncutter location surface\ndeformation vector\ndeformation vector is computed\ninversely converted method\nmodel building\nobtain the deformed surface\nscallop height\nslicing procedure\nsome examples with actual machining\nsubdivision surface\nsubdivision surfaces\ntested by some examples with actual machining\ntool path generation\n"}, {"id": "S2212671612001709", "text": "An algorithm of multi-axis NC tool-path generation for subdivision surfaces is proposed. The algorithm includes two steps: model building and tool path generation. In the section of model building, in order to obtain the deformed surface, the deformation vector is computed which is associated with the curvature and the slope of cutter location surface. In the procedure of tool path generation, the slicing procedure is adopted to get the CL points. In addition, the inversely converted method is used. The method is tested by some examples with actual machining. The results show that the method can effectively reduce the error of the scallop height for subdivision surface and obtain the better shape and quality. In addition, the computational complexity and is scalable and robust.", "keywords": "algorithm\nalgorithm of multi-axis NC tool-path generation\nalgorithm of multi-axis NC tool-path generation for subdivision surfaces\nCL points\ncurvature and the slope of cutter location surface\ncutter location surface\ndeformation vector\ndeformation vector is computed\ninversely converted method\nmodel building\nobtain the deformed surface\nscallop height\nslicing procedure\nsome examples with actual machining\nsubdivision surface\nsubdivision surfaces\ntested by some examples with actual machining\ntool path generation\n"}, {"id": "S0010938X14002157", "text": "A surfactant is a surface active agent. In this work a surfactant term will be used for compounds which improve the dispersability of the CI in the acid (as emulsifiers providing dispersed emulsion \u2013 not separated) while wetting the surface of the metallic material [14,20,24]. However, surfactants can offer corrosion protection themselves. Some examples when the same compound was used as a surfactant or active corrosion inhibitor ingredient are given below. Typical surfactants in the oilfield services industry are alkylphenol ethoxylates, e.g. nonylphenol ethoxylate (NPE) [14,15,30,106,107]. However, NPEs have been banned from use in the North Sea because of their toxicity. On the other hand, ethoxylated linear alcohols are more acceptable [20]. The quaternary ammonium salts and amines (when protonated) are the most used compounds of the cationic surfactants class, where the cation is the surface active specie. As the amines only function as a surfactant in the protonated state, they cannot be used at high pH. On the other hand, quaternary ammonium compounds, frequently abbreviated as \u201cquats\u201d, are not pH sensitive. Long-chain quaternary ammonium bromides were also reported to work as efficient CIs for steel materials [106]. A frequently employed surfactant was N-dodecylpyridinium bromide (DDPB) [9,60,61,108,109]. Anionic sulphates, anionic sulphonates, alkoxylated alkylphenol resins, and polyoxyethylene sorbitan oleates are also useful surfactants. Ali reported that a particularly useful surfactant is a blend of polyethylene glycol esters of fatty acids and ethoxylated alkylphenols [15]. Several examples of the surfactants used are given below in Section 5.6.\n", "keywords": "acid\nalkoxylated alkylphenol resins\nalkylphenol ethoxylates\namines\namines (when protonated)\nAnionic sulphates\nanionic sulphonates\nblend of polyethylene glycol esters of fatty acids and ethoxylated alkylphenols\ncation\ncationic surfactants class\nCI\nCIs\ncompounds which improve the dispersability\ncorrosion protection\nDDPB\ndispersed emulsion\nemulsifiers\nethoxylated linear alcohols\nimprove the dispersability\nLong-chain quaternary ammonium bromides\nmetallic material\nN-dodecylpyridinium bromide\nnonylphenol ethoxylate\nNPE\nNPEs\npolyoxyethylene sorbitan oleates\nprotonated state\nquaternary ammonium compounds\nquaternary ammonium salts\nquats\nsteel materials\nsurface active agent\nsurface active specie\nsurfactant\nsurfactant or active corrosion inhibitor\nsurfactants\n"}, {"id": "S0010938X14002157", "text": "A surfactant is a surface active agent. In this work a surfactant term will be used for compounds which improve the dispersability of the CI in the acid (as emulsifiers providing dispersed emulsion \u2013 not separated) while wetting the surface of the metallic material [14,20,24]. However, surfactants can offer corrosion protection themselves. Some examples when the same compound was used as a surfactant or active corrosion inhibitor ingredient are given below. Typical surfactants in the oilfield services industry are alkylphenol ethoxylates, e.g. nonylphenol ethoxylate (NPE) [14,15,30,106,107]. However, NPEs have been banned from use in the North Sea because of their toxicity. On the other hand, ethoxylated linear alcohols are more acceptable [20]. The quaternary ammonium salts and amines (when protonated) are the most used compounds of the cationic surfactants class, where the cation is the surface active specie. As the amines only function as a surfactant in the protonated state, they cannot be used at high pH. On the other hand, quaternary ammonium compounds, frequently abbreviated as \u201cquats\u201d, are not pH sensitive. Long-chain quaternary ammonium bromides were also reported to work as efficient CIs for steel materials [106]. A frequently employed surfactant was N-dodecylpyridinium bromide (DDPB) [9,60,61,108,109]. Anionic sulphates, anionic sulphonates, alkoxylated alkylphenol resins, and polyoxyethylene sorbitan oleates are also useful surfactants. Ali reported that a particularly useful surfactant is a blend of polyethylene glycol esters of fatty acids and ethoxylated alkylphenols [15]. Several examples of the surfactants used are given below in Section 5.6.\n", "keywords": "acid\nalkoxylated alkylphenol resins\nalkylphenol ethoxylates\namines\namines (when protonated)\nAnionic sulphates\nanionic sulphonates\nblend of polyethylene glycol esters of fatty acids and ethoxylated alkylphenols\ncation\ncationic surfactants class\nCI\nCIs\ncompounds which improve the dispersability\ncorrosion protection\nDDPB\ndispersed emulsion\nemulsifiers\nethoxylated linear alcohols\nimprove the dispersability\nLong-chain quaternary ammonium bromides\nmetallic material\nN-dodecylpyridinium bromide\nnonylphenol ethoxylate\nNPE\nNPEs\npolyoxyethylene sorbitan oleates\nprotonated state\nquaternary ammonium compounds\nquaternary ammonium salts\nquats\nsteel materials\nsurface active agent\nsurface active specie\nsurfactant\nsurfactant or active corrosion inhibitor\nsurfactants\n"}, {"id": "S0010938X14002157", "text": "A surfactant is a surface active agent. In this work a surfactant term will be used for compounds which improve the dispersability of the CI in the acid (as emulsifiers providing dispersed emulsion \u2013 not separated) while wetting the surface of the metallic material [14,20,24]. However, surfactants can offer corrosion protection themselves. Some examples when the same compound was used as a surfactant or active corrosion inhibitor ingredient are given below. Typical surfactants in the oilfield services industry are alkylphenol ethoxylates, e.g. nonylphenol ethoxylate (NPE) [14,15,30,106,107]. However, NPEs have been banned from use in the North Sea because of their toxicity. On the other hand, ethoxylated linear alcohols are more acceptable [20]. The quaternary ammonium salts and amines (when protonated) are the most used compounds of the cationic surfactants class, where the cation is the surface active specie. As the amines only function as a surfactant in the protonated state, they cannot be used at high pH. On the other hand, quaternary ammonium compounds, frequently abbreviated as \u201cquats\u201d, are not pH sensitive. Long-chain quaternary ammonium bromides were also reported to work as efficient CIs for steel materials [106]. A frequently employed surfactant was N-dodecylpyridinium bromide (DDPB) [9,60,61,108,109]. Anionic sulphates, anionic sulphonates, alkoxylated alkylphenol resins, and polyoxyethylene sorbitan oleates are also useful surfactants. Ali reported that a particularly useful surfactant is a blend of polyethylene glycol esters of fatty acids and ethoxylated alkylphenols [15]. Several examples of the surfactants used are given below in Section 5.6.\n", "keywords": "acid\nalkoxylated alkylphenol resins\nalkylphenol ethoxylates\namines\namines (when protonated)\nAnionic sulphates\nanionic sulphonates\nblend of polyethylene glycol esters of fatty acids and ethoxylated alkylphenols\ncation\ncationic surfactants class\nCI\nCIs\ncompounds which improve the dispersability\ncorrosion protection\nDDPB\ndispersed emulsion\nemulsifiers\nethoxylated linear alcohols\nimprove the dispersability\nLong-chain quaternary ammonium bromides\nmetallic material\nN-dodecylpyridinium bromide\nnonylphenol ethoxylate\nNPE\nNPEs\npolyoxyethylene sorbitan oleates\nprotonated state\nquaternary ammonium compounds\nquaternary ammonium salts\nquats\nsteel materials\nsurface active agent\nsurface active specie\nsurfactant\nsurfactant or active corrosion inhibitor\nsurfactants\n"}, {"id": "S0010938X14002157", "text": "A surfactant is a surface active agent. In this work a surfactant term will be used for compounds which improve the dispersability of the CI in the acid (as emulsifiers providing dispersed emulsion \u2013 not separated) while wetting the surface of the metallic material [14,20,24]. However, surfactants can offer corrosion protection themselves. Some examples when the same compound was used as a surfactant or active corrosion inhibitor ingredient are given below. Typical surfactants in the oilfield services industry are alkylphenol ethoxylates, e.g. nonylphenol ethoxylate (NPE) [14,15,30,106,107]. However, NPEs have been banned from use in the North Sea because of their toxicity. On the other hand, ethoxylated linear alcohols are more acceptable [20]. The quaternary ammonium salts and amines (when protonated) are the most used compounds of the cationic surfactants class, where the cation is the surface active specie. As the amines only function as a surfactant in the protonated state, they cannot be used at high pH. On the other hand, quaternary ammonium compounds, frequently abbreviated as \u201cquats\u201d, are not pH sensitive. Long-chain quaternary ammonium bromides were also reported to work as efficient CIs for steel materials [106]. A frequently employed surfactant was N-dodecylpyridinium bromide (DDPB) [9,60,61,108,109]. Anionic sulphates, anionic sulphonates, alkoxylated alkylphenol resins, and polyoxyethylene sorbitan oleates are also useful surfactants. Ali reported that a particularly useful surfactant is a blend of polyethylene glycol esters of fatty acids and ethoxylated alkylphenols [15]. Several examples of the surfactants used are given below in Section 5.6.\n", "keywords": "acid\nalkoxylated alkylphenol resins\nalkylphenol ethoxylates\namines\namines (when protonated)\nAnionic sulphates\nanionic sulphonates\nblend of polyethylene glycol esters of fatty acids and ethoxylated alkylphenols\ncation\ncationic surfactants class\nCI\nCIs\ncompounds which improve the dispersability\ncorrosion protection\nDDPB\ndispersed emulsion\nemulsifiers\nethoxylated linear alcohols\nimprove the dispersability\nLong-chain quaternary ammonium bromides\nmetallic material\nN-dodecylpyridinium bromide\nnonylphenol ethoxylate\nNPE\nNPEs\npolyoxyethylene sorbitan oleates\nprotonated state\nquaternary ammonium compounds\nquaternary ammonium salts\nquats\nsteel materials\nsurface active agent\nsurface active specie\nsurfactant\nsurfactant or active corrosion inhibitor\nsurfactants\n"}, {"id": "S0370269304009803", "text": "Our aim is to introduce vector mesons in terms of a Lagrangian which satisfies the low energy current algebra. One consistent method is in terms of a non-linear chiral Lagrangian with a hidden local symmetry [6]. In this theory the vector mesons emerge as dynamical vector mesons. The three point vector-pseudo scalar interaction is given by (11)ih4\u3008V\u03bc(P\u2202\u03bcP\u2212\u2202\u03bcPP)\u3009, where h stands for the vector-pseudoscalar coupling. Some typical vertices of \u03c1's to pseudoscalar mesons are (12)\u03c0+(p1)\u03c0\u2212(p2)\u03c10:h(p1\u2212p2)\u03bc\u03b5\u03bc,\u03c0+(p1)\u03c00(p2)\u03c1\u2212:h(p1\u2212p2)\u03bc\u03b5\u03bc,K+(p1)K\u00af0(p2)\u03c1\u2212:h2(p1\u2212p2)\u03bc\u03b5\u03bc,etc., which is directly related to the \u03c1 decay width: \u0393(\u03c1)=h2(|p\u03c0|)3/(6\u03c0m\u03c12), where p\u03c0 is the momentum of final state pions in the \u03c1 rest frame. With \u0393(\u03c1)=149.2MeV, we find h=5.95. We note in passing that the Kawarabayashi\u2013Suzuki\u2013Riazuddin\u2013Fayyazuddin relation gives the value h=m\u03c1/(2f\u03c0)[12]. Thus the value of h in Eq.\u00a0(4) and the two values in this paragraph differ by small amounts (\u223c19%). The strong four-point vertices involving pions are obtained from the first two terms of Eq.\u00a0(5). The weak vertices are obtained from the definitions of Q6 and Q8. In the numerical work we shall use the value of h from Eq.\u00a0(4) and also h=5.95 obtained from the decay width.\n", "keywords": "(12)\u03c0+(p1)\u03c0\u2212(p2)\u03c10:h(p1\u2212p2)\u03bc\u03b5\u03bc\ndiffer by small amounts\ndynamical vector mesons\nh\nhidden local symmetry\nih4\u3008V\u03bc(P\u2202\u03bcP\u2212\u2202\u03bcPP)\u3009\nintroduce vector mesons\nKawarabayashi\u2013Suzuki\u2013Riazuddin\u2013Fayyazuddin relation\nK+(p1)K\u00af0(p2)\u03c1\u2212:h2(p1\u2212p2)\u03bc\u03b5\u03bc\nLagrangian\nlow energy current algebra\nmomentum of final state pions in the \u03c1 rest frame\nnon-linear chiral Lagrangian\np\u03c0\nstrong four-point vertices involving pions are obtained\nvector mesons\nvector-pseudoscalar coupling\nvector-pseudo scalar interaction\nvertices of \u03c1's to pseudoscalar mesons\nweak vertices are obtained\n\u0393(\u03c1)=h2(|p\u03c0|)3/(6\u03c0m\u03c12)\n\u03c0+(p1)\u03c00(p2)\u03c1\u2212:h(p1\u2212p2)\u03bc\u03b5\u03bc\n\u03c1 decay width\n"}, {"id": "S0370269304009803", "text": "Our aim is to introduce vector mesons in terms of a Lagrangian which satisfies the low energy current algebra. One consistent method is in terms of a non-linear chiral Lagrangian with a hidden local symmetry [6]. In this theory the vector mesons emerge as dynamical vector mesons. The three point vector-pseudo scalar interaction is given by (11)ih4\u3008V\u03bc(P\u2202\u03bcP\u2212\u2202\u03bcPP)\u3009, where h stands for the vector-pseudoscalar coupling. Some typical vertices of \u03c1's to pseudoscalar mesons are (12)\u03c0+(p1)\u03c0\u2212(p2)\u03c10:h(p1\u2212p2)\u03bc\u03b5\u03bc,\u03c0+(p1)\u03c00(p2)\u03c1\u2212:h(p1\u2212p2)\u03bc\u03b5\u03bc,K+(p1)K\u00af0(p2)\u03c1\u2212:h2(p1\u2212p2)\u03bc\u03b5\u03bc,etc., which is directly related to the \u03c1 decay width: \u0393(\u03c1)=h2(|p\u03c0|)3/(6\u03c0m\u03c12), where p\u03c0 is the momentum of final state pions in the \u03c1 rest frame. With \u0393(\u03c1)=149.2MeV, we find h=5.95. We note in passing that the Kawarabayashi\u2013Suzuki\u2013Riazuddin\u2013Fayyazuddin relation gives the value h=m\u03c1/(2f\u03c0)[12]. Thus the value of h in Eq.\u00a0(4) and the two values in this paragraph differ by small amounts (\u223c19%). The strong four-point vertices involving pions are obtained from the first two terms of Eq.\u00a0(5). The weak vertices are obtained from the definitions of Q6 and Q8. In the numerical work we shall use the value of h from Eq.\u00a0(4) and also h=5.95 obtained from the decay width.\n", "keywords": "(12)\u03c0+(p1)\u03c0\u2212(p2)\u03c10:h(p1\u2212p2)\u03bc\u03b5\u03bc\ndiffer by small amounts\ndynamical vector mesons\nh\nhidden local symmetry\nih4\u3008V\u03bc(P\u2202\u03bcP\u2212\u2202\u03bcPP)\u3009\nintroduce vector mesons\nKawarabayashi\u2013Suzuki\u2013Riazuddin\u2013Fayyazuddin relation\nK+(p1)K\u00af0(p2)\u03c1\u2212:h2(p1\u2212p2)\u03bc\u03b5\u03bc\nLagrangian\nlow energy current algebra\nmomentum of final state pions in the \u03c1 rest frame\nnon-linear chiral Lagrangian\np\u03c0\nstrong four-point vertices involving pions are obtained\nvector mesons\nvector-pseudoscalar coupling\nvector-pseudo scalar interaction\nvertices of \u03c1's to pseudoscalar mesons\nweak vertices are obtained\n\u0393(\u03c1)=h2(|p\u03c0|)3/(6\u03c0m\u03c12)\n\u03c0+(p1)\u03c00(p2)\u03c1\u2212:h(p1\u2212p2)\u03bc\u03b5\u03bc\n\u03c1 decay width\n"}, {"id": "S0010938X15301268", "text": "Fig. 9 displays the growth of two of the main corrosion products that develop or form on the surface of Cu40Zn with time, hydrozincite (Fig. 9a) and Cu2O (Fig. 9b). It should be remembered that both phases were present already from start of the exposure. The data is presented in absorbance units and allows comparisons to be made of the amounts of each species between the two Cu40Zn surfaces investigated, DP and HZ7. The tendency is very clear that the formation rates of both hydrozincite and cuprite are quite suppressed for Cu40Zn with preformed hydrozincite (HZ7) compared to the diamond polished surface (DP). In summary, without being able to consider the formation of simonkolleite, it can be concluded that an increased surface coverage of hydrozincite reduces the initial spreading ability of the NaCl-containing droplets and thereby lowers the overall formation rate of hydrozincite and cuprite.\n", "keywords": "absorbance units\ncomparisons to be made of the amounts of each species between the two Cu40Zn surfaces investigated\ncorrosion products\nCu2O\nCu40Zn\ncuprite\ndiamond polished surface\nDP\nformation of simonkolleite\nformation rate\nformation rates\nhydrozincite\nHZ7\ninitial spreading\nNaCl-containing droplets\nsimonkolleite\n"}, {"id": "S0010938X15301268", "text": "Fig. 9 displays the growth of two of the main corrosion products that develop or form on the surface of Cu40Zn with time, hydrozincite (Fig. 9a) and Cu2O (Fig. 9b). It should be remembered that both phases were present already from start of the exposure. The data is presented in absorbance units and allows comparisons to be made of the amounts of each species between the two Cu40Zn surfaces investigated, DP and HZ7. The tendency is very clear that the formation rates of both hydrozincite and cuprite are quite suppressed for Cu40Zn with preformed hydrozincite (HZ7) compared to the diamond polished surface (DP). In summary, without being able to consider the formation of simonkolleite, it can be concluded that an increased surface coverage of hydrozincite reduces the initial spreading ability of the NaCl-containing droplets and thereby lowers the overall formation rate of hydrozincite and cuprite.\n", "keywords": "absorbance units\ncomparisons to be made of the amounts of each species between the two Cu40Zn surfaces investigated\ncorrosion products\nCu2O\nCu40Zn\ncuprite\ndiamond polished surface\nDP\nformation of simonkolleite\nformation rate\nformation rates\nhydrozincite\nHZ7\ninitial spreading\nNaCl-containing droplets\nsimonkolleite\n"}, {"id": "S0010938X15301268", "text": "Fig. 9 displays the growth of two of the main corrosion products that develop or form on the surface of Cu40Zn with time, hydrozincite (Fig. 9a) and Cu2O (Fig. 9b). It should be remembered that both phases were present already from start of the exposure. The data is presented in absorbance units and allows comparisons to be made of the amounts of each species between the two Cu40Zn surfaces investigated, DP and HZ7. The tendency is very clear that the formation rates of both hydrozincite and cuprite are quite suppressed for Cu40Zn with preformed hydrozincite (HZ7) compared to the diamond polished surface (DP). In summary, without being able to consider the formation of simonkolleite, it can be concluded that an increased surface coverage of hydrozincite reduces the initial spreading ability of the NaCl-containing droplets and thereby lowers the overall formation rate of hydrozincite and cuprite.\n", "keywords": "absorbance units\ncomparisons to be made of the amounts of each species between the two Cu40Zn surfaces investigated\ncorrosion products\nCu2O\nCu40Zn\ncuprite\ndiamond polished surface\nDP\nformation of simonkolleite\nformation rate\nformation rates\nhydrozincite\nHZ7\ninitial spreading\nNaCl-containing droplets\nsimonkolleite\n"}, {"id": "S0010938X15301268", "text": "Fig. 9 displays the growth of two of the main corrosion products that develop or form on the surface of Cu40Zn with time, hydrozincite (Fig. 9a) and Cu2O (Fig. 9b). It should be remembered that both phases were present already from start of the exposure. The data is presented in absorbance units and allows comparisons to be made of the amounts of each species between the two Cu40Zn surfaces investigated, DP and HZ7. The tendency is very clear that the formation rates of both hydrozincite and cuprite are quite suppressed for Cu40Zn with preformed hydrozincite (HZ7) compared to the diamond polished surface (DP). In summary, without being able to consider the formation of simonkolleite, it can be concluded that an increased surface coverage of hydrozincite reduces the initial spreading ability of the NaCl-containing droplets and thereby lowers the overall formation rate of hydrozincite and cuprite.\n", "keywords": "absorbance units\ncomparisons to be made of the amounts of each species between the two Cu40Zn surfaces investigated\ncorrosion products\nCu2O\nCu40Zn\ncuprite\ndiamond polished surface\nDP\nformation of simonkolleite\nformation rate\nformation rates\nhydrozincite\nHZ7\ninitial spreading\nNaCl-containing droplets\nsimonkolleite\n"}, {"id": "S0039602899010493", "text": "Although the basic mechanisms of the AD process are reasonably well understood, it has not proved simple to apply existing theories to the interpretation of experimental data. What is needed is a combination of the AD theory and the electronic structure of realistic systems, including surface defects and adsorbed species. Such electronic structure calculations are still complex and time-consuming. In many cases, especially for insulating surfaces, attempts to model MIES spectra use simple or intuitive models. In Refs. [4,6,23] it is assumed that the main transition mechanism is Auger de-excitation, and the MIES spectra have been simulated by the surface density of states (DOS) projected on the surface oxygen ions of the uppermost surface layer using a Hartree\u2013Fock method (the crystal code [24,25]) and a density functional theory (DFT) method (the cetep code [26]). The effect of the overlap between the surface and He(1s) wavefunctions was taken into account only approximately by applying an additional z-dependent exponential factor to the surface DOS. Other workers [5,6] estimated the AD transition probability using a DOS projected on to the projectile 1s atomic orbital. However, they were not able to use state-of-the-art methods for the surface electronic structure. Yet the success of the simplified treatments [4\u20136], especially for MIES features such as relative energies of the different peaks, suggests that real spectra are indeed related to the projection of the surface DOS on to the projectile orbital.\n", "keywords": "AD\nadsorbed species\nAD transition\nAuger de-excitation\ncetep code\ncrystal code\ndensity functional theory\ndensity of states\nDFT\nDOS\nHartree\u2013Fock method\nMIES\nMIES spectra\nsurface defects\nsurface oxygen ions\n"}, {"id": "S0039602899010493", "text": "Although the basic mechanisms of the AD process are reasonably well understood, it has not proved simple to apply existing theories to the interpretation of experimental data. What is needed is a combination of the AD theory and the electronic structure of realistic systems, including surface defects and adsorbed species. Such electronic structure calculations are still complex and time-consuming. In many cases, especially for insulating surfaces, attempts to model MIES spectra use simple or intuitive models. In Refs. [4,6,23] it is assumed that the main transition mechanism is Auger de-excitation, and the MIES spectra have been simulated by the surface density of states (DOS) projected on the surface oxygen ions of the uppermost surface layer using a Hartree\u2013Fock method (the crystal code [24,25]) and a density functional theory (DFT) method (the cetep code [26]). The effect of the overlap between the surface and He(1s) wavefunctions was taken into account only approximately by applying an additional z-dependent exponential factor to the surface DOS. Other workers [5,6] estimated the AD transition probability using a DOS projected on to the projectile 1s atomic orbital. However, they were not able to use state-of-the-art methods for the surface electronic structure. Yet the success of the simplified treatments [4\u20136], especially for MIES features such as relative energies of the different peaks, suggests that real spectra are indeed related to the projection of the surface DOS on to the projectile orbital.\n", "keywords": "AD\nadsorbed species\nAD transition\nAuger de-excitation\ncetep code\ncrystal code\ndensity functional theory\ndensity of states\nDFT\nDOS\nHartree\u2013Fock method\nMIES\nMIES spectra\nsurface defects\nsurface oxygen ions\n"}, {"id": "S0039602899010493", "text": "Although the basic mechanisms of the AD process are reasonably well understood, it has not proved simple to apply existing theories to the interpretation of experimental data. What is needed is a combination of the AD theory and the electronic structure of realistic systems, including surface defects and adsorbed species. Such electronic structure calculations are still complex and time-consuming. In many cases, especially for insulating surfaces, attempts to model MIES spectra use simple or intuitive models. In Refs. [4,6,23] it is assumed that the main transition mechanism is Auger de-excitation, and the MIES spectra have been simulated by the surface density of states (DOS) projected on the surface oxygen ions of the uppermost surface layer using a Hartree\u2013Fock method (the crystal code [24,25]) and a density functional theory (DFT) method (the cetep code [26]). The effect of the overlap between the surface and He(1s) wavefunctions was taken into account only approximately by applying an additional z-dependent exponential factor to the surface DOS. Other workers [5,6] estimated the AD transition probability using a DOS projected on to the projectile 1s atomic orbital. However, they were not able to use state-of-the-art methods for the surface electronic structure. Yet the success of the simplified treatments [4\u20136], especially for MIES features such as relative energies of the different peaks, suggests that real spectra are indeed related to the projection of the surface DOS on to the projectile orbital.\n", "keywords": "AD\nadsorbed species\nAD transition\nAuger de-excitation\ncetep code\ncrystal code\ndensity functional theory\ndensity of states\nDFT\nDOS\nHartree\u2013Fock method\nMIES\nMIES spectra\nsurface defects\nsurface oxygen ions\n"}, {"id": "S0370269304007634", "text": "I also could not resist mentioning another wild speculation [10]. Many years ago, inspired by the almost exact correspondence between Einstein's post-Newtonian equations of gravity and Maxwell's equations of motion I proposed the gravitipole in analogy with Dirac's magnetic monopole. After Dirac there was considerable debate on how a field theory of magnetic monopoles may be formulated. Eventually, 't Hooft and Polyakov showed that the magnetic monopole exists as an extended solution in certain non-abelian gauge theories. Most theorists now believe that electromagnetism is merely a piece of a grand unified theory and that magnetic monopoles exist. Might it not turn out that Einstein's theory is but a piece of a bigger theory and that gravitipoles exist? In grand unified theory the electromagnetic field is a component of a multiplet. Could it be that the gravitational field also somehow carries an internal index and that the field we observe is just a component of a multiplet? Throwing caution to the wind, I also asked in [10] if the gravitipole and the graviton might not form a representation under some dual group just as the magnetic monopole and the photon form a triplet under the dual group of Montonen and Olive\u00a0[11].\n", "keywords": "Dirac's magnetic monopole\nEinstein's post-Newtonian equations of gravity\nelectromagnetic field\nelectromagnetism\nfield theory of magnetic monopoles\ngravitational field\ngravitipole\ngravitipoles\ngraviton\nmagnetic monopole\nmagnetic monopoles\nMaxwell's equations of motion\nnon-abelian gauge theories\nphoton\n"}, {"id": "S0370269304007634", "text": "I also could not resist mentioning another wild speculation [10]. Many years ago, inspired by the almost exact correspondence between Einstein's post-Newtonian equations of gravity and Maxwell's equations of motion I proposed the gravitipole in analogy with Dirac's magnetic monopole. After Dirac there was considerable debate on how a field theory of magnetic monopoles may be formulated. Eventually, 't Hooft and Polyakov showed that the magnetic monopole exists as an extended solution in certain non-abelian gauge theories. Most theorists now believe that electromagnetism is merely a piece of a grand unified theory and that magnetic monopoles exist. Might it not turn out that Einstein's theory is but a piece of a bigger theory and that gravitipoles exist? In grand unified theory the electromagnetic field is a component of a multiplet. Could it be that the gravitational field also somehow carries an internal index and that the field we observe is just a component of a multiplet? Throwing caution to the wind, I also asked in [10] if the gravitipole and the graviton might not form a representation under some dual group just as the magnetic monopole and the photon form a triplet under the dual group of Montonen and Olive\u00a0[11].\n", "keywords": "Dirac's magnetic monopole\nEinstein's post-Newtonian equations of gravity\nelectromagnetic field\nelectromagnetism\nfield theory of magnetic monopoles\ngravitational field\ngravitipole\ngravitipoles\ngraviton\nmagnetic monopole\nmagnetic monopoles\nMaxwell's equations of motion\nnon-abelian gauge theories\nphoton\n"}, {"id": "S0370269304007634", "text": "I also could not resist mentioning another wild speculation [10]. Many years ago, inspired by the almost exact correspondence between Einstein's post-Newtonian equations of gravity and Maxwell's equations of motion I proposed the gravitipole in analogy with Dirac's magnetic monopole. After Dirac there was considerable debate on how a field theory of magnetic monopoles may be formulated. Eventually, 't Hooft and Polyakov showed that the magnetic monopole exists as an extended solution in certain non-abelian gauge theories. Most theorists now believe that electromagnetism is merely a piece of a grand unified theory and that magnetic monopoles exist. Might it not turn out that Einstein's theory is but a piece of a bigger theory and that gravitipoles exist? In grand unified theory the electromagnetic field is a component of a multiplet. Could it be that the gravitational field also somehow carries an internal index and that the field we observe is just a component of a multiplet? Throwing caution to the wind, I also asked in [10] if the gravitipole and the graviton might not form a representation under some dual group just as the magnetic monopole and the photon form a triplet under the dual group of Montonen and Olive\u00a0[11].\n", "keywords": "Dirac's magnetic monopole\nEinstein's post-Newtonian equations of gravity\nelectromagnetic field\nelectromagnetism\nfield theory of magnetic monopoles\ngravitational field\ngravitipole\ngravitipoles\ngraviton\nmagnetic monopole\nmagnetic monopoles\nMaxwell's equations of motion\nnon-abelian gauge theories\nphoton\n"}, {"id": "S0927025614007137", "text": "Structural adhesives are increasingly used for bonding components within critical load bearing engineering structures such as aerospace and automotives. Typically these adhesives are based on epoxy polymers. Epoxies are inherently brittle due to their homogeneous microstructure and highly cross linked nature. Thus, there has been much research focused on improving the fracture toughness of epoxy polymers by incorporating a second minority phase at the nano-scale. These modifiers fall into one of two main categories: inorganic additives, e.g. silica [1,2], glass [3], alumina [4], nano-clays [5] and carbon nanotubes [6,7] or organic, usually rubber particles. Rubbery additives can be either core\u2013shell rubber particles [8\u201310] or can form during curing via reaction induced phase separation mechanisms [11,12]. The primary energy dissipation mechanisms for rubber toughened epoxies are known to be both plastic void growth and shear band development [13]. It has also been shown that a combination of the above additives to create a hybrid material can provide synergistic toughening effects, e.g. carbon nanotubes and silica nanoparticles [14] or rubber with silica nanoparticles [15\u201317].\n", "keywords": "additives\nadhesives\naerospace\nalumina\nautomotives\nbonding components\ncarbon nanotubes\ncarbon nanotubes and silica nanoparticles\ncore\u2013shell rubber particles\ncritical load bearing engineering structures\ncuring\nenergy dissipation mechanisms\nEpoxies\nepoxy polymers\nfracture toughness\nglass\nhybrid material\nimproving the fracture toughness of epoxy polymers\nincorporating a second minority phase at the nano-scale\ninorganic additives\nmicrostructure\nnano-clays\norganic\nplastic void growth\nreaction induced phase separation mechanisms\nrubber particles\nrubber toughened epoxies\nrubber with silica nanoparticles\nRubbery additives\nshear band development\nsilica\nsilica nanoparticles\nStructural adhesives\nsynergistic toughening effects\n"}, {"id": "S2212671612002375", "text": "Power Grid reasoning expert system is a complex system. To solve knowledge sharing of knowledge Base in expert system, we abstract and analyze the power grid security investigation procedure by using ontology Technology. With ontology-based Power Grid knowledge base, we establish associated relationship of procedure vocabularies. In this paper, we introduce and analyze of semantic reasoning tools such as Jena. The reasoner mechanism and inference rules of grammar has been included and explained. At last we give a specific application of security investigation procedure ontology and reasoning.", "keywords": "abstract and analyze the power grid security investigation procedure\nassociated relationship of procedure vocabularies\nexpert system\ninference rules of grammar\nintroduce and analyze of semantic reasoning tools\nJena\nknowledge Base\nknowledge sharing of knowledge Base in expert system\nontology-based Power Grid knowledge base\nontology Technology\nPower Grid reasoning expert system\npower grid security investigation procedure\nreasoner mechanism\nsecurity investigation procedure ontology and reasoning\nsemantic reasoning tools\nspecific application of security investigation procedure ontology and reasoning\nsystem\n"}, {"id": "S2212671612002375", "text": "Power Grid reasoning expert system is a complex system. To solve knowledge sharing of knowledge Base in expert system, we abstract and analyze the power grid security investigation procedure by using ontology Technology. With ontology-based Power Grid knowledge base, we establish associated relationship of procedure vocabularies. In this paper, we introduce and analyze of semantic reasoning tools such as Jena. The reasoner mechanism and inference rules of grammar has been included and explained. At last we give a specific application of security investigation procedure ontology and reasoning.", "keywords": "abstract and analyze the power grid security investigation procedure\nassociated relationship of procedure vocabularies\nexpert system\ninference rules of grammar\nintroduce and analyze of semantic reasoning tools\nJena\nknowledge Base\nknowledge sharing of knowledge Base in expert system\nontology-based Power Grid knowledge base\nontology Technology\nPower Grid reasoning expert system\npower grid security investigation procedure\nreasoner mechanism\nsecurity investigation procedure ontology and reasoning\nsemantic reasoning tools\nspecific application of security investigation procedure ontology and reasoning\nsystem\n"}, {"id": "S2212671612002375", "text": "Power Grid reasoning expert system is a complex system. To solve knowledge sharing of knowledge Base in expert system, we abstract and analyze the power grid security investigation procedure by using ontology Technology. With ontology-based Power Grid knowledge base, we establish associated relationship of procedure vocabularies. In this paper, we introduce and analyze of semantic reasoning tools such as Jena. The reasoner mechanism and inference rules of grammar has been included and explained. At last we give a specific application of security investigation procedure ontology and reasoning.", "keywords": "abstract and analyze the power grid security investigation procedure\nassociated relationship of procedure vocabularies\nexpert system\ninference rules of grammar\nintroduce and analyze of semantic reasoning tools\nJena\nknowledge Base\nknowledge sharing of knowledge Base in expert system\nontology-based Power Grid knowledge base\nontology Technology\nPower Grid reasoning expert system\npower grid security investigation procedure\nreasoner mechanism\nsecurity investigation procedure ontology and reasoning\nsemantic reasoning tools\nspecific application of security investigation procedure ontology and reasoning\nsystem\n"}, {"id": "S0009261415008362", "text": "For any quantum dynamical method, existing or emerging, reliable benchmarks are required to assess their accuracy. A model Hamiltonian exhibiting tunnelling dynamics through a multidimensional asymmetric double well potential has been used as a test by the MP/SOFT [18] and CCS methods [19] mentioned above, and also more recently by a configuration interaction (CI) expansion method [20] and two-layer version of CCS (2L-CCS). [21] The Hamiltonian consists of a 1-dimensional tunnelling mode coupled to an (M\u22121)-dimensional harmonic bath, hence it is a system-bath problem which bears some similarity to the Caldeira-Leggett model of tunnelling in a dissipative system [22,23]. This Hamiltonian is non-dissipative, however and the harmonic modes all have the same frequency. System-bath models play an important role in physics, being used to describe superconductivity at a Josephson junction in a superconducting quantum interface device (SQUID) [24], for which the Caldeira-Leggett model provides a theoretical basis, and magnetic and conductance phenomena in the spin-bath regime [25].\n", "keywords": "1-dimensional tunnelling mode\n2L-CCS\nA model Hamiltonian\nbenchmarks\nCaldeira-Leggett model\nCaldeira-Leggett model of tunnelling\nCCS methods\nCI\nconfiguration interaction\nconfiguration interaction (CI) expansion method\nHamiltonian\nharmonic modes\n(M\u22121)-dimensional harmonic bath\nmagnetic and conductance phenomena\nMP/SOFT\nmultidimensional asymmetric double well potential\nphysics\nquantum dynamical method\nspin-bath regime\nSQUID\nsuperconducting quantum interface device\nsuperconductivity at a Josephson junction\nSystem-bath models\nsystem-bath problem\nThe Hamiltonian\ntunnelling dynamics\ntwo-layer version of CCS\n"}, {"id": "S0009261415008362", "text": "For any quantum dynamical method, existing or emerging, reliable benchmarks are required to assess their accuracy. A model Hamiltonian exhibiting tunnelling dynamics through a multidimensional asymmetric double well potential has been used as a test by the MP/SOFT [18] and CCS methods [19] mentioned above, and also more recently by a configuration interaction (CI) expansion method [20] and two-layer version of CCS (2L-CCS). [21] The Hamiltonian consists of a 1-dimensional tunnelling mode coupled to an (M\u22121)-dimensional harmonic bath, hence it is a system-bath problem which bears some similarity to the Caldeira-Leggett model of tunnelling in a dissipative system [22,23]. This Hamiltonian is non-dissipative, however and the harmonic modes all have the same frequency. System-bath models play an important role in physics, being used to describe superconductivity at a Josephson junction in a superconducting quantum interface device (SQUID) [24], for which the Caldeira-Leggett model provides a theoretical basis, and magnetic and conductance phenomena in the spin-bath regime [25].\n", "keywords": "1-dimensional tunnelling mode\n2L-CCS\nA model Hamiltonian\nbenchmarks\nCaldeira-Leggett model\nCaldeira-Leggett model of tunnelling\nCCS methods\nCI\nconfiguration interaction\nconfiguration interaction (CI) expansion method\nHamiltonian\nharmonic modes\n(M\u22121)-dimensional harmonic bath\nmagnetic and conductance phenomena\nMP/SOFT\nmultidimensional asymmetric double well potential\nphysics\nquantum dynamical method\nspin-bath regime\nSQUID\nsuperconducting quantum interface device\nsuperconductivity at a Josephson junction\nSystem-bath models\nsystem-bath problem\nThe Hamiltonian\ntunnelling dynamics\ntwo-layer version of CCS\n"}, {"id": "S0022311515002391", "text": "Spark plasma sintering (SPS) is a relatively new sintering-based technique [17] in which the powder to be consolidated is loaded into an electrically and thermally conductive graphite mould and a large DC pulsed current (1000\u20135000A) is applied under a uniaxial pressure. When current passes through the graphite mould (and the powder if it is electrically conductive), the powder is heated both from the outside (the mould acts as a heating element) and inside (due to Joule heating from the intrinsic electrical resistance of the powder material). SPS is characterised by very fast heating (up to 2000\u00b0C/min) and cooling rates and short holding times (minutes) to achieve near theoretical density [17]. Thus SPS occupies a very different time\u2013temperature\u2013density space in powder consolidation maps when compared with conventional methods, such as hot pressing sintering and HIP with ramp rate of 50\u201380\u00b0C/min and a few hours holding time. Although SPS has been studied for a rapidly growing number of materials [17], there are only a small number of studies on the fabrication and microstructural characterisation of ODS steels processed by SPS, briefly reviewed below.\n", "keywords": "conventional methods\nelectrically and thermally conductive graphite mould and a large DC pulsed current (1000\u20135000A) is applied under a uniaxial pressure\nfabrication and microstructural characterisation of ODS steels processed by SPS\ngraphite mould\nheating element\nHIP\nhot pressing sintering\nmould\nODS steels\npowder\npowder consolidation maps\npowder material\nsintering-based technique\nSpark plasma sintering\nSPS\n"}, {"id": "S092702561300760X", "text": "Due to the complex nature of the thermal spray process, modelling has been playing a key role in providing some key insights for process design and operations. The relationships among processing conditions, particle characteristics, and the resulting coating properties are nonlinear and might be difficult to be unravelled by the experimental studies alone (e.g. [5\u20137]) Detailed information on the atomic level changes leading to changes observed at macroscale can appropriately be obtained by MD simulation and the effect of temperature and velocity can be determined more precisely. In this work, relatively simpler spray system of copper\u2013copper particle was simulated to obtain a better understanding of particle recrystallization and solidification, and deformation mechanics and topography of the impacting particles. Using state-of-the-art methods to examine the physical mechanisms involved in the impacting behavior and structure\u2013property relationship, it can be suggested that the consecutive layer deposition of particles can better be understood by understanding individual particle impacts. The particle\u2013surface interaction mechanism and its relation to Reynolds number can offer information on the quality of the coating through its response to shock heating. As a general practice, engineering components are thermally sprayed in a continuous multilayer mode with cooling; therefore there is an opportunity for developing richer theoretical models for single or multiple particle impact in conjunction with actual spraying tests, so as to identify cohesive and adhesive strength, hardness and residual stresses.\n", "keywords": "atomic level changes\ncoating\nconsecutive layer deposition\ncooling\ncopper\u2013copper particle\ndeformation\ndeveloping richer theoretical models for single or multiple particle impact\nengineering components\nexperimental studies\nMD\nMD simulation\nmodelling\nparticle\nparticle recrystallization and solidification\nparticles\nparticle\u2013surface interaction mechanism\nprocessing\nproviding some key insights for process design and operations\nReynolds number\nshock heating\nspraying tests\nspray system\nsurface\nthermally sprayed\nthermal spray\nthermal spray process\nunderstanding individual particle impacts\n"}, {"id": "S0301679X13003289", "text": "Nanoparticle Tracking Analysis (NTA) has been applied to characterising soot agglomerates of particles and compared with Transmission Electron Microscoscopy (TEM). Soot nanoparticles were extracted from used oil drawn from the sump of a light duty automotive diesel engine. The samples were prepared for analysis by diluting with heptane. Individual tracking of soot agglomerates allows for size distribution analysis. The size of soot was compared with length measurements of projected two-dimensional TEM images of agglomerates. Both the techniques show that soot-in-oil exists as agglomerates with average size of 120nm. NTA is able to measure particles in polydisperse solutions and reports the size and volume distribution of soot-in-oil aggregates; it has the advantages of being fast and relatively low cost if compared with TEM.Nanoparticle Tracking Analysis (NTA) has been applied to characterising soot agglomerates of particles and compared with Transmission Electron Microscoscopy (TEM). Soot nanoparticles were extracted from used oil drawn from the sump of a light duty automotive diesel engine. The samples were prepared for analysis by diluting with heptane. Individual tracking of soot agglomerates allows for size distribution analysis. The size of soot was compared with length measurements of projected two-dimensional TEM images of agglomerates. Both the techniques show that soot-in-oil exists as agglomerates with average size of 120nm. NTA is able to measure particles in polydisperse solutions and reports the size and volume distribution of soot-in-oil aggregates; it has the advantages of being fast and relatively low cost if compared with TEM.\n", "keywords": "agglomerates\nheptane\nNanoparticle Tracking Analysis\nNTA\npolydisperse solutions\nsize distribution analysis\nsoot agglomerates\nsoot-in-oil aggregates\nSoot nanoparticles\nTEM\nTransmission Electron Microscoscopy\n"}, {"id": "S0301679X13003289", "text": "Nanoparticle Tracking Analysis (NTA) has been applied to characterising soot agglomerates of particles and compared with Transmission Electron Microscoscopy (TEM). Soot nanoparticles were extracted from used oil drawn from the sump of a light duty automotive diesel engine. The samples were prepared for analysis by diluting with heptane. Individual tracking of soot agglomerates allows for size distribution analysis. The size of soot was compared with length measurements of projected two-dimensional TEM images of agglomerates. Both the techniques show that soot-in-oil exists as agglomerates with average size of 120nm. NTA is able to measure particles in polydisperse solutions and reports the size and volume distribution of soot-in-oil aggregates; it has the advantages of being fast and relatively low cost if compared with TEM.Nanoparticle Tracking Analysis (NTA) has been applied to characterising soot agglomerates of particles and compared with Transmission Electron Microscoscopy (TEM). Soot nanoparticles were extracted from used oil drawn from the sump of a light duty automotive diesel engine. The samples were prepared for analysis by diluting with heptane. Individual tracking of soot agglomerates allows for size distribution analysis. The size of soot was compared with length measurements of projected two-dimensional TEM images of agglomerates. Both the techniques show that soot-in-oil exists as agglomerates with average size of 120nm. NTA is able to measure particles in polydisperse solutions and reports the size and volume distribution of soot-in-oil aggregates; it has the advantages of being fast and relatively low cost if compared with TEM.\n", "keywords": "agglomerates\nheptane\nNanoparticle Tracking Analysis\nNTA\npolydisperse solutions\nsize distribution analysis\nsoot agglomerates\nsoot-in-oil aggregates\nSoot nanoparticles\nTEM\nTransmission Electron Microscoscopy\n"}, {"id": "S0301679X13003289", "text": "Nanoparticle Tracking Analysis (NTA) has been applied to characterising soot agglomerates of particles and compared with Transmission Electron Microscoscopy (TEM). Soot nanoparticles were extracted from used oil drawn from the sump of a light duty automotive diesel engine. The samples were prepared for analysis by diluting with heptane. Individual tracking of soot agglomerates allows for size distribution analysis. The size of soot was compared with length measurements of projected two-dimensional TEM images of agglomerates. Both the techniques show that soot-in-oil exists as agglomerates with average size of 120nm. NTA is able to measure particles in polydisperse solutions and reports the size and volume distribution of soot-in-oil aggregates; it has the advantages of being fast and relatively low cost if compared with TEM.Nanoparticle Tracking Analysis (NTA) has been applied to characterising soot agglomerates of particles and compared with Transmission Electron Microscoscopy (TEM). Soot nanoparticles were extracted from used oil drawn from the sump of a light duty automotive diesel engine. The samples were prepared for analysis by diluting with heptane. Individual tracking of soot agglomerates allows for size distribution analysis. The size of soot was compared with length measurements of projected two-dimensional TEM images of agglomerates. Both the techniques show that soot-in-oil exists as agglomerates with average size of 120nm. NTA is able to measure particles in polydisperse solutions and reports the size and volume distribution of soot-in-oil aggregates; it has the advantages of being fast and relatively low cost if compared with TEM.\n", "keywords": "agglomerates\nheptane\nNanoparticle Tracking Analysis\nNTA\npolydisperse solutions\nsize distribution analysis\nsoot agglomerates\nsoot-in-oil aggregates\nSoot nanoparticles\nTEM\nTransmission Electron Microscoscopy\n"}, {"id": "S0301679X13003289", "text": "Nanoparticle Tracking Analysis (NTA) has been applied to characterising soot agglomerates of particles and compared with Transmission Electron Microscoscopy (TEM). Soot nanoparticles were extracted from used oil drawn from the sump of a light duty automotive diesel engine. The samples were prepared for analysis by diluting with heptane. Individual tracking of soot agglomerates allows for size distribution analysis. The size of soot was compared with length measurements of projected two-dimensional TEM images of agglomerates. Both the techniques show that soot-in-oil exists as agglomerates with average size of 120nm. NTA is able to measure particles in polydisperse solutions and reports the size and volume distribution of soot-in-oil aggregates; it has the advantages of being fast and relatively low cost if compared with TEM.Nanoparticle Tracking Analysis (NTA) has been applied to characterising soot agglomerates of particles and compared with Transmission Electron Microscoscopy (TEM). Soot nanoparticles were extracted from used oil drawn from the sump of a light duty automotive diesel engine. The samples were prepared for analysis by diluting with heptane. Individual tracking of soot agglomerates allows for size distribution analysis. The size of soot was compared with length measurements of projected two-dimensional TEM images of agglomerates. Both the techniques show that soot-in-oil exists as agglomerates with average size of 120nm. NTA is able to measure particles in polydisperse solutions and reports the size and volume distribution of soot-in-oil aggregates; it has the advantages of being fast and relatively low cost if compared with TEM.\n", "keywords": "agglomerates\nheptane\nNanoparticle Tracking Analysis\nNTA\npolydisperse solutions\nsize distribution analysis\nsoot agglomerates\nsoot-in-oil aggregates\nSoot nanoparticles\nTEM\nTransmission Electron Microscoscopy\n"}, {"id": "S0301679X13003289", "text": "Nanoparticle Tracking Analysis (NTA) has been applied to characterising soot agglomerates of particles and compared with Transmission Electron Microscoscopy (TEM). Soot nanoparticles were extracted from used oil drawn from the sump of a light duty automotive diesel engine. The samples were prepared for analysis by diluting with heptane. Individual tracking of soot agglomerates allows for size distribution analysis. The size of soot was compared with length measurements of projected two-dimensional TEM images of agglomerates. Both the techniques show that soot-in-oil exists as agglomerates with average size of 120nm. NTA is able to measure particles in polydisperse solutions and reports the size and volume distribution of soot-in-oil aggregates; it has the advantages of being fast and relatively low cost if compared with TEM.Nanoparticle Tracking Analysis (NTA) has been applied to characterising soot agglomerates of particles and compared with Transmission Electron Microscoscopy (TEM). Soot nanoparticles were extracted from used oil drawn from the sump of a light duty automotive diesel engine. The samples were prepared for analysis by diluting with heptane. Individual tracking of soot agglomerates allows for size distribution analysis. The size of soot was compared with length measurements of projected two-dimensional TEM images of agglomerates. Both the techniques show that soot-in-oil exists as agglomerates with average size of 120nm. NTA is able to measure particles in polydisperse solutions and reports the size and volume distribution of soot-in-oil aggregates; it has the advantages of being fast and relatively low cost if compared with TEM.\n", "keywords": "agglomerates\nheptane\nNanoparticle Tracking Analysis\nNTA\npolydisperse solutions\nsize distribution analysis\nsoot agglomerates\nsoot-in-oil aggregates\nSoot nanoparticles\nTEM\nTransmission Electron Microscoscopy\n"}, {"id": "S0301679X13003289", "text": "Nanoparticle Tracking Analysis (NTA) has been applied to characterising soot agglomerates of particles and compared with Transmission Electron Microscoscopy (TEM). Soot nanoparticles were extracted from used oil drawn from the sump of a light duty automotive diesel engine. The samples were prepared for analysis by diluting with heptane. Individual tracking of soot agglomerates allows for size distribution analysis. The size of soot was compared with length measurements of projected two-dimensional TEM images of agglomerates. Both the techniques show that soot-in-oil exists as agglomerates with average size of 120nm. NTA is able to measure particles in polydisperse solutions and reports the size and volume distribution of soot-in-oil aggregates; it has the advantages of being fast and relatively low cost if compared with TEM.Nanoparticle Tracking Analysis (NTA) has been applied to characterising soot agglomerates of particles and compared with Transmission Electron Microscoscopy (TEM). Soot nanoparticles were extracted from used oil drawn from the sump of a light duty automotive diesel engine. The samples were prepared for analysis by diluting with heptane. Individual tracking of soot agglomerates allows for size distribution analysis. The size of soot was compared with length measurements of projected two-dimensional TEM images of agglomerates. Both the techniques show that soot-in-oil exists as agglomerates with average size of 120nm. NTA is able to measure particles in polydisperse solutions and reports the size and volume distribution of soot-in-oil aggregates; it has the advantages of being fast and relatively low cost if compared with TEM.\n", "keywords": "agglomerates\nheptane\nNanoparticle Tracking Analysis\nNTA\npolydisperse solutions\nsize distribution analysis\nsoot agglomerates\nsoot-in-oil aggregates\nSoot nanoparticles\nTEM\nTransmission Electron Microscoscopy\n"}, {"id": "S0011227515001216", "text": "Prior to assembling the miniature ADR, the mKCC MR heat switch could not be fully thermally characterised due to cryostat constraints. However, based on experiments and research conducted at MSSL on a range of tungsten heat switches, the thermal conductivity has been estimated. In Hills et al. [8], an equation is derived which allows the thermal conductivity (\u03ba) below 6K to be calculated as a function of magnetic field (B) and temperature (T) (see Eq. (1)). To estimate the performance of the mKCC heat switch, the parameters in Eq. (1) have been taken from the measured thermal conductivity of another MSSL heat switch with the same 1.5mm square cross section, a free path length of 43cm and a RRR of 20,000; it has been observed from experiments conducted at MSSL that there is little change in the thermal performance for tungsten heat switches with a RRR between 20,000 and 32,000 (subject of a future publication) and therefore the performance of the 20,000 RRR heat switch has been assumed to be a good approximation. Fig. 5 gives the calculated thermal conductivity of the mKCC switch at 0, 1, 2 and 3T based on Eq. (1), where the constants b0, a1, a2, a3, a4 and n have the values 0.0328, 1.19\u00d710\u22124, 3.57\u00d710\u22126, 1.36, 0.000968 and 1.7 respectively. It should be noted that the calculated thermal conductivity of the mKCC switch presented in Fig. 5 has been validated by comparing the experimental results of the miniature ADR with modelled predictions (this is discussed in Section 3.3).(1)\u03ba(T)=b0T2+1a1+a2T2T+Bna3T+a4T4\n", "keywords": "20,000 RRR heat switch\nassembling the miniature ADR\nB\ncryostat constraints\nmagnetic field\nminiature ADR\nmKCC heat switch\nmKCC MR heat switch\nmKCC switch\nMSSL heat switch\nthermal conductivity\nthermally characterised\nthermal performance\ntungsten heat switches\n\u03ba\n"}, {"id": "S0011227515001216", "text": "Prior to assembling the miniature ADR, the mKCC MR heat switch could not be fully thermally characterised due to cryostat constraints. However, based on experiments and research conducted at MSSL on a range of tungsten heat switches, the thermal conductivity has been estimated. In Hills et al. [8], an equation is derived which allows the thermal conductivity (\u03ba) below 6K to be calculated as a function of magnetic field (B) and temperature (T) (see Eq. (1)). To estimate the performance of the mKCC heat switch, the parameters in Eq. (1) have been taken from the measured thermal conductivity of another MSSL heat switch with the same 1.5mm square cross section, a free path length of 43cm and a RRR of 20,000; it has been observed from experiments conducted at MSSL that there is little change in the thermal performance for tungsten heat switches with a RRR between 20,000 and 32,000 (subject of a future publication) and therefore the performance of the 20,000 RRR heat switch has been assumed to be a good approximation. Fig. 5 gives the calculated thermal conductivity of the mKCC switch at 0, 1, 2 and 3T based on Eq. (1), where the constants b0, a1, a2, a3, a4 and n have the values 0.0328, 1.19\u00d710\u22124, 3.57\u00d710\u22126, 1.36, 0.000968 and 1.7 respectively. It should be noted that the calculated thermal conductivity of the mKCC switch presented in Fig. 5 has been validated by comparing the experimental results of the miniature ADR with modelled predictions (this is discussed in Section 3.3).(1)\u03ba(T)=b0T2+1a1+a2T2T+Bna3T+a4T4\n", "keywords": "20,000 RRR heat switch\nassembling the miniature ADR\nB\ncryostat constraints\nmagnetic field\nminiature ADR\nmKCC heat switch\nmKCC MR heat switch\nmKCC switch\nMSSL heat switch\nthermal conductivity\nthermally characterised\nthermal performance\ntungsten heat switches\n\u03ba\n"}, {"id": "S0011227515001216", "text": "Prior to assembling the miniature ADR, the mKCC MR heat switch could not be fully thermally characterised due to cryostat constraints. However, based on experiments and research conducted at MSSL on a range of tungsten heat switches, the thermal conductivity has been estimated. In Hills et al. [8], an equation is derived which allows the thermal conductivity (\u03ba) below 6K to be calculated as a function of magnetic field (B) and temperature (T) (see Eq. (1)). To estimate the performance of the mKCC heat switch, the parameters in Eq. (1) have been taken from the measured thermal conductivity of another MSSL heat switch with the same 1.5mm square cross section, a free path length of 43cm and a RRR of 20,000; it has been observed from experiments conducted at MSSL that there is little change in the thermal performance for tungsten heat switches with a RRR between 20,000 and 32,000 (subject of a future publication) and therefore the performance of the 20,000 RRR heat switch has been assumed to be a good approximation. Fig. 5 gives the calculated thermal conductivity of the mKCC switch at 0, 1, 2 and 3T based on Eq. (1), where the constants b0, a1, a2, a3, a4 and n have the values 0.0328, 1.19\u00d710\u22124, 3.57\u00d710\u22126, 1.36, 0.000968 and 1.7 respectively. It should be noted that the calculated thermal conductivity of the mKCC switch presented in Fig. 5 has been validated by comparing the experimental results of the miniature ADR with modelled predictions (this is discussed in Section 3.3).(1)\u03ba(T)=b0T2+1a1+a2T2T+Bna3T+a4T4\n", "keywords": "20,000 RRR heat switch\nassembling the miniature ADR\nB\ncryostat constraints\nmagnetic field\nminiature ADR\nmKCC heat switch\nmKCC MR heat switch\nmKCC switch\nMSSL heat switch\nthermal conductivity\nthermally characterised\nthermal performance\ntungsten heat switches\n\u03ba\n"}, {"id": "S2212667814001245", "text": "Sentence reduction is one of approaches for text summarization that has been attracted many researchers and scholars of natural language processing field. In this paper, we present a method that generates sentence reduction and applying in Vietnamese text summarization using Bayesian Network model. Bayesian network model is used to find the best likelihood short sentence through compare difference of probability. Experimental results with 980 sentences, show that our method really effectively in generating sentence reduction that understandable, readable and exactly grammar.", "keywords": "980 sentences\nBayesian network model\nBayesian Network model\nbest likelihood short sentence\ncompare difference of probability\ngenerates sentence reduction\ngenerating sentence reduction\nnatural language processing\nSentence reduction\ntext summarization\nVietnamese text summarization\n"}, {"id": "S2212667814001245", "text": "Sentence reduction is one of approaches for text summarization that has been attracted many researchers and scholars of natural language processing field. In this paper, we present a method that generates sentence reduction and applying in Vietnamese text summarization using Bayesian Network model. Bayesian network model is used to find the best likelihood short sentence through compare difference of probability. Experimental results with 980 sentences, show that our method really effectively in generating sentence reduction that understandable, readable and exactly grammar.", "keywords": "980 sentences\nBayesian network model\nBayesian Network model\nbest likelihood short sentence\ncompare difference of probability\ngenerates sentence reduction\ngenerating sentence reduction\nnatural language processing\nSentence reduction\ntext summarization\nVietnamese text summarization\n"}, {"id": "S2212667814001245", "text": "Sentence reduction is one of approaches for text summarization that has been attracted many researchers and scholars of natural language processing field. In this paper, we present a method that generates sentence reduction and applying in Vietnamese text summarization using Bayesian Network model. Bayesian network model is used to find the best likelihood short sentence through compare difference of probability. Experimental results with 980 sentences, show that our method really effectively in generating sentence reduction that understandable, readable and exactly grammar.", "keywords": "980 sentences\nBayesian network model\nBayesian Network model\nbest likelihood short sentence\ncompare difference of probability\ngenerates sentence reduction\ngenerating sentence reduction\nnatural language processing\nSentence reduction\ntext summarization\nVietnamese text summarization\n"}, {"id": "S0167931714000203", "text": "To restrict pollen tube growth to a single focal plane is an important subject to enable their accurate growth analysis under microscopic observation. In the conventional method to assay pollen tube growth, the pollen tubes grow in a disorderly manner on solid medium, rendering it impossible to observe their growth in detail. Here, we present a new method to assay pollen tube growth using poly-dimethylsiloxane microchannel device to isolate individual pollen tubes. The growth of the pollen tube is confined to the microchannel and to the same focal plane, allowing accurate microscopic observations. This methodology has the potential for analyses of pollen tube growth in microfluidic environments in response to chemical products and signaling molecules, which paves the way for various experiments on plant reproduction.\n", "keywords": "accurate microscopic observations\nanalyses of pollen tube growth\nassay pollen tube\nassay pollen tube growth\nassay pollen tube growth,\nchemical products\nexperiments on plant reproduction\ngrowth analysis under microscopic observation\nisolate individual pollen tubes\nnew method to assay pollen tube growth\nobserve their growth in detail.\nplant reproduction\npollen tube\npollen tube growth\npollen tubes\npollen tubes grow\npoly-dimethylsiloxane microchannel device\nrestrict pollen tube growth to a single focal plane\nsignaling molecules\n"}, {"id": "S1359028614000989", "text": "The data acquisition strategies must balance the relevant scales and volumes of the datasets to be used in the physical and statistical modeling. Approaches for extraction of the necessary information must be able to disregard spurious information, so as to develop a working network of models for each active mechanism related to each degradation pathway on the mesoscopic physical level and the data-driven statistical model level. To capture the temporal evolution of the energy material over long time frames, appropriate informatics methods are needed to balance data volume (e.g., simple univariate time-series data streams with high-dimensional volumetric imaging datasets) while considering their respective information contents [68,69]. The raw data and extracted information must be accessible for query and modeling. Similarly, the modeling approaches used to understand and parameterize active mechanisms and phenomena over lifetime fall into the broad categories of micro-, meso- and macroscopic approaches. Laboratory and real-world experimentation, informatics, analytics, and the development of network models for mesoscopic evolution of energy materials over lifetime together constitute the field of degradation science.\n", "keywords": "analytics\nappropriate informatics methods\ncapture the temporal evolution\ndata acquisition strategies\ndegradation science\ndevelopment of network models\nenergy material\nenergy materials\nextraction of the necessary information\nhigh-dimensional volumetric imaging datasets\ninformatics\nLaboratory and real-world experimentation\nmicro-, meso- and macroscopic approaches\nmodeling\nmodeling approaches\nnetwork of models\nphysical and statistical modeling\nquery\nstatistical model\nunivariate time-series data streams\n"}, {"id": "S0021999113005846", "text": "Although the free Kelvin wave problem is of considerable theoretical importance, problems with forcing and damping have greater practical importance. In nature, the forcing could be due to a wind stress at the free surface or an astronomical tidal potential, and the damping could be due to the turbulent stress of a bottom boundary layer. Regardless of the details, the forced response is composed of shallow-water waves, possibly including Kelvin waves, with the largest amplitudes in waves with a natural frequency \u03c9f close to that of the forcing frequency \u03c9; various examples of this sort are given in Chapters 9 and 10 of Gill [16]. When \u03c9\u2248\u03c9f, there is a large amplitude near-resonant response, the size of which is sensitive to the weak damping and |\u03c9\u2212\u03c9f|. Thus, in numerical solutions of near-resonantly forced waves, we anticipate that errors in \u03c9f (associated with the spatial discretisation) could lead to non-trivial errors in the forced response.\n", "keywords": "astronomical tidal potential\ndamping\nforced response\nforcing\nforcing frequency\nfree Kelvin wave problem\nKelvin wave\nKelvin waves\nnatural frequency\nnear-resonantly forced waves\nnumerical solutions\nproblems with forcing and damping\nshallow-water waves\nspatial discretisation\nturbulent stress of a bottom boundary layer\nwaves\nweak damping\nwind\nwind stress at the free surface\n\u03c9\n\u03c9f\n|\u03c9\u2212\u03c9f|\n"}, {"id": "S0167273813005298", "text": "At 200 \u2013 300\u00b0C: nuclear densities are localised in the tetrahedral volume roughly covering the 8c and 32f positions with \u201cbulges\u201d of nuclear densities pointing toward the 48i position, while at 400 and 500\u00b0C continuous nuclear densities forming a straight line along the <100> direction are found, indicative of oxide-ion diffusion pathway along that direction. In the literature, curved pathways along the <100> direction passing through the 48i site are generally observed in fluorite materials [20], the prevalence of curve pathway as opposed from straight pathway is explained by the repulsion between cation and anions, the curved pathway allowing the cation\u2013anion to maintain a reasonable distance. However, a straight pathway is observed for Y0.785Ta0.215O1.715 [23], as is the case for the present material. This suggests that Ta and Re cations might play a similar role in these systems.\n", "keywords": "anion\nanions\n\u201cbulges\u201d\ncation\nfluorite materials\nion\noxide\noxide-ion diffusion\npresent material\nTa and Re cations\n"}, {"id": "S0370269304009037", "text": "Within a coalescence approach as successfully applied earlier in the light-quark sector, we have evaluated transverse-momentum dependencies of charmed hadrons in central heavy-ion reactions at RHIC. For the charm-quark distributions at hadronization we have considered two limiting scenarios, i.e., no reinteractions (using spectra from PYTHIA) and complete thermalization with transverse flow of the bulk matter. The resulting J/\u03c8 (mT-)spectra differ in slope by up to a factor of\u00a02 (harder for pQCD c-quarks), and the integrated yield is about a factor of\u00a03 larger in the thermal case. For D-mesons, we found that the difference in the slope parameters of the pT-spectra in the two scenarios is less pronounced, but their elliptic flow is about a factor of\u00a02 larger for pT\u2a7e1.5\u00a0GeV in the thermalized case. The elliptic flow pattern of D-mesons was found to be essentially preserved in the single-electron decay spectra, rendering the latter a very promising observable to address the strength of charm reinteractions in the QGP. The present study can be straightforwardly generalized to charmed baryons (\u039bc), which may serve as a complimentary probe for charm-quark reinteractions in the QGP.\n", "keywords": "baryons\ncentral heavy-ion reactions\ncharmed hadrons\ncharm-quark distributions\ncharm-quark reinteractions\ncharm reinteractions\ncoalescence approach\ncomplete thermalization\ncomplimentary probe\ndifference in the slope parameters\nD-mesons\nelliptic flow\nelliptic flow pattern\nevaluated transverse-momentum dependencies\nhadronization\nintegrated yield\nlight-quark sector\npQCD c-quarks\npT-spectra\nQGP\nquark\nsingle-electron decay spectra\nspectra\ntransverse flow\n"}, {"id": "S0370269304009037", "text": "Within a coalescence approach as successfully applied earlier in the light-quark sector, we have evaluated transverse-momentum dependencies of charmed hadrons in central heavy-ion reactions at RHIC. For the charm-quark distributions at hadronization we have considered two limiting scenarios, i.e., no reinteractions (using spectra from PYTHIA) and complete thermalization with transverse flow of the bulk matter. The resulting J/\u03c8 (mT-)spectra differ in slope by up to a factor of\u00a02 (harder for pQCD c-quarks), and the integrated yield is about a factor of\u00a03 larger in the thermal case. For D-mesons, we found that the difference in the slope parameters of the pT-spectra in the two scenarios is less pronounced, but their elliptic flow is about a factor of\u00a02 larger for pT\u2a7e1.5\u00a0GeV in the thermalized case. The elliptic flow pattern of D-mesons was found to be essentially preserved in the single-electron decay spectra, rendering the latter a very promising observable to address the strength of charm reinteractions in the QGP. The present study can be straightforwardly generalized to charmed baryons (\u039bc), which may serve as a complimentary probe for charm-quark reinteractions in the QGP.\n", "keywords": "baryons\ncentral heavy-ion reactions\ncharmed hadrons\ncharm-quark distributions\ncharm-quark reinteractions\ncharm reinteractions\ncoalescence approach\ncomplete thermalization\ncomplimentary probe\ndifference in the slope parameters\nD-mesons\nelliptic flow\nelliptic flow pattern\nevaluated transverse-momentum dependencies\nhadronization\nintegrated yield\nlight-quark sector\npQCD c-quarks\npT-spectra\nQGP\nquark\nsingle-electron decay spectra\nspectra\ntransverse flow\n"}, {"id": "S0370269304009037", "text": "Within a coalescence approach as successfully applied earlier in the light-quark sector, we have evaluated transverse-momentum dependencies of charmed hadrons in central heavy-ion reactions at RHIC. For the charm-quark distributions at hadronization we have considered two limiting scenarios, i.e., no reinteractions (using spectra from PYTHIA) and complete thermalization with transverse flow of the bulk matter. The resulting J/\u03c8 (mT-)spectra differ in slope by up to a factor of\u00a02 (harder for pQCD c-quarks), and the integrated yield is about a factor of\u00a03 larger in the thermal case. For D-mesons, we found that the difference in the slope parameters of the pT-spectra in the two scenarios is less pronounced, but their elliptic flow is about a factor of\u00a02 larger for pT\u2a7e1.5\u00a0GeV in the thermalized case. The elliptic flow pattern of D-mesons was found to be essentially preserved in the single-electron decay spectra, rendering the latter a very promising observable to address the strength of charm reinteractions in the QGP. The present study can be straightforwardly generalized to charmed baryons (\u039bc), which may serve as a complimentary probe for charm-quark reinteractions in the QGP.\n", "keywords": "baryons\ncentral heavy-ion reactions\ncharmed hadrons\ncharm-quark distributions\ncharm-quark reinteractions\ncharm reinteractions\ncoalescence approach\ncomplete thermalization\ncomplimentary probe\ndifference in the slope parameters\nD-mesons\nelliptic flow\nelliptic flow pattern\nevaluated transverse-momentum dependencies\nhadronization\nintegrated yield\nlight-quark sector\npQCD c-quarks\npT-spectra\nQGP\nquark\nsingle-electron decay spectra\nspectra\ntransverse flow\n"}, {"id": "S2212671612001291", "text": "In the paper we propose a conceptual methodology to control liquid state of Al-Si alloys in melting and holding sub-process of the pressure die-casting process. Given that, we determine the characteristic of the holding furnace based on weight percent (wt %) of the certain alloys and their elements. Subsequently the paper introduces an application of methodology of research for establishing characteristic of holding furnace. The application was realized under real conditions in foundry that uses horizontal cold chamber machine CLH 400.1. The chemical analysis was performed by spectrophotometer SPECTROLAB JR.CCD 2000. Finally the last part of the paper lists overall findings with possible future direction to extend this methodology in practice.", "keywords": "alloys\nAl-Si alloys\napplication of methodology of research\ncharacteristic of holding furnace\nchemical analysis\nCLH 400.1\nconceptual methodology to control liquid state\ndetermine the characteristic of the holding furnace\nextend this methodology in practice\nfoundry\nholding furnace\nhorizontal cold chamber machine\nmelting and holding sub-process\npressure die-casting process\nreal condition\nSPECTROLAB JR.CCD 2000\nspectrophotometer\nweight percent\nwt %\n"}, {"id": "S2212671612001291", "text": "In the paper we propose a conceptual methodology to control liquid state of Al-Si alloys in melting and holding sub-process of the pressure die-casting process. Given that, we determine the characteristic of the holding furnace based on weight percent (wt %) of the certain alloys and their elements. Subsequently the paper introduces an application of methodology of research for establishing characteristic of holding furnace. The application was realized under real conditions in foundry that uses horizontal cold chamber machine CLH 400.1. The chemical analysis was performed by spectrophotometer SPECTROLAB JR.CCD 2000. Finally the last part of the paper lists overall findings with possible future direction to extend this methodology in practice.", "keywords": "alloys\nAl-Si alloys\napplication of methodology of research\ncharacteristic of holding furnace\nchemical analysis\nCLH 400.1\nconceptual methodology to control liquid state\ndetermine the characteristic of the holding furnace\nextend this methodology in practice\nfoundry\nholding furnace\nhorizontal cold chamber machine\nmelting and holding sub-process\npressure die-casting process\nreal condition\nSPECTROLAB JR.CCD 2000\nspectrophotometer\nweight percent\nwt %\n"}, {"id": "S2212671612001291", "text": "In the paper we propose a conceptual methodology to control liquid state of Al-Si alloys in melting and holding sub-process of the pressure die-casting process. Given that, we determine the characteristic of the holding furnace based on weight percent (wt %) of the certain alloys and their elements. Subsequently the paper introduces an application of methodology of research for establishing characteristic of holding furnace. The application was realized under real conditions in foundry that uses horizontal cold chamber machine CLH 400.1. The chemical analysis was performed by spectrophotometer SPECTROLAB JR.CCD 2000. Finally the last part of the paper lists overall findings with possible future direction to extend this methodology in practice.", "keywords": "alloys\nAl-Si alloys\napplication of methodology of research\ncharacteristic of holding furnace\nchemical analysis\nCLH 400.1\nconceptual methodology to control liquid state\ndetermine the characteristic of the holding furnace\nextend this methodology in practice\nfoundry\nholding furnace\nhorizontal cold chamber machine\nmelting and holding sub-process\npressure die-casting process\nreal condition\nSPECTROLAB JR.CCD 2000\nspectrophotometer\nweight percent\nwt %\n"}, {"id": "S2212671612001291", "text": "In the paper we propose a conceptual methodology to control liquid state of Al-Si alloys in melting and holding sub-process of the pressure die-casting process. Given that, we determine the characteristic of the holding furnace based on weight percent (wt %) of the certain alloys and their elements. Subsequently the paper introduces an application of methodology of research for establishing characteristic of holding furnace. The application was realized under real conditions in foundry that uses horizontal cold chamber machine CLH 400.1. The chemical analysis was performed by spectrophotometer SPECTROLAB JR.CCD 2000. Finally the last part of the paper lists overall findings with possible future direction to extend this methodology in practice.", "keywords": "alloys\nAl-Si alloys\napplication of methodology of research\ncharacteristic of holding furnace\nchemical analysis\nCLH 400.1\nconceptual methodology to control liquid state\ndetermine the characteristic of the holding furnace\nextend this methodology in practice\nfoundry\nholding furnace\nhorizontal cold chamber machine\nmelting and holding sub-process\npressure die-casting process\nreal condition\nSPECTROLAB JR.CCD 2000\nspectrophotometer\nweight percent\nwt %\n"}, {"id": "S2212671612001291", "text": "In the paper we propose a conceptual methodology to control liquid state of Al-Si alloys in melting and holding sub-process of the pressure die-casting process. Given that, we determine the characteristic of the holding furnace based on weight percent (wt %) of the certain alloys and their elements. Subsequently the paper introduces an application of methodology of research for establishing characteristic of holding furnace. The application was realized under real conditions in foundry that uses horizontal cold chamber machine CLH 400.1. The chemical analysis was performed by spectrophotometer SPECTROLAB JR.CCD 2000. Finally the last part of the paper lists overall findings with possible future direction to extend this methodology in practice.", "keywords": "alloys\nAl-Si alloys\napplication of methodology of research\ncharacteristic of holding furnace\nchemical analysis\nCLH 400.1\nconceptual methodology to control liquid state\ndetermine the characteristic of the holding furnace\nextend this methodology in practice\nfoundry\nholding furnace\nhorizontal cold chamber machine\nmelting and holding sub-process\npressure die-casting process\nreal condition\nSPECTROLAB JR.CCD 2000\nspectrophotometer\nweight percent\nwt %\n"}, {"id": "S2212671612001291", "text": "In the paper we propose a conceptual methodology to control liquid state of Al-Si alloys in melting and holding sub-process of the pressure die-casting process. Given that, we determine the characteristic of the holding furnace based on weight percent (wt %) of the certain alloys and their elements. Subsequently the paper introduces an application of methodology of research for establishing characteristic of holding furnace. The application was realized under real conditions in foundry that uses horizontal cold chamber machine CLH 400.1. The chemical analysis was performed by spectrophotometer SPECTROLAB JR.CCD 2000. Finally the last part of the paper lists overall findings with possible future direction to extend this methodology in practice.", "keywords": "alloys\nAl-Si alloys\napplication of methodology of research\ncharacteristic of holding furnace\nchemical analysis\nCLH 400.1\nconceptual methodology to control liquid state\ndetermine the characteristic of the holding furnace\nextend this methodology in practice\nfoundry\nholding furnace\nhorizontal cold chamber machine\nmelting and holding sub-process\npressure die-casting process\nreal condition\nSPECTROLAB JR.CCD 2000\nspectrophotometer\nweight percent\nwt %\n"}, {"id": "S0370269303015478", "text": "Since perturbative expansion is used, it is impossible to find the exact bounds; instead, one can derive tree-level unitarity bounds or loop-improved unitarity bounds. In this study, we will use unitarity bounds coming from a tree-level analysis\u00a0[20]. This tree level analysis is derived with the help of the equivalence theorem [21], which itself is a high-energy approximation where it is assumed that the energy scale is much larger than the Z0 and W\u00b1 gauge-boson masses. We will consider here this \u201chigh-energy\u201d hypothesis that both the equivalence theorem and the decoupling regime are well settled, but in such a way that the unitarity constraint is also fulfilled. Our purpose is to investigate the quantum effects in the decays of the light CP-even Higgs boson h0, especially looking for sizeable differences with respect to the SM in the decoupling regime.\n", "keywords": "decays of the light CP-even Higgs boson h0\ndecoupling regime\nequivalence theorem\nexact bounds\ngauge-boson masses\nh0\nHiggs boson\nhigh-energy approximation\ninvestigate the quantum effects in the decays of the light CP-even Higgs boson h0\nlooking for sizeable differences with respect to the SM in the decoupling regime\nloop-improved unitarity bounds\nperturbative expansion\nquantum effects\nSM\ntree level analysis\ntree-level analysis\ntree-level unitarity bounds\nunitarity bounds\nunitarity constraint\n"}, {"id": "S0021999113004555", "text": "Three Runge\u2013Kutta IMEX schemes were tested by Ullrich and Jablonowski [23] for the HEVI solution of the equations governing atmospheric motion. They tested the ARS(2,3,2) scheme of Ascher et al. [1] and also suggested the less computationally expensive but nearly as accurate Strang carryover scheme. This involves Strang splitting but the first implicit stage is cleverly re-used from the final implicit stage of the previous time-step and so there is only one implicit solution per time-step. Another novel approach taken by Ullrich and Jablonowski [23] is to use a Rosenbrock solution in order to treat all of the vertical terms implicitly rather than just the terms involved in wave propagation. A Rosenbrock solution is one iteration of a Newton solver. This circumvents the time-step restriction associated with vertical advection at the cost of slowing the vertical advection.\n", "keywords": "ARS(2,3,2) scheme\natmospheric motion\nHEVI solution\nNewton solver\none iteration of a Newton solver\nRosenbrock solution\nRunge\u2013Kutta IMEX schemes\nslowing the vertical advection\nStrang carryover scheme\nStrang splitting\ntime-step restriction\nvertical advection\nwave propagation\n"}, {"id": "S0021999113004555", "text": "Three Runge\u2013Kutta IMEX schemes were tested by Ullrich and Jablonowski [23] for the HEVI solution of the equations governing atmospheric motion. They tested the ARS(2,3,2) scheme of Ascher et al. [1] and also suggested the less computationally expensive but nearly as accurate Strang carryover scheme. This involves Strang splitting but the first implicit stage is cleverly re-used from the final implicit stage of the previous time-step and so there is only one implicit solution per time-step. Another novel approach taken by Ullrich and Jablonowski [23] is to use a Rosenbrock solution in order to treat all of the vertical terms implicitly rather than just the terms involved in wave propagation. A Rosenbrock solution is one iteration of a Newton solver. This circumvents the time-step restriction associated with vertical advection at the cost of slowing the vertical advection.\n", "keywords": "ARS(2,3,2) scheme\natmospheric motion\nHEVI solution\nNewton solver\none iteration of a Newton solver\nRosenbrock solution\nRunge\u2013Kutta IMEX schemes\nslowing the vertical advection\nStrang carryover scheme\nStrang splitting\ntime-step restriction\nvertical advection\nwave propagation\n"}, {"id": "S0021999113004555", "text": "Three Runge\u2013Kutta IMEX schemes were tested by Ullrich and Jablonowski [23] for the HEVI solution of the equations governing atmospheric motion. They tested the ARS(2,3,2) scheme of Ascher et al. [1] and also suggested the less computationally expensive but nearly as accurate Strang carryover scheme. This involves Strang splitting but the first implicit stage is cleverly re-used from the final implicit stage of the previous time-step and so there is only one implicit solution per time-step. Another novel approach taken by Ullrich and Jablonowski [23] is to use a Rosenbrock solution in order to treat all of the vertical terms implicitly rather than just the terms involved in wave propagation. A Rosenbrock solution is one iteration of a Newton solver. This circumvents the time-step restriction associated with vertical advection at the cost of slowing the vertical advection.\n", "keywords": "ARS(2,3,2) scheme\natmospheric motion\nHEVI solution\nNewton solver\none iteration of a Newton solver\nRosenbrock solution\nRunge\u2013Kutta IMEX schemes\nslowing the vertical advection\nStrang carryover scheme\nStrang splitting\ntime-step restriction\nvertical advection\nwave propagation\n"}, {"id": "S0377025713001031", "text": "Flow-induced deformations can lead to irreversible changes in the structure of a polymeric fluid; if the rate of extension far exceeds the rate of relaxation, then the polymer chain can be broken. Mechanical degradation of polymers in extensional flow has long been recognised [30] and leads to a reduction in the average molecular weight. A-Alamry et al. [1] have recently reported evidence of flow-induced polymer degradation in DoD jetting. Central scission is observed for polystyrene in a number of good solvents under certain jetting conditions for a bounded range of molecular weights. Since only those molecules that are fully extended can be fractured at the centre of the polymer chain [29], in this paper we investigate whether flow-induced central scission is possible under the conditions of DoD jetting.\n", "keywords": "Central scission\nDoD jetting\nflow-induced central scission\npolymeric fluid\npolymers\npolystyrene\nreduction in the average molecular weight\n"}, {"id": "S0009261412006513", "text": "Water is the most important liquid, and the nature of its structure remains a topic of keen debate and an active area of research [1\u20139]. Much of this debate centers around whether water has a mainly tetrahedral structure with a continuum of distorted hydrogen bonds, or if it contains a mixture of two distinct components. One major development in recent years is the application of inner-shell spectroscopic techniques, such as X-ray absorption spectroscopy (XAS) and X-ray emission spectroscopy (XES) at the oxygen K-edge to investigate the structure of water [2,10\u201312]. These methods can provide a direct structural probe of water, providing insight into the nature of its hydrogen bonding network. Theoretical studies play a critical role in these studies, since the analysis of the experimental data requires calculations to provide a link between the observed spectral features and the underlying structure. However, the simulation of the XAS or XES for liquid water presents a difficult challenge because it requires accurate molecular dynamics simulations to provide a correct description of the molecular structure coupled with accurate calculations of the spectral properties, i.e. excitation energies and line intensities. Furthermore, adequate sampling over molecular configurations also needs to be accounted for.\n", "keywords": "accurate molecular dynamics simulations\nadequate sampling\ncalculations\ndirect structural probe of water\ndistorted hydrogen bonds\nexcitation energies\nexperimental data\nhydrogen bonding network\ninner-shell spectroscopic techniques\nline intensities\nliquid\nliquid water\nmolecular configurations\nmolecular structure\nobserved spectral features\noxygen K-edge\nspectral properties\nunderlying structure\nwater\nWater\nXAS\nXES\nX-ray absorption spectroscopy\nX-ray emission spectroscopy\n"}, {"id": "S0009261412006513", "text": "Water is the most important liquid, and the nature of its structure remains a topic of keen debate and an active area of research [1\u20139]. Much of this debate centers around whether water has a mainly tetrahedral structure with a continuum of distorted hydrogen bonds, or if it contains a mixture of two distinct components. One major development in recent years is the application of inner-shell spectroscopic techniques, such as X-ray absorption spectroscopy (XAS) and X-ray emission spectroscopy (XES) at the oxygen K-edge to investigate the structure of water [2,10\u201312]. These methods can provide a direct structural probe of water, providing insight into the nature of its hydrogen bonding network. Theoretical studies play a critical role in these studies, since the analysis of the experimental data requires calculations to provide a link between the observed spectral features and the underlying structure. However, the simulation of the XAS or XES for liquid water presents a difficult challenge because it requires accurate molecular dynamics simulations to provide a correct description of the molecular structure coupled with accurate calculations of the spectral properties, i.e. excitation energies and line intensities. Furthermore, adequate sampling over molecular configurations also needs to be accounted for.\n", "keywords": "accurate molecular dynamics simulations\nadequate sampling\ncalculations\ndirect structural probe of water\ndistorted hydrogen bonds\nexcitation energies\nexperimental data\nhydrogen bonding network\ninner-shell spectroscopic techniques\nline intensities\nliquid\nliquid water\nmolecular configurations\nmolecular structure\nobserved spectral features\noxygen K-edge\nspectral properties\nunderlying structure\nwater\nWater\nXAS\nXES\nX-ray absorption spectroscopy\nX-ray emission spectroscopy\n"}, {"id": "S0009261412006513", "text": "Water is the most important liquid, and the nature of its structure remains a topic of keen debate and an active area of research [1\u20139]. Much of this debate centers around whether water has a mainly tetrahedral structure with a continuum of distorted hydrogen bonds, or if it contains a mixture of two distinct components. One major development in recent years is the application of inner-shell spectroscopic techniques, such as X-ray absorption spectroscopy (XAS) and X-ray emission spectroscopy (XES) at the oxygen K-edge to investigate the structure of water [2,10\u201312]. These methods can provide a direct structural probe of water, providing insight into the nature of its hydrogen bonding network. Theoretical studies play a critical role in these studies, since the analysis of the experimental data requires calculations to provide a link between the observed spectral features and the underlying structure. However, the simulation of the XAS or XES for liquid water presents a difficult challenge because it requires accurate molecular dynamics simulations to provide a correct description of the molecular structure coupled with accurate calculations of the spectral properties, i.e. excitation energies and line intensities. Furthermore, adequate sampling over molecular configurations also needs to be accounted for.\n", "keywords": "accurate molecular dynamics simulations\nadequate sampling\ncalculations\ndirect structural probe of water\ndistorted hydrogen bonds\nexcitation energies\nexperimental data\nhydrogen bonding network\ninner-shell spectroscopic techniques\nline intensities\nliquid\nliquid water\nmolecular configurations\nmolecular structure\nobserved spectral features\noxygen K-edge\nspectral properties\nunderlying structure\nwater\nWater\nXAS\nXES\nX-ray absorption spectroscopy\nX-ray emission spectroscopy\n"}, {"id": "S0009261412006513", "text": "Water is the most important liquid, and the nature of its structure remains a topic of keen debate and an active area of research [1\u20139]. Much of this debate centers around whether water has a mainly tetrahedral structure with a continuum of distorted hydrogen bonds, or if it contains a mixture of two distinct components. One major development in recent years is the application of inner-shell spectroscopic techniques, such as X-ray absorption spectroscopy (XAS) and X-ray emission spectroscopy (XES) at the oxygen K-edge to investigate the structure of water [2,10\u201312]. These methods can provide a direct structural probe of water, providing insight into the nature of its hydrogen bonding network. Theoretical studies play a critical role in these studies, since the analysis of the experimental data requires calculations to provide a link between the observed spectral features and the underlying structure. However, the simulation of the XAS or XES for liquid water presents a difficult challenge because it requires accurate molecular dynamics simulations to provide a correct description of the molecular structure coupled with accurate calculations of the spectral properties, i.e. excitation energies and line intensities. Furthermore, adequate sampling over molecular configurations also needs to be accounted for.\n", "keywords": "accurate molecular dynamics simulations\nadequate sampling\ncalculations\ndirect structural probe of water\ndistorted hydrogen bonds\nexcitation energies\nexperimental data\nhydrogen bonding network\ninner-shell spectroscopic techniques\nline intensities\nliquid\nliquid water\nmolecular configurations\nmolecular structure\nobserved spectral features\noxygen K-edge\nspectral properties\nunderlying structure\nwater\nWater\nXAS\nXES\nX-ray absorption spectroscopy\nX-ray emission spectroscopy\n"}, {"id": "S0038092X14004770", "text": "Shading can be the most detrimental factor on performance for a domestic system. The impact of shading on performance varies depending on the electrical series and parallel arrangement of cells within a module and modules within an installed array. Whilst many approaches to shading analysis have been proposed, computational efficiency is not reported despite being of high importance when incorporating shading algorithms into an overall energy yield model. The lack of consideration of the non-linear impacts of shading on smaller systems for example means that the shading loss is significantly underestimated, especially from supposedly small obstacles such as antennas or chimneys. As an example, the system shown in Fig. 1 illustrates the case where the installer may have attested a shade loss factor close to unity under UK microgeneration guidelines (Microgeneration Certification Scheme, 2013), i.e. negligible, but the performance of the system is severely compromised due to the non-linear cell mismatch effects. An effective shading sub-model therefore needs to give feedback to inform decisions of array layout in the proximity of obstructions but must not rely on high power computing.\n", "keywords": "antennas\ncells\nchimneys\ndomestic system\neffective shading sub-model\nenergy yield model\ngive feedback to inform decisions of array layout\nhigh power computing\ninstalled array\nmodule\nmodules\nnon-linear cell mismatch effects\nshade loss\nshading\nShading\nshading algorithms\nshading analysis\nsystem\nsystem shown in Fig. 1\n"}, {"id": "S1687850714000405", "text": "Xylanases have potential applications in various fields. Some of the important applications are as fallows. Xylanases are used as bleaching agent in the pulp and paper industry. Mostly they are used to hydrolyzed the xylan component from wood which facilitate in removal of lignin (Viikari, Kantelinen, Buchert, & Puls, 1994). It also helps in brightening of the pulp to avoid the chlorine free bleaching operations (Paice, Jurasek, Ho, Bourbonnais, & Archibald, 1989). In bakeries the xylanase act on the gluten fraction of the dough and help in the even redistribution of the water content of the bread (Wong & Saddler, 1992). Xylanases also have potential application in animal feed industry. They are used for the hydrolysis of non-starchy polysaccharides such as arabinoxylan in monogastric diets (Walsh, Power, & Headon, 1993). Xylanases also play a key role in the maceration of vegetable matter (Beck & Scoot, 1974), protoplastation of plant cells, clarification of juices and wine (Biely, 1985) liquefaction of coffee mucilage for making liquid coffee, recovery of oil from subterranian mines, extraction of flavors and pigments, plant oils and starch (McCleary, 1986) and to improve the efficiency of agricultural silage production (Wong & Saddler, 1992).\n", "keywords": "animal feed industry\narabinoxylan\nbleaching agent\nbrightening of the pulp\nchlorine free bleaching operations\nclarification of juices and wine\nefficiency of agricultural silage production\neven redistribution of the water content of the bread\nextraction of flavors and pigments, plant oils and starch\nhydrolysis of non-starchy polysaccharides\nhydrolyzed the xylan component\nlignin\nliquefaction of coffee mucilage\nmaceration of vegetable matte\nmaking liquid coffee\nnon-starchy polysaccharides\nprotoplastation of plant cells\npulp\npulp and paper industry\nrecovery of oil from subterranian mines\nremoval of lignin\nwood\nxylanase\nXylanases\nxylan component\n"}, {"id": "S1687850714000405", "text": "Xylanases have potential applications in various fields. Some of the important applications are as fallows. Xylanases are used as bleaching agent in the pulp and paper industry. Mostly they are used to hydrolyzed the xylan component from wood which facilitate in removal of lignin (Viikari, Kantelinen, Buchert, & Puls, 1994). It also helps in brightening of the pulp to avoid the chlorine free bleaching operations (Paice, Jurasek, Ho, Bourbonnais, & Archibald, 1989). In bakeries the xylanase act on the gluten fraction of the dough and help in the even redistribution of the water content of the bread (Wong & Saddler, 1992). Xylanases also have potential application in animal feed industry. They are used for the hydrolysis of non-starchy polysaccharides such as arabinoxylan in monogastric diets (Walsh, Power, & Headon, 1993). Xylanases also play a key role in the maceration of vegetable matter (Beck & Scoot, 1974), protoplastation of plant cells, clarification of juices and wine (Biely, 1985) liquefaction of coffee mucilage for making liquid coffee, recovery of oil from subterranian mines, extraction of flavors and pigments, plant oils and starch (McCleary, 1986) and to improve the efficiency of agricultural silage production (Wong & Saddler, 1992).\n", "keywords": "animal feed industry\narabinoxylan\nbleaching agent\nbrightening of the pulp\nchlorine free bleaching operations\nclarification of juices and wine\nefficiency of agricultural silage production\neven redistribution of the water content of the bread\nextraction of flavors and pigments, plant oils and starch\nhydrolysis of non-starchy polysaccharides\nhydrolyzed the xylan component\nlignin\nliquefaction of coffee mucilage\nmaceration of vegetable matte\nmaking liquid coffee\nnon-starchy polysaccharides\nprotoplastation of plant cells\npulp\npulp and paper industry\nrecovery of oil from subterranian mines\nremoval of lignin\nwood\nxylanase\nXylanases\nxylan component\n"}, {"id": "S0370269304006070", "text": "The purpose of this Letter is to answer the above question and to confront those six-zero textures of lepton mass matrices with the latest experimental data. First, we shall present a concise analysis of the lepton mass matrices in Table\u00a01 and reveal their isomeric features, namely, they have the same phenomenological consequences, although their structures are apparently different. Second, we shall examine the predictions of these lepton mass matrices by comparing them with the 2\u03c3 and 3\u03c3 intervals of two neutrino mass-squared differences and three lepton flavor mixing angles,22To be specific, we make use of the 2\u03c3 and 3\u03c3 intervals of two neutrino mass-squared differences and three lepton flavor mixing angles given by M. Maltoni et al. in Ref.\u00a0[5]. which are obtained from a global analysis of the latest solar, atmospheric, reactor (KamLAND and CHOOZ [10]) and accelerator (K2K) neutrino data. We find no parameter space allowed for six isomeric lepton mass matrices at the 2\u03c3 level. At the 3\u03c3 level, however, their results for neutrino masses and lepton flavor mixing angles can be compatible with current data. Third, we incorporate the seesaw mechanism and the Fukugita\u2013Tanimoto\u2013Yanagida hypothesis [9] in the charged lepton and Dirac neutrino mass matrices with six texture zeros. It turns out that their predictions, including \u03b823\u224845\u00b0, are in good agreement with the present experimental data even at the 2\u03c3 level.\n", "keywords": "charged lepton\nconfront those six-zero textures of lepton mass matrices\ncurrent data\nDirac neutrino mass matrices\nexamine the predictions of these lepton mass matrices\nexperimental data\nFukugita\u2013Tanimoto\u2013Yanagida hypothesis\nglobal analysis\nisomeric lepton mass matrices\nlepton flavor mixing angles\nlepton mass matrices\nneutrino\nneutrino mass-squared differences\nreveal their isomeric features\nseesaw mechanism\nsolar, atmospheric, reactor (KamLAND and CHOOZ [10]) and accelerator (K2K) neutrino data\n"}, {"id": "S0168365913009589", "text": "Ultrasound (US) can initiate the release of drugs from liposomes via an event called inertial cavitation, whereby the rarefactional phase of an ultrasound wave causes the expansion of a gas bubble followed by a violent collapse due to the inertia of the surrounding media. This collapse creates shock waves which can disrupt the stability of co-localised liposomal drug carriers. To date, studies have concentrated on the use of low frequency or high intensity US to generate gas bubbles in situ, and most recently such parameters have been used to achieve a variable level of triggered drug release following an intratumoral injection of liposomes [14]. However, concerns persist over the damage to non-target tissue that such US exposure parameters may cause and whether ultimately they will be widely clinically applicable. An alternative strategy is to utilise high-frequency US pulses at pressures in the diagnostic range in the presence of pre-existing gas bubbles. This provides an inertial cavitation stimulus for drug release using safe, clinically achievable US exposure conditions and approved US contrast agents [15]. Indeed, in the context of improving the delivery of therapeutics such as oncolytic viruses, this approach has already shown great promise [16]. A further advantage of this approach is that US-induced cavitation events produce distinct acoustic emissions that can be recorded and characterised providing non-invasive feedback, a feature which has proven useful in ablative US applications [17\u201319].\n", "keywords": "gas bubble\ngas bubbles\ninertial cavitation\nintratumoral injection\nliposomes\noncolytic viruses\nUltrasound\nUS\nUS contrast agents\nUS-induced cavitation\n"}, {"id": "S0168365913009589", "text": "Ultrasound (US) can initiate the release of drugs from liposomes via an event called inertial cavitation, whereby the rarefactional phase of an ultrasound wave causes the expansion of a gas bubble followed by a violent collapse due to the inertia of the surrounding media. This collapse creates shock waves which can disrupt the stability of co-localised liposomal drug carriers. To date, studies have concentrated on the use of low frequency or high intensity US to generate gas bubbles in situ, and most recently such parameters have been used to achieve a variable level of triggered drug release following an intratumoral injection of liposomes [14]. However, concerns persist over the damage to non-target tissue that such US exposure parameters may cause and whether ultimately they will be widely clinically applicable. An alternative strategy is to utilise high-frequency US pulses at pressures in the diagnostic range in the presence of pre-existing gas bubbles. This provides an inertial cavitation stimulus for drug release using safe, clinically achievable US exposure conditions and approved US contrast agents [15]. Indeed, in the context of improving the delivery of therapeutics such as oncolytic viruses, this approach has already shown great promise [16]. A further advantage of this approach is that US-induced cavitation events produce distinct acoustic emissions that can be recorded and characterised providing non-invasive feedback, a feature which has proven useful in ablative US applications [17\u201319].\n", "keywords": "gas bubble\ngas bubbles\ninertial cavitation\nintratumoral injection\nliposomes\noncolytic viruses\nUltrasound\nUS\nUS contrast agents\nUS-induced cavitation\n"}, {"id": "S0021999114002587", "text": "Designers of microfluidic devices are in need of computational tools that can be used to analyse problems that involve rarefied gas flows in complex micro geometries. Numerical simulation of the gas flow through such geometries is, however, extremely challenging. Conventional continuum fluid dynamics (CFD) becomes invalid or inaccurate as the characteristic scale of the geometry (e.g. the channel height, h) approaches the molecular mean free path, \u03bb [1,2]. When \u03bb/h\u22730.1, the error in solutions obtained from CFD may be significant, and we must consider the fluid for what it is: a collection of interacting particles. However, the computational expense of simulating the flow of a rarefied gas in high-aspect-ratio micro geometries (i.e. ones that are long, relative to their cross section) using a particle method, such as the direct simulation Monte Carlo (DSMC) method [2], can be prohibitively high [3,4]. The computational intensity of the particle method is greater still when simulating low-speed microfluidic devices where there are only small deviations from equilibrium, characterised by extremely low Mach numbers and weak temperature gradients.\n", "keywords": "analyse problems that involve rarefied gas flows in complex micro geometries\nCFD\ncollection of interacting particles\nConventional continuum fluid dynamics\ndirect simulation Monte Carlo\nDSMC\nextremely low Mach numbers and weak temperature gradients\nflow of a rarefied gas\ngas flow\nhigh-aspect-ratio micro geometries\nlong, relative to their cross section\nmicrofluidic devices\nNumerical simulation\nparticle method\nrarefied gas flows\nsmall deviations from equilibrium\n"}, {"id": "S0021999114002587", "text": "Designers of microfluidic devices are in need of computational tools that can be used to analyse problems that involve rarefied gas flows in complex micro geometries. Numerical simulation of the gas flow through such geometries is, however, extremely challenging. Conventional continuum fluid dynamics (CFD) becomes invalid or inaccurate as the characteristic scale of the geometry (e.g. the channel height, h) approaches the molecular mean free path, \u03bb [1,2]. When \u03bb/h\u22730.1, the error in solutions obtained from CFD may be significant, and we must consider the fluid for what it is: a collection of interacting particles. However, the computational expense of simulating the flow of a rarefied gas in high-aspect-ratio micro geometries (i.e. ones that are long, relative to their cross section) using a particle method, such as the direct simulation Monte Carlo (DSMC) method [2], can be prohibitively high [3,4]. The computational intensity of the particle method is greater still when simulating low-speed microfluidic devices where there are only small deviations from equilibrium, characterised by extremely low Mach numbers and weak temperature gradients.\n", "keywords": "analyse problems that involve rarefied gas flows in complex micro geometries\nCFD\ncollection of interacting particles\nConventional continuum fluid dynamics\ndirect simulation Monte Carlo\nDSMC\nextremely low Mach numbers and weak temperature gradients\nflow of a rarefied gas\ngas flow\nhigh-aspect-ratio micro geometries\nlong, relative to their cross section\nmicrofluidic devices\nNumerical simulation\nparticle method\nrarefied gas flows\nsmall deviations from equilibrium\n"}, {"id": "S0021999114002587", "text": "Designers of microfluidic devices are in need of computational tools that can be used to analyse problems that involve rarefied gas flows in complex micro geometries. Numerical simulation of the gas flow through such geometries is, however, extremely challenging. Conventional continuum fluid dynamics (CFD) becomes invalid or inaccurate as the characteristic scale of the geometry (e.g. the channel height, h) approaches the molecular mean free path, \u03bb [1,2]. When \u03bb/h\u22730.1, the error in solutions obtained from CFD may be significant, and we must consider the fluid for what it is: a collection of interacting particles. However, the computational expense of simulating the flow of a rarefied gas in high-aspect-ratio micro geometries (i.e. ones that are long, relative to their cross section) using a particle method, such as the direct simulation Monte Carlo (DSMC) method [2], can be prohibitively high [3,4]. The computational intensity of the particle method is greater still when simulating low-speed microfluidic devices where there are only small deviations from equilibrium, characterised by extremely low Mach numbers and weak temperature gradients.\n", "keywords": "analyse problems that involve rarefied gas flows in complex micro geometries\nCFD\ncollection of interacting particles\nConventional continuum fluid dynamics\ndirect simulation Monte Carlo\nDSMC\nextremely low Mach numbers and weak temperature gradients\nflow of a rarefied gas\ngas flow\nhigh-aspect-ratio micro geometries\nlong, relative to their cross section\nmicrofluidic devices\nNumerical simulation\nparticle method\nrarefied gas flows\nsmall deviations from equilibrium\n"}, {"id": "S2212671612001497", "text": "Our country is rich of line galloping, there are many important galloping data failed to collect systematically and completely because there is no unified management platform. After the galloping occurrence in 2009\u20132010's winter the department of productive of the State Grid Corporation organized a lot of human to carry out the research of galloping information, this work is time\u2013consuming and inefficient. The State Grid Corporation has used the production management system (PMS) which is a powerful and easy to use. With the help of the system we can create a galloping database which can save resources and storage the galloping data. To build and put it into application of database can provide technical support for line galloping prevention and galloping research work.", "keywords": "application of database\ncarry out the research of galloping information,\ncollect systematically and completely\ncreate a galloping database\ngalloping\ngalloping data\ngalloping occurrence\ngalloping research work\nline galloping\nline galloping prevention\nPMS\nproduction management system\nsave resources and storage the galloping data\ntechnical support\nunified management platform\n"}, {"id": "S0304399111001811", "text": "We have developed the theory of electrons carrying quantized orbital angular momentum. To make connection to realistic situations, we considered a plane wave moving along the optic axis of a lens system, intercepted by a round, centered aperture.88In the experiment, this aperture carries the holographic mask. It turns out that the movement along the optic axis can be separated off; the reduced Schr\u00f6dinger equation operating in the plane of the aperture can be mapped onto Bessel's differential equation. The ensuing eigenfunctions fall into families with discrete orbital angular momentum \u210fm along the optic axis where m is a magnetic quantum number. Those vortices can be produced by matching a plane wave after passage through a holographic mask with a fork dislocation to the eigenfunctions of the cylindrical problem. Vortices can be focussed by magnetic lenses into volcano-like charge distributions with very narrow angular divergence, resembling loop currents in the diffraction plane. Inclusion of spherical aberration changes the ringlike shape but does not destroy the central zero intensity of vortices with m\u22600. Partial coherence of the incident wave leads to a rise of the central intensity minimum. It is shown that a very small source angle (i.e. a very high coherence) is necessary so as to keep the volcano structure intact. Their small angular width in the far field may allow the creation of nm-sized or smaller electron vortices but the demand for extremely high coherence of the source poses a serious difficulty.\n", "keywords": "angular divergence\naperture\ncreation of nm-sized or smaller electron vortices\nelectrons carrying quantized orbital angular momentum.\nholographic mask\nincident wave\nInclusion of spherical aberration\nlens system\nmagnetic lenses\nPartial coherence\nplane\nplane wave\nreduced Schr\u00f6dinger equation\nround, centered aperture\nvortices\n"}, {"id": "S0370269304009141", "text": "Longitudinal beam and target single-spin asymmetries have been at the center of the attention lately, since they have been measured by the HERMES and CLAS experimental Collaborations\u00a0[1\u20134] and more measurements are planned. They were originally believed to be signals of the so-called T-odd fragmentation functions\u00a0[5], in particular, of the Collins function\u00a0[6\u201312]. However, both types of asymmetry can receive contributions also from T-odd distribution functions\u00a0[13\u201316], a fact that has often been neglected in analyses. An exhaustive treatment of the contributions of T-odd distribution functions has not been carried out completely so far, especially up to subleading order in an expansion in\u00a01/Q, Q2 being the virtuality of the incident photon and the only hard scale of the process, and including quark mass corrections. It is the purpose of the present work to describe the longitudinal beam and target spin asymmetries in a complete way in terms of leading and subleading twist distribution and fragmentation functions. We consider both single-particle inclusive DIS, e+p\u2192e\u2032+h+X, and single-jet inclusive DIS, e+p\u2192e\u2032+jet+X. We assume factorization holds for these processes, even though at present there is no factorization proof for observables containing subleading-twist transverse-momentum dependent functions (only recently proofs for the leading-twist case have been presented in Refs.\u00a0[17,18]).\n", "keywords": "Collins function\ndescribe the longitudinal beam and target spin asymmetries\ne+p\u2192e\u2032+h+X\ne+p\u2192e\u2032+jet+X\nexhaustive treatment of the contributions of T-odd distribution functions\nfactorization\nfactorization proof for observables containing subleading-twist transverse-momentum dependent functions\nfragmentation functions\nHERMES and CLAS experimental Collaborations\u00a0\nincident photon\nLongitudinal beam and target single-spin asymmetries\nproofs for the leading-twist case\nquark\nquark mass corrections\nreceive contributions also from T-odd distribution functions\nsingle-jet inclusive DIS\nsingle-particle inclusive DIS\nsubleading order in an expansion in\u00a01/Q\nsubleading-twist transverse-momentum dependent functions\nT-odd distribution functions\nT-odd fragmentation functions\ntwist distribution\n"}, {"id": "S0370269304009141", "text": "Longitudinal beam and target single-spin asymmetries have been at the center of the attention lately, since they have been measured by the HERMES and CLAS experimental Collaborations\u00a0[1\u20134] and more measurements are planned. They were originally believed to be signals of the so-called T-odd fragmentation functions\u00a0[5], in particular, of the Collins function\u00a0[6\u201312]. However, both types of asymmetry can receive contributions also from T-odd distribution functions\u00a0[13\u201316], a fact that has often been neglected in analyses. An exhaustive treatment of the contributions of T-odd distribution functions has not been carried out completely so far, especially up to subleading order in an expansion in\u00a01/Q, Q2 being the virtuality of the incident photon and the only hard scale of the process, and including quark mass corrections. It is the purpose of the present work to describe the longitudinal beam and target spin asymmetries in a complete way in terms of leading and subleading twist distribution and fragmentation functions. We consider both single-particle inclusive DIS, e+p\u2192e\u2032+h+X, and single-jet inclusive DIS, e+p\u2192e\u2032+jet+X. We assume factorization holds for these processes, even though at present there is no factorization proof for observables containing subleading-twist transverse-momentum dependent functions (only recently proofs for the leading-twist case have been presented in Refs.\u00a0[17,18]).\n", "keywords": "Collins function\ndescribe the longitudinal beam and target spin asymmetries\ne+p\u2192e\u2032+h+X\ne+p\u2192e\u2032+jet+X\nexhaustive treatment of the contributions of T-odd distribution functions\nfactorization\nfactorization proof for observables containing subleading-twist transverse-momentum dependent functions\nfragmentation functions\nHERMES and CLAS experimental Collaborations\u00a0\nincident photon\nLongitudinal beam and target single-spin asymmetries\nproofs for the leading-twist case\nquark\nquark mass corrections\nreceive contributions also from T-odd distribution functions\nsingle-jet inclusive DIS\nsingle-particle inclusive DIS\nsubleading order in an expansion in\u00a01/Q\nsubleading-twist transverse-momentum dependent functions\nT-odd distribution functions\nT-odd fragmentation functions\ntwist distribution\n"}, {"id": "S0370269304009141", "text": "Longitudinal beam and target single-spin asymmetries have been at the center of the attention lately, since they have been measured by the HERMES and CLAS experimental Collaborations\u00a0[1\u20134] and more measurements are planned. They were originally believed to be signals of the so-called T-odd fragmentation functions\u00a0[5], in particular, of the Collins function\u00a0[6\u201312]. However, both types of asymmetry can receive contributions also from T-odd distribution functions\u00a0[13\u201316], a fact that has often been neglected in analyses. An exhaustive treatment of the contributions of T-odd distribution functions has not been carried out completely so far, especially up to subleading order in an expansion in\u00a01/Q, Q2 being the virtuality of the incident photon and the only hard scale of the process, and including quark mass corrections. It is the purpose of the present work to describe the longitudinal beam and target spin asymmetries in a complete way in terms of leading and subleading twist distribution and fragmentation functions. We consider both single-particle inclusive DIS, e+p\u2192e\u2032+h+X, and single-jet inclusive DIS, e+p\u2192e\u2032+jet+X. We assume factorization holds for these processes, even though at present there is no factorization proof for observables containing subleading-twist transverse-momentum dependent functions (only recently proofs for the leading-twist case have been presented in Refs.\u00a0[17,18]).\n", "keywords": "Collins function\ndescribe the longitudinal beam and target spin asymmetries\ne+p\u2192e\u2032+h+X\ne+p\u2192e\u2032+jet+X\nexhaustive treatment of the contributions of T-odd distribution functions\nfactorization\nfactorization proof for observables containing subleading-twist transverse-momentum dependent functions\nfragmentation functions\nHERMES and CLAS experimental Collaborations\u00a0\nincident photon\nLongitudinal beam and target single-spin asymmetries\nproofs for the leading-twist case\nquark\nquark mass corrections\nreceive contributions also from T-odd distribution functions\nsingle-jet inclusive DIS\nsingle-particle inclusive DIS\nsubleading order in an expansion in\u00a01/Q\nsubleading-twist transverse-momentum dependent functions\nT-odd distribution functions\nT-odd fragmentation functions\ntwist distribution\n"}, {"id": "S0370269304009141", "text": "Longitudinal beam and target single-spin asymmetries have been at the center of the attention lately, since they have been measured by the HERMES and CLAS experimental Collaborations\u00a0[1\u20134] and more measurements are planned. They were originally believed to be signals of the so-called T-odd fragmentation functions\u00a0[5], in particular, of the Collins function\u00a0[6\u201312]. However, both types of asymmetry can receive contributions also from T-odd distribution functions\u00a0[13\u201316], a fact that has often been neglected in analyses. An exhaustive treatment of the contributions of T-odd distribution functions has not been carried out completely so far, especially up to subleading order in an expansion in\u00a01/Q, Q2 being the virtuality of the incident photon and the only hard scale of the process, and including quark mass corrections. It is the purpose of the present work to describe the longitudinal beam and target spin asymmetries in a complete way in terms of leading and subleading twist distribution and fragmentation functions. We consider both single-particle inclusive DIS, e+p\u2192e\u2032+h+X, and single-jet inclusive DIS, e+p\u2192e\u2032+jet+X. We assume factorization holds for these processes, even though at present there is no factorization proof for observables containing subleading-twist transverse-momentum dependent functions (only recently proofs for the leading-twist case have been presented in Refs.\u00a0[17,18]).\n", "keywords": "Collins function\ndescribe the longitudinal beam and target spin asymmetries\ne+p\u2192e\u2032+h+X\ne+p\u2192e\u2032+jet+X\nexhaustive treatment of the contributions of T-odd distribution functions\nfactorization\nfactorization proof for observables containing subleading-twist transverse-momentum dependent functions\nfragmentation functions\nHERMES and CLAS experimental Collaborations\u00a0\nincident photon\nLongitudinal beam and target single-spin asymmetries\nproofs for the leading-twist case\nquark\nquark mass corrections\nreceive contributions also from T-odd distribution functions\nsingle-jet inclusive DIS\nsingle-particle inclusive DIS\nsubleading order in an expansion in\u00a01/Q\nsubleading-twist transverse-momentum dependent functions\nT-odd distribution functions\nT-odd fragmentation functions\ntwist distribution\n"}, {"id": "S0168874X1630049X", "text": "The crack band approach for producing mesh independent load\u2013displacement curves for fracture in plain concrete is based on the idea that the crack opening is transformed into inelastic strain by distributing it over an element length dependent zone [5]. This approach will only produce mesh independent load\u2013displacement curves, if the inelastic strain profiles in the finite element analysis are mesh size dependent. This requirement is an important difference to the nonlocal model which is designed to produce both mesh size independent load\u2013displacement curves and strain profiles. In CDPM2, the crack band approach is applied only to the tensile part of the damage algorithm by replacing the stress\u2013inelastic strain law shown in Fig. 2(b) by a stress\u2013inelastic displacement law of the form(13)\u03c3=ftexp(\u2212\u03f5inhwft)if(\u03f5in>0)Here, wft is a crack opening threshold used to control the slope of the softening curve and h is the width of the crack-band, which in the present study is equal to the maximum dimension of the element along the principal direction of the strain tensor corresponding to the maximum tensile principal strain at the onset of damage. For the compressive part, a stress\u2013inelastic strain law was used to determine the compressive damage parameter, since it was reported in [14] for columns subjected to eccentric compression that inelastic strain profiles in compression do not exhibit a mesh dependence which would satisfy the assumptions of the crack-band approach. This approach of applying the crack-band approach only to the tensile part has already been successfully used in\u00a0Grassl et al. [16].\n", "keywords": "(13)\u03c3=ftexp(\u2212\u03f5inhwft)if(\u03f5in>0)\ncrack band approach\ncrack-band approach\ninelastic strain profiles\nstress\u2013inelastic displacement law\nstress\u2013inelastic strain law\ntensile part\n"}, {"id": "S0168874X1630049X", "text": "The crack band approach for producing mesh independent load\u2013displacement curves for fracture in plain concrete is based on the idea that the crack opening is transformed into inelastic strain by distributing it over an element length dependent zone [5]. This approach will only produce mesh independent load\u2013displacement curves, if the inelastic strain profiles in the finite element analysis are mesh size dependent. This requirement is an important difference to the nonlocal model which is designed to produce both mesh size independent load\u2013displacement curves and strain profiles. In CDPM2, the crack band approach is applied only to the tensile part of the damage algorithm by replacing the stress\u2013inelastic strain law shown in Fig. 2(b) by a stress\u2013inelastic displacement law of the form(13)\u03c3=ftexp(\u2212\u03f5inhwft)if(\u03f5in>0)Here, wft is a crack opening threshold used to control the slope of the softening curve and h is the width of the crack-band, which in the present study is equal to the maximum dimension of the element along the principal direction of the strain tensor corresponding to the maximum tensile principal strain at the onset of damage. For the compressive part, a stress\u2013inelastic strain law was used to determine the compressive damage parameter, since it was reported in [14] for columns subjected to eccentric compression that inelastic strain profiles in compression do not exhibit a mesh dependence which would satisfy the assumptions of the crack-band approach. This approach of applying the crack-band approach only to the tensile part has already been successfully used in\u00a0Grassl et al. [16].\n", "keywords": "(13)\u03c3=ftexp(\u2212\u03f5inhwft)if(\u03f5in>0)\ncrack band approach\ncrack-band approach\ninelastic strain profiles\nstress\u2013inelastic displacement law\nstress\u2013inelastic strain law\ntensile part\n"}, {"id": "S0168874X1630049X", "text": "The crack band approach for producing mesh independent load\u2013displacement curves for fracture in plain concrete is based on the idea that the crack opening is transformed into inelastic strain by distributing it over an element length dependent zone [5]. This approach will only produce mesh independent load\u2013displacement curves, if the inelastic strain profiles in the finite element analysis are mesh size dependent. This requirement is an important difference to the nonlocal model which is designed to produce both mesh size independent load\u2013displacement curves and strain profiles. In CDPM2, the crack band approach is applied only to the tensile part of the damage algorithm by replacing the stress\u2013inelastic strain law shown in Fig. 2(b) by a stress\u2013inelastic displacement law of the form(13)\u03c3=ftexp(\u2212\u03f5inhwft)if(\u03f5in>0)Here, wft is a crack opening threshold used to control the slope of the softening curve and h is the width of the crack-band, which in the present study is equal to the maximum dimension of the element along the principal direction of the strain tensor corresponding to the maximum tensile principal strain at the onset of damage. For the compressive part, a stress\u2013inelastic strain law was used to determine the compressive damage parameter, since it was reported in [14] for columns subjected to eccentric compression that inelastic strain profiles in compression do not exhibit a mesh dependence which would satisfy the assumptions of the crack-band approach. This approach of applying the crack-band approach only to the tensile part has already been successfully used in\u00a0Grassl et al. [16].\n", "keywords": "(13)\u03c3=ftexp(\u2212\u03f5inhwft)if(\u03f5in>0)\ncrack band approach\ncrack-band approach\ninelastic strain profiles\nstress\u2013inelastic displacement law\nstress\u2013inelastic strain law\ntensile part\n"}, {"id": "S0168874X1630049X", "text": "The crack band approach for producing mesh independent load\u2013displacement curves for fracture in plain concrete is based on the idea that the crack opening is transformed into inelastic strain by distributing it over an element length dependent zone [5]. This approach will only produce mesh independent load\u2013displacement curves, if the inelastic strain profiles in the finite element analysis are mesh size dependent. This requirement is an important difference to the nonlocal model which is designed to produce both mesh size independent load\u2013displacement curves and strain profiles. In CDPM2, the crack band approach is applied only to the tensile part of the damage algorithm by replacing the stress\u2013inelastic strain law shown in Fig. 2(b) by a stress\u2013inelastic displacement law of the form(13)\u03c3=ftexp(\u2212\u03f5inhwft)if(\u03f5in>0)Here, wft is a crack opening threshold used to control the slope of the softening curve and h is the width of the crack-band, which in the present study is equal to the maximum dimension of the element along the principal direction of the strain tensor corresponding to the maximum tensile principal strain at the onset of damage. For the compressive part, a stress\u2013inelastic strain law was used to determine the compressive damage parameter, since it was reported in [14] for columns subjected to eccentric compression that inelastic strain profiles in compression do not exhibit a mesh dependence which would satisfy the assumptions of the crack-band approach. This approach of applying the crack-band approach only to the tensile part has already been successfully used in\u00a0Grassl et al. [16].\n", "keywords": "(13)\u03c3=ftexp(\u2212\u03f5inhwft)if(\u03f5in>0)\ncrack band approach\ncrack-band approach\ninelastic strain profiles\nstress\u2013inelastic displacement law\nstress\u2013inelastic strain law\ntensile part\n"}, {"id": "S2212667814000987", "text": "In this paper we consider problems of creating and introducing intelligent management systems as one of the most important mechanism of increasing energy efficiency in industry. Operating principles of intelligent electric power distribution systems developed in MSTU \u00abSTANKIN\u00bb for AC and DC grids on industrial plants are described. Essential devices composing the systems are considered, their technical characteristics are described. Experimental results are presented.In this paper we consider problems of creating and introducing intelligent management systems as one of the most important mechanism of increasing energy efficiency in industry. Operating principles of intelligent electric power distribution systems developed in MSTU \u00abSTANKIN\u00bb for AC and DC grids on industrial plants are described. Essential devices composing the systems are considered, their technical characteristics are described. Experimental results are presented.\n", "keywords": "AC and DC grids\ncreating and introducing intelligent management systems\ndevices composing the systems\ndevices composing the systems are considered\nincreasing energy efficiency\nintelligent electric power distribution systems\nintelligent management systems\nMSTU \u00abSTANKIN\u00bb\nntelligent management systems\nproblems of creating and introducing intelligent management systems\ntechnical characteristics are described\n"}, {"id": "S2212667814000987", "text": "In this paper we consider problems of creating and introducing intelligent management systems as one of the most important mechanism of increasing energy efficiency in industry. Operating principles of intelligent electric power distribution systems developed in MSTU \u00abSTANKIN\u00bb for AC and DC grids on industrial plants are described. Essential devices composing the systems are considered, their technical characteristics are described. Experimental results are presented.In this paper we consider problems of creating and introducing intelligent management systems as one of the most important mechanism of increasing energy efficiency in industry. Operating principles of intelligent electric power distribution systems developed in MSTU \u00abSTANKIN\u00bb for AC and DC grids on industrial plants are described. Essential devices composing the systems are considered, their technical characteristics are described. Experimental results are presented.\n", "keywords": "AC and DC grids\ncreating and introducing intelligent management systems\ndevices composing the systems\ndevices composing the systems are considered\nincreasing energy efficiency\nintelligent electric power distribution systems\nintelligent management systems\nMSTU \u00abSTANKIN\u00bb\nntelligent management systems\nproblems of creating and introducing intelligent management systems\ntechnical characteristics are described\n"}, {"id": "S2212667814000987", "text": "In this paper we consider problems of creating and introducing intelligent management systems as one of the most important mechanism of increasing energy efficiency in industry. Operating principles of intelligent electric power distribution systems developed in MSTU \u00abSTANKIN\u00bb for AC and DC grids on industrial plants are described. Essential devices composing the systems are considered, their technical characteristics are described. Experimental results are presented.In this paper we consider problems of creating and introducing intelligent management systems as one of the most important mechanism of increasing energy efficiency in industry. Operating principles of intelligent electric power distribution systems developed in MSTU \u00abSTANKIN\u00bb for AC and DC grids on industrial plants are described. Essential devices composing the systems are considered, their technical characteristics are described. Experimental results are presented.\n", "keywords": "AC and DC grids\ncreating and introducing intelligent management systems\ndevices composing the systems\ndevices composing the systems are considered\nincreasing energy efficiency\nintelligent electric power distribution systems\nintelligent management systems\nMSTU \u00abSTANKIN\u00bb\nntelligent management systems\nproblems of creating and introducing intelligent management systems\ntechnical characteristics are described\n"}, {"id": "S2212667814000987", "text": "In this paper we consider problems of creating and introducing intelligent management systems as one of the most important mechanism of increasing energy efficiency in industry. Operating principles of intelligent electric power distribution systems developed in MSTU \u00abSTANKIN\u00bb for AC and DC grids on industrial plants are described. Essential devices composing the systems are considered, their technical characteristics are described. Experimental results are presented.In this paper we consider problems of creating and introducing intelligent management systems as one of the most important mechanism of increasing energy efficiency in industry. Operating principles of intelligent electric power distribution systems developed in MSTU \u00abSTANKIN\u00bb for AC and DC grids on industrial plants are described. Essential devices composing the systems are considered, their technical characteristics are described. Experimental results are presented.\n", "keywords": "AC and DC grids\ncreating and introducing intelligent management systems\ndevices composing the systems\ndevices composing the systems are considered\nincreasing energy efficiency\nintelligent electric power distribution systems\nintelligent management systems\nMSTU \u00abSTANKIN\u00bb\nntelligent management systems\nproblems of creating and introducing intelligent management systems\ntechnical characteristics are described\n"}, {"id": "S2212667814001440", "text": "In this paper, we present a tele-operated mobile robot system for old age surveillance. The robot operates in autonomous mode in which the robots navigates in the environment and search for unusual situation of elderly people. If a patient is lying on the floor, the robot informs the user. The user switches the control mode from autonomous to haptic based user control. In the autonomous mode, the robot utilizes the visual sensor and landmarks to monitor the entire environment. The robot is equipped microphone, speaker and monitor making it possible to communicate with the user in remote place. In addition, the robot utilizes the vital sensors to check the patient's condition. The preliminary surveillance experiments show a good performance.", "keywords": "autonomous mode\ncommunicate\nelderly people\ninforms the user\nlying on the floor\nmicrophone\nmonitor\nnavigates in the environment\nold age surveillance\npatient\npatient's condition\nremote place\nrobot\nsearch for unusual situation\nspeaker\nswitches the control mode from autonomous to haptic based\ntele-operated mobile robot system\nvisual sensor and landmarks to monitor\nvital sensors\n"}, {"id": "S2212667814001440", "text": "In this paper, we present a tele-operated mobile robot system for old age surveillance. The robot operates in autonomous mode in which the robots navigates in the environment and search for unusual situation of elderly people. If a patient is lying on the floor, the robot informs the user. The user switches the control mode from autonomous to haptic based user control. In the autonomous mode, the robot utilizes the visual sensor and landmarks to monitor the entire environment. The robot is equipped microphone, speaker and monitor making it possible to communicate with the user in remote place. In addition, the robot utilizes the vital sensors to check the patient's condition. The preliminary surveillance experiments show a good performance.", "keywords": "autonomous mode\ncommunicate\nelderly people\ninforms the user\nlying on the floor\nmicrophone\nmonitor\nnavigates in the environment\nold age surveillance\npatient\npatient's condition\nremote place\nrobot\nsearch for unusual situation\nspeaker\nswitches the control mode from autonomous to haptic based\ntele-operated mobile robot system\nvisual sensor and landmarks to monitor\nvital sensors\n"}, {"id": "S2212667814001440", "text": "In this paper, we present a tele-operated mobile robot system for old age surveillance. The robot operates in autonomous mode in which the robots navigates in the environment and search for unusual situation of elderly people. If a patient is lying on the floor, the robot informs the user. The user switches the control mode from autonomous to haptic based user control. In the autonomous mode, the robot utilizes the visual sensor and landmarks to monitor the entire environment. The robot is equipped microphone, speaker and monitor making it possible to communicate with the user in remote place. In addition, the robot utilizes the vital sensors to check the patient's condition. The preliminary surveillance experiments show a good performance.", "keywords": "autonomous mode\ncommunicate\nelderly people\ninforms the user\nlying on the floor\nmicrophone\nmonitor\nnavigates in the environment\nold age surveillance\npatient\npatient's condition\nremote place\nrobot\nsearch for unusual situation\nspeaker\nswitches the control mode from autonomous to haptic based\ntele-operated mobile robot system\nvisual sensor and landmarks to monitor\nvital sensors\n"}, {"id": "S2212667814000951", "text": "Design semantics is an integration of human mode of existence and view on culture and art, which means it is a unity of art and science. Design semantics is the annotation of form and the reflection of its symbolic meaning, which means it is an explanation of the deposited human cultural spirit. Chinese art stresses Expression, Force and Qi. In China, people advocate \u201cto learn from nature\u201d, \u201cto look up to observe the sun, the moon and stars, and look down to observe the surroundings\u201d, and take \u201cNature and Man in One\u201d as the highest state of spirit. Design semantics is expressed in space environment design through a symbiotic philosophical view that natural and artificial forms are complementary and interactive. This form of design leads humans back to a better state of living, i.e. Nature and Man in One.", "keywords": "annotation of form and the reflection of its symbolic meaning\nDesign semantics\nexplanation of the deposited human cultural spirit\nintegration of human mode of existence and view on culture and art\nnatural and artificial forms\nobserve the surroundings\nspace environment design\nto learn from nature\nunity of art and science\n"}, {"id": "S2212667814000951", "text": "Design semantics is an integration of human mode of existence and view on culture and art, which means it is a unity of art and science. Design semantics is the annotation of form and the reflection of its symbolic meaning, which means it is an explanation of the deposited human cultural spirit. Chinese art stresses Expression, Force and Qi. In China, people advocate \u201cto learn from nature\u201d, \u201cto look up to observe the sun, the moon and stars, and look down to observe the surroundings\u201d, and take \u201cNature and Man in One\u201d as the highest state of spirit. Design semantics is expressed in space environment design through a symbiotic philosophical view that natural and artificial forms are complementary and interactive. This form of design leads humans back to a better state of living, i.e. Nature and Man in One.", "keywords": "annotation of form and the reflection of its symbolic meaning\nDesign semantics\nexplanation of the deposited human cultural spirit\nintegration of human mode of existence and view on culture and art\nnatural and artificial forms\nobserve the surroundings\nspace environment design\nto learn from nature\nunity of art and science\n"}, {"id": "S2212667814000951", "text": "Design semantics is an integration of human mode of existence and view on culture and art, which means it is a unity of art and science. Design semantics is the annotation of form and the reflection of its symbolic meaning, which means it is an explanation of the deposited human cultural spirit. Chinese art stresses Expression, Force and Qi. In China, people advocate \u201cto learn from nature\u201d, \u201cto look up to observe the sun, the moon and stars, and look down to observe the surroundings\u201d, and take \u201cNature and Man in One\u201d as the highest state of spirit. Design semantics is expressed in space environment design through a symbiotic philosophical view that natural and artificial forms are complementary and interactive. This form of design leads humans back to a better state of living, i.e. Nature and Man in One.", "keywords": "annotation of form and the reflection of its symbolic meaning\nDesign semantics\nexplanation of the deposited human cultural spirit\nintegration of human mode of existence and view on culture and art\nnatural and artificial forms\nobserve the surroundings\nspace environment design\nto learn from nature\nunity of art and science\n"}, {"id": "S2212667814000951", "text": "Design semantics is an integration of human mode of existence and view on culture and art, which means it is a unity of art and science. Design semantics is the annotation of form and the reflection of its symbolic meaning, which means it is an explanation of the deposited human cultural spirit. Chinese art stresses Expression, Force and Qi. In China, people advocate \u201cto learn from nature\u201d, \u201cto look up to observe the sun, the moon and stars, and look down to observe the surroundings\u201d, and take \u201cNature and Man in One\u201d as the highest state of spirit. Design semantics is expressed in space environment design through a symbiotic philosophical view that natural and artificial forms are complementary and interactive. This form of design leads humans back to a better state of living, i.e. Nature and Man in One.", "keywords": "annotation of form and the reflection of its symbolic meaning\nDesign semantics\nexplanation of the deposited human cultural spirit\nintegration of human mode of existence and view on culture and art\nnatural and artificial forms\nobserve the surroundings\nspace environment design\nto learn from nature\nunity of art and science\n"}, {"id": "S2212667812000822", "text": "It has been more than a century since the emergence of the lettered words. After that, with the development of economy and culture, the increase of international contacts and communication between China and foreign countries, lettered words have been appearing more frequently. Lettered words have become an indispensable part of Chinese vocabulary, such as WTO, Ka la OK and MP3. As a new phenomenon in the vocabulary system of the modern Chinese, the lettered words draws a lot of academic attention. Ecolinguistics is a new branch of linguistic, which combine the linguistic with the ecology. This paper is trying to analyze the lettered words from the perspective of Ecolinguistics. This paper will discuss the reasons of appearing the lettered words and the influence may give to modern Chinese form the ecolinguistic view.", "keywords": "analyze the lettered words from the perspective of Ecolinguistics\nbranch of linguistic\nChinese vocabulary\ncombine the linguistic with the ecology\ncommunication\ndevelopment of economy and culture\ndiscuss the reasons of appearing the lettered words\nEcolinguistics\nKa la OK\nlettered words\nLettered words\nMP3\nvocabulary system\nWTO\n"}, {"id": "S0167931713005042", "text": "We used 2\u03bcm of ultra-nanocrystalline diamond (UNCD) grown by chemical vapour deposition (CVD) on a \u223c520\u03bcm silicon carrier wafer from Advanced Diamond Technologies Ltd. Detailed information about the material and the stamp fabrication can be found in our earlier paper [16]. The UNCD wafer was scribed into 1\u00d71cm2 samples and subjected to RCA cleaning (SC-1), followed by ultrasonic solvent cleaning. Nanofeature stamps were then created from the samples using conventional electron beam lithography (EBL) with negative tone electron sensitive resist, hydrogen silsesquioxane (HSQ). An Al discharge layer was required above the resist to prevent e-beam deflection due to charge build-up on the surface [17]. Several stamps were produced with this process and the pattern written varied in design but consisted of arrays of circular pillars. After EBL and HSQ development, the HSQ was used as an etch mask for RIE with a mixture of oxygen and argon gas. The etched diamond nanopillars were typically 225nm high. Fig. 1 displays a scanning electron micrograph of some typical stamp features.\n", "keywords": "Al discharge layer\nargon gas\nchemical vapour deposition\ncircular pillars\nconventional electron beam lithography\nCVD\nEBL\nEBL and HSQ development, t\netched diamond nanopillars\netch mask\nHSQ\nhydrogen silsesquioxane\nNanofeature stamps were then created from the samples\nnegative tone electron sensitive resist\noxygen\nprevent e-beam deflection due to charge build-up on the surface\nRCA cleaning\nRIE\nscanning electron micrograph\nSeveral stamps were produced\nultra-nanocrystalline diamond\nultrasonic solvent cleaning\nUNCD\nUNCD wafer\nvapour\n"}, {"id": "S0167931713005042", "text": "We used 2\u03bcm of ultra-nanocrystalline diamond (UNCD) grown by chemical vapour deposition (CVD) on a \u223c520\u03bcm silicon carrier wafer from Advanced Diamond Technologies Ltd. Detailed information about the material and the stamp fabrication can be found in our earlier paper [16]. The UNCD wafer was scribed into 1\u00d71cm2 samples and subjected to RCA cleaning (SC-1), followed by ultrasonic solvent cleaning. Nanofeature stamps were then created from the samples using conventional electron beam lithography (EBL) with negative tone electron sensitive resist, hydrogen silsesquioxane (HSQ). An Al discharge layer was required above the resist to prevent e-beam deflection due to charge build-up on the surface [17]. Several stamps were produced with this process and the pattern written varied in design but consisted of arrays of circular pillars. After EBL and HSQ development, the HSQ was used as an etch mask for RIE with a mixture of oxygen and argon gas. The etched diamond nanopillars were typically 225nm high. Fig. 1 displays a scanning electron micrograph of some typical stamp features.\n", "keywords": "Al discharge layer\nargon gas\nchemical vapour deposition\ncircular pillars\nconventional electron beam lithography\nCVD\nEBL\nEBL and HSQ development, t\netched diamond nanopillars\netch mask\nHSQ\nhydrogen silsesquioxane\nNanofeature stamps were then created from the samples\nnegative tone electron sensitive resist\noxygen\nprevent e-beam deflection due to charge build-up on the surface\nRCA cleaning\nRIE\nscanning electron micrograph\nSeveral stamps were produced\nultra-nanocrystalline diamond\nultrasonic solvent cleaning\nUNCD\nUNCD wafer\nvapour\n"}, {"id": "S0167931713005042", "text": "We used 2\u03bcm of ultra-nanocrystalline diamond (UNCD) grown by chemical vapour deposition (CVD) on a \u223c520\u03bcm silicon carrier wafer from Advanced Diamond Technologies Ltd. Detailed information about the material and the stamp fabrication can be found in our earlier paper [16]. The UNCD wafer was scribed into 1\u00d71cm2 samples and subjected to RCA cleaning (SC-1), followed by ultrasonic solvent cleaning. Nanofeature stamps were then created from the samples using conventional electron beam lithography (EBL) with negative tone electron sensitive resist, hydrogen silsesquioxane (HSQ). An Al discharge layer was required above the resist to prevent e-beam deflection due to charge build-up on the surface [17]. Several stamps were produced with this process and the pattern written varied in design but consisted of arrays of circular pillars. After EBL and HSQ development, the HSQ was used as an etch mask for RIE with a mixture of oxygen and argon gas. The etched diamond nanopillars were typically 225nm high. Fig. 1 displays a scanning electron micrograph of some typical stamp features.\n", "keywords": "Al discharge layer\nargon gas\nchemical vapour deposition\ncircular pillars\nconventional electron beam lithography\nCVD\nEBL\nEBL and HSQ development, t\netched diamond nanopillars\netch mask\nHSQ\nhydrogen silsesquioxane\nNanofeature stamps were then created from the samples\nnegative tone electron sensitive resist\noxygen\nprevent e-beam deflection due to charge build-up on the surface\nRCA cleaning\nRIE\nscanning electron micrograph\nSeveral stamps were produced\nultra-nanocrystalline diamond\nultrasonic solvent cleaning\nUNCD\nUNCD wafer\nvapour\n"}, {"id": "S0370269304008706", "text": "The agreement between the new data and the calculations with the relativistic deuteron wave function should not be considered as accidental one; in this connection other results should be mentioned. Previously it was shown [15] that calculations within the framework of light-front dynamics with Karmanov's deuteron wave function are in reasonably good agreement with the experimental data on the T20 parameter of deuteron breakup on H and C targets with the emission of protons at 0\u00b0 in the k region from 0.4 to 0.8\u00a0GeV/c. Furthermore, within the same approach a qualitative description of the momentum behaviour of the Ayy parameter of the 9Be(d,p)X reaction at a deuteron momentum of 4.5\u00a0GeV/c and a detected proton angle of 80\u00a0mr and a rather good description of the Ayy data for the 12C(d,p)X reaction at 9\u00a0GeV/c and 85\u00a0mr were obtained\u00a0[16].\n", "keywords": "12C(d,p)X reaction\nagreement between the new data and the calculations with the relativistic deuteron wave function\nAyy data\ncalculations within the framework of light-front dynamics with Karmanov's deuteron wave function\ndeuteron momentum\nemission of protons at 0\u00b0 in the k region from 0.4 to 0.8\u00a0GeV/c\nKarmanov's deuteron wave function\nlight-front dynamics\nqualitative description of the momentum behaviour of the Ayy parameter of the 9Be(d,p)X reaction\nrelativistic deuteron wave function\nT20 parameter of deuteron breakup on H and C targets\n"}, {"id": "S0370269304008706", "text": "The agreement between the new data and the calculations with the relativistic deuteron wave function should not be considered as accidental one; in this connection other results should be mentioned. Previously it was shown [15] that calculations within the framework of light-front dynamics with Karmanov's deuteron wave function are in reasonably good agreement with the experimental data on the T20 parameter of deuteron breakup on H and C targets with the emission of protons at 0\u00b0 in the k region from 0.4 to 0.8\u00a0GeV/c. Furthermore, within the same approach a qualitative description of the momentum behaviour of the Ayy parameter of the 9Be(d,p)X reaction at a deuteron momentum of 4.5\u00a0GeV/c and a detected proton angle of 80\u00a0mr and a rather good description of the Ayy data for the 12C(d,p)X reaction at 9\u00a0GeV/c and 85\u00a0mr were obtained\u00a0[16].\n", "keywords": "12C(d,p)X reaction\nagreement between the new data and the calculations with the relativistic deuteron wave function\nAyy data\ncalculations within the framework of light-front dynamics with Karmanov's deuteron wave function\ndeuteron momentum\nemission of protons at 0\u00b0 in the k region from 0.4 to 0.8\u00a0GeV/c\nKarmanov's deuteron wave function\nlight-front dynamics\nqualitative description of the momentum behaviour of the Ayy parameter of the 9Be(d,p)X reaction\nrelativistic deuteron wave function\nT20 parameter of deuteron breakup on H and C targets\n"}, {"id": "S1361841516300822", "text": "For all volunteers the AAMM technique significantly (p < 0.01) outperformed the other two methods in all of the intervals as can be seen by comparing to the error curves shown in Fig.\u00a08 and the figures in Table\u00a01 in the supplementary materials. Significance was assessed using a 1-tailed Wilcoxon signed rank test since the error distributions were generally not symmetric. The estimation errors for AAMM and its non-adaptive counterpart, AAMM (no adapt.), were similar in the beginning of the application phase, but as anticipated, as the application phase went on, the AAMM technique continually improved its accuracy by incorporating more and more data into the model. On average the motion estimation of AAMM improved by 22.94% in T5 with respect to its non-adaptive counterpart. However, the method has already significantly adapted to the breathing pattern in T2, i.e. after between 3 and 7\u00a0min of imaging, where motion estimations where on average 16.87% more accurate than at the beginning of the adaptation phase. By visually inspecting the curves for AAMM in Fig.\u00a08 it can be seen that for many volunteers (in particular volunteers A, D, E, and F) the error curves start to flatten approximately around the 7\u00a0min mark. From this it can be concluded that a longer calibration scan of around 12\u00a0min would be optimal, that is the 5\u00a0min that were used for calibration in this experiment plus 7\u00a0min worth of data added during the application phase. Note that this time could be significantly reduced if a non-cardiac-gated sequence was used.\n", "keywords": "1-tailed Wilcoxon signed rank test\nAAMM\nAAMM (no adapt.)\nAAMM technique\ncalibration\ncalibration scan\nerror curves\nimaging\nmotion estimation\nmotion estimations\nnon-cardiac-gated sequence\n"}, {"id": "S0009261414000372", "text": "It is well-known that the optical properties of atoms and molecules can be influenced by their electronic environment. Local field effects on spontaneous emission rates within nanostructured photonic materials for example are familiar, and have been well summarized [1]. Optical processes, including resonance energy transfer are similarly dependent on the local environment of molecular chromophores [2\u20134]. Many biological systems are known to contain complex organizations of molecules with absorption bands shifted due to the electronic influence of other, nearby optical centres. For instance, in widely studied light-harvesting complexes, there are two identifiable forms of the photosynthetic antenna molecule bacteriochlorophyll, with absorption bands centred on 800 and 850nm; it has been shown that the most efficient forms of energy transfer between the two occurs when there is a neighbouring carotenoid species 5\u20137. Until now, research on the broader influence of a neighbouring, off-resonant, molecule on photon absorption has mostly centred on the phenomenon of induced circular dichroism, where both quantum electrodynamic (QED) calculations [8\u201310] and experimental procedures [11\u201313] predict and verify that a chiral mediator confers the capacity for an achiral acceptor to exhibit circular differential absorption.\n", "keywords": "absorption bands\nachiral acceptor\natoms\ncarotenoid species\nchiral mediator\ncircular differential absorption\nelectronic environment\nelectronic influence\nenergy transfer\nexperimental procedures\ninduced circular dichroism\ninfluence of a neighbouring, off-resonant, molecule on photon absorption\nlight-harvesting complexes\nlocal environment of molecular chromophores\nLocal field effects\nmolecule\nmolecules\nnanostructured photonic materials\nOptical processes\noptical properties\nphoton\nphoton absorption\nphotosynthetic antenna molecule bacteriochlorophyll\nQED\nquantum electrodynamic\nresonance energy transfer\n"}, {"id": "S0370269302014880", "text": "Production of charmonium states J/\u03c8 and \u03c8\u2032 in nucleus\u2013nucleus collisions has been studied at CERN SPS over the previous 15 years by the NA38 and NA50 Collaborations. This experimental program was mainly motivated by the suggestion [1] to use the J/\u03c8 as a probe of the state of matter created at the early stage of the collision. The original picture [1] (see also [2] for a modern review) assumes that charmonia are created exclusively at the initial stage of the reaction in primary nucleon\u2013nucleon collisions. During the subsequent evolution of the system, the number of hidden charm mesons is reduced because of: (a)\u00a0absorption of pre-resonance charmonium states by nuclear nucleons (normal nuclear suppression), (b)\u00a0interactions of charmonia with secondary hadrons (comovers), (c)\u00a0dissociation of cc\u0304 bound states in deconfined medium (anomalous suppression). It was found [3] that J/\u03c8 suppression with respect to Drell\u2013Yan muon pairs measured in proton\u2013nucleus and nucleus\u2013nucleus collisions with light projectiles can be explained by the so-called \u201cnormal\u201d (due to sweeping nucleons) nuclear suppression alone. In contrast, the NA50 experiment with a heavy projectile and target (Pb+Pb) revealed essentially stronger J/\u03c8 suppression for central collisions [4\u20137]. This anomalous J/\u03c8 suppression was attributed to formation of quark\u2013gluon plasma (QGP) [7], but a comover scenario cannot be excluded [8].\n", "keywords": "absorption of pre-resonance charmonium states by nuclear nucleons\nanomalous suppression\ncc\u0304\ncentral collisions\ncharmonia\ncharmonium\ncharmonium states\ncomovers\ncomover scenario\ndissociation of cc\u0304 bound states in deconfined medium\nDrell\u2013Yan muon pairs\nexperimental program\nformation of quark\u2013gluon plasma\nhadrons\nheavy projectile and target\nhidden charm mesons\ninteractions of charmonia with secondary hadrons\nJ/\u03c8\nJ/\u03c8 suppression\nlight projectiles\nNA50 experiment\nnormal nuclear suppression\nnuclear nucleons\nnuclear suppression\nnucleon\nnucleus\nnucleus\u2013nucleus collisions\noriginal picture\nPb+Pb\nprimary nucleon\u2013nucleon collisions\nProduction of charmonium states J/\u03c8 and \u03c8\u2032 in nucleus\u2013nucleus collisions\nproton\u2013nucleus\nQGP\nquark\u2013gluon plasma\nsubsequent evolution of the system,\nsweeping nucleons\nuse the J/\u03c8 as a probe of the state of matter created at the early stage of the collision\n\u03c8\u2032\n"}, {"id": "S0377025714002213", "text": "The first of these systems, a biopolymer gel, involves the thermoreversible gelation of aqueous gelatin solutions to form a physical gel, whereas the other systems considered herein involve the formation of chemical gels featuring permanent cross-linked branching networks. The second system is a commercial silicone dielectric gel (SDG) which is used in the production of electronic products created by industrial printing processes. The third experimental system is a fibrin gel formed by the thrombin-induced polymerisation of fibrinogen molecules. The gel network product in the latter case forms the principal microstructural component of a blood clot [8]. The latter case is particularly interesting as the critical-gel which is established at the GP serves as a \u2018template\u2019 for the ensuing development of microstructure and associated rheological properties in the post-GP phase of fibrin clot evolution [9].\n", "keywords": "biopolymer gel\nblood clot\ncommercial silicone dielectric gel\ncritical-gel\ncross-linked branching networks\nfibrin clot\nfibrin gel\nfibrinogen molecules\ngel network product\nGP\nmicrostructural component\nproduction of electronic products\nSDG\n\u2018template\u2019 for the ensuing development of microstructure and associated rheological properties\nthermoreversible gelation\nthrombin-induced polymerisation\n"}, {"id": "S0377025714002213", "text": "The first of these systems, a biopolymer gel, involves the thermoreversible gelation of aqueous gelatin solutions to form a physical gel, whereas the other systems considered herein involve the formation of chemical gels featuring permanent cross-linked branching networks. The second system is a commercial silicone dielectric gel (SDG) which is used in the production of electronic products created by industrial printing processes. The third experimental system is a fibrin gel formed by the thrombin-induced polymerisation of fibrinogen molecules. The gel network product in the latter case forms the principal microstructural component of a blood clot [8]. The latter case is particularly interesting as the critical-gel which is established at the GP serves as a \u2018template\u2019 for the ensuing development of microstructure and associated rheological properties in the post-GP phase of fibrin clot evolution [9].\n", "keywords": "biopolymer gel\nblood clot\ncommercial silicone dielectric gel\ncritical-gel\ncross-linked branching networks\nfibrin clot\nfibrin gel\nfibrinogen molecules\ngel network product\nGP\nmicrostructural component\nproduction of electronic products\nSDG\n\u2018template\u2019 for the ensuing development of microstructure and associated rheological properties\nthermoreversible gelation\nthrombin-induced polymerisation\n"}, {"id": "S0377025714002213", "text": "The first of these systems, a biopolymer gel, involves the thermoreversible gelation of aqueous gelatin solutions to form a physical gel, whereas the other systems considered herein involve the formation of chemical gels featuring permanent cross-linked branching networks. The second system is a commercial silicone dielectric gel (SDG) which is used in the production of electronic products created by industrial printing processes. The third experimental system is a fibrin gel formed by the thrombin-induced polymerisation of fibrinogen molecules. The gel network product in the latter case forms the principal microstructural component of a blood clot [8]. The latter case is particularly interesting as the critical-gel which is established at the GP serves as a \u2018template\u2019 for the ensuing development of microstructure and associated rheological properties in the post-GP phase of fibrin clot evolution [9].\n", "keywords": "biopolymer gel\nblood clot\ncommercial silicone dielectric gel\ncritical-gel\ncross-linked branching networks\nfibrin clot\nfibrin gel\nfibrinogen molecules\ngel network product\nGP\nmicrostructural component\nproduction of electronic products\nSDG\n\u2018template\u2019 for the ensuing development of microstructure and associated rheological properties\nthermoreversible gelation\nthrombin-induced polymerisation\n"}, {"id": "S0377025714002213", "text": "The first of these systems, a biopolymer gel, involves the thermoreversible gelation of aqueous gelatin solutions to form a physical gel, whereas the other systems considered herein involve the formation of chemical gels featuring permanent cross-linked branching networks. The second system is a commercial silicone dielectric gel (SDG) which is used in the production of electronic products created by industrial printing processes. The third experimental system is a fibrin gel formed by the thrombin-induced polymerisation of fibrinogen molecules. The gel network product in the latter case forms the principal microstructural component of a blood clot [8]. The latter case is particularly interesting as the critical-gel which is established at the GP serves as a \u2018template\u2019 for the ensuing development of microstructure and associated rheological properties in the post-GP phase of fibrin clot evolution [9].\n", "keywords": "biopolymer gel\nblood clot\ncommercial silicone dielectric gel\ncritical-gel\ncross-linked branching networks\nfibrin clot\nfibrin gel\nfibrinogen molecules\ngel network product\nGP\nmicrostructural component\nproduction of electronic products\nSDG\n\u2018template\u2019 for the ensuing development of microstructure and associated rheological properties\nthermoreversible gelation\nthrombin-induced polymerisation\n"}, {"id": "S0375960113004908", "text": "Topological insulators (TIs) are promising candidates of spintronics materials because of their robust helical surface states and the extremely strong spin\u2013orbit interaction [1\u20133]. Initially, binary chalcogenides Bi2Te3, Sb2Te3 and Bi2Se3 have been identified as three-dimensional TIs by surface sensitive probes such as angle resolved photoemission spectroscopy and scanning tunneling microscopy/spectroscopy. Later, ternary chalcogenide (BixSb1\u2212x)2Te3 [4,5], which has similar tetradymite structure to the parent compounds Bi2Te3 and Sb2Te3, was predicted by ab initio calculations and confirmed by ARPES measurements as a tunable topological insulator whose Fermi energy and carrier density can be adjusted via changing the Bi/Sb composition ratio with stable topological surface state for the entire composition range. Combined with magnetism or superconductivity, TIs have attracted great attention due to the rich variety of new physics and applications. The ferromagnetism in several transition metal (TM) doped TIs, which breaks the time-reversal symmetry, has been reported [6\u201313]. Ferromagnetism in TIs is important because the combination of magnetism with TIs makes a good platform to study fundamental physical phenomena, such as the quantum anomalous Hall effect [14\u201317], Majorana fermions [18], image magnetic monopole effect [19], and topological contributions to the Faraday and Kerr magneto-optical effect [20].\n", "keywords": "ab initio calculations\nangle resolved photoemission spectroscopy\nARPES measurements\nBi2Se3\nBi2Te3\nbinary chalcogenides\n(BixSb1\u2212x)2Te3\nchanging the Bi/Sb composition ratio\nFaraday and Kerr magneto-optical effect\nferromagnetism\nFerromagnetism\nfundamental physical phenomena\nHall effect\nimage magnetic monopole effect\nmagnetism\nMajorana fermions\nSb2Te3\nscanning tunneling microscopy/spectroscopy\nspin\u2013orbit interaction\nspintronics materials\nsuperconductivity\nsurface sensitive probes\nternary chalcogenide\nTIs\nTM\ntopological insulator\nTopological insulators\ntransition metal\n"}, {"id": "S0370269304005829", "text": "Let us now consider the case of a beta-beam source. Similarly to the case of a static tritium source, an advantage of the beta-beams is that the neutrino fluxes can be very accurately calculated. Fig.\u00a03 shows the electron\u2013neutrino scattering events in the range of 0.1\u00a0MeV to 1\u00a0MeV and 1\u00a0keV to 10\u00a0keV, respectively. (In Fig.\u00a03(b) we have rounded to the nearest integer number of counts.) The shape of the flux-averaged cross sections is very similar to the reactor case as reflected in the event rates shown in the figures. As can be seen, by measuring electron recoils in the keV range with a beta-beam source one could, with a sufficiently strong source, have a very clear signature for a neutrino magnetic moment of 5\u00d710\u221211\u03bcB. These figures are for Helium-6 ions, however, similar results can be obtained using neutrinos from 18Ne. The results shown are obtained for an intensity of 1015\u00a0\u03bd/s (i.e., 1015\u00a0\u00a0ions/s). If there is no magnetic moment, this intensity will produce about 170 events in the 0.1 MeV to 1 MeV range per year and 3 events in the 1 keV to 10 keV range per year. These numbers increase to 210 and 55, respectively, in the case of a magnetic moment of 5\u00d710\u221211\u03bcB.\n", "keywords": "beta-beams\nbeta-beam source\nelectron\nelectron\u2013neutrino scattering\nelectron recoils\nHelium-6 ions\nneutrino\nneutrino fluxes\nneutrino magnetic moment\nneutrinos\nreactor case\nstatic tritium source\n"}, {"id": "S0370269304005829", "text": "Let us now consider the case of a beta-beam source. Similarly to the case of a static tritium source, an advantage of the beta-beams is that the neutrino fluxes can be very accurately calculated. Fig.\u00a03 shows the electron\u2013neutrino scattering events in the range of 0.1\u00a0MeV to 1\u00a0MeV and 1\u00a0keV to 10\u00a0keV, respectively. (In Fig.\u00a03(b) we have rounded to the nearest integer number of counts.) The shape of the flux-averaged cross sections is very similar to the reactor case as reflected in the event rates shown in the figures. As can be seen, by measuring electron recoils in the keV range with a beta-beam source one could, with a sufficiently strong source, have a very clear signature for a neutrino magnetic moment of 5\u00d710\u221211\u03bcB. These figures are for Helium-6 ions, however, similar results can be obtained using neutrinos from 18Ne. The results shown are obtained for an intensity of 1015\u00a0\u03bd/s (i.e., 1015\u00a0\u00a0ions/s). If there is no magnetic moment, this intensity will produce about 170 events in the 0.1 MeV to 1 MeV range per year and 3 events in the 1 keV to 10 keV range per year. These numbers increase to 210 and 55, respectively, in the case of a magnetic moment of 5\u00d710\u221211\u03bcB.\n", "keywords": "beta-beams\nbeta-beam source\nelectron\nelectron\u2013neutrino scattering\nelectron recoils\nHelium-6 ions\nneutrino\nneutrino fluxes\nneutrino magnetic moment\nneutrinos\nreactor case\nstatic tritium source\n"}, {"id": "S0038092X15001681", "text": "For the reverse current analysis, for both scenarios (shading and short circuits) were tested on two systems, one system using standard silicon modules and another system using high efficiency modules. For the standard silicon system, a power of 50kWp was considered, with a system composed by 10 strings of 24 modules per string and an approximate system Voc of 864 [VDC]. For the high efficiency system, a power of 40kWp was considered, with a system composed by 10 strings of 18 modules per string and an approximate system Voc of 873 [VDC]. Fig. 5(a) shows the reverse current present in one string when different numbers of modules in the string are shaded by 90%. Fig. 5(b) shows the reverse current present in one string when different numbers of modules of the string are short-circuited. For both figures the continuous lines are for the standard silicon system and the dashed lines are for the high efficiency system.\n", "keywords": "10 strings of 18 modules per string\n10 strings of 24 modules per string\ndifferent numbers of modules of the string are short-circuited\nhigh efficiency modules\nhigh efficiency system\nreverse current\nreverse current analysis\nstandard silicon modules\nstandard silicon system\nsystem\nVDC\nVoc of 864\nVoc of 873\n"}, {"id": "S0038092X15001681", "text": "For the reverse current analysis, for both scenarios (shading and short circuits) were tested on two systems, one system using standard silicon modules and another system using high efficiency modules. For the standard silicon system, a power of 50kWp was considered, with a system composed by 10 strings of 24 modules per string and an approximate system Voc of 864 [VDC]. For the high efficiency system, a power of 40kWp was considered, with a system composed by 10 strings of 18 modules per string and an approximate system Voc of 873 [VDC]. Fig. 5(a) shows the reverse current present in one string when different numbers of modules in the string are shaded by 90%. Fig. 5(b) shows the reverse current present in one string when different numbers of modules of the string are short-circuited. For both figures the continuous lines are for the standard silicon system and the dashed lines are for the high efficiency system.\n", "keywords": "10 strings of 18 modules per string\n10 strings of 24 modules per string\ndifferent numbers of modules of the string are short-circuited\nhigh efficiency modules\nhigh efficiency system\nreverse current\nreverse current analysis\nstandard silicon modules\nstandard silicon system\nsystem\nVDC\nVoc of 864\nVoc of 873\n"}, {"id": "S0038092X15001681", "text": "For the reverse current analysis, for both scenarios (shading and short circuits) were tested on two systems, one system using standard silicon modules and another system using high efficiency modules. For the standard silicon system, a power of 50kWp was considered, with a system composed by 10 strings of 24 modules per string and an approximate system Voc of 864 [VDC]. For the high efficiency system, a power of 40kWp was considered, with a system composed by 10 strings of 18 modules per string and an approximate system Voc of 873 [VDC]. Fig. 5(a) shows the reverse current present in one string when different numbers of modules in the string are shaded by 90%. Fig. 5(b) shows the reverse current present in one string when different numbers of modules of the string are short-circuited. For both figures the continuous lines are for the standard silicon system and the dashed lines are for the high efficiency system.\n", "keywords": "10 strings of 18 modules per string\n10 strings of 24 modules per string\ndifferent numbers of modules of the string are short-circuited\nhigh efficiency modules\nhigh efficiency system\nreverse current\nreverse current analysis\nstandard silicon modules\nstandard silicon system\nsystem\nVDC\nVoc of 864\nVoc of 873\n"}, {"id": "S0010938X14000420", "text": "One surface was then polished and cleaned using a protocol designed to eliminate as much preparation-related contamination as possible. This is as follows: The lead surface was polished by hand using a damp abrasive disc (BuehlerMet II \u00ae) to remove visible surface defects and to expose a fresh metal surface. Coupons were then polished using a sequence of diamond polishes with decreasing particle sizes (6\u03bcm, 3\u03bcm, 1\u03bcm Buehler MetaDi \u00ae polycrystalline diamond suspension). A polishing cloth (Buehler MicroCloth \u00ae) was saturated with the appropriate diamond suspension. A custom-made jig fitted to an automatic polisher (Buehler Minimet \u00ae 1000) was used to hold the coupons in place during automated polishing. Coupons were polished for 15min using each diamond suspension followed by rinsing with 2-propanol (99.5%, reagent grade) and cleaning in 2-propanol for 5min in an ultrasonic bath. After polishing with the 1\u03bcm diamond suspension, the coupons were ultrasonically cleaned in 2-propanol for 3\u00d75min, with fresh propanol for each cleaning cycle. Polished coupons were stored in 2-propanol until required.\n", "keywords": "2-propanol\nautomated polishing\ncleaning\ncoupons\nCoupons\ncustom-made jig\ndamp abrasive disc\ndiamond\ndiamond polishes\ndiamond suspension\nmetal surface\npolished\npolisher\npolishing\npolishing cloth\npolycrystalline diamond\npropanol\nsurface\nultrasonic bath\n"}, {"id": "S0010938X14000420", "text": "One surface was then polished and cleaned using a protocol designed to eliminate as much preparation-related contamination as possible. This is as follows: The lead surface was polished by hand using a damp abrasive disc (BuehlerMet II \u00ae) to remove visible surface defects and to expose a fresh metal surface. Coupons were then polished using a sequence of diamond polishes with decreasing particle sizes (6\u03bcm, 3\u03bcm, 1\u03bcm Buehler MetaDi \u00ae polycrystalline diamond suspension). A polishing cloth (Buehler MicroCloth \u00ae) was saturated with the appropriate diamond suspension. A custom-made jig fitted to an automatic polisher (Buehler Minimet \u00ae 1000) was used to hold the coupons in place during automated polishing. Coupons were polished for 15min using each diamond suspension followed by rinsing with 2-propanol (99.5%, reagent grade) and cleaning in 2-propanol for 5min in an ultrasonic bath. After polishing with the 1\u03bcm diamond suspension, the coupons were ultrasonically cleaned in 2-propanol for 3\u00d75min, with fresh propanol for each cleaning cycle. Polished coupons were stored in 2-propanol until required.\n", "keywords": "2-propanol\nautomated polishing\ncleaning\ncoupons\nCoupons\ncustom-made jig\ndamp abrasive disc\ndiamond\ndiamond polishes\ndiamond suspension\nmetal surface\npolished\npolisher\npolishing\npolishing cloth\npolycrystalline diamond\npropanol\nsurface\nultrasonic bath\n"}, {"id": "S1364815216303243", "text": "Typical physically-based 2D flood models solve the Shallow Water Equations (SWEs), requiring high computational resources. Many of these models have been developed to obtain better performance, while maintaining the required accuracy, by reducing the complexity of the SWEs. This reduction is usually achieved by approximating or neglecting less significant terms of the equations (Hunter et\u00a0al., 2007; Yen and Tsai, 2001). The JFLOW model (Bradbrook et\u00a0al., 2004), Urban Inundation Model (UIM) (Chen et\u00a0al., 2007), and the diffusive version of LISFLOOD-FP (Hunter et\u00a0al., 2005) solve the 2D diffusion wave equations that neglect the inertial (local acceleration) and advection (convective acceleration) terms (Yen and Tsai, 2001). The inertial version of LISFLOOD-FP (Bates et\u00a0al., 2010) solves the SWEs without the advection term. In either version of LISFLOOD-FP the flow is decoupled in the Cartesian directions. Other models use the full SWEs but focus on the use of multi resolution grids or irregular mesh, like InfoWorks ICM (Innovyze, 2012) and MIKE FLOOD (DHI Software, 2014; H\u00e9nonin et\u00a0al., 2013). These last two models are commercial packages, and the code applied in the optimisation techniques is not in the public domain.\n", "keywords": "2D diffusion wave equations\nCartesian directions\nInfoWorks ICM\nJFLOW\nLISFLOOD-FP\nMIKE FLOOD\nmulti resolution grids or irregular mesh\nphysically-based 2D flood models\nreducing the complexity of the SWEs\nShallow Water Equations\nsolve the Shallow Water Equations (SWEs)\nSWEs\nUIM\nUrban Inundation Model\n"}, {"id": "S0010938X13003818", "text": "Based on the theoretical analysis, the value of the measuring resistor, Rm, has no effect on the corrosion process and on the estimated value of noise resistance. In order to validate this conclusion, the experiment of Fig. 9 was performed. Specifically, a pair of nominally identical specimens was initially coupled by a 4.7k\u03a9 resistor and their potential with respect to a saturated calomel electrode was recorded by using a NI-USB 6009 analog-to-digital converter. The electrochemical noise signal was recorded using in-house developed software, acquiring at 1023Hz segments of 1000 points at each iteration. Between iterations, the 1000 values acquired were averaged to obtain a single value of potential, subsequently saved to the file used for later processing. The final dataset comprised potential values spaced 1\u00b10.05s in time. Under the assumption that the noise present above 1023Hz is negligible compared with the noise present below 0.5Hz, this procedure enables an accurate recording of the potential noise in the frequencies of interest, avoiding aliasing of frequencies between 0.5 and 1023Hz and minimizing the 50Hz interference from the mains supply.\n", "keywords": "4.7k\u03a9 resistor\naccurate recording of the potential noise in the frequencies of interest\ncorrosion process\ndataset\nelectrochemical noise signal\nin-house developed software\nNI-USB 6009 analog-to-digital converter\nobtain a single value of potential\npair of nominally identical specimens\nRm\nsaturated calomel electrode\ntheoretical analysis\nvalidate this conclusion\nvalue of noise resistance\nvalue of the measuring resistor\n"}, {"id": "S0010938X13003818", "text": "Based on the theoretical analysis, the value of the measuring resistor, Rm, has no effect on the corrosion process and on the estimated value of noise resistance. In order to validate this conclusion, the experiment of Fig. 9 was performed. Specifically, a pair of nominally identical specimens was initially coupled by a 4.7k\u03a9 resistor and their potential with respect to a saturated calomel electrode was recorded by using a NI-USB 6009 analog-to-digital converter. The electrochemical noise signal was recorded using in-house developed software, acquiring at 1023Hz segments of 1000 points at each iteration. Between iterations, the 1000 values acquired were averaged to obtain a single value of potential, subsequently saved to the file used for later processing. The final dataset comprised potential values spaced 1\u00b10.05s in time. Under the assumption that the noise present above 1023Hz is negligible compared with the noise present below 0.5Hz, this procedure enables an accurate recording of the potential noise in the frequencies of interest, avoiding aliasing of frequencies between 0.5 and 1023Hz and minimizing the 50Hz interference from the mains supply.\n", "keywords": "4.7k\u03a9 resistor\naccurate recording of the potential noise in the frequencies of interest\ncorrosion process\ndataset\nelectrochemical noise signal\nin-house developed software\nNI-USB 6009 analog-to-digital converter\nobtain a single value of potential\npair of nominally identical specimens\nRm\nsaturated calomel electrode\ntheoretical analysis\nvalidate this conclusion\nvalue of noise resistance\nvalue of the measuring resistor\n"}, {"id": "S0010938X13003818", "text": "Based on the theoretical analysis, the value of the measuring resistor, Rm, has no effect on the corrosion process and on the estimated value of noise resistance. In order to validate this conclusion, the experiment of Fig. 9 was performed. Specifically, a pair of nominally identical specimens was initially coupled by a 4.7k\u03a9 resistor and their potential with respect to a saturated calomel electrode was recorded by using a NI-USB 6009 analog-to-digital converter. The electrochemical noise signal was recorded using in-house developed software, acquiring at 1023Hz segments of 1000 points at each iteration. Between iterations, the 1000 values acquired were averaged to obtain a single value of potential, subsequently saved to the file used for later processing. The final dataset comprised potential values spaced 1\u00b10.05s in time. Under the assumption that the noise present above 1023Hz is negligible compared with the noise present below 0.5Hz, this procedure enables an accurate recording of the potential noise in the frequencies of interest, avoiding aliasing of frequencies between 0.5 and 1023Hz and minimizing the 50Hz interference from the mains supply.\n", "keywords": "4.7k\u03a9 resistor\naccurate recording of the potential noise in the frequencies of interest\ncorrosion process\ndataset\nelectrochemical noise signal\nin-house developed software\nNI-USB 6009 analog-to-digital converter\nobtain a single value of potential\npair of nominally identical specimens\nRm\nsaturated calomel electrode\ntheoretical analysis\nvalidate this conclusion\nvalue of noise resistance\nvalue of the measuring resistor\n"}, {"id": "S0010938X13003818", "text": "Based on the theoretical analysis, the value of the measuring resistor, Rm, has no effect on the corrosion process and on the estimated value of noise resistance. In order to validate this conclusion, the experiment of Fig. 9 was performed. Specifically, a pair of nominally identical specimens was initially coupled by a 4.7k\u03a9 resistor and their potential with respect to a saturated calomel electrode was recorded by using a NI-USB 6009 analog-to-digital converter. The electrochemical noise signal was recorded using in-house developed software, acquiring at 1023Hz segments of 1000 points at each iteration. Between iterations, the 1000 values acquired were averaged to obtain a single value of potential, subsequently saved to the file used for later processing. The final dataset comprised potential values spaced 1\u00b10.05s in time. Under the assumption that the noise present above 1023Hz is negligible compared with the noise present below 0.5Hz, this procedure enables an accurate recording of the potential noise in the frequencies of interest, avoiding aliasing of frequencies between 0.5 and 1023Hz and minimizing the 50Hz interference from the mains supply.\n", "keywords": "4.7k\u03a9 resistor\naccurate recording of the potential noise in the frequencies of interest\ncorrosion process\ndataset\nelectrochemical noise signal\nin-house developed software\nNI-USB 6009 analog-to-digital converter\nobtain a single value of potential\npair of nominally identical specimens\nRm\nsaturated calomel electrode\ntheoretical analysis\nvalidate this conclusion\nvalue of noise resistance\nvalue of the measuring resistor\n"}, {"id": "S0010938X13003818", "text": "Based on the theoretical analysis, the value of the measuring resistor, Rm, has no effect on the corrosion process and on the estimated value of noise resistance. In order to validate this conclusion, the experiment of Fig. 9 was performed. Specifically, a pair of nominally identical specimens was initially coupled by a 4.7k\u03a9 resistor and their potential with respect to a saturated calomel electrode was recorded by using a NI-USB 6009 analog-to-digital converter. The electrochemical noise signal was recorded using in-house developed software, acquiring at 1023Hz segments of 1000 points at each iteration. Between iterations, the 1000 values acquired were averaged to obtain a single value of potential, subsequently saved to the file used for later processing. The final dataset comprised potential values spaced 1\u00b10.05s in time. Under the assumption that the noise present above 1023Hz is negligible compared with the noise present below 0.5Hz, this procedure enables an accurate recording of the potential noise in the frequencies of interest, avoiding aliasing of frequencies between 0.5 and 1023Hz and minimizing the 50Hz interference from the mains supply.\n", "keywords": "4.7k\u03a9 resistor\naccurate recording of the potential noise in the frequencies of interest\ncorrosion process\ndataset\nelectrochemical noise signal\nin-house developed software\nNI-USB 6009 analog-to-digital converter\nobtain a single value of potential\npair of nominally identical specimens\nRm\nsaturated calomel electrode\ntheoretical analysis\nvalidate this conclusion\nvalue of noise resistance\nvalue of the measuring resistor\n"}, {"id": "S0010938X13003818", "text": "Based on the theoretical analysis, the value of the measuring resistor, Rm, has no effect on the corrosion process and on the estimated value of noise resistance. In order to validate this conclusion, the experiment of Fig. 9 was performed. Specifically, a pair of nominally identical specimens was initially coupled by a 4.7k\u03a9 resistor and their potential with respect to a saturated calomel electrode was recorded by using a NI-USB 6009 analog-to-digital converter. The electrochemical noise signal was recorded using in-house developed software, acquiring at 1023Hz segments of 1000 points at each iteration. Between iterations, the 1000 values acquired were averaged to obtain a single value of potential, subsequently saved to the file used for later processing. The final dataset comprised potential values spaced 1\u00b10.05s in time. Under the assumption that the noise present above 1023Hz is negligible compared with the noise present below 0.5Hz, this procedure enables an accurate recording of the potential noise in the frequencies of interest, avoiding aliasing of frequencies between 0.5 and 1023Hz and minimizing the 50Hz interference from the mains supply.\n", "keywords": "4.7k\u03a9 resistor\naccurate recording of the potential noise in the frequencies of interest\ncorrosion process\ndataset\nelectrochemical noise signal\nin-house developed software\nNI-USB 6009 analog-to-digital converter\nobtain a single value of potential\npair of nominally identical specimens\nRm\nsaturated calomel electrode\ntheoretical analysis\nvalidate this conclusion\nvalue of noise resistance\nvalue of the measuring resistor\n"}, {"id": "S0010938X13003818", "text": "Based on the theoretical analysis, the value of the measuring resistor, Rm, has no effect on the corrosion process and on the estimated value of noise resistance. In order to validate this conclusion, the experiment of Fig. 9 was performed. Specifically, a pair of nominally identical specimens was initially coupled by a 4.7k\u03a9 resistor and their potential with respect to a saturated calomel electrode was recorded by using a NI-USB 6009 analog-to-digital converter. The electrochemical noise signal was recorded using in-house developed software, acquiring at 1023Hz segments of 1000 points at each iteration. Between iterations, the 1000 values acquired were averaged to obtain a single value of potential, subsequently saved to the file used for later processing. The final dataset comprised potential values spaced 1\u00b10.05s in time. Under the assumption that the noise present above 1023Hz is negligible compared with the noise present below 0.5Hz, this procedure enables an accurate recording of the potential noise in the frequencies of interest, avoiding aliasing of frequencies between 0.5 and 1023Hz and minimizing the 50Hz interference from the mains supply.\n", "keywords": "4.7k\u03a9 resistor\naccurate recording of the potential noise in the frequencies of interest\ncorrosion process\ndataset\nelectrochemical noise signal\nin-house developed software\nNI-USB 6009 analog-to-digital converter\nobtain a single value of potential\npair of nominally identical specimens\nRm\nsaturated calomel electrode\ntheoretical analysis\nvalidate this conclusion\nvalue of noise resistance\nvalue of the measuring resistor\n"}, {"id": "S0010938X13003818", "text": "Based on the theoretical analysis, the value of the measuring resistor, Rm, has no effect on the corrosion process and on the estimated value of noise resistance. In order to validate this conclusion, the experiment of Fig. 9 was performed. Specifically, a pair of nominally identical specimens was initially coupled by a 4.7k\u03a9 resistor and their potential with respect to a saturated calomel electrode was recorded by using a NI-USB 6009 analog-to-digital converter. The electrochemical noise signal was recorded using in-house developed software, acquiring at 1023Hz segments of 1000 points at each iteration. Between iterations, the 1000 values acquired were averaged to obtain a single value of potential, subsequently saved to the file used for later processing. The final dataset comprised potential values spaced 1\u00b10.05s in time. Under the assumption that the noise present above 1023Hz is negligible compared with the noise present below 0.5Hz, this procedure enables an accurate recording of the potential noise in the frequencies of interest, avoiding aliasing of frequencies between 0.5 and 1023Hz and minimizing the 50Hz interference from the mains supply.\n", "keywords": "4.7k\u03a9 resistor\naccurate recording of the potential noise in the frequencies of interest\ncorrosion process\ndataset\nelectrochemical noise signal\nin-house developed software\nNI-USB 6009 analog-to-digital converter\nobtain a single value of potential\npair of nominally identical specimens\nRm\nsaturated calomel electrode\ntheoretical analysis\nvalidate this conclusion\nvalue of noise resistance\nvalue of the measuring resistor\n"}, {"id": "S0010938X13003818", "text": "Based on the theoretical analysis, the value of the measuring resistor, Rm, has no effect on the corrosion process and on the estimated value of noise resistance. In order to validate this conclusion, the experiment of Fig. 9 was performed. Specifically, a pair of nominally identical specimens was initially coupled by a 4.7k\u03a9 resistor and their potential with respect to a saturated calomel electrode was recorded by using a NI-USB 6009 analog-to-digital converter. The electrochemical noise signal was recorded using in-house developed software, acquiring at 1023Hz segments of 1000 points at each iteration. Between iterations, the 1000 values acquired were averaged to obtain a single value of potential, subsequently saved to the file used for later processing. The final dataset comprised potential values spaced 1\u00b10.05s in time. Under the assumption that the noise present above 1023Hz is negligible compared with the noise present below 0.5Hz, this procedure enables an accurate recording of the potential noise in the frequencies of interest, avoiding aliasing of frequencies between 0.5 and 1023Hz and minimizing the 50Hz interference from the mains supply.\n", "keywords": "4.7k\u03a9 resistor\naccurate recording of the potential noise in the frequencies of interest\ncorrosion process\ndataset\nelectrochemical noise signal\nin-house developed software\nNI-USB 6009 analog-to-digital converter\nobtain a single value of potential\npair of nominally identical specimens\nRm\nsaturated calomel electrode\ntheoretical analysis\nvalidate this conclusion\nvalue of noise resistance\nvalue of the measuring resistor\n"}, {"id": "S0010938X13003818", "text": "Based on the theoretical analysis, the value of the measuring resistor, Rm, has no effect on the corrosion process and on the estimated value of noise resistance. In order to validate this conclusion, the experiment of Fig. 9 was performed. Specifically, a pair of nominally identical specimens was initially coupled by a 4.7k\u03a9 resistor and their potential with respect to a saturated calomel electrode was recorded by using a NI-USB 6009 analog-to-digital converter. The electrochemical noise signal was recorded using in-house developed software, acquiring at 1023Hz segments of 1000 points at each iteration. Between iterations, the 1000 values acquired were averaged to obtain a single value of potential, subsequently saved to the file used for later processing. The final dataset comprised potential values spaced 1\u00b10.05s in time. Under the assumption that the noise present above 1023Hz is negligible compared with the noise present below 0.5Hz, this procedure enables an accurate recording of the potential noise in the frequencies of interest, avoiding aliasing of frequencies between 0.5 and 1023Hz and minimizing the 50Hz interference from the mains supply.\n", "keywords": "4.7k\u03a9 resistor\naccurate recording of the potential noise in the frequencies of interest\ncorrosion process\ndataset\nelectrochemical noise signal\nin-house developed software\nNI-USB 6009 analog-to-digital converter\nobtain a single value of potential\npair of nominally identical specimens\nRm\nsaturated calomel electrode\ntheoretical analysis\nvalidate this conclusion\nvalue of noise resistance\nvalue of the measuring resistor\n"}, {"id": "S0010938X13003818", "text": "Based on the theoretical analysis, the value of the measuring resistor, Rm, has no effect on the corrosion process and on the estimated value of noise resistance. In order to validate this conclusion, the experiment of Fig. 9 was performed. Specifically, a pair of nominally identical specimens was initially coupled by a 4.7k\u03a9 resistor and their potential with respect to a saturated calomel electrode was recorded by using a NI-USB 6009 analog-to-digital converter. The electrochemical noise signal was recorded using in-house developed software, acquiring at 1023Hz segments of 1000 points at each iteration. Between iterations, the 1000 values acquired were averaged to obtain a single value of potential, subsequently saved to the file used for later processing. The final dataset comprised potential values spaced 1\u00b10.05s in time. Under the assumption that the noise present above 1023Hz is negligible compared with the noise present below 0.5Hz, this procedure enables an accurate recording of the potential noise in the frequencies of interest, avoiding aliasing of frequencies between 0.5 and 1023Hz and minimizing the 50Hz interference from the mains supply.\n", "keywords": "4.7k\u03a9 resistor\naccurate recording of the potential noise in the frequencies of interest\ncorrosion process\ndataset\nelectrochemical noise signal\nin-house developed software\nNI-USB 6009 analog-to-digital converter\nobtain a single value of potential\npair of nominally identical specimens\nRm\nsaturated calomel electrode\ntheoretical analysis\nvalidate this conclusion\nvalue of noise resistance\nvalue of the measuring resistor\n"}, {"id": "S0011227514002136", "text": "Measuring and analysing the hold time of the CPA pill allows the thermal boundary resistance within the pill to be assessed; the thermal boundary dictates the actual temperature of the CPA crystals in comparison to the temperature of the cold finger, which is maintained at a constant temperature by a servo control program. Fig. 17 shows the temperature profile during the recycling of the CPA pill and subsequent operation at 200mK. During the hold time, the servo control program maintained the CPA pill temperature to within a millikelvin. It is expected that microkelvin stability can be achieved with fast read-out thermometry (which was not available at the time of testing but which will be used for the mKCC), as this would allow for temperature control on much faster (millisecond) timescales than the current (approximately 1s) thermometry readout used.\n", "keywords": "cold finger\nCPA crystals\nCPA pill\nfast read-out thermometry\nMeasuring and analysing the hold time of the CPA pill\nmicrokelvin stability\nmKCC\noperation at 200mK\npill\nrecycling\nservo control program\ntemperature\nthermal boundary\nthermal boundary resistance\n"}, {"id": "S0011227514002136", "text": "Measuring and analysing the hold time of the CPA pill allows the thermal boundary resistance within the pill to be assessed; the thermal boundary dictates the actual temperature of the CPA crystals in comparison to the temperature of the cold finger, which is maintained at a constant temperature by a servo control program. Fig. 17 shows the temperature profile during the recycling of the CPA pill and subsequent operation at 200mK. During the hold time, the servo control program maintained the CPA pill temperature to within a millikelvin. It is expected that microkelvin stability can be achieved with fast read-out thermometry (which was not available at the time of testing but which will be used for the mKCC), as this would allow for temperature control on much faster (millisecond) timescales than the current (approximately 1s) thermometry readout used.\n", "keywords": "cold finger\nCPA crystals\nCPA pill\nfast read-out thermometry\nMeasuring and analysing the hold time of the CPA pill\nmicrokelvin stability\nmKCC\noperation at 200mK\npill\nrecycling\nservo control program\ntemperature\nthermal boundary\nthermal boundary resistance\n"}, {"id": "S0003491615001955", "text": "A fluctuating vacuum is a general feature of quantum fields, of which the free Maxwell field considered in\u00a0 [1\u201312] is but one example. Fermionic fields such as that describing the electron, also undergo vacuum fluctuations, consequently one expects to find Casimir effects associated with such fields whenever they are confined in some way. Such effects were first investigated in the context of nuclear physics, within the so-called \u201cMIT bag model\u201d of the nucleon\u00a0 [13]. In the bag-model one envisages the nucleon as a collection of fermionic fields describing confined quarks. These quarks are subject to a boundary condition at the surface of the \u2018bag\u2019 that represents the nucleon\u2019s surface. Just as in the electromagnetic case, the bag boundary condition modifies the vacuum fluctuations of the field, which results in the appearance of a Casimir force\u00a0 [14\u201318]. This force, although very weak at a macroscopic scale, can be significant on the small length scales encountered in nuclear physics. It therefore has important consequences for the physics of the bag-model nucleon\u00a0 [19].\n", "keywords": "a collection of fermionic fields describing confined quarks\nbag-model nucleon\nCasimir effects\nCasimir force\nFermionic fields\nfluctuating vacuum\nfree Maxwell field\n\u201cMIT bag model\u201d of the nucleon\nnuclear physics\nnucleon\nquantum fields\nsuch fields\nthe bag boundary condition modifies the vacuum fluctuations of the field\nundergo vacuum fluctuations\n"}, {"id": "S0003491615001955", "text": "A fluctuating vacuum is a general feature of quantum fields, of which the free Maxwell field considered in\u00a0 [1\u201312] is but one example. Fermionic fields such as that describing the electron, also undergo vacuum fluctuations, consequently one expects to find Casimir effects associated with such fields whenever they are confined in some way. Such effects were first investigated in the context of nuclear physics, within the so-called \u201cMIT bag model\u201d of the nucleon\u00a0 [13]. In the bag-model one envisages the nucleon as a collection of fermionic fields describing confined quarks. These quarks are subject to a boundary condition at the surface of the \u2018bag\u2019 that represents the nucleon\u2019s surface. Just as in the electromagnetic case, the bag boundary condition modifies the vacuum fluctuations of the field, which results in the appearance of a Casimir force\u00a0 [14\u201318]. This force, although very weak at a macroscopic scale, can be significant on the small length scales encountered in nuclear physics. It therefore has important consequences for the physics of the bag-model nucleon\u00a0 [19].\n", "keywords": "a collection of fermionic fields describing confined quarks\nbag-model nucleon\nCasimir effects\nCasimir force\nFermionic fields\nfluctuating vacuum\nfree Maxwell field\n\u201cMIT bag model\u201d of the nucleon\nnuclear physics\nnucleon\nquantum fields\nsuch fields\nthe bag boundary condition modifies the vacuum fluctuations of the field\nundergo vacuum fluctuations\n"}, {"id": "S0032386110001667", "text": "Note that the quantitative introduction of a reactive functionality into the polymer chain end can be easily achieved by adopting the living ROMP technique especially using the Schrock type molybdenum alkylidene initiator [7,12,21,61\u201365]. The exclusive preparation of end-functionalized ring-opened polymers (realized by a living polymerization with quantitative initiation) can be applied not only to prepare block copolymers (ABCs) coupled with another living polymerization techniques [66], but also for preparation of macromonomers, as described below. In contrast, the initiation efficiency is not always perfect as seen in the molybdenum alkylidene initiators, because dissociation of ligand (PR3 etc.) should be required to generate the catalytically active species in the ROMP with the ruthenium carbene catalysts (Scheme 2) [67\u201369]. An equilibrium between coordination and dissociation of PR3 should be present even in the propagation process, and replacement of halogen with the other anionic ligand (and/or replacement of PR3 with the other neutral donor ligands/substrates) can also be considered as the probable side reactions. Importance of using the molybdenum catalysts should be thus emphasized for their precise preparations, although the initiators are highly sensitive to moisture and both monomers and solvent have to be thus strictly purified to avoid the catalyst decomposition (deactivation).\n", "keywords": "ABCs\nanionic ligand\nblock copolymers\ncatalyst\ncoordination\ndeactivation\ndecomposition\ndissociation\ngenerate the catalytically active species in the ROMP\nhalogen\nligand\nliving polymerization techniques\nliving polymerization with quantitative initiation\nliving ROMP technique\nmacromonomers\nmolybdenum alkylidene initiators\nmolybdenum catalysts\nmonomers\nneutral donor ligands/substrates\npolymer chain end\nPR3\npreparation of end-functionalized ring-opened polymers\npreparation of macromonomers\npropagation process\nquantitative introduction of a reactive functionality into the polymer chain end\nring-opened polymers\nROMP\nruthenium carbene catalysts\nSchrock type molybdenum alkylidene initiator\nsolvent\n"}, {"id": "S0032386110001667", "text": "Note that the quantitative introduction of a reactive functionality into the polymer chain end can be easily achieved by adopting the living ROMP technique especially using the Schrock type molybdenum alkylidene initiator [7,12,21,61\u201365]. The exclusive preparation of end-functionalized ring-opened polymers (realized by a living polymerization with quantitative initiation) can be applied not only to prepare block copolymers (ABCs) coupled with another living polymerization techniques [66], but also for preparation of macromonomers, as described below. In contrast, the initiation efficiency is not always perfect as seen in the molybdenum alkylidene initiators, because dissociation of ligand (PR3 etc.) should be required to generate the catalytically active species in the ROMP with the ruthenium carbene catalysts (Scheme 2) [67\u201369]. An equilibrium between coordination and dissociation of PR3 should be present even in the propagation process, and replacement of halogen with the other anionic ligand (and/or replacement of PR3 with the other neutral donor ligands/substrates) can also be considered as the probable side reactions. Importance of using the molybdenum catalysts should be thus emphasized for their precise preparations, although the initiators are highly sensitive to moisture and both monomers and solvent have to be thus strictly purified to avoid the catalyst decomposition (deactivation).\n", "keywords": "ABCs\nanionic ligand\nblock copolymers\ncatalyst\ncoordination\ndeactivation\ndecomposition\ndissociation\ngenerate the catalytically active species in the ROMP\nhalogen\nligand\nliving polymerization techniques\nliving polymerization with quantitative initiation\nliving ROMP technique\nmacromonomers\nmolybdenum alkylidene initiators\nmolybdenum catalysts\nmonomers\nneutral donor ligands/substrates\npolymer chain end\nPR3\npreparation of end-functionalized ring-opened polymers\npreparation of macromonomers\npropagation process\nquantitative introduction of a reactive functionality into the polymer chain end\nring-opened polymers\nROMP\nruthenium carbene catalysts\nSchrock type molybdenum alkylidene initiator\nsolvent\n"}, {"id": "S2212671612001618", "text": "This paper presents a non-fragile controller design method based on system quadratic performance optimization. For the additive controller gain variations, the necessary and sufficient conditions for the existence of non-fragile state feedback controller are given and transformed to the LMI problems, which simplifies the solutions to obtain non-fragile state feedback controllers. The flight control simulation results prove the reliability and validity of the method.", "keywords": "additive controller gain variations\nflight control simulation\nLMI problems\nnon-fragile controller design method\nnon-fragile state feedback controller\nnon-fragile state feedback controllers\nsimplifies the solutions\nsystem quadratic performance optimization\ntransformed\n"}, {"id": "S0168365913004975", "text": "The \u03b1-\u03c9-aminohexylcarbamate derivative of cyanocobalamin was prepared using a method described previously [18]. Briefly, solid CDI (260mg, 0.32mmol) was added to cyanocobalamin (1.0g, 0.148mmol) previously dissolved in anhydrous dimethyl sulfoxide. The mixture was stirred for up to 2h at 30\u00b0C, followed by the addition of dry 1,6-hexanediamine (314mg, 0.54mmol) and stirring of the mixture at room temperature over 24h. The mixture was poured into ethyl acetate (30ml) and left to stand. Following centrifugation and decanting of the supernatant, the residue was sonicated for 5min in acetone (50ml). The resulting precipitate was filtered and the solid washed in acetone. The crude product was purified by silica column chromatography (45% v/v 2-propanol, 30% v/v n-butanol, 2% v/v ammonia and 25% v/v water) followed by lyophilisation.\n", "keywords": "acetone\nammonia\nanhydrous dimethyl sulfoxide\nbutanol\ncentrifugation\ncrude product\ncyanocobalamin\ndecanting\ndry 1,6-hexanediamine\nethyl acetate\nlyophilisation\nprecipitate\npropanol\nresidue\nsilica column chromatography\nsolid\nsolid CDI\nsupernatant\nwater\n\u03b1-\u03c9-aminohexylcarbamate derivative of cyanocobalamin\n"}, {"id": "S0168365913004975", "text": "The \u03b1-\u03c9-aminohexylcarbamate derivative of cyanocobalamin was prepared using a method described previously [18]. Briefly, solid CDI (260mg, 0.32mmol) was added to cyanocobalamin (1.0g, 0.148mmol) previously dissolved in anhydrous dimethyl sulfoxide. The mixture was stirred for up to 2h at 30\u00b0C, followed by the addition of dry 1,6-hexanediamine (314mg, 0.54mmol) and stirring of the mixture at room temperature over 24h. The mixture was poured into ethyl acetate (30ml) and left to stand. Following centrifugation and decanting of the supernatant, the residue was sonicated for 5min in acetone (50ml). The resulting precipitate was filtered and the solid washed in acetone. The crude product was purified by silica column chromatography (45% v/v 2-propanol, 30% v/v n-butanol, 2% v/v ammonia and 25% v/v water) followed by lyophilisation.\n", "keywords": "acetone\nammonia\nanhydrous dimethyl sulfoxide\nbutanol\ncentrifugation\ncrude product\ncyanocobalamin\ndecanting\ndry 1,6-hexanediamine\nethyl acetate\nlyophilisation\nprecipitate\npropanol\nresidue\nsilica column chromatography\nsolid\nsolid CDI\nsupernatant\nwater\n\u03b1-\u03c9-aminohexylcarbamate derivative of cyanocobalamin\n"}, {"id": "S0888327016302333", "text": "The GFRFs of nonlinear systems can be determined by either a parametric-model-based method or a nonparametric-model-based method [8]. In the parametric approach, a nonlinear parametric model is first identified from the input\u2013output data. The GFRFs are then obtained by mapping the resultant model into the frequency domain using the probing method [9]. The nonparametric approach is often referred to as frequency-domain Volterra system identification and is based on the observation that the Volterra model of nonlinear systems is linear in terms of the unknown Volterra kernels, which, in the frequency domain, corresponds to a linear relation between the output frequency response and linear, quadratic, and higher order GFRFs. This linear relationship allows the use of a least squares (LS) approach to solve for the GFRFs. Several researchers [10\u201312] have used this method to estimate the GFRFs. But they usually made the assumption that it is known a priori that the system under study can be represented by just two or three terms. However, such information is rarely available a priori.\n", "keywords": "a least squares (LS) approach\na nonlinear parametric model\na nonparametric-model-based method\na parametric-model-based method\ncorresponds to a linear relation\nfrequency-domain Volterra system identification\nGFRFs\nlinear relationship\nmade the assumption\nmapping the resultant model into the frequency domain\nnonparametric approach\nprobing method\nThe GFRFs of nonlinear systems\nthe system\ntwo or three terms\nVolterra model of nonlinear systems\n"}, {"id": "S0888327016302333", "text": "The GFRFs of nonlinear systems can be determined by either a parametric-model-based method or a nonparametric-model-based method [8]. In the parametric approach, a nonlinear parametric model is first identified from the input\u2013output data. The GFRFs are then obtained by mapping the resultant model into the frequency domain using the probing method [9]. The nonparametric approach is often referred to as frequency-domain Volterra system identification and is based on the observation that the Volterra model of nonlinear systems is linear in terms of the unknown Volterra kernels, which, in the frequency domain, corresponds to a linear relation between the output frequency response and linear, quadratic, and higher order GFRFs. This linear relationship allows the use of a least squares (LS) approach to solve for the GFRFs. Several researchers [10\u201312] have used this method to estimate the GFRFs. But they usually made the assumption that it is known a priori that the system under study can be represented by just two or three terms. However, such information is rarely available a priori.\n", "keywords": "a least squares (LS) approach\na nonlinear parametric model\na nonparametric-model-based method\na parametric-model-based method\ncorresponds to a linear relation\nfrequency-domain Volterra system identification\nGFRFs\nlinear relationship\nmade the assumption\nmapping the resultant model into the frequency domain\nnonparametric approach\nprobing method\nThe GFRFs of nonlinear systems\nthe system\ntwo or three terms\nVolterra model of nonlinear systems\n"}, {"id": "S0888327016302333", "text": "The GFRFs of nonlinear systems can be determined by either a parametric-model-based method or a nonparametric-model-based method [8]. In the parametric approach, a nonlinear parametric model is first identified from the input\u2013output data. The GFRFs are then obtained by mapping the resultant model into the frequency domain using the probing method [9]. The nonparametric approach is often referred to as frequency-domain Volterra system identification and is based on the observation that the Volterra model of nonlinear systems is linear in terms of the unknown Volterra kernels, which, in the frequency domain, corresponds to a linear relation between the output frequency response and linear, quadratic, and higher order GFRFs. This linear relationship allows the use of a least squares (LS) approach to solve for the GFRFs. Several researchers [10\u201312] have used this method to estimate the GFRFs. But they usually made the assumption that it is known a priori that the system under study can be represented by just two or three terms. However, such information is rarely available a priori.\n", "keywords": "a least squares (LS) approach\na nonlinear parametric model\na nonparametric-model-based method\na parametric-model-based method\ncorresponds to a linear relation\nfrequency-domain Volterra system identification\nGFRFs\nlinear relationship\nmade the assumption\nmapping the resultant model into the frequency domain\nnonparametric approach\nprobing method\nThe GFRFs of nonlinear systems\nthe system\ntwo or three terms\nVolterra model of nonlinear systems\n"}, {"id": "S0032386109010386", "text": "From a general point of view, polymerization techniques can be divided into two types of chemical reactions: step-growth polymerization and free radical polymerization. Step-growth polymerization is widely used for synthesis of polyesters, polyamide and epoxies while the synthesis of polyacrylics requires the use of free radical polymerization. These polymerization reactions can be performed either in bulk or in solution or in dispersed media. Heterophase polymerizations (i.e. emulsion, dispersion and miniemulsion polymerizations) present the advantage of easier removal of the resulting product from the reactor compared to bulk polymerization thanks to the low viscosity of the reaction medium. Polymerization in solution also induces lower viscosity but also lower reaction rates due to dilution of the reactants and higher cost and environmental impact due to the use of organic solvents. These problems are solved in the case of heterophase polymerizations where the reactants are confined inside droplets (no dilution effect) and water is used as medium. The use of surfactant molecules are usually needed for the stabilization of the monomer droplet and subsequent polymer particles in the water phase.\n", "keywords": "bulk\nbulk polymerization\nchemical reactions\ndilution\ndilution effect\ndispersed media\ndispersion\ndroplets\nemulsion\nepoxies\nfree radical polymerization\nheterophase polymerizations\nHeterophase polymerizations\nminiemulsion polymerizations\nmonomer droplet\norganic solvents\npolyacrylics\npolyamide\npolyesters\nPolymerization\npolymerization reactions\npolymerization techniques\npolymer particles\nreactants\nreaction\nreaction medium\nreactor\nremoval of the resulting product\nsolution\nstabilization of the monomer droplet\nstep-growth polymerization\nStep-growth polymerization\nsurfactant molecules\nsynthesis of polyacrylics\nsynthesis of polyesters\nwater\n"}, {"id": "S0032386109010386", "text": "From a general point of view, polymerization techniques can be divided into two types of chemical reactions: step-growth polymerization and free radical polymerization. Step-growth polymerization is widely used for synthesis of polyesters, polyamide and epoxies while the synthesis of polyacrylics requires the use of free radical polymerization. These polymerization reactions can be performed either in bulk or in solution or in dispersed media. Heterophase polymerizations (i.e. emulsion, dispersion and miniemulsion polymerizations) present the advantage of easier removal of the resulting product from the reactor compared to bulk polymerization thanks to the low viscosity of the reaction medium. Polymerization in solution also induces lower viscosity but also lower reaction rates due to dilution of the reactants and higher cost and environmental impact due to the use of organic solvents. These problems are solved in the case of heterophase polymerizations where the reactants are confined inside droplets (no dilution effect) and water is used as medium. The use of surfactant molecules are usually needed for the stabilization of the monomer droplet and subsequent polymer particles in the water phase.\n", "keywords": "bulk\nbulk polymerization\nchemical reactions\ndilution\ndilution effect\ndispersed media\ndispersion\ndroplets\nemulsion\nepoxies\nfree radical polymerization\nheterophase polymerizations\nHeterophase polymerizations\nminiemulsion polymerizations\nmonomer droplet\norganic solvents\npolyacrylics\npolyamide\npolyesters\nPolymerization\npolymerization reactions\npolymerization techniques\npolymer particles\nreactants\nreaction\nreaction medium\nreactor\nremoval of the resulting product\nsolution\nstabilization of the monomer droplet\nstep-growth polymerization\nStep-growth polymerization\nsurfactant molecules\nsynthesis of polyacrylics\nsynthesis of polyesters\nwater\n"}, {"id": "S2212671612000704", "text": "The load of beam pumping unit is changeable, which is often in a state of light load. Reducing a certain voltage can improve the power factor and efficiency of the beam pumping unit when in light load .We can change the voltage by changing the thyristor trigger angle. It is complex and unacceptable to analyze the change of the cycles of the load overall. So we can divide the load of the whole cycle into several equal parts, each can be thought of as a constant load. The most optimal voltage for the current load can be calculated by genetic algorithm. When each load is in the most optimal voltage, we can get the whole optimal voltage changeable rule. Then it produces the result of energy saving.", "keywords": "analyze the change of the cycles of the load overall\nbeam pumping\nbeam pumping unit\ncalculated by genetic algorithm\nchange the voltage\nchanging the thyristor trigger angle\nconstant load\ncurrent load\ndivide the load\nenergy saving\nequal parts\ngenetic algorithm\nimprove the power factor and efficiency of the beam pumping unit\nlight load\noptimal voltage\nReducing a certain voltage\nthyristor trigger angle\nwhole optimal voltage changeable rule.\n"}, {"id": "S0168365913002848", "text": "Mice bearing the orthotopic model were treated starting from day 21 after NB cell implant; mice with the pseudo-metastatic model received the first treatment 4h after NB cell injection. These therapeutic schedules were designed to test the effects of our targeted formulations against both established and pseudo-metastatic preclinical models of human NB, as described [16,19]. Animals were treated i.v. once a week for 3 weeks with untargeted (SL[DXR]) or peptide-targeted SL[DXR] (5mg/kg). Scrambled peptide-functionalized liposomes were used as a control, and in every experiment a group of control mice received HEPES-buffered saline. Survival times were used as the main criterion for determining treatment efficacy. In the orthotopic model, time-dependent anti-tumor activity was also evaluated by bioluminescence imaging (BLI) and X-ray analyses. For this purpose, the GI-LI-N cell line was infected with a retrovirus expressing the firefly luciferase gene, as previously reported [17]; luciferase activity of retrovirally-transduced cells was visualized in vivo by BLI (IVIS Caliper Life Sciences, Hopkinton, MA) after a 10min incubation with 150\u03bcg/mL of d-luciferin (Caliper Life Sciences), as described [17]. X-ray analysis was superimposed to the luminescence for a better visualization of the tumors.\n", "keywords": "Animals\ndetermining treatment efficacy\nd-luciferin\nHEPES-buffered saline\ninfected\nluciferase activity\nMice bearing the orthotopic model\nmice with the pseudo-metastatic model\northotopic model\nour targeted formulations\npeptide-targeted SL[DXR]\npreclinical models\nretrovirally-transduced cells\nScrambled peptide-functionalized liposomes\nSL[DXR]\nSurvival times\ntest the effects\nthe GI-LI-N cell line\nThese therapeutic schedules\ntime-dependent anti-tumor activity\nuntargeted\nvisualized\n"}, {"id": "S0168365913002848", "text": "Mice bearing the orthotopic model were treated starting from day 21 after NB cell implant; mice with the pseudo-metastatic model received the first treatment 4h after NB cell injection. These therapeutic schedules were designed to test the effects of our targeted formulations against both established and pseudo-metastatic preclinical models of human NB, as described [16,19]. Animals were treated i.v. once a week for 3 weeks with untargeted (SL[DXR]) or peptide-targeted SL[DXR] (5mg/kg). Scrambled peptide-functionalized liposomes were used as a control, and in every experiment a group of control mice received HEPES-buffered saline. Survival times were used as the main criterion for determining treatment efficacy. In the orthotopic model, time-dependent anti-tumor activity was also evaluated by bioluminescence imaging (BLI) and X-ray analyses. For this purpose, the GI-LI-N cell line was infected with a retrovirus expressing the firefly luciferase gene, as previously reported [17]; luciferase activity of retrovirally-transduced cells was visualized in vivo by BLI (IVIS Caliper Life Sciences, Hopkinton, MA) after a 10min incubation with 150\u03bcg/mL of d-luciferin (Caliper Life Sciences), as described [17]. X-ray analysis was superimposed to the luminescence for a better visualization of the tumors.\n", "keywords": "Animals\ndetermining treatment efficacy\nd-luciferin\nHEPES-buffered saline\ninfected\nluciferase activity\nMice bearing the orthotopic model\nmice with the pseudo-metastatic model\northotopic model\nour targeted formulations\npeptide-targeted SL[DXR]\npreclinical models\nretrovirally-transduced cells\nScrambled peptide-functionalized liposomes\nSL[DXR]\nSurvival times\ntest the effects\nthe GI-LI-N cell line\nThese therapeutic schedules\ntime-dependent anti-tumor activity\nuntargeted\nvisualized\n"}, {"id": "S0168365913002848", "text": "Mice bearing the orthotopic model were treated starting from day 21 after NB cell implant; mice with the pseudo-metastatic model received the first treatment 4h after NB cell injection. These therapeutic schedules were designed to test the effects of our targeted formulations against both established and pseudo-metastatic preclinical models of human NB, as described [16,19]. Animals were treated i.v. once a week for 3 weeks with untargeted (SL[DXR]) or peptide-targeted SL[DXR] (5mg/kg). Scrambled peptide-functionalized liposomes were used as a control, and in every experiment a group of control mice received HEPES-buffered saline. Survival times were used as the main criterion for determining treatment efficacy. In the orthotopic model, time-dependent anti-tumor activity was also evaluated by bioluminescence imaging (BLI) and X-ray analyses. For this purpose, the GI-LI-N cell line was infected with a retrovirus expressing the firefly luciferase gene, as previously reported [17]; luciferase activity of retrovirally-transduced cells was visualized in vivo by BLI (IVIS Caliper Life Sciences, Hopkinton, MA) after a 10min incubation with 150\u03bcg/mL of d-luciferin (Caliper Life Sciences), as described [17]. X-ray analysis was superimposed to the luminescence for a better visualization of the tumors.\n", "keywords": "Animals\ndetermining treatment efficacy\nd-luciferin\nHEPES-buffered saline\ninfected\nluciferase activity\nMice bearing the orthotopic model\nmice with the pseudo-metastatic model\northotopic model\nour targeted formulations\npeptide-targeted SL[DXR]\npreclinical models\nretrovirally-transduced cells\nScrambled peptide-functionalized liposomes\nSL[DXR]\nSurvival times\ntest the effects\nthe GI-LI-N cell line\nThese therapeutic schedules\ntime-dependent anti-tumor activity\nuntargeted\nvisualized\n"}, {"id": "S2212667814001397", "text": "In this paper, a regression analysis based method is proposed to calculate the Journal Influence Score. This Influence Score is used to measure the\u00a0scientific influence of\u00a0scholarly journals. Journal Influence Score is calculated by using various factors in a weighted manner. The Score is then compared with the SCImago Journal Score. The results show that the error is small between the existing and proposed methods, proving that the model is a feasible and effective way of calculating scientific impact of journals.", "keywords": "calculate the Journal Influence Score\ncalculating scientific impact of journals\ncompared with the SCImago Journal Score\nInfluence Score\nJournal Influence Score\nmeasure the\u00a0scientific influence of\u00a0scholarly journals\nregression analysis\nregression analysis based method\nscholarly journals\nscientific influence\nSCImago Journal Score\nusing various factors in a weighted manner\n"}, {"id": "S2212667814001397", "text": "In this paper, a regression analysis based method is proposed to calculate the Journal Influence Score. This Influence Score is used to measure the\u00a0scientific influence of\u00a0scholarly journals. Journal Influence Score is calculated by using various factors in a weighted manner. The Score is then compared with the SCImago Journal Score. The results show that the error is small between the existing and proposed methods, proving that the model is a feasible and effective way of calculating scientific impact of journals.", "keywords": "calculate the Journal Influence Score\ncalculating scientific impact of journals\ncompared with the SCImago Journal Score\nInfluence Score\nJournal Influence Score\nmeasure the\u00a0scientific influence of\u00a0scholarly journals\nregression analysis\nregression analysis based method\nscholarly journals\nscientific influence\nSCImago Journal Score\nusing various factors in a weighted manner\n"}, {"id": "S2212667813000774", "text": "Evolutionary Algorithms are the stochastic optimization methods, simulating the behavior of natural evolution. These algorithms are basically population based search procedures efficiently dealing with complex search spaces having robust and powerful search mechanism. EAs are highly applicable in multiobjective optimization problem which are having conflicting objectives. This paper reviews the work carried out for diversity and convergence issues in EMO.", "keywords": "EAs\nEMO\nEvolutionary Algorithms\nnatural evolution\noptimization problem\npopulation based search procedures\nreviews the work carried out for diversity and convergence issues in EMO\nsearch mechanism\nstochastic optimization methods\n"}, {"id": "S074756321630348X", "text": "Social network gaming, which refers to playing games that are connected to social networking services (SNS) directly, or through mobile applications (apps), is a popular online activity. Social network games (SNG) are generally free-to-play and do not award monetary prizes, but users can make in-game purchases to advance within the game, customise the game, give gifts to friends, and access other exclusive benefits and features, leading to these games being referred to as \u2018freemium\u2019. Although SNG are connected to a SNS and encourage users to interact with their connections, most SNG can be played without any social interaction. SNG have grown rapidly in popularity and the global SNG market is predicted to grow annually at 16% from 2013 to 2019 to reach a total market value of US$17.4 billion (Transparency Market Research, 2015). A survey of Facebook users in Australia in November 2012 reported that there are over 3.5 million social gamers across Australia and almost 70% play SNG daily (Spiral Media, 2013), and it is highly likely that the use of SNG has increased since this time.\n", "keywords": "apps\nmobile applications\nSNG\nSNG market\nSNS\nSocial network games\nSocial network gaming\nsocial networking services\nsurvey\n"}, {"id": "S2212671612002338", "text": "Robust and automatic thresholding of gray level images has been commonly used in the field of pattern recognition and computer vision for objects detecting, tracking and recognizing. The Otsu scheme, a widely used image thresholding technique, provides approving results for segmenting a gray level image with only one modal distribution in gray level histogram. However, it provides poor results if the histogram of a gray level is non-bimodal. For enhancing the performance of the Otsu algorithm further, in this work, an improved median-based Otsu image thresholding algorithm is presented. Finally extensive tests are performed and the experiments show that our method obtain more satisfactory results than the original Otsu thresholding algorithm.", "keywords": "computer vision\ndetecting\nenhancing the performance\nextensive tests\ngray level histogram.\ngray level image\ngray level images\nhistogram\nimage thresholding technique\nmedian-based Otsu image thresholding algorithm\nmodal distribution\nobjects detecting, tracking and recognizing\nOtsu algorithm\nOtsu scheme\nOtsu thresholding algorithm\npattern recognition\nrecognizing\nRobust and automatic thresholding\nsegmenting\nthresholding\ntracking\n"}, {"id": "S2212671612002338", "text": "Robust and automatic thresholding of gray level images has been commonly used in the field of pattern recognition and computer vision for objects detecting, tracking and recognizing. The Otsu scheme, a widely used image thresholding technique, provides approving results for segmenting a gray level image with only one modal distribution in gray level histogram. However, it provides poor results if the histogram of a gray level is non-bimodal. For enhancing the performance of the Otsu algorithm further, in this work, an improved median-based Otsu image thresholding algorithm is presented. Finally extensive tests are performed and the experiments show that our method obtain more satisfactory results than the original Otsu thresholding algorithm.", "keywords": "computer vision\ndetecting\nenhancing the performance\nextensive tests\ngray level histogram.\ngray level image\ngray level images\nhistogram\nimage thresholding technique\nmedian-based Otsu image thresholding algorithm\nmodal distribution\nobjects detecting, tracking and recognizing\nOtsu algorithm\nOtsu scheme\nOtsu thresholding algorithm\npattern recognition\nrecognizing\nRobust and automatic thresholding\nsegmenting\nthresholding\ntracking\n"}, {"id": "S0301010415002189", "text": "The sodium trimer has a long history of theoretical and experimental studies. A pioneering theoretical paper of Martin and Davidson published in 1978 showed that the obtuse isosceles geometry is lower in energy than the linear conformation [6]. Several extended PES scans of Na3 and other alkali trimers followed this initial study, employing DFT [7], complete active space SCF [8], or a configuration interaction approach based on valence bond wave functions [9]. Recently, the applicability of density functional theory (DFT) to JT-distorted systems has also been tested for Na3 [10], and the B-X transition has been revisited as well, applying state-averaged multi-reference configuration interaction with a large active space in order to derive more accurate non-adiabatic coupling terms for an improved interpretation of photoabsorption spectra [11\u201313].\n", "keywords": "alkali trimers\nB-X transition\ncomplete active space SCF\nconfiguration interaction approach\ndensity functional theory\nderive more accurate non-adiabatic coupling terms\nDFT\nJT-distorted systems\nlinear conformation\nNa3\nobtuse isosceles geometry\nPES scans\nphotoabsorption spectra\nsodium trimer\nstate-averaged multi-reference configuration interaction\nvalence bond wave functions\n"}, {"id": "S0301010415002189", "text": "The sodium trimer has a long history of theoretical and experimental studies. A pioneering theoretical paper of Martin and Davidson published in 1978 showed that the obtuse isosceles geometry is lower in energy than the linear conformation [6]. Several extended PES scans of Na3 and other alkali trimers followed this initial study, employing DFT [7], complete active space SCF [8], or a configuration interaction approach based on valence bond wave functions [9]. Recently, the applicability of density functional theory (DFT) to JT-distorted systems has also been tested for Na3 [10], and the B-X transition has been revisited as well, applying state-averaged multi-reference configuration interaction with a large active space in order to derive more accurate non-adiabatic coupling terms for an improved interpretation of photoabsorption spectra [11\u201313].\n", "keywords": "alkali trimers\nB-X transition\ncomplete active space SCF\nconfiguration interaction approach\ndensity functional theory\nderive more accurate non-adiabatic coupling terms\nDFT\nJT-distorted systems\nlinear conformation\nNa3\nobtuse isosceles geometry\nPES scans\nphotoabsorption spectra\nsodium trimer\nstate-averaged multi-reference configuration interaction\nvalence bond wave functions\n"}, {"id": "S0301010415002189", "text": "The sodium trimer has a long history of theoretical and experimental studies. A pioneering theoretical paper of Martin and Davidson published in 1978 showed that the obtuse isosceles geometry is lower in energy than the linear conformation [6]. Several extended PES scans of Na3 and other alkali trimers followed this initial study, employing DFT [7], complete active space SCF [8], or a configuration interaction approach based on valence bond wave functions [9]. Recently, the applicability of density functional theory (DFT) to JT-distorted systems has also been tested for Na3 [10], and the B-X transition has been revisited as well, applying state-averaged multi-reference configuration interaction with a large active space in order to derive more accurate non-adiabatic coupling terms for an improved interpretation of photoabsorption spectra [11\u201313].\n", "keywords": "alkali trimers\nB-X transition\ncomplete active space SCF\nconfiguration interaction approach\ndensity functional theory\nderive more accurate non-adiabatic coupling terms\nDFT\nJT-distorted systems\nlinear conformation\nNa3\nobtuse isosceles geometry\nPES scans\nphotoabsorption spectra\nsodium trimer\nstate-averaged multi-reference configuration interaction\nvalence bond wave functions\n"}, {"id": "S0301010415002189", "text": "The sodium trimer has a long history of theoretical and experimental studies. A pioneering theoretical paper of Martin and Davidson published in 1978 showed that the obtuse isosceles geometry is lower in energy than the linear conformation [6]. Several extended PES scans of Na3 and other alkali trimers followed this initial study, employing DFT [7], complete active space SCF [8], or a configuration interaction approach based on valence bond wave functions [9]. Recently, the applicability of density functional theory (DFT) to JT-distorted systems has also been tested for Na3 [10], and the B-X transition has been revisited as well, applying state-averaged multi-reference configuration interaction with a large active space in order to derive more accurate non-adiabatic coupling terms for an improved interpretation of photoabsorption spectra [11\u201313].\n", "keywords": "alkali trimers\nB-X transition\ncomplete active space SCF\nconfiguration interaction approach\ndensity functional theory\nderive more accurate non-adiabatic coupling terms\nDFT\nJT-distorted systems\nlinear conformation\nNa3\nobtuse isosceles geometry\nPES scans\nphotoabsorption spectra\nsodium trimer\nstate-averaged multi-reference configuration interaction\nvalence bond wave functions\n"}, {"id": "S0098300412001793", "text": "MINERAL (MINeral ERror AnaLysis) is a new MATLAB\u00ae based program that provides mineral formula recalculations combined with the associated propagation of the analytical uncertainties. Methods are based on the work of Giamarita and Day (1990). However, additional features have been added to provide users with greater flexibility in data reporting. Many programs exist to recalculate wt% data into formula unit cations. Some generalized programs can be used to recalculate the formula of multiple minerals e.g. CALCMIN (Brandelik, 2009) and HYPER-FORM (De Bjerg et al., 1992). Other programs are mineral specific e.g. AMPH CLASS (Esawi, 2004) and PROBE AMPH (Tindle and Webb, 1994) for the recalculation of amphibole analyses; ILMAT (Lepage, 2003) for the recalculation of magnetite and ilmenite; and PX-NOM (Sturm, 2002) for the recalculation of pyroxene analyses. MINERAL provides a rapid method for the recalculation of multiple common minerals. However, its strength lies in the fact that is the first tool to incorporate the associated uncertainty propagation calculations. As these are performed concurrently with the standard recalculations, no additional time is needed to perform uncertainty propagation. While an understanding of the underlying calculations is strongly recommended, MINERAL is designed to allow users with little or no experience operating MATLAB\u00ae and/or performing mineral formula recalculations and uncertainty propagation to undertake both with ease.\n", "keywords": "allow users with little or no experience operating MATLAB\u00ae and/or performing mineral formula recalculations and uncertainty propagation to undertake both with ease\nAMPH CLASS\na new MATLAB\u00ae based program\nCALCMIN\nHYPER-FORM\nILMAT\nincorporate the associated uncertainty propagation calculations.\nMethods\nMINERAL\nMINERAL (MINeral ERror AnaLysis)\nPROBE AMPH\nprovides mineral formula recalculations combined with the associated propagation of the analytical uncertainties\nPX-NOM\nrecalculate the formula of multiple minerals\nrecalculation of amphibole analyses\nrecalculation of magnetite and ilmenite\nrecalculation of multiple common minerals\nrecalculation of pyroxene analyses\nSome generalized programs\nto provide users with greater flexibility in data reporting\n"}, {"id": "S0098300412001793", "text": "MINERAL (MINeral ERror AnaLysis) is a new MATLAB\u00ae based program that provides mineral formula recalculations combined with the associated propagation of the analytical uncertainties. Methods are based on the work of Giamarita and Day (1990). However, additional features have been added to provide users with greater flexibility in data reporting. Many programs exist to recalculate wt% data into formula unit cations. Some generalized programs can be used to recalculate the formula of multiple minerals e.g. CALCMIN (Brandelik, 2009) and HYPER-FORM (De Bjerg et al., 1992). Other programs are mineral specific e.g. AMPH CLASS (Esawi, 2004) and PROBE AMPH (Tindle and Webb, 1994) for the recalculation of amphibole analyses; ILMAT (Lepage, 2003) for the recalculation of magnetite and ilmenite; and PX-NOM (Sturm, 2002) for the recalculation of pyroxene analyses. MINERAL provides a rapid method for the recalculation of multiple common minerals. However, its strength lies in the fact that is the first tool to incorporate the associated uncertainty propagation calculations. As these are performed concurrently with the standard recalculations, no additional time is needed to perform uncertainty propagation. While an understanding of the underlying calculations is strongly recommended, MINERAL is designed to allow users with little or no experience operating MATLAB\u00ae and/or performing mineral formula recalculations and uncertainty propagation to undertake both with ease.\n", "keywords": "allow users with little or no experience operating MATLAB\u00ae and/or performing mineral formula recalculations and uncertainty propagation to undertake both with ease\nAMPH CLASS\na new MATLAB\u00ae based program\nCALCMIN\nHYPER-FORM\nILMAT\nincorporate the associated uncertainty propagation calculations.\nMethods\nMINERAL\nMINERAL (MINeral ERror AnaLysis)\nPROBE AMPH\nprovides mineral formula recalculations combined with the associated propagation of the analytical uncertainties\nPX-NOM\nrecalculate the formula of multiple minerals\nrecalculation of amphibole analyses\nrecalculation of magnetite and ilmenite\nrecalculation of multiple common minerals\nrecalculation of pyroxene analyses\nSome generalized programs\nto provide users with greater flexibility in data reporting\n"}, {"id": "S0098300412001793", "text": "MINERAL (MINeral ERror AnaLysis) is a new MATLAB\u00ae based program that provides mineral formula recalculations combined with the associated propagation of the analytical uncertainties. Methods are based on the work of Giamarita and Day (1990). However, additional features have been added to provide users with greater flexibility in data reporting. Many programs exist to recalculate wt% data into formula unit cations. Some generalized programs can be used to recalculate the formula of multiple minerals e.g. CALCMIN (Brandelik, 2009) and HYPER-FORM (De Bjerg et al., 1992). Other programs are mineral specific e.g. AMPH CLASS (Esawi, 2004) and PROBE AMPH (Tindle and Webb, 1994) for the recalculation of amphibole analyses; ILMAT (Lepage, 2003) for the recalculation of magnetite and ilmenite; and PX-NOM (Sturm, 2002) for the recalculation of pyroxene analyses. MINERAL provides a rapid method for the recalculation of multiple common minerals. However, its strength lies in the fact that is the first tool to incorporate the associated uncertainty propagation calculations. As these are performed concurrently with the standard recalculations, no additional time is needed to perform uncertainty propagation. While an understanding of the underlying calculations is strongly recommended, MINERAL is designed to allow users with little or no experience operating MATLAB\u00ae and/or performing mineral formula recalculations and uncertainty propagation to undertake both with ease.\n", "keywords": "allow users with little or no experience operating MATLAB\u00ae and/or performing mineral formula recalculations and uncertainty propagation to undertake both with ease\nAMPH CLASS\na new MATLAB\u00ae based program\nCALCMIN\nHYPER-FORM\nILMAT\nincorporate the associated uncertainty propagation calculations.\nMethods\nMINERAL\nMINERAL (MINeral ERror AnaLysis)\nPROBE AMPH\nprovides mineral formula recalculations combined with the associated propagation of the analytical uncertainties\nPX-NOM\nrecalculate the formula of multiple minerals\nrecalculation of amphibole analyses\nrecalculation of magnetite and ilmenite\nrecalculation of multiple common minerals\nrecalculation of pyroxene analyses\nSome generalized programs\nto provide users with greater flexibility in data reporting\n"}, {"id": "S0370269304009293", "text": "An OPE of VQCD(r) was developed in [3]. In this and the next paragraph, we review the content of that paper relevant to our analysis. Within this framework, short-distance contributions are contained in the potentials, which are in fact the Wilson coefficients, while non-perturbative contributions are contained in the matrix elements that are organized in multipole expansion in r\u2192 at r\u226a\u039bQCD\u22121. The following relation was derived: (16)VQCD(r)=VS(r)+\u03b4EUS(r),(17)\u03b4EUS=\u2212ig2TFNC\u222b0\u221edte\u2212i\u0394V(r)t\u00d7\u3008r\u2192\u22c5E\u2192a(t)\u03c6adj(t,0)abr\u2192\u22c5E\u2192b(0)\u3009+O(r3). VS(r) denotes the singlet potential. \u03b4EUS(r) denotes the non-perturbative contribution to the QCD potential, which starts at O(\u039bQCD3r2) in the multipole expansion. \u0394V(r)=VO(r)\u2212VS(r) denotes the difference between the octet and singlet potentials; see [3] for details. Intuitively VS(r) corresponds to VUV(r;\u03bcf) and \u03b4EUS(r) to VIR(r;\u03bcf). We adopt dimensional regularization in our analysis; we also refer to hard cutoff schemes when discussing conceptual aspects.\n", "keywords": "dimensional regularization\nhard cutoff schemes\nmultipole expansion\nOPE\nQCD potential\nVQCD(r)\n"}, {"id": "S0098300413002951", "text": "Hitherto, the investigation of fossil-orientation was only used for the topmost surface of fossil mass occurrences, deposited directly on the sea floor. Due to the fast development of virtual methods (e.g., macro-CT, \u00b5-CT, nano-CT, etc.) it became possible, to investigate the interior orientation of such fossil mass occurrences in three-dimensional detail. Although, a series of paleontological studies deal with 3D-visualization of fossil-elements, no mass occurrence has previously been reconstructed three dimensionally for investigating their interior orientation. This study illustrates an interdisciplinary approach of virtual reconstruction, analyses and interpretation of the interior orientation of an ammonoid mass occurrence. The method established herein produces clear and consistent results using planispirally coiled ammonoid shells \u2013 fossils, that so far would have been used only with caution for depositional interpretations. This method can be applied to any kind of fossil mass occurrence, or even other abundant organic elements and particles, to examine their orientation and depositional conditions to conclude on their paleoenvironment, particularly on paleocurrents.\n", "keywords": "3D-visualization\nammonoid mass occurrence\nanalyses and interpretation of the interior orientation\n\u00b5-CT\ndepositional interpretations\nfossil-elements\nfossil mass occurrence\nfossil mass occurrences\ninvestigating their interior orientation\ninvestigation of fossil-orientation\nmacro-CT\nnano-CT\nossil mass occurrences\nother abundant organic elements\nparticles\nplanispirally coiled ammonoid shells\nvirtual methods\nvirtual reconstruction\n"}, {"id": "S0009261412013838", "text": "Experimental studies of the dynamics of individual carbon atoms in graphene have been empowered by the recent progress in aberration-corrected transmission electron microscopy (AC-TEM) capable of sub-\u00c5ngstrom resolution. The examples include AC-TEM observations of the formation and annealing of Stone\u2013Wales defects [1], edge reconstruction [2,3] and formation of a large hole in graphene sheet from a single vacancy defect [3]. The AC-TEM has been also exploited in visualization in real time of the process of self-assembly of graphene nanoribbons from molecular precursors [4,5] and formation of nanometre size hollow protrusion on the nanotube sidewall [6]. Based on AC-TEM observations of transformation of small finite graphene flake into fullerene, a new \u2018top-down\u2019 mechanism for the formation of fullerene under the electron beam radiation has been proposed [7]. The critical step in the proposed \u2018top-down\u2019 mechanism of the fullerene formation is creation of vacancies in small graphene flake as a result of knock-on damage by electrons of the imaging electron beam (e-beam). The subsequent formation of pentagons at the vacancy sites near the edge reduces the number of dangling bonds and triggers the curving process of graphene flake into a closed fullerene structure [7]. Thus, dynamic behaviour of vacancies near graphene edge plays a crucial role in explaining mechanisms of the e-beam assisted self-assembly and structural transformations in graphene-like structures.\n", "keywords": "aberration-corrected transmission electron microscopy\nAC-TEM\nAC-TEM observations\nAC-TEM observations of the formation and annealing of Stone\u2013Wales defects\ncarbon atoms\ncreation of vacancies in small graphene flake\ne-beam assisted self-assembly\nedge reconstruction\nformation of a large hole in graphene sheet from a single vacancy defect\nformation of fullerene\nformation of nanometre size hollow protrusion on the nanotube sidewall\nfullerene\ngraphene\ngraphene-like structures\ngraphene nanoribbons\ngraphene sheet\nknock-on damage by electrons of the imaging electron beam (e-beam)\nmolecular precursors\nnanotube sidewall\nprocess of self-assembly of graphene nanoribbons\nproposed \u2018top-down\u2019 mechanism of the fullerene formation\nsmall finite graphene flake\nstructural transformations in graphene-like structures\nsub-\u00c5ngstrom resolution\nsubsequent formation of pentagons\n\u2018top-down\u2019 mechanism for the formation of fullerene\n"}, {"id": "S0022311515303640", "text": "Ferritic and martensitic steels are candidate materials for use in nuclear reactors [1,2]. The transmutation-created inert gas, especially He, plays an important role in the microstructural evolution of these steels under neutron irradiation. In a previous paper [3] the mechanisms by which He in a perfect body-centred-cubic (bcc) Fe lattice, can agglomerate into bubbles was discussed. It was shown that small He interstitial clusters are highly mobile but become effectively pinned with the emission of Fe interstitials when the clusters contain 5 or more He atoms. Small bubbles up to around 1.5\u00a0nm in diameter can easily form at room temperature from such seed points but larger bubbles are more difficult to form by diffusion alone due to the induced strain in the bcc lattice which increases the energy barriers for diffusion towards the bubbles whilst reducing them in a direction away from the bubbles. Subsequent bubble enlargement can then only occur either through increased temperature or by radiation induced mechanisms which increase the number of vacancies in the bubble and reduce the lattice strain. Emission of interracial loops from such a bubble was not observed in molecular dynamics simulations.\n", "keywords": "bcc\nbcc lattice\nbody-centred-cubic\nbubble\nbubble enlargement\nbubbles\ndiffusion\nEmission of interracial loops\nFe interstitials\nFe lattice\nFerritic and martensitic steels\nHe\nHe atoms\nHe interstitial clusters\ninduced strain\nlattice strain\nmechanisms by which He in a perfect body-centred-cubic (bcc) Fe lattice, can agglomerate into bubbles\nmicrostructural evolution of these steels under neutron irradiation\nmolecular dynamics simulations\nneutron irradiation\nnuclear reactors\nradiation induced mechanisms\nreduce the lattice strain\nsteels\ntransmutation-created inert gas\n"}, {"id": "S0167273814004408", "text": "Thin MIEC layers of GDC and STFO on single-crystalline YSZ substrates were exposed to H2/H218O atmosphere for thermally and electrochemically driven tracer exchange experiments. Rectangular noble metal thin film current collectors were deposited on top and beneath the MIEC layer and used for polarization. The lateral distribution of the tracer revealed several interesting features: (i) In case of thermal tracer exchange, an enhanced tracer fraction is found on top of the metallic current collector due to its ionically blocking nature. At the edges of the current collector, the concentration of 18O decreases with a finite step width that is correlated with in-plane diffusion of oxygen ions. (ii) Due to the low electronic conductivity of STFO and GDC, the MIEC area that is influenced by an applied bias is restricted to a region close to the current collector. The width of this active region depends on the bias. It amounts to only 10\u201315\u03bcm for STFO but more than 100\u03bcm for GDC at a cathodic bias of \u2212500mV. (iii) Not only enhanced tracer incorporation due to cathodic bias but also reduced incorporation due to anodic bias could be experimentally resolved in the active region.\n", "keywords": "18O\napplied bias\ncurrent collector\ndeposited on top and beneath the MIEC layer\nenhanced tracer incorporation\nexposed to H2/H218O atmosphere\nGDC\nH2/H218O atmosphere\nin-plane diffusion of oxygen ions\nlateral distribution of the tracer\nlow electronic conductivity\nmetallic current collector\nMIEC layer\noxygen ions\npolarization\nRectangular noble metal thin film current collectors\nreduced incorporation\nsingle-crystalline YSZ substrates\nSTFO\nthermal tracer exchange\nThin MIEC layers\ntracer exchange experiments\ntracer fraction\nwidth of this active region\n"}, {"id": "S0167273814004408", "text": "Thin MIEC layers of GDC and STFO on single-crystalline YSZ substrates were exposed to H2/H218O atmosphere for thermally and electrochemically driven tracer exchange experiments. Rectangular noble metal thin film current collectors were deposited on top and beneath the MIEC layer and used for polarization. The lateral distribution of the tracer revealed several interesting features: (i) In case of thermal tracer exchange, an enhanced tracer fraction is found on top of the metallic current collector due to its ionically blocking nature. At the edges of the current collector, the concentration of 18O decreases with a finite step width that is correlated with in-plane diffusion of oxygen ions. (ii) Due to the low electronic conductivity of STFO and GDC, the MIEC area that is influenced by an applied bias is restricted to a region close to the current collector. The width of this active region depends on the bias. It amounts to only 10\u201315\u03bcm for STFO but more than 100\u03bcm for GDC at a cathodic bias of \u2212500mV. (iii) Not only enhanced tracer incorporation due to cathodic bias but also reduced incorporation due to anodic bias could be experimentally resolved in the active region.\n", "keywords": "18O\napplied bias\ncurrent collector\ndeposited on top and beneath the MIEC layer\nenhanced tracer incorporation\nexposed to H2/H218O atmosphere\nGDC\nH2/H218O atmosphere\nin-plane diffusion of oxygen ions\nlateral distribution of the tracer\nlow electronic conductivity\nmetallic current collector\nMIEC layer\noxygen ions\npolarization\nRectangular noble metal thin film current collectors\nreduced incorporation\nsingle-crystalline YSZ substrates\nSTFO\nthermal tracer exchange\nThin MIEC layers\ntracer exchange experiments\ntracer fraction\nwidth of this active region\n"}, {"id": "S0377025715000993", "text": "Equilibrium surface tension was measured at 21\u00b0C with a SITA pro line t-15 bubble tensiometer. Rheological measurements were performed with an ARES rheometer at shear rates up to 15s\u22121 and with a piezo axial vibrator [21] (PAV) at frequencies up to 6kHz. Table 1 shows the measured values of viscosity (the real component \u03b7\u2032 of complex viscosity) at 1s\u22121 and 4000s\u22121 and of surface tension for the solutions with and without the surfactant mixture. For the most concentrated (1.1wt%) solution, viscosity fell from >60mPas at low shear rate to about 4mPas at the highest shear rates. The PEDOT:PSS fluids also exhibited elasticity that steadily reduced with increasing frequency [4]. All the aqueous PEDOT:PSS solutions shear-thinned significantly, but the presence of surfactants did not affect the trends in the rheological behaviour, particularly at the higher frequencies (10\u20134000s\u22121).\n", "keywords": "ARES rheometer\nEquilibrium surface tension\nmost concentrated (1.1wt%) solution\nPAV\nPEDOT:PSS fluids\nPEDOT:PSS solutions\npiezo axial vibrator\nRheological measurements\nSITA pro line t-15 bubble tensiometer\nsurface tension\nvalues of viscosity\n"}, {"id": "S0370157314001318", "text": "Despite the ubiquity of time-dependent dynamical systems in nature, there has been relatively little work done on the analysis of time series from such systems. Mathematically they are known as non-autonomous systems, which are named as such because, unlike autonomous systems, in addition to the points in space over which they are observed they are also influenced by the points in time. Recently there has been much work on the direct \u2018bottom-up\u2019 approach to these systems, which includes the introduction of a subclass known as chronotaxic systems that are able to model the stable but time-varying frequencies of oscillations in living systems\u00a0 [8,9]. In contrast, the time series analysis of these systems, referred to as the inverse or \u2018top-down\u2019 approach, has not been studied in detail before. This is partly because non-autonomous systems can still be analysed in the same way as other types of systems in both the deterministic\u00a0 [10] and the stochastic\u00a0 [11] regime. However, it is now argued that this type of analysis is insufficient and that an entirely new analytical framework is required to provide a more useful picture of such systems. In the case of chronotaxic systems some methods have already been developed for the inverse approach and they have shown to be useful in analysing heart rate variability\u00a0 [12]. A general and dedicated procedure for analysing non-autonomous systems has still not been tackled though.\n", "keywords": "analysis of time series\nanalytical framework\nautonomous systems\n\u2018bottom-up\u2019 approach\nchronotaxic systems\ndeterministic\u00a0\nheart rate variability\u00a0\ninverse\ninverse approach\nliving systems\nmodel the stable but time-varying frequencies\nnon-autonomous systems\nstochastic\u00a0\ntime-dependent dynamical systems\ntime series analysis\n\u2018top-down\u2019\n"}, {"id": "S0370157314001318", "text": "Despite the ubiquity of time-dependent dynamical systems in nature, there has been relatively little work done on the analysis of time series from such systems. Mathematically they are known as non-autonomous systems, which are named as such because, unlike autonomous systems, in addition to the points in space over which they are observed they are also influenced by the points in time. Recently there has been much work on the direct \u2018bottom-up\u2019 approach to these systems, which includes the introduction of a subclass known as chronotaxic systems that are able to model the stable but time-varying frequencies of oscillations in living systems\u00a0 [8,9]. In contrast, the time series analysis of these systems, referred to as the inverse or \u2018top-down\u2019 approach, has not been studied in detail before. This is partly because non-autonomous systems can still be analysed in the same way as other types of systems in both the deterministic\u00a0 [10] and the stochastic\u00a0 [11] regime. However, it is now argued that this type of analysis is insufficient and that an entirely new analytical framework is required to provide a more useful picture of such systems. In the case of chronotaxic systems some methods have already been developed for the inverse approach and they have shown to be useful in analysing heart rate variability\u00a0 [12]. A general and dedicated procedure for analysing non-autonomous systems has still not been tackled though.\n", "keywords": "analysis of time series\nanalytical framework\nautonomous systems\n\u2018bottom-up\u2019 approach\nchronotaxic systems\ndeterministic\u00a0\nheart rate variability\u00a0\ninverse\ninverse approach\nliving systems\nmodel the stable but time-varying frequencies\nnon-autonomous systems\nstochastic\u00a0\ntime-dependent dynamical systems\ntime series analysis\n\u2018top-down\u2019\n"}, {"id": "S2212667814001208", "text": "Improving as well as evaluating the performance of High Performance Computing (HPC) applications by migrating them to Cloud environments are widely considered as critical issues in the field of high performance and Cloud computing. However, poor network performance, heterogeneous and dynamic environments are some series of pitfalls for execution of HPC applications in Cloud. This paper proposes a new approach to improve the performance and scalability of HPC applications on Amazon's HPC Cloud. The evidence from our approach points a significant improvement in speed up and scale up with the response rate of more than 20 percent parallel ef\ufb01ciency on the Cloud in comparison to dedicated HPC cluster. We state that the EC2 Cloud system is a feasible platform for deploying on-demand, small sized HPC applications.", "keywords": "Amazon's HPC Cloud\nCloud\nCloud environments\ndeploying on-demand, small sized HPC applications\nEC2 Cloud system\nexecution of HPC applications in Cloud\nheterogeneous and dynamic environments\nhigh performance and Cloud computing\nHigh Performance Computing\nHPC\nHPC applications\nHPC cluster\nImproving as well as evaluating the performance\nmigrating them to Cloud environments\nparallel ef\ufb01ciency\npoor network performance\nproposes a new approach to improve the performance and scalability of HPC applications\nresponse rate\nscale up\nspeed up\n"}, {"id": "S2212667814001208", "text": "Improving as well as evaluating the performance of High Performance Computing (HPC) applications by migrating them to Cloud environments are widely considered as critical issues in the field of high performance and Cloud computing. However, poor network performance, heterogeneous and dynamic environments are some series of pitfalls for execution of HPC applications in Cloud. This paper proposes a new approach to improve the performance and scalability of HPC applications on Amazon's HPC Cloud. The evidence from our approach points a significant improvement in speed up and scale up with the response rate of more than 20 percent parallel ef\ufb01ciency on the Cloud in comparison to dedicated HPC cluster. We state that the EC2 Cloud system is a feasible platform for deploying on-demand, small sized HPC applications.", "keywords": "Amazon's HPC Cloud\nCloud\nCloud environments\ndeploying on-demand, small sized HPC applications\nEC2 Cloud system\nexecution of HPC applications in Cloud\nheterogeneous and dynamic environments\nhigh performance and Cloud computing\nHigh Performance Computing\nHPC\nHPC applications\nHPC cluster\nImproving as well as evaluating the performance\nmigrating them to Cloud environments\nparallel ef\ufb01ciency\npoor network performance\nproposes a new approach to improve the performance and scalability of HPC applications\nresponse rate\nscale up\nspeed up\n"}, {"id": "S2212667814001208", "text": "Improving as well as evaluating the performance of High Performance Computing (HPC) applications by migrating them to Cloud environments are widely considered as critical issues in the field of high performance and Cloud computing. However, poor network performance, heterogeneous and dynamic environments are some series of pitfalls for execution of HPC applications in Cloud. This paper proposes a new approach to improve the performance and scalability of HPC applications on Amazon's HPC Cloud. The evidence from our approach points a significant improvement in speed up and scale up with the response rate of more than 20 percent parallel ef\ufb01ciency on the Cloud in comparison to dedicated HPC cluster. We state that the EC2 Cloud system is a feasible platform for deploying on-demand, small sized HPC applications.", "keywords": "Amazon's HPC Cloud\nCloud\nCloud environments\ndeploying on-demand, small sized HPC applications\nEC2 Cloud system\nexecution of HPC applications in Cloud\nheterogeneous and dynamic environments\nhigh performance and Cloud computing\nHigh Performance Computing\nHPC\nHPC applications\nHPC cluster\nImproving as well as evaluating the performance\nmigrating them to Cloud environments\nparallel ef\ufb01ciency\npoor network performance\nproposes a new approach to improve the performance and scalability of HPC applications\nresponse rate\nscale up\nspeed up\n"}, {"id": "S2212667814001208", "text": "Improving as well as evaluating the performance of High Performance Computing (HPC) applications by migrating them to Cloud environments are widely considered as critical issues in the field of high performance and Cloud computing. However, poor network performance, heterogeneous and dynamic environments are some series of pitfalls for execution of HPC applications in Cloud. This paper proposes a new approach to improve the performance and scalability of HPC applications on Amazon's HPC Cloud. The evidence from our approach points a significant improvement in speed up and scale up with the response rate of more than 20 percent parallel ef\ufb01ciency on the Cloud in comparison to dedicated HPC cluster. We state that the EC2 Cloud system is a feasible platform for deploying on-demand, small sized HPC applications.", "keywords": "Amazon's HPC Cloud\nCloud\nCloud environments\ndeploying on-demand, small sized HPC applications\nEC2 Cloud system\nexecution of HPC applications in Cloud\nheterogeneous and dynamic environments\nhigh performance and Cloud computing\nHigh Performance Computing\nHPC\nHPC applications\nHPC cluster\nImproving as well as evaluating the performance\nmigrating them to Cloud environments\nparallel ef\ufb01ciency\npoor network performance\nproposes a new approach to improve the performance and scalability of HPC applications\nresponse rate\nscale up\nspeed up\n"}, {"id": "S2212667814001208", "text": "Improving as well as evaluating the performance of High Performance Computing (HPC) applications by migrating them to Cloud environments are widely considered as critical issues in the field of high performance and Cloud computing. However, poor network performance, heterogeneous and dynamic environments are some series of pitfalls for execution of HPC applications in Cloud. This paper proposes a new approach to improve the performance and scalability of HPC applications on Amazon's HPC Cloud. The evidence from our approach points a significant improvement in speed up and scale up with the response rate of more than 20 percent parallel ef\ufb01ciency on the Cloud in comparison to dedicated HPC cluster. We state that the EC2 Cloud system is a feasible platform for deploying on-demand, small sized HPC applications.", "keywords": "Amazon's HPC Cloud\nCloud\nCloud environments\ndeploying on-demand, small sized HPC applications\nEC2 Cloud system\nexecution of HPC applications in Cloud\nheterogeneous and dynamic environments\nhigh performance and Cloud computing\nHigh Performance Computing\nHPC\nHPC applications\nHPC cluster\nImproving as well as evaluating the performance\nmigrating them to Cloud environments\nparallel ef\ufb01ciency\npoor network performance\nproposes a new approach to improve the performance and scalability of HPC applications\nresponse rate\nscale up\nspeed up\n"}, {"id": "S2212667814001208", "text": "Improving as well as evaluating the performance of High Performance Computing (HPC) applications by migrating them to Cloud environments are widely considered as critical issues in the field of high performance and Cloud computing. However, poor network performance, heterogeneous and dynamic environments are some series of pitfalls for execution of HPC applications in Cloud. This paper proposes a new approach to improve the performance and scalability of HPC applications on Amazon's HPC Cloud. The evidence from our approach points a significant improvement in speed up and scale up with the response rate of more than 20 percent parallel ef\ufb01ciency on the Cloud in comparison to dedicated HPC cluster. We state that the EC2 Cloud system is a feasible platform for deploying on-demand, small sized HPC applications.", "keywords": "Amazon's HPC Cloud\nCloud\nCloud environments\ndeploying on-demand, small sized HPC applications\nEC2 Cloud system\nexecution of HPC applications in Cloud\nheterogeneous and dynamic environments\nhigh performance and Cloud computing\nHigh Performance Computing\nHPC\nHPC applications\nHPC cluster\nImproving as well as evaluating the performance\nmigrating them to Cloud environments\nparallel ef\ufb01ciency\npoor network performance\nproposes a new approach to improve the performance and scalability of HPC applications\nresponse rate\nscale up\nspeed up\n"}, {"id": "S2212667814001208", "text": "Improving as well as evaluating the performance of High Performance Computing (HPC) applications by migrating them to Cloud environments are widely considered as critical issues in the field of high performance and Cloud computing. However, poor network performance, heterogeneous and dynamic environments are some series of pitfalls for execution of HPC applications in Cloud. This paper proposes a new approach to improve the performance and scalability of HPC applications on Amazon's HPC Cloud. The evidence from our approach points a significant improvement in speed up and scale up with the response rate of more than 20 percent parallel ef\ufb01ciency on the Cloud in comparison to dedicated HPC cluster. We state that the EC2 Cloud system is a feasible platform for deploying on-demand, small sized HPC applications.", "keywords": "Amazon's HPC Cloud\nCloud\nCloud environments\ndeploying on-demand, small sized HPC applications\nEC2 Cloud system\nexecution of HPC applications in Cloud\nheterogeneous and dynamic environments\nhigh performance and Cloud computing\nHigh Performance Computing\nHPC\nHPC applications\nHPC cluster\nImproving as well as evaluating the performance\nmigrating them to Cloud environments\nparallel ef\ufb01ciency\npoor network performance\nproposes a new approach to improve the performance and scalability of HPC applications\nresponse rate\nscale up\nspeed up\n"}, {"id": "S2212667814001208", "text": "Improving as well as evaluating the performance of High Performance Computing (HPC) applications by migrating them to Cloud environments are widely considered as critical issues in the field of high performance and Cloud computing. However, poor network performance, heterogeneous and dynamic environments are some series of pitfalls for execution of HPC applications in Cloud. This paper proposes a new approach to improve the performance and scalability of HPC applications on Amazon's HPC Cloud. The evidence from our approach points a significant improvement in speed up and scale up with the response rate of more than 20 percent parallel ef\ufb01ciency on the Cloud in comparison to dedicated HPC cluster. We state that the EC2 Cloud system is a feasible platform for deploying on-demand, small sized HPC applications.", "keywords": "Amazon's HPC Cloud\nCloud\nCloud environments\ndeploying on-demand, small sized HPC applications\nEC2 Cloud system\nexecution of HPC applications in Cloud\nheterogeneous and dynamic environments\nhigh performance and Cloud computing\nHigh Performance Computing\nHPC\nHPC applications\nHPC cluster\nImproving as well as evaluating the performance\nmigrating them to Cloud environments\nparallel ef\ufb01ciency\npoor network performance\nproposes a new approach to improve the performance and scalability of HPC applications\nresponse rate\nscale up\nspeed up\n"}, {"id": "S1877750315000575", "text": "There are some relevant studies on information dissemination in transportation systems using simulations. One category of studies look at how either local information (only about the neighbours) or global information (about the entire network) affects the global network performance. Our approach is different in the sense that we investigate the impact of information on the global network performance depending on the fraction of people that receive information. We analyse what is the effect of real time information dissemination and explain why this effect appears. Information is disseminated in real time and contains global details about how congested the roads are. This approach is important as it gives insights on the impact that massive use of real-time information can have on traffic. This can be useful for building more intelligent traffic control mechanisms where information is a steering tool.\n", "keywords": "analyse what is the effect of real time information dissemination\nbuilding more intelligent traffic control mechanisms\ninformation dissemination in transportation systems\ninvestigate the impact of information on the global network performance\nmassive use of real-time information\nsimulations\n"}, {"id": "S1364815216303541", "text": "As a particular case of survey data, we used the iUTAH \u201cUtah Water Survey,\u201d which was implemented by participating researchers from several Utah institutions of higher education. The objectives of the survey were to document how a representative cross-section of Utah's adult population thinks about water issues. The survey included three core blocks of questions: perceptions of the adequacy of local water supplies, perceptions of the quality of local water resources, and concern about a range of water and non-water issues. A number of additional questions captured information about respondents' familiarity with water cost, lawn-watering behaviors, participation in water based recreation, and demographic attributes. Supplementary material to this paper includes a document with a description of the dataset as a whole, a document containing the complete survey instrument, and two data files containing the results and an associated codebook (see Section 4.3).\n", "keywords": "associated codebook\nawn-watering behaviors\ncomplete survey instrument\nconcern about a range of water and non-water issues\ndemographic attributes\ndescription of the dataset\nfamiliarity with water cost\niUTAH \u201cUtah Water Survey,\u201d\nparticipation in water based recreation\nperceptions of the adequacy of local water supplies\nperceptions of the quality of local water resources\nSupplementary material\nsurvey\nUtah's adult population thinks about water issues\n"}, {"id": "S0254058415304235", "text": "The observed conductivity of A2FeMoO6\u2013\u03b4 (A\u00a0=\u00a0Ca, Sr, Ba) [7] was linked to a potential double exchange mechanism, with conduction between Fe3+-O-Mo-O-Fe2+. Double-exchange mechanisms, as proposed by Zener [23], posit that electron transfer between ions in different oxidation states may be facilitated if the electron does not have to alter its spin state. Replacement of Mo with Fe in this mechanism would be expected to result in a reduction of the conductivity through reduction of the available percolation pathways, unless delocalisation of Fe electrons through Fe2+-O-Fe3+ exchange could also occur. Double exchange mechanisms have been observed previously for mixed valent iron in iron oxides [24], and, as iron is known to exist in a mixed valent state for Ca2\u2013xSrxFeMoO6\u2013\u03b4 [25], this provides a plausible explanation for the observed metallic conductivity. Band structure calculations and Mossbauer spectroscopy could be utilised to further elucidate the conduction mechanism for these compounds, however this is outside the scope of this enquiry.\n", "keywords": "A2FeMoO6\u2013\u03b4\nBa\nBand structure calculations\nCa\nCa2\u2013xSrxFeMoO6\u2013\u03b4\ncompounds\nconduction mechanism\ndelocalisation of Fe electrons\ndouble exchange mechanism\nDouble exchange mechanisms\nDouble-exchange mechanisms\nelectron\nelectron transfer\nFe\nFe2+-O-Fe3+\nFe2+-O-Fe3+ exchange\nFe3+-O-Mo-O-Fe2+\nFe electrons\nions\niron\niron in iron oxides\nmetallic conductivity\nMo\nMossbauer spectroscopy\nobserved conductivity\noxidation\npercolation pathways\nreduction of the available percolation pathways\nreduction of the conductivity\nReplacement\nSr\n"}, {"id": "S0029549314002854", "text": "The pipes under pressure in the RCS or connected to RCS are usually made of austenitic or austenitic & ferritic stainless steel. Most connections are welded. The pipes may be exposed to various degradation phenomena (diverse hazards, mechanical fatigue, thermal fatigue, stress corrosion, etc.). Event screening in the databases showed a total of 116 events (33 related to cracks and 83 to leaks). Three main causes for failure were identified, namely, fatigue, corrosion and the presence of manufacturing defects. Human factor induced defects proved to have little impact \u2013 less than 10% of the cases could be attributed to operation errors. Fatigue was found being induced by several factors: excessive vibration, pressure shocks and the thermal regime of operating the pipe, as well as by combinations of these factors. Corrosion was induced, in most of the cases, by a non-appropriate choice of alloys while not taking into account the chemical parameters of the fluid inside pipes. Manufacturing defects mostly dealt with welding related problems and deviation from the design documentation during post-weld heat treatment.\n", "keywords": "alloys\naustenitic or austenitic & ferritic stainless steel\nEvent screening\nfluid\npipe\npipes\nRCS\n"}, {"id": "S0029549314002854", "text": "The pipes under pressure in the RCS or connected to RCS are usually made of austenitic or austenitic & ferritic stainless steel. Most connections are welded. The pipes may be exposed to various degradation phenomena (diverse hazards, mechanical fatigue, thermal fatigue, stress corrosion, etc.). Event screening in the databases showed a total of 116 events (33 related to cracks and 83 to leaks). Three main causes for failure were identified, namely, fatigue, corrosion and the presence of manufacturing defects. Human factor induced defects proved to have little impact \u2013 less than 10% of the cases could be attributed to operation errors. Fatigue was found being induced by several factors: excessive vibration, pressure shocks and the thermal regime of operating the pipe, as well as by combinations of these factors. Corrosion was induced, in most of the cases, by a non-appropriate choice of alloys while not taking into account the chemical parameters of the fluid inside pipes. Manufacturing defects mostly dealt with welding related problems and deviation from the design documentation during post-weld heat treatment.\n", "keywords": "alloys\naustenitic or austenitic & ferritic stainless steel\nEvent screening\nfluid\npipe\npipes\nRCS\n"}, {"id": "S2212671612000698", "text": "In Obstacle detection is based on inverse perspective mapping and homography. Obstacle classification is based on fuzzy neural network. The estimation of the vanishing point relies on feature extraction strategy. The method exploits the geometrical relations between the elements in the scene so that obstacle can be detected. The estimated homography of the road plane between successive images is used for image alignment. A new fuzzy decision fusion method with fuzzy attribution for obstacle detection and classification application is described The fuzzy decision function modifies parameters with auto-adapted algorithm to get better classification probability It is shown that the method can achieve better classification result.", "keywords": "auto-adapted algorithm\nclassification\ndetected\nelements\nestimated homography\nestimation of the vanishing point\nfeature extraction strategy\nfuzzy attribution for obstacle detection and classification application\nfuzzy decision fusion method\nfuzzy neural network\ngeometrical relations\nimage alignment\nimages\ninverse perspective mapping and homography\nobstacle\nObstacle classification\nObstacle detection\nobstacle detection and classification application\nroad plane\nscene\nThe fuzzy decision function\n"}, {"id": "S0167273813006735", "text": "While impedance spectroscopy is a quite common method to investigate mixed conducting thin film electrodes, [6,10\u201312] oxygen tracer experiments are often performed on bulk samples [13\u201316]. Recently, several IEDP measurements of mixed conducting cathode materials were published with the oxide films being deposited on insulating substrates [17\u201319]. However, to the best of the authors' knowledge no study so far reported experiments with both techniques being applied on the same films at the same temperature. This contribution reports the results of a study applying EIS and IEDP to one and the same La0.6Sr0.4CoO3\u2212\u03b4 (LSC) thin film in order to get complementary results on the resistive contributions of the oxygen reduction kinetics on such films. As electrical measurements require an oxygen ion conductor, yttria stabilized zirconia (YSZ) was used as substrate for LSC films with two different grain sizes. Quantitative material parameters are deduced from both types of experiments and comparison of the data allowed testing the appropriateness of analysis models.\n", "keywords": "analysis models\napplying EIS and IEDP to one and the same La0.6Sr0.4CoO3\u2212\u03b4 (LSC) thin film\nappropriateness of analysis models\nbulk samples\ncomparison of the data\nEIS\nelectrical measurements\nfilms\ngrain\nIEDP\nIEDP measurements\nimpedance spectroscopy\ninsulating substrates\nLa0.6Sr0.4CoO3\u2212\u03b4\nLSC\nLSC films\nmixed conducting cathode materials\nmixed conducting thin film electrodes\noxide films\noxygen\noxygen ion conductor\noxygen reduction kinetics\noxygen tracer experiments\nQuantitative material parameters\nresistive contributions of the oxygen reduction kinetics\nthin film\nYSZ\nyttria stabilized zirconia\n"}, {"id": "S0167273813006735", "text": "While impedance spectroscopy is a quite common method to investigate mixed conducting thin film electrodes, [6,10\u201312] oxygen tracer experiments are often performed on bulk samples [13\u201316]. Recently, several IEDP measurements of mixed conducting cathode materials were published with the oxide films being deposited on insulating substrates [17\u201319]. However, to the best of the authors' knowledge no study so far reported experiments with both techniques being applied on the same films at the same temperature. This contribution reports the results of a study applying EIS and IEDP to one and the same La0.6Sr0.4CoO3\u2212\u03b4 (LSC) thin film in order to get complementary results on the resistive contributions of the oxygen reduction kinetics on such films. As electrical measurements require an oxygen ion conductor, yttria stabilized zirconia (YSZ) was used as substrate for LSC films with two different grain sizes. Quantitative material parameters are deduced from both types of experiments and comparison of the data allowed testing the appropriateness of analysis models.\n", "keywords": "analysis models\napplying EIS and IEDP to one and the same La0.6Sr0.4CoO3\u2212\u03b4 (LSC) thin film\nappropriateness of analysis models\nbulk samples\ncomparison of the data\nEIS\nelectrical measurements\nfilms\ngrain\nIEDP\nIEDP measurements\nimpedance spectroscopy\ninsulating substrates\nLa0.6Sr0.4CoO3\u2212\u03b4\nLSC\nLSC films\nmixed conducting cathode materials\nmixed conducting thin film electrodes\noxide films\noxygen\noxygen ion conductor\noxygen reduction kinetics\noxygen tracer experiments\nQuantitative material parameters\nresistive contributions of the oxygen reduction kinetics\nthin film\nYSZ\nyttria stabilized zirconia\n"}, {"id": "S0167273813006735", "text": "While impedance spectroscopy is a quite common method to investigate mixed conducting thin film electrodes, [6,10\u201312] oxygen tracer experiments are often performed on bulk samples [13\u201316]. Recently, several IEDP measurements of mixed conducting cathode materials were published with the oxide films being deposited on insulating substrates [17\u201319]. However, to the best of the authors' knowledge no study so far reported experiments with both techniques being applied on the same films at the same temperature. This contribution reports the results of a study applying EIS and IEDP to one and the same La0.6Sr0.4CoO3\u2212\u03b4 (LSC) thin film in order to get complementary results on the resistive contributions of the oxygen reduction kinetics on such films. As electrical measurements require an oxygen ion conductor, yttria stabilized zirconia (YSZ) was used as substrate for LSC films with two different grain sizes. Quantitative material parameters are deduced from both types of experiments and comparison of the data allowed testing the appropriateness of analysis models.\n", "keywords": "analysis models\napplying EIS and IEDP to one and the same La0.6Sr0.4CoO3\u2212\u03b4 (LSC) thin film\nappropriateness of analysis models\nbulk samples\ncomparison of the data\nEIS\nelectrical measurements\nfilms\ngrain\nIEDP\nIEDP measurements\nimpedance spectroscopy\ninsulating substrates\nLa0.6Sr0.4CoO3\u2212\u03b4\nLSC\nLSC films\nmixed conducting cathode materials\nmixed conducting thin film electrodes\noxide films\noxygen\noxygen ion conductor\noxygen reduction kinetics\noxygen tracer experiments\nQuantitative material parameters\nresistive contributions of the oxygen reduction kinetics\nthin film\nYSZ\nyttria stabilized zirconia\n"}, {"id": "S096386951400070X", "text": "This paper has highlighted a band of frequencies, outside the conventional operation range, and close to electrical resonance of an eddy current probe, where the magnitude of impedance SNR reaches a peak. The SNR of scans of three slots of varying depth were enhanced by a factor of up to 3.7, from the SNR measured at 1MHz. This is a result of a defect-decoupling resonance-shift effect and is referred to as the near electrical resonance signal enhancement (NERSE) phenomenon. NERSE frequency operation has significant potential for ECT inspection, and opens up a range of investigative possibilities. Within this investigation, only the magnitude of the electrical impedance has been analyzed. An immediate extension of this investigation will be to consider phase information, and determine whether a similar exploitable NERSE effect exists.\n", "keywords": "defect-decoupling resonance-shift effect\nECT inspection\neddy current probe\nelectrical impedance\nelectrical resonance of an eddy current probe\nhighlighted a band of frequencies\nimpedance SNR\nnear electrical resonance signal enhancement\nNERSE\nSNR\n"}, {"id": "S096386951400070X", "text": "This paper has highlighted a band of frequencies, outside the conventional operation range, and close to electrical resonance of an eddy current probe, where the magnitude of impedance SNR reaches a peak. The SNR of scans of three slots of varying depth were enhanced by a factor of up to 3.7, from the SNR measured at 1MHz. This is a result of a defect-decoupling resonance-shift effect and is referred to as the near electrical resonance signal enhancement (NERSE) phenomenon. NERSE frequency operation has significant potential for ECT inspection, and opens up a range of investigative possibilities. Within this investigation, only the magnitude of the electrical impedance has been analyzed. An immediate extension of this investigation will be to consider phase information, and determine whether a similar exploitable NERSE effect exists.\n", "keywords": "defect-decoupling resonance-shift effect\nECT inspection\neddy current probe\nelectrical impedance\nelectrical resonance of an eddy current probe\nhighlighted a band of frequencies\nimpedance SNR\nnear electrical resonance signal enhancement\nNERSE\nSNR\n"}, {"id": "S0263822312001468", "text": "Recently together with structural efficiency, passenger safety is also an important issue in application of material to transportation industries. Hence, the crashworthiness parameters are introducing to predict the capability of structure to prevent the massive damage and protect the passenger in the event of a crash. Crashworthiness parameters for various thin-walled tubes made from metal or fibre/resin composites in different geometries have been studied. A critical difference of tubular composites failure modes compared with metallic is the brittle collapse. In addition, in composites, tubular failure modes are involved with micro-cracking development, delamination, fibre breakage, etc., instead of plastic deformation. Implementation of composite materials in the field of crashworthiness is attributed to Hull, who in 80s and 90s of the last century studied extensively the crushing behaviour of fibre reinforced composite material. He found that the composite materials absorbed high energy in the face of the fracture surface energy mechanism rather than plastic deformation as observed for metals [1]. This observation has inspired others to further investigation about crashworthiness characteristics of composite materials. Studies have examined the axial crushing behaviour of fibre-reinforced tubes [2], fibreglass tubes [3,4], PVC tubes [5] and carbon fibre reinforced plastic (CFRP) tubes [6].\n", "keywords": "axial crushing behaviour\nbrittle collapse\ncarbon fibre reinforced plastic\ncarbon fibre reinforced plastic (CFRP) tubes\nCFRP\ncomposite materials\ncomposites\ncrashworthiness characteristics\ncrashworthiness parameters\nCrashworthiness parameters\ncrushing behaviour\ndelamination\nfibre breakage\nfibreglass tubes\nfibre reinforced composite material\nfibre-reinforced tubes\nfibre/resin composites\nfracture surface energy\nImplementation of composite materials\nmetal\nmetals\nmicro-cracking development\npassenger safety\nplastic deformation\nprevent the massive damage\nprotect the passenger\nPVC tubes\nstructural efficiency\ntransportation industries\ntubular composites failure\ntubular failure modes\n"}, {"id": "S0370269303017222", "text": "In the bag model and in linear or harmonic oscillator confining potentials, the first excited S-state lies above the lowest P-state, making the predicted Roper mass heavier than the lightest negative parity baryon mass. Pairwise spin-dependent interactions must reverse the level ordering. As mentioned earlier, color-spin interactions fail in this regard\u00a0[29], while flavor-spin interactions produce the desired effect. Since the q3 color wave function is antisymmetric, the flavor-spin-orbital wave function is totally symmetric. For all quarks in an S-state, the flavor-spin wave function is totally symmetric all by itself and leads to the most attractive flavor-spin interaction. If one quark is in a P-state, the orbital wave function is mixed symmetry and so is the flavor-spin wave function, and the flavor-spin interaction is a less attractive. In the SU(3)F symmetric case, Eq.\u00a0(1), one obtains mass splittings (2)\u0394M\u03c7=\u221214C\u03c7,N(939),N\u2217(1440),\u22124C\u03c7,\u0394(1232),\u22122C\u03c7,N\u2217(1535). Here we have approximated the N\u2217(1535) as a state with total quark spin-1/2.\n", "keywords": "bag model\ncolor-spin interactions\nflavor-spin interaction\nflavor-spin interactions\nflavor-spin-orbital wave function\nflavor-spin wave function\nlevel ordering\nmass splittings\norbital wave function\noscillator confining potentials\nPairwise spin-dependent interactions\nq3 color wave function\nquark\nquarks\nquark spin\n"}, {"id": "S0370269304006161", "text": "Absorption events through the charged current reactions (2)\u03bde+40Ar\u2192e\u2212+40K\u2217and\u03bd\u0304e+40Ar\u2192e++40Cl\u2217. There is some uncertainty in predicting e\u2212(e+) event rates for these processes which arise due to the nuclear model dependencies of the absorption cross section and the treatment of the Coulomb distortion of electron (positron) in the field of the residual nucleus. The nuclear absorption cross section for the charged current neutrino reactions in 40Ar relevant to supernova neutrino energies was first calculated by Raghavan\u00a0[10] and Bahcall et al.\u00a0[11] for Fermi transitions leading to isobaric analogue state (IAS) at 4.38\u00a0MeV in 40K\u2217. Later Ormand et al.\u00a0[12] used a shell model to calculate the Fermi and Gamow\u2013Teller transitions. In these calculations Fermi function F(Z,Ee) was used to take into account the Coulomb effects. In a recent paper Bueno et al.\u00a0[13] make use of a calculation by Martinez-Pinedo et al.\u00a0[14] who use a shell model for Fermi and Gamow\u2013Teller transitions and a continuum random phase approximation (CRPA) for forbidden transitions to calculate the absorption cross sections. In this calculation the Coulomb distortion of the produced electron is treated with a hybrid model where a Fermi function is used for lower electron energies and modified effective momentum approximation (MEMA) for higher electron energies\u00a0[14\u201317]. In a recent work Bhattacharya et al.\u00a0[18] have measured the Fermi and Gamow\u2013Teller transition strengths leading to excited states up to 6\u00a0MeV in 40K\u2217 and obtained the neutrino absorption cross section for supernova neutrinos in 40Ar.\n", "keywords": "4.38\u00a0MeV in 40K\nAbsorption events through the charged current reactions\ncalculation by Martinez-Pinedo et al.\ncontinuum random phase approximation\nCoulomb effects\nCRPA\nelectron\nFermi and Gamow\u2013Teller transition strengths\nIAS\nisobaric analogue state\nMEMA\nmodified effective momentum approximation\nneutrino absorption cross section\nnuclear absorption cross section for the charged current neutrino reactions\nnuclear model dependencies\npositron\npredicting e\u2212(e+) event rates\nresidual nucleus\nshell model\nshell model for Fermi and Gamow\u2013Teller transitions\nsupernova neutrino energies\nthe absorption cross sections\nthe Fermi and Gamow\u2013Teller transitions\ntreatment of the Coulomb distortion\n"}, {"id": "S0022311515301069", "text": "The fluence of each capsule was determined by using activation monitor sets. These monitor sets consist of different metal wire pieces that have an activation reaction at a specific energy range. The different activation energies are chosen in such a way that the spectrum can be reconstructed. In BODEX, each capsule contained a flux monitor set on the \u2018back side\u2019 (as seen from the core) and one on the front side, positioned at the central height of the capsules. Additionally, one detector was placed at the top and one at the bottom, resulting in a total of 6 monitor sets per leg. The fluence in each capsule was determined as the average between the two flux monitor located in each capsule. The sets have been analysed by determining the activation of each wire piece, which indicates the fluence of a specific energy range. Table\u00a03 show the values of the fluences for the two capsules containing molybdenum.\n", "keywords": "activation\nactivation monitor sets.\ncapsule\ncapsules\ndetector\ndetermining the activation of each wire piece\nfluence\nfluence of each capsule\nfluences\nflux monitor\nflux monitor set\nmetal wire pieces\nmolybdenum\nmonitor\nmonitor sets\nwire piece\n"}, {"id": "S0022311515301069", "text": "The fluence of each capsule was determined by using activation monitor sets. These monitor sets consist of different metal wire pieces that have an activation reaction at a specific energy range. The different activation energies are chosen in such a way that the spectrum can be reconstructed. In BODEX, each capsule contained a flux monitor set on the \u2018back side\u2019 (as seen from the core) and one on the front side, positioned at the central height of the capsules. Additionally, one detector was placed at the top and one at the bottom, resulting in a total of 6 monitor sets per leg. The fluence in each capsule was determined as the average between the two flux monitor located in each capsule. The sets have been analysed by determining the activation of each wire piece, which indicates the fluence of a specific energy range. Table\u00a03 show the values of the fluences for the two capsules containing molybdenum.\n", "keywords": "activation\nactivation monitor sets.\ncapsule\ncapsules\ndetector\ndetermining the activation of each wire piece\nfluence\nfluence of each capsule\nfluences\nflux monitor\nflux monitor set\nmetal wire pieces\nmolybdenum\nmonitor\nmonitor sets\nwire piece\n"}, {"id": "S0045782513000546", "text": "In this article we consider an extension to the equations of poroelasticity by modelling the flow of a slightly compressible single phase fluid in a viscoelastic porous medium. The constitutive equations therefore allow for the presence of viscoelastic relaxation effects in the porous media (but not the fluid). Fully discrete numerical schemes are derived based on a lagged and non-lagged backward Euler time stepping method applied to a mixed and Galerkin finite element spatial discretization. We show that the lagged scheme is unconditionally stable and give an optimal a priori error bound for it. Furthermore, this scheme is practical and useful in the sense that it can be easily implemented in existing poroelasticity software because the coupling between the viscous stresses and pressures and the elasticity and flow equations is \u2018lagged\u2019 by one time step. The required additional coding therefore takes the form of extra \u2018right hand side loads\u2019 together with some updating subroutines for the viscoelastic internal variables, but the solver and assembly engines remain intact. This idea of lagging has been used before for nonlinearly viscoelastic diffusion problems in [3,24] but, of course, is not new. Lagging in numerical schemes is discussed more widely by Lowrie in [14].\n", "keywords": "assembly engines\nconstitutive equations\ncoupling\nextension to the equations of poroelasticity\nflow equations\nfluid\nFully discrete numerical schemes\nlagged and non-lagged backward Euler time stepping method\nmixed and Galerkin finite element spatial discretization\nmodelling the flow of a slightly compressible single phase fluid in a viscoelastic porous medium\nnonlinearly viscoelastic diffusion problems\nnumerical schemes\nporous media\nsingle phase fluid\nupdating subroutines\nviscoelastic porous medium\nviscoelastic relaxation effects\n"}, {"id": "S0045782512003428", "text": "The choice of the interpolation functions and support point coordinates for the gradient field is crucial to ensure stability and accuracy of the formulation. For example, nodal integration and NS-FEM are unstable involving the appearance of spurious low-energy modes. They need non-physical penalty energy functions that stabilize them. The articles [2,28] numerically verify the stability, convergence and accuracy of several W2 variants including new elements which can be constructed based on the idea of assumed continuous deformation gradients. For first order hexahedral elements, [2,28] found good results for the element types C3D_8N_27C and C3D_8N_8I. The first is defined by 27 support points and a second order tensor-product interpolation of the deformation gradient by Lagrange polynomials. The latter element type is defined by 16 support points with 8 points being coincident with the nodes and 8 additional points in the element interior. Among the tested first order tetrahedra, the nodally integrated tetrahedron with an additional bubble mode in the gradients was found to be most accurate. It turned out to be even the most efficient with respect to computing time in explicit analysis [28] because the enlarged critical time step compensates the slightly increased numerical cost per restoring force assembly. Fig. 1 illustrates the positions of support points for various CAG and SFEM formulations.\n", "keywords": "C3D_8N_27C\nC3D_8N_8I\nCAG and SFEM formulations\ncomputing time in explicit analysis\ncontinuous deformation gradients\nelement types\nfirst order hexahedral elements\nfirst order tetrahedra\ninterpolation functions\nnodal integration and NS-FEM\nnodally integrated tetrahedron\nnon-physical penalty energy functions\nnumerically verify the stability, convergence and accuracy of several W2 variants\nsecond order tensor-product interpolation\nstability and accuracy of the formulation\nstabilize them\nsupport point coordinates for the gradient field\nW2 variants\n"}, {"id": "S0370269304007257", "text": "Some non-standard couplings, which should be determined here, could also be studied in the standard e+e\u2212 option of a linear collider. Therefore, it is worth while to compare the potential power of the two options. As far as the parameter \u03b1\u03b31 is concerned, the \u03b3\u03b3 collider does not allow for its determination, while it could be determined at e+e\u2212. The second tt\u0304\u03b3 coupling \u03b1\u03b32, which is proportional to the real part of the top-quark electric dipole moment,44See [23] taking into account that the operators OuB, OqB and OqW are redundant. can be measured here. It should be recalled that energy and polar-angle distributions of leptons and b-quarks in e+e\u2212 colliders are sensitive only to the imaginary part of the electric dipole moment,55However, it should be emphasized that there exist observables sensitive also to the real part of the top-quark electric dipole moment, see [24]. while here the real part could be determined. For the measurement of \u03b3\u03b3H couplings, e+e\u2212 colliders are, of course, useless, while here, for the bX final state both \u03b1h1 and \u03b1h2 could be measured. In the case of the decay form factor \u03b1d measurement, the e+e\u2212 option seems to be a little more advantageous, especially if e+e\u2212 polarization can be tuned appropriately\u00a0[25].\n", "keywords": "coupling \u03b1\u03b32\ne+e\u2212\ne+e\u2212 colliders\ne+e\u2212 option\ne+e\u2212 polarization\nhe real part of the top-quark electric dipole moment\nimaginary part of the electric dipole moment\nlinear collider\nmeasurement of \u03b3\u03b3H couplings\nnon-standard couplings\nparameter \u03b1\u03b31\npolar-angle distributions of leptons and b-quarks\npotential power\nstandard e+e\u2212 option\nthe bX final state\nthe decay form factor \u03b1d measurement\n\u03b1h1 and \u03b1h2 could be measured\n\u03b3\u03b3 collider\n"}, {"id": "S0370269304007257", "text": "Some non-standard couplings, which should be determined here, could also be studied in the standard e+e\u2212 option of a linear collider. Therefore, it is worth while to compare the potential power of the two options. As far as the parameter \u03b1\u03b31 is concerned, the \u03b3\u03b3 collider does not allow for its determination, while it could be determined at e+e\u2212. The second tt\u0304\u03b3 coupling \u03b1\u03b32, which is proportional to the real part of the top-quark electric dipole moment,44See [23] taking into account that the operators OuB, OqB and OqW are redundant. can be measured here. It should be recalled that energy and polar-angle distributions of leptons and b-quarks in e+e\u2212 colliders are sensitive only to the imaginary part of the electric dipole moment,55However, it should be emphasized that there exist observables sensitive also to the real part of the top-quark electric dipole moment, see [24]. while here the real part could be determined. For the measurement of \u03b3\u03b3H couplings, e+e\u2212 colliders are, of course, useless, while here, for the bX final state both \u03b1h1 and \u03b1h2 could be measured. In the case of the decay form factor \u03b1d measurement, the e+e\u2212 option seems to be a little more advantageous, especially if e+e\u2212 polarization can be tuned appropriately\u00a0[25].\n", "keywords": "coupling \u03b1\u03b32\ne+e\u2212\ne+e\u2212 colliders\ne+e\u2212 option\ne+e\u2212 polarization\nhe real part of the top-quark electric dipole moment\nimaginary part of the electric dipole moment\nlinear collider\nmeasurement of \u03b3\u03b3H couplings\nnon-standard couplings\nparameter \u03b1\u03b31\npolar-angle distributions of leptons and b-quarks\npotential power\nstandard e+e\u2212 option\nthe bX final state\nthe decay form factor \u03b1d measurement\n\u03b1h1 and \u03b1h2 could be measured\n\u03b3\u03b3 collider\n"}, {"id": "S0045782514001947", "text": "Our procedure does not address the issue of how parameterizations can vary for different flow types. However, Edeling et\u00a0al.\u00a0 [9] carried out separate calibrations for a set of 13 boundary-layer flows. They summarized this information across calibrations by computing Highest Posterior-Density (HPD) intervals, and subsequently represent the total solution uncertainty with a probability-box (p-box). This p-box represents both parameter variability across flows, and epistemic uncertainty within each calibration. A prediction of a new boundary-layer flow is made with uncertainty bars generated from this uncertainty information, and the resulting error estimate is shown to be consistent with measurement data. This approach is helpful, but it might be extended further by modelling proximity across flows through a distance that would relate to the flow characteristics in order to borrow strength across calibrations instead of splitting the calibrations and then merging the outcomes afterwards. This is a challenging but attractive venue for future research.\n", "keywords": "boundary-layer flow\nboundary-layer flows\nflow\nflows\nHighest Posterior-Density\nHPD\nmodelling proximity across flows\nparameterizations\np-box\nprobability-box\nsplitting the calibrations and then merging the outcomes\n"}, {"id": "S0045782514001947", "text": "Our procedure does not address the issue of how parameterizations can vary for different flow types. However, Edeling et\u00a0al.\u00a0 [9] carried out separate calibrations for a set of 13 boundary-layer flows. They summarized this information across calibrations by computing Highest Posterior-Density (HPD) intervals, and subsequently represent the total solution uncertainty with a probability-box (p-box). This p-box represents both parameter variability across flows, and epistemic uncertainty within each calibration. A prediction of a new boundary-layer flow is made with uncertainty bars generated from this uncertainty information, and the resulting error estimate is shown to be consistent with measurement data. This approach is helpful, but it might be extended further by modelling proximity across flows through a distance that would relate to the flow characteristics in order to borrow strength across calibrations instead of splitting the calibrations and then merging the outcomes afterwards. This is a challenging but attractive venue for future research.\n", "keywords": "boundary-layer flow\nboundary-layer flows\nflow\nflows\nHighest Posterior-Density\nHPD\nmodelling proximity across flows\nparameterizations\np-box\nprobability-box\nsplitting the calibrations and then merging the outcomes\n"}, {"id": "S0022311515301653", "text": "Uranium carbide was traditionally used as fuel kernel for the US version of pebble bed reactors as opposed to the German version based on uranium dioxide. For the Generation IV nuclear systems, mixed uranium\u2013plutonium carbides (U, Pu) C constitute the primary option for the gas fast reactors (GFR) and UCO is the first candidate for the very high temperature reactor (VHTR). In the former case the fuel high actinide density and thermal conductivity are exploited in view of high burnup performance. In the latter, UCO is a good compromise between oxides and carbides both in terms of thermal conductivity and fissile density. However, in the American VHTR design, the fuel is a 3:1 ratio of UO2:UC2 for one essential reason, well explained by Olander [2] in a recent publication. During burnup, pure UO2 fuel tends to oxidize to UO2+x. UO2+x reacts with the pyrocarbon coating layer according to the equilibrium:(1)UO2+x\u00a0+\u00a0xC\u00a0\u2192\u00a0UO2\u00a0+\u00a0xCO\n", "keywords": "actinide\nburnup\ncarbides\nfuel\nfuel kernel\ngas fast reactors\nGeneration IV nuclear systems\nGFR\nmixed uranium\u2013plutonium carbides\noxides\npebble bed reactors\npyrocarbon coating layer\nUC2\nUCO\nUO2\nUO2 fuel\nUO2+x\n(U, Pu) C\nUranium carbide\nuranium dioxide\nvery high temperature reactor\nVHTR\nxC\nxCO\n"}, {"id": "S0022311513011951", "text": "The displacement cascade is a rapid process (of order picoseconds). Further migration of vacancies and SIAs, mainly by diffusion, happens over a timescale of order nanoseconds [17]. This is still short compared to operating times, so is important to consider the equilibrium result of such processes: If the vacancies and SIAs were likely to find their Frenkel partner, recombine, and annihilate, then the metal should essentially return to its original structure; however, if defects instead formed large clusters of a single type this could result in formation of voids, dislocation loops or swelling, possibly weakening the material in the process. Defects can be trapped at grain boundaries or surface, so for an ODS particle to effect the diffusion, there concentration must be such that there are many such particles in each grain.\n", "keywords": "defects\ndiffusion\ndislocation loops\ndisplacement cascade\ngrain\nmigration of vacancies and SIAs\nODS particle\nstructure\nswelling\nvoids\n"}, {"id": "S0927025614003322", "text": "A principle of high-throughput materials science is that one does not know a priori where the value of the data lies for any specific application. Trends and insights are deduced a posteriori. This requires efficient interfaces to interrogate available data on various levels. We have developed a simple WEB-based API to greatly improve the accessibility and utility of the AFLOWLIB database [14] to the scientific community. Through it, the client can access calculated physical properties (thermodynamic, crystallographic, or mechanical properties), as well as simulation provenance and runtime properties of the included systems. The data may be used directly (e.g., to browse a class of materials with a desired property) or integrated into higher level work-flows. The interface also allows for the sharing of updates of data used in previous published works, e.g., previously calculated alloy phase diagrams [19\u201331], thus the database can be expanded systematically.\n", "keywords": "AFLOWLIB database\nalloy phase diagrams\nbrowse a class of materials with a desired property\nefficient interfaces to interrogate available data on various levels\nhigher level work-flows\nhigh-throughput materials science\nimprove the accessibility and utility of the AFLOWLIB database\ninterface\nphysical properties\nruntime properties\nsharing of updates of data\nsimulation provenance\nthermodynamic, crystallographic, or mechanical properties\nWEB-based API\n"}, {"id": "S0165212515000931", "text": "We describe three ways to solve the reflection problem. The first way is very simple (Section\u00a0 4). We exploit the consequences of shifting the semi-infinite row by one period (to the right or left). In effect, we regard the semi-infinite row as two scatterers, one of which is another semi-infinite row. This idea goes back to a series of papers by Millar in the 1960s, starting with\u00a0 [2]. He used it for several two-dimensional grating problems. A similar approach was used for layered media by Shenderov\u00a0 [3]. In our one-dimensional context, we obtain a quadratic equation for R; we show how to select the correct solution. We remark that there has been much recent interest in related two-dimensional waveguide problems; see, for example,\u00a0 [4\u20136], where the shifting-by-one-period idea is again employed, leading to a quadratic equation for a certain operator.\n", "keywords": "layered media\nobtain a quadratic equation\nquadratic equation\nreflection problem\nsemi-infinite row\nshifting-by-one-period idea\nshifting the semi-infinite row\ntwo-dimensional grating problems\ntwo-dimensional waveguide problems\ntwo scatterers\nwaveguide problems\n"}, {"id": "S2212667813000762", "text": "Analyzing the significance of macroscopical dynamic monitoring of new add construction land, considering the influence of various factors, this paper selected Yinchuan Plain for a typical experimental zone, built knowledge base of remote sensing images interpretation, used multi-temporal remote sensing images, carried through interactive interpretation of change patterns of new add construction land and field validation. Interpretation results of 20m scale remote sensing image show that the minimum spot average area of new construction land change monitored by 20m scale remote sensing data is about 6 acres. The ability 20m scale remote sensing data identifies new increased construction land change further strengthens, shows in the recognition of the smallest spot area reduces and the recognition accuracy increases.", "keywords": "construction land\nfield validation\nidentifies new increased construction land\ninteractive interpretation of change patterns\nInterpretation\nknowledge base\nmulti-temporal remote sensing images\nrecognition\nrecognition of the smallest spot area\nremote sensing data\nremote sensing image\nremote sensing images\nremote sensing images interpretation\nYinchuan Plain\n"}, {"id": "S2212667813000762", "text": "Analyzing the significance of macroscopical dynamic monitoring of new add construction land, considering the influence of various factors, this paper selected Yinchuan Plain for a typical experimental zone, built knowledge base of remote sensing images interpretation, used multi-temporal remote sensing images, carried through interactive interpretation of change patterns of new add construction land and field validation. Interpretation results of 20m scale remote sensing image show that the minimum spot average area of new construction land change monitored by 20m scale remote sensing data is about 6 acres. The ability 20m scale remote sensing data identifies new increased construction land change further strengthens, shows in the recognition of the smallest spot area reduces and the recognition accuracy increases.", "keywords": "construction land\nfield validation\nidentifies new increased construction land\ninteractive interpretation of change patterns\nInterpretation\nknowledge base\nmulti-temporal remote sensing images\nrecognition\nrecognition of the smallest spot area\nremote sensing data\nremote sensing image\nremote sensing images\nremote sensing images interpretation\nYinchuan Plain\n"}, {"id": "S2212667813000762", "text": "Analyzing the significance of macroscopical dynamic monitoring of new add construction land, considering the influence of various factors, this paper selected Yinchuan Plain for a typical experimental zone, built knowledge base of remote sensing images interpretation, used multi-temporal remote sensing images, carried through interactive interpretation of change patterns of new add construction land and field validation. Interpretation results of 20m scale remote sensing image show that the minimum spot average area of new construction land change monitored by 20m scale remote sensing data is about 6 acres. The ability 20m scale remote sensing data identifies new increased construction land change further strengthens, shows in the recognition of the smallest spot area reduces and the recognition accuracy increases.", "keywords": "construction land\nfield validation\nidentifies new increased construction land\ninteractive interpretation of change patterns\nInterpretation\nknowledge base\nmulti-temporal remote sensing images\nrecognition\nrecognition of the smallest spot area\nremote sensing data\nremote sensing image\nremote sensing images\nremote sensing images interpretation\nYinchuan Plain\n"}, {"id": "S0098300414000259", "text": "Apache Pig is a platform for creating MapReduce workflows with Hadoop. These workflows are expressed as directed acyclic graphs (DAGs) of tasks that exist at a conceptually higher level than their implementations as series of MapReduce jobs. Pig Latin is the procedural language used for building these workflows, providing syntax similar to the declarative SQL commonly used for relational database systems. In addition to standard SQL operations, Pig can be extended with user-defined functions (UDFs) commonly written in Java. We adopted Pig for our implementation of the correlator to speed up development time, allow for ad hoc workflow changes, and to embrace the Hadoop community\u05f3s migration away from MapReduce towards more generalized DAG processing (Mayer, 2013). Specifically, in the event that future versions of Hadoop are optimized to support paradigms other than MapReduce, Pig scripts could take advantage of these advances without recoding, whereas explicit Java MapReduce jobs would need to be rewritten.\n", "keywords": "Apache Pig\nDAG processing\nDAGs\ndirected acyclic graphs\nHadoop\nJava MapReduce\nMapReduce\nMapReduce workflows\nPig\nPig Latin\nPig scripts\nSQL\nSQL operations\nUDFs\nuser-defined functions\n"}, {"id": "S0098300414000259", "text": "Apache Pig is a platform for creating MapReduce workflows with Hadoop. These workflows are expressed as directed acyclic graphs (DAGs) of tasks that exist at a conceptually higher level than their implementations as series of MapReduce jobs. Pig Latin is the procedural language used for building these workflows, providing syntax similar to the declarative SQL commonly used for relational database systems. In addition to standard SQL operations, Pig can be extended with user-defined functions (UDFs) commonly written in Java. We adopted Pig for our implementation of the correlator to speed up development time, allow for ad hoc workflow changes, and to embrace the Hadoop community\u05f3s migration away from MapReduce towards more generalized DAG processing (Mayer, 2013). Specifically, in the event that future versions of Hadoop are optimized to support paradigms other than MapReduce, Pig scripts could take advantage of these advances without recoding, whereas explicit Java MapReduce jobs would need to be rewritten.\n", "keywords": "Apache Pig\nDAG processing\nDAGs\ndirected acyclic graphs\nHadoop\nJava MapReduce\nMapReduce\nMapReduce workflows\nPig\nPig Latin\nPig scripts\nSQL\nSQL operations\nUDFs\nuser-defined functions\n"}, {"id": "S0022311514006722", "text": "Zirconium alloys are used as fuel cladding in pressurised and boiling water nuclear reactors. As such these materials are exposed to a large number of environmental factors that will promote degradation mechanisms such as oxidation. At high burn-ups, i.e. extended service life, oxidation and the associated hydrogen pick-up can be a limiting factor in terms of fuel efficiency and safety. The oxidation kinetics for many zirconium alloys are cyclical, demonstrating a series of approximately cubic kinetic curves separated by transitions [1\u20135]. These transitions are typified by a breakdown in the protective character of the oxide and are potentially linked to a number of mechanical issues. Understanding how these issues influence oxidation is a key to developing a full mechanistic understanding of the corrosion process.\n", "keywords": "corrosion process\ndegradation mechanisms\ndeveloping a full mechanistic understanding of the corrosion process\nfuel\nhydrogen pick-up\nnuclear reactors\noxidation\noxide\nUnderstanding how these issues influence oxidation\nzirconium alloys\nZirconium alloys\n"}, {"id": "S004578251400334X", "text": "In this article we propose a method which adopts a different approach to the generation procedure outlined above and that helps to address the problem of generating high-order meshes for high Reynolds number flows. The method is conceptually simple, cheap to implement and does not require a dense linear boundary-layer mesh. It is based on the use of an isoparametric [17] or, in general, a transfinite interpolation [18] where a high-order coarse boundary-layer prismatic mesh is subdivided into either prisms or tetrahedra using the mapping that defines the coarse high-order prisms. The procedure is also very versatile as it permits meshes with different distributions of y+ to be generated with ease and furthermore, the validity of these meshes is guaranteed if the initial mesh is valid and the polynomial space is chosen appropriately.\n", "keywords": "coarse high-order prisms\ndense linear boundary-layer mesh\ngenerating high-order meshes for high Reynolds number flows\ngeneration procedure\nhigh-order coarse boundary-layer prismatic mesh\nhigh-order meshes\ninitial mesh\nisoparametric [17] or, in general, a transfinite interpolation\nmeshes\npolynomial space\nprisms\nReynolds number flows\nsubdivided into either prisms or tetrahedra\ntetrahedra\n"}, {"id": "S0045782515002418", "text": "FR schemes are similar to nodal DG schemes, which are arguably the most popular type of unstructured high-order method (at least in the field of computational aerodynamics). Like nodal DG schemes, FR schemes utilise a high-order (nodal) polynomial basis to approximate the solution within each element of the computational domain, and like nodal DG schemes, FR schemes do not explicitly enforce inter-element solution continuity. However, unlike nodal DG schemes, FR methods are based solely on the governing system in a differential form. A description of the FR approach in 1D is presented below. For further information see the original paper of Huynh\u00a0 [2].\n", "keywords": "approximate the solution\ncomputational aerodynamics\ncomputational domain\nenforce inter-element solution continuity\nFR approach\nFR methods\nFR schemes\nhigh-order\nnodal\nnodal DG schemes\nunstructured high-order method\n"}, {"id": "S0038092X14000942", "text": "Thermal performance of the smart window has been predicted under different simulated parameters namely, direct solar radiation intensity, ambient temperature, water inlet temperature, and water flow rate. Fig. 11 shows a sample of temperature distribution of all window components at a plane passing through the horizontal window segment based on simulation physical conditions listed on Table 3. Simulation data has been collected from all successive simulations. The effect of increasing direct solar radiation on both solar cells and water temperatures is shown in Fig. 12. Three different simulations were performed assuming direct solar radiation intensities of 400, 600, and 800W/m2 incident on the window\u2019s front pane with set ambient temperature, water inlet temperature and water flow rate of respectively 273K, 283K and 0.01kg/s. Water temperature was found to increase by 5\u00b0C as it passed through the tube, carrying the solar cells, from left to right for the bottom most units at 800W/m2 of direct incident solar radiation.\n", "keywords": "ambient temperature\ndirect solar radiation intensity\nincreasing direct solar radiation on both solar cells and water temperatures\nsimulated parameters\nSimulation data\nsimulation physical conditions\nsimulations\nsolar cells\nsolar radiation\nThermal performance of the smart window\ntube\nwater flow rate\nwater inlet temperature\nwindow components\nwindow\u2019s front pane\n"}, {"id": "S0010938X15301554", "text": "AA 2024-T3 aluminium alloy is widely used for aerospace applications due to its high strength to weight ratio and high damage tolerance that result from copper and magnesium as the principal alloying elements and appropriate thermomechanical processing. The microstructure of the alloy is relatively complex and a number of compositionally-distinct phases have been identified [1]. Although possessing favourable mechanical properties, the alloy is relatively susceptible to corrosion and generally requires surface treatment in practical applications. The corrosion behaviour of the alloy is particularly affected by the presence of the intermetallic particles due to their differing potentials with respect to the alloy matrix [2\u20139]. Copper-containing second phase particles at the alloy surface are particularly detrimental to the corrosion resistance as they provide preferential cathodic sites [2,10]. One of the principle types of second phase particle that is important to the corrosion behaviour of the alloy is the S phase (Al2CuMg) particle [1,11]. Dealloying of S phase particles, which may account for \u223c60% of the constituent particles in AA2024 alloys [11], is commonly observed when the alloy is exposed to an aggressive environment. The particles are considered as important initiation sites for severe localized corrosion in the alloy [11\u201322]. The dealloying of the S phase particles and the resulting enrichment of copper result in a decrease of the Volta potential with respect to the matrix and hence the dealloyed particles become active cathodic sites [23\u201325].\n", "keywords": "AA2024 alloys\nAA 2024-T3 aluminium alloy\naerospace applications\nAl2CuMg\nalloy\nalloying elements\nalloy matrix\ncathodic sites\ncompositionally-distinct phases\nconstituent particles\ncopper\nCopper-containing second phase particles\ncorrosion\ncorrosion behaviour\ncorrosion resistance\ndealloyed particles\ndealloying\ndecrease of the Volta potential\nenrichment\nintermetallic particles\nmagnesium\nparticles\nsecond phase particle\nS phase\nS phase particles\nsurface treatment\nthermomechanical processing\n"}, {"id": "S0010938X15301554", "text": "AA 2024-T3 aluminium alloy is widely used for aerospace applications due to its high strength to weight ratio and high damage tolerance that result from copper and magnesium as the principal alloying elements and appropriate thermomechanical processing. The microstructure of the alloy is relatively complex and a number of compositionally-distinct phases have been identified [1]. Although possessing favourable mechanical properties, the alloy is relatively susceptible to corrosion and generally requires surface treatment in practical applications. The corrosion behaviour of the alloy is particularly affected by the presence of the intermetallic particles due to their differing potentials with respect to the alloy matrix [2\u20139]. Copper-containing second phase particles at the alloy surface are particularly detrimental to the corrosion resistance as they provide preferential cathodic sites [2,10]. One of the principle types of second phase particle that is important to the corrosion behaviour of the alloy is the S phase (Al2CuMg) particle [1,11]. Dealloying of S phase particles, which may account for \u223c60% of the constituent particles in AA2024 alloys [11], is commonly observed when the alloy is exposed to an aggressive environment. The particles are considered as important initiation sites for severe localized corrosion in the alloy [11\u201322]. The dealloying of the S phase particles and the resulting enrichment of copper result in a decrease of the Volta potential with respect to the matrix and hence the dealloyed particles become active cathodic sites [23\u201325].\n", "keywords": "AA2024 alloys\nAA 2024-T3 aluminium alloy\naerospace applications\nAl2CuMg\nalloy\nalloying elements\nalloy matrix\ncathodic sites\ncompositionally-distinct phases\nconstituent particles\ncopper\nCopper-containing second phase particles\ncorrosion\ncorrosion behaviour\ncorrosion resistance\ndealloyed particles\ndealloying\ndecrease of the Volta potential\nenrichment\nintermetallic particles\nmagnesium\nparticles\nsecond phase particle\nS phase\nS phase particles\nsurface treatment\nthermomechanical processing\n"}, {"id": "S0010938X15301554", "text": "AA 2024-T3 aluminium alloy is widely used for aerospace applications due to its high strength to weight ratio and high damage tolerance that result from copper and magnesium as the principal alloying elements and appropriate thermomechanical processing. The microstructure of the alloy is relatively complex and a number of compositionally-distinct phases have been identified [1]. Although possessing favourable mechanical properties, the alloy is relatively susceptible to corrosion and generally requires surface treatment in practical applications. The corrosion behaviour of the alloy is particularly affected by the presence of the intermetallic particles due to their differing potentials with respect to the alloy matrix [2\u20139]. Copper-containing second phase particles at the alloy surface are particularly detrimental to the corrosion resistance as they provide preferential cathodic sites [2,10]. One of the principle types of second phase particle that is important to the corrosion behaviour of the alloy is the S phase (Al2CuMg) particle [1,11]. Dealloying of S phase particles, which may account for \u223c60% of the constituent particles in AA2024 alloys [11], is commonly observed when the alloy is exposed to an aggressive environment. The particles are considered as important initiation sites for severe localized corrosion in the alloy [11\u201322]. The dealloying of the S phase particles and the resulting enrichment of copper result in a decrease of the Volta potential with respect to the matrix and hence the dealloyed particles become active cathodic sites [23\u201325].\n", "keywords": "AA2024 alloys\nAA 2024-T3 aluminium alloy\naerospace applications\nAl2CuMg\nalloy\nalloying elements\nalloy matrix\ncathodic sites\ncompositionally-distinct phases\nconstituent particles\ncopper\nCopper-containing second phase particles\ncorrosion\ncorrosion behaviour\ncorrosion resistance\ndealloyed particles\ndealloying\ndecrease of the Volta potential\nenrichment\nintermetallic particles\nmagnesium\nparticles\nsecond phase particle\nS phase\nS phase particles\nsurface treatment\nthermomechanical processing\n"}, {"id": "S0010938X15301554", "text": "AA 2024-T3 aluminium alloy is widely used for aerospace applications due to its high strength to weight ratio and high damage tolerance that result from copper and magnesium as the principal alloying elements and appropriate thermomechanical processing. The microstructure of the alloy is relatively complex and a number of compositionally-distinct phases have been identified [1]. Although possessing favourable mechanical properties, the alloy is relatively susceptible to corrosion and generally requires surface treatment in practical applications. The corrosion behaviour of the alloy is particularly affected by the presence of the intermetallic particles due to their differing potentials with respect to the alloy matrix [2\u20139]. Copper-containing second phase particles at the alloy surface are particularly detrimental to the corrosion resistance as they provide preferential cathodic sites [2,10]. One of the principle types of second phase particle that is important to the corrosion behaviour of the alloy is the S phase (Al2CuMg) particle [1,11]. Dealloying of S phase particles, which may account for \u223c60% of the constituent particles in AA2024 alloys [11], is commonly observed when the alloy is exposed to an aggressive environment. The particles are considered as important initiation sites for severe localized corrosion in the alloy [11\u201322]. The dealloying of the S phase particles and the resulting enrichment of copper result in a decrease of the Volta potential with respect to the matrix and hence the dealloyed particles become active cathodic sites [23\u201325].\n", "keywords": "AA2024 alloys\nAA 2024-T3 aluminium alloy\naerospace applications\nAl2CuMg\nalloy\nalloying elements\nalloy matrix\ncathodic sites\ncompositionally-distinct phases\nconstituent particles\ncopper\nCopper-containing second phase particles\ncorrosion\ncorrosion behaviour\ncorrosion resistance\ndealloyed particles\ndealloying\ndecrease of the Volta potential\nenrichment\nintermetallic particles\nmagnesium\nparticles\nsecond phase particle\nS phase\nS phase particles\nsurface treatment\nthermomechanical processing\n"}, {"id": "S221450951400031X", "text": "This phase was completed in 2005. Previous contracts had been procured with the contractor providing the detailed design. For this system the design was undertaken by Mott MacDonald. It was developed by looking at the systems installed previously and calculating what was actually required to achieve cathodic protection of the piers. This resulted in a significant reduction in the number of zones and monitoring probes. The varying amounts of steelwork in the beams had previously lead to up to 5 zones per beam, with multiple layers of mesh to achieve the design current density. On review of the data the operating current density was similar in all zones and so this was reduced to a single zone per beam. The encapsulation was susceptible to ASR and contained post tensioning and so it was decided to use a galvanic system based on Galvashield CC anodes from Fosroc. Our design did not include an option to allow depolarization of the galvanic system, but the contractor supplied one, such that the anodes could be remotely disconnected. The control unit was from Electrotech CP and operated via a broadband connection provided by the contractor.\n", "keywords": "ASR\nbroadband connection\ncathodic protection of the piers\ndata\ndepolarization of the galvanic system\ndetailed design\nencapsulation\ngalvanic system\nGalvashield CC anodes\nlooking at the systems installed previously and calculating\nmultiple layers of mesh\nsignificant reduction in the number of zones and monitoring probes\nsystems installed previously\nvarying amounts of steelwork\n"}, {"id": "S0021999115000546", "text": "The particular phase field model we employ is an extension of [6], and is based on the three dimensional thermal phase field model of [7] and two dimensional thermal-solutal phase field model of [8]. One feature of the physical problem is that it is purely dissipative, or entropy increasing, as all natural relaxational phenomena are. The resulting PDEs are of Allen\u2013Cahn [9] and Carn\u2013Hilliard type [10]. That is to say, the model involves time derivatives of the three fields coupled to forms involving variational derivatives of some functional \u2013 typically the free energy functional. As the dendrite grows the free energy reduces monotonically with time but never achieves equilibrium if the domain boundary is far from the dendrite. Although we have listed some of the difficult aspects of this model, the relaxational aspect is typically an asset and results in stable numerical schemes: there is no convection, for example (at least in the absence of flow in the melt).\n", "keywords": "dendrite\nfree energy functional\nfunctional\nmelt\nnumerical schemes\nPDEs\nphase field model\nthree dimensional thermal phase field model\ntime derivatives of the three fields coupled to forms involving variational derivatives\ntwo dimensional thermal-solutal phase field model\n"}, {"id": "S0021999115000546", "text": "The particular phase field model we employ is an extension of [6], and is based on the three dimensional thermal phase field model of [7] and two dimensional thermal-solutal phase field model of [8]. One feature of the physical problem is that it is purely dissipative, or entropy increasing, as all natural relaxational phenomena are. The resulting PDEs are of Allen\u2013Cahn [9] and Carn\u2013Hilliard type [10]. That is to say, the model involves time derivatives of the three fields coupled to forms involving variational derivatives of some functional \u2013 typically the free energy functional. As the dendrite grows the free energy reduces monotonically with time but never achieves equilibrium if the domain boundary is far from the dendrite. Although we have listed some of the difficult aspects of this model, the relaxational aspect is typically an asset and results in stable numerical schemes: there is no convection, for example (at least in the absence of flow in the melt).\n", "keywords": "dendrite\nfree energy functional\nfunctional\nmelt\nnumerical schemes\nPDEs\nphase field model\nthree dimensional thermal phase field model\ntime derivatives of the three fields coupled to forms involving variational derivatives\ntwo dimensional thermal-solutal phase field model\n"}, {"id": "S0370269304009657", "text": "In this Letter, we present results of a relativistic calculation of decay constants in the framework of full Salpeter equation. The full Salpeter equation is a relativistic equation describing a bound state. Since this method has a very solid basis in quantum field theory, it is very good in describing a bound state which is a relativistic system. In a previous paper [16], we solved the instantaneous Bethe\u2013Salpeter equation [17], which is also called full Salpeter equation [18]. After we solved the full Salpeter equation, we obtained the relativistic wave function of the bound state. We used this wave function to calculate the average kinetic energy of the heavy quark inside a heavy meson in 0\u2212 state, and obtained values which agree very well with recent experiments. We also found there that the relativistic corrections are quite large and cannot be ignored [16]. In this Letter we use this method to predict the values of decay constants of heavy mesons in 0\u2212 state.\n", "keywords": "Bethe\u2013Salpeter equation\ncalculate the average kinetic energy\ndescribing a bound state\nframework of full Salpeter equation\nfull Salpeter equation\nheavy meson\nheavy mesons\nheavy quark\nobtained values\npredict the values of decay constants\nquantum field theory\nrelativistic calculation of decay constants\nrelativistic corrections are quite large\nrelativistic equation\nrelativistic system\nrelativistic wave function of the bound state\nSalpeter equation\nsolved the full Salpeter equation\nsolved the instantaneous Bethe\u2013Salpeter equation\nuse this method\nwave function\n"}, {"id": "S0370269304009657", "text": "In this Letter, we present results of a relativistic calculation of decay constants in the framework of full Salpeter equation. The full Salpeter equation is a relativistic equation describing a bound state. Since this method has a very solid basis in quantum field theory, it is very good in describing a bound state which is a relativistic system. In a previous paper [16], we solved the instantaneous Bethe\u2013Salpeter equation [17], which is also called full Salpeter equation [18]. After we solved the full Salpeter equation, we obtained the relativistic wave function of the bound state. We used this wave function to calculate the average kinetic energy of the heavy quark inside a heavy meson in 0\u2212 state, and obtained values which agree very well with recent experiments. We also found there that the relativistic corrections are quite large and cannot be ignored [16]. In this Letter we use this method to predict the values of decay constants of heavy mesons in 0\u2212 state.\n", "keywords": "Bethe\u2013Salpeter equation\ncalculate the average kinetic energy\ndescribing a bound state\nframework of full Salpeter equation\nfull Salpeter equation\nheavy meson\nheavy mesons\nheavy quark\nobtained values\npredict the values of decay constants\nquantum field theory\nrelativistic calculation of decay constants\nrelativistic corrections are quite large\nrelativistic equation\nrelativistic system\nrelativistic wave function of the bound state\nSalpeter equation\nsolved the full Salpeter equation\nsolved the instantaneous Bethe\u2013Salpeter equation\nuse this method\nwave function\n"}, {"id": "S0032386109001712", "text": "With ever increasing computer performance, simulations in much larger systems have become feasible. However, full-atomistic approaches to polymer crystallization need extremely large computer power even in the case of simple polymers, and appropriate modeling or coarse-graining of the system is imperative. From a series of work on the development of coarse-grained models for polymers, Mayer and Muller-Plathe have build up a model of poly(vinyl alcohol) (PVA) for studying early stage of crystallization. They investigated the emergence of crystalline order from the isotropic melt by rapid quenching [51,52]. They could reproduce many elementary processes of homogenous nucleation that showed good correspondence with experiments and other simulations, in temperature dependence of lamella thickness, structure of fold surface, etc. In their work, they neglected long-range force (van der Waals attraction) to accelerate computation. Their model has the energy contribution due to intrachain interactions only and the dominant driving force for crystallization is entropic, which seems to ignore dominant driving force for polymer crystallization in conventional sense. However, their work is reminiscent of the classical solid\u2013liquid transition in systems of repulsive spherical atoms [53] and poses an intriguing problem as to the intrinsic driving force for polymer crystallization.\n", "keywords": "appropriate modeling\nclassical solid\u2013liquid transition\ncoarse-grained models\ncoarse-graining of the system\ncrystalline\ncrystallization\nfold surface\nfull-atomistic approaches\nhomogenous nucleation\nintrachain interactions\nisotropic melt\nlamella\nliquid\npolymer\npolymer crystallization\npolymers\npoly(vinyl alcohol)\nPVA\nrapid quenching\nrepulsive spherical atoms\nsimple polymers\nsimulations\nsolid\n"}, {"id": "S0032386109001712", "text": "With ever increasing computer performance, simulations in much larger systems have become feasible. However, full-atomistic approaches to polymer crystallization need extremely large computer power even in the case of simple polymers, and appropriate modeling or coarse-graining of the system is imperative. From a series of work on the development of coarse-grained models for polymers, Mayer and Muller-Plathe have build up a model of poly(vinyl alcohol) (PVA) for studying early stage of crystallization. They investigated the emergence of crystalline order from the isotropic melt by rapid quenching [51,52]. They could reproduce many elementary processes of homogenous nucleation that showed good correspondence with experiments and other simulations, in temperature dependence of lamella thickness, structure of fold surface, etc. In their work, they neglected long-range force (van der Waals attraction) to accelerate computation. Their model has the energy contribution due to intrachain interactions only and the dominant driving force for crystallization is entropic, which seems to ignore dominant driving force for polymer crystallization in conventional sense. However, their work is reminiscent of the classical solid\u2013liquid transition in systems of repulsive spherical atoms [53] and poses an intriguing problem as to the intrinsic driving force for polymer crystallization.\n", "keywords": "appropriate modeling\nclassical solid\u2013liquid transition\ncoarse-grained models\ncoarse-graining of the system\ncrystalline\ncrystallization\nfold surface\nfull-atomistic approaches\nhomogenous nucleation\nintrachain interactions\nisotropic melt\nlamella\nliquid\npolymer\npolymer crystallization\npolymers\npoly(vinyl alcohol)\nPVA\nrapid quenching\nrepulsive spherical atoms\nsimple polymers\nsimulations\nsolid\n"}, {"id": "S0032386109001712", "text": "With ever increasing computer performance, simulations in much larger systems have become feasible. However, full-atomistic approaches to polymer crystallization need extremely large computer power even in the case of simple polymers, and appropriate modeling or coarse-graining of the system is imperative. From a series of work on the development of coarse-grained models for polymers, Mayer and Muller-Plathe have build up a model of poly(vinyl alcohol) (PVA) for studying early stage of crystallization. They investigated the emergence of crystalline order from the isotropic melt by rapid quenching [51,52]. They could reproduce many elementary processes of homogenous nucleation that showed good correspondence with experiments and other simulations, in temperature dependence of lamella thickness, structure of fold surface, etc. In their work, they neglected long-range force (van der Waals attraction) to accelerate computation. Their model has the energy contribution due to intrachain interactions only and the dominant driving force for crystallization is entropic, which seems to ignore dominant driving force for polymer crystallization in conventional sense. However, their work is reminiscent of the classical solid\u2013liquid transition in systems of repulsive spherical atoms [53] and poses an intriguing problem as to the intrinsic driving force for polymer crystallization.\n", "keywords": "appropriate modeling\nclassical solid\u2013liquid transition\ncoarse-grained models\ncoarse-graining of the system\ncrystalline\ncrystallization\nfold surface\nfull-atomistic approaches\nhomogenous nucleation\nintrachain interactions\nisotropic melt\nlamella\nliquid\npolymer\npolymer crystallization\npolymers\npoly(vinyl alcohol)\nPVA\nrapid quenching\nrepulsive spherical atoms\nsimple polymers\nsimulations\nsolid\n"}, {"id": "S0166218X14003011", "text": "We study sequences of optimal walks of a growing length in weighted digraphs, or equivalently, sequences of entries of max-algebraic matrix powers with growing exponents. It is known that these sequences are eventually periodic when the digraphs are strongly connected. The transient of such periodicity depends, in general, both on the size of digraph and on the magnitude of the weights. In this paper, we show that some bounds on the indices of periodicity of (unweighted) digraphs, such as the bounds of Wielandt, Dulmage\u2013Mendelsohn, Schwarz, Kim and Gregory\u2013Kirkland\u2013Pullman, apply to the weights of optimal walks when one of their ends is a critical node.\n", "keywords": "digraph\ndigraphs are strongly connected\nsequences of entries of max-algebraic matrix powers with growing exponents\nsequences of optimal walks of a growing length in weighted digraphs\n(unweighted) digraphs\n"}, {"id": "S0167273815004130", "text": "Room temperature powder X-ray diffraction (XRD) was performed on a PANalytical Empyrean diffractometer. The obtained XRD patterns were analysed with STOE Win XPOW software in order to determine phase purity, the crystal structure and the cell parameters of the samples. Thermogravimetric analysis (TGA) was performed using a Netzsch STA 449C instrument equipped with Proteus thermal analysis software. The TGA studies were carried out under reducing conditions (5% H2/Ar) from room temperature to 900\u00b0C, in order to determine the weight change of the perovskite during the reduction. The microstructure of the samples' surface was analysed using a JEOL JSM-6700 field emission 74 scanning electron microscope (FEG-SEM). The total conductivity of the samples was measured using a conventional four-terminal method. Bar samples were prepared by calcination at 1300\u00b0C for 1h. Gold wire contacts were attached to the bars, which then were cured at 850\u00b0C for 1h. The conductivity of the samples was measured under a redox cycle at 900\u00b0C. Low oxygen partial pressure was achieved by using a continuous flow of 5% H2/Ar.\n", "keywords": "5% H2/Ar\ncalcination\ncell parameters\nconductivity\ncrystal structure\nFEG-SEM\nfield emission 74 scanning electron microscope\nflow\nfour-terminal method\nGold wire contacts\nH2/Ar\nLow oxygen partial pressure\nNetzsch STA 449C instrument\nPANalytical Empyrean diffractometer\nperovskite\nphase purity\npowder X-ray diffraction\nProteus thermal analysis software\nredox cycle\nreducing conditions\nSTOE Win XPOW software\nTGA\nThermogravimetric analysis\ntotal conductivity\nweight change\nXRD\nXRD patterns\n"}, {"id": "S0166361516300926", "text": "This research traces the implementation of an information system in the form of ERP modules covering tenant and contract management in a Chinese service company. Misalignments between the ERP system specification and user needs led to the adoption of informal processes within the organisation. These processes are facilitated within an informal organisational structure and are based on human interactions undertaken within the formal organisation. Rather than to attempt to suppress the emergence of the informal organisation the company decided to channel the energies of staff involved in informal processes towards organisational goals. The company achieved this by harnessing the capabilities of what we term a hybrid ERP system, combining the functionality of a traditional (formal) ERP installation with the capabilities of Enterprise Social Software (ESS). However the company recognised that the successful operation of the hybrid ERP system would require a number of changes in organisational design in areas such as reporting structures and communication channels. A narrative provided by interviews with company personnel is thematised around the formal and informal characteristics of the organisation as defined in the literature. This leads to a definition of the characteristics of the hybrid organisation and strategies for enabling a hybrid organisation, facilitated by a hybrid ERP system, which directs formal and informal behaviour towards organisational goals and provides a template for future hybrid implementations.\n", "keywords": "Enterprise Social Software\nERP installation\nERP modules\nERP system\nESS\nhybrid ERP system\n"}, {"id": "S0166361516300926", "text": "This research traces the implementation of an information system in the form of ERP modules covering tenant and contract management in a Chinese service company. Misalignments between the ERP system specification and user needs led to the adoption of informal processes within the organisation. These processes are facilitated within an informal organisational structure and are based on human interactions undertaken within the formal organisation. Rather than to attempt to suppress the emergence of the informal organisation the company decided to channel the energies of staff involved in informal processes towards organisational goals. The company achieved this by harnessing the capabilities of what we term a hybrid ERP system, combining the functionality of a traditional (formal) ERP installation with the capabilities of Enterprise Social Software (ESS). However the company recognised that the successful operation of the hybrid ERP system would require a number of changes in organisational design in areas such as reporting structures and communication channels. A narrative provided by interviews with company personnel is thematised around the formal and informal characteristics of the organisation as defined in the literature. This leads to a definition of the characteristics of the hybrid organisation and strategies for enabling a hybrid organisation, facilitated by a hybrid ERP system, which directs formal and informal behaviour towards organisational goals and provides a template for future hybrid implementations.\n", "keywords": "Enterprise Social Software\nERP installation\nERP modules\nERP system\nESS\nhybrid ERP system\n"}, {"id": "S037026930400680X", "text": "Certainly therefore the see-saw mechanism is an attractive explanation of why the light neutrino masses are so small. However, it is not without its faults. In particular, there is a tension between the strongly hierarchical nature of the observed Yukawa couplings in the quark and charged lepton sectors, and the essentially hierarchy-free masses implied by the \u0394m2's. Moreover, both the \u03b812 and \u03b823 mixing angles are large while the angle\u00a0\u03b813 is small which is in sharp contrast with the corresponding mixings in the quark sector which are all small. These problems can be solved in specific models, for example, the \u0394m2 values can be fitted by taking the spectrum of rhd neutrino masses to be hierarchical in such a way as to almost compensate for the hierarchical neutrino Yukawa couplings. But this has the price of introducing a wide range of rhd neutrino masses MR\u223c1010\u20131015 which then require explanation.\n", "keywords": "charged lepton sectors\nhierarchical neutrino Yukawa couplings\nhierarchy-free masses implied by the \u0394m2's\nlight neutrino masses\nMR\u223c1010\u20131015\nquark\nquark sector\nrhd neutrino masses\nsee-saw mechanism\nthe angle\u00a0\u03b813\nYukawa couplings\nYukawa couplings in the quark and charged lepton sectors\n\u0394m2 values can be fitted by taking the spectrum of rhd neutrino masses\n\u03b812 and \u03b823 mixing angles\n"}, {"id": "S0957417416303773", "text": "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen. Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002). In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances. The result is a set of orthogonal vectors sorted in descending order of achieved variance. The first of these vectors is that onto which the variance of the projection of the samples is maximum. In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance. To be rigorous, up to N synthetic orthogonal KPIs may be computed. However, only a small set of them, the first N^, is enough to account for most of the variance of the data.\n", "keywords": "a set of orthogonal vectors\ndata mining\ndimensionality reduction\nfinds the mutually-uncorrelated vectors\nlinear PCA\nN-dimensional vector space basis\nN^ synthetic KPIs\nPCA\nPrincipal Component Analysis method\nthe first N^\nthe projection of the samples generates the highest variances\nthe simplest version of PCA\nthe variance of the projection of the samples is maximum\nup to N synthetic orthogonal KPIs\n"}, {"id": "S0957417416303773", "text": "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen. Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002). In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances. The result is a set of orthogonal vectors sorted in descending order of achieved variance. The first of these vectors is that onto which the variance of the projection of the samples is maximum. In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance. To be rigorous, up to N synthetic orthogonal KPIs may be computed. However, only a small set of them, the first N^, is enough to account for most of the variance of the data.\n", "keywords": "a set of orthogonal vectors\ndata mining\ndimensionality reduction\nfinds the mutually-uncorrelated vectors\nlinear PCA\nN-dimensional vector space basis\nN^ synthetic KPIs\nPCA\nPrincipal Component Analysis method\nthe first N^\nthe projection of the samples generates the highest variances\nthe simplest version of PCA\nthe variance of the projection of the samples is maximum\nup to N synthetic orthogonal KPIs\n"}, {"id": "S0957417416303773", "text": "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen. Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002). In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances. The result is a set of orthogonal vectors sorted in descending order of achieved variance. The first of these vectors is that onto which the variance of the projection of the samples is maximum. In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance. To be rigorous, up to N synthetic orthogonal KPIs may be computed. However, only a small set of them, the first N^, is enough to account for most of the variance of the data.\n", "keywords": "a set of orthogonal vectors\ndata mining\ndimensionality reduction\nfinds the mutually-uncorrelated vectors\nlinear PCA\nN-dimensional vector space basis\nN^ synthetic KPIs\nPCA\nPrincipal Component Analysis method\nthe first N^\nthe projection of the samples generates the highest variances\nthe simplest version of PCA\nthe variance of the projection of the samples is maximum\nup to N synthetic orthogonal KPIs\n"}, {"id": "S0957417416303773", "text": "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen. Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002). In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances. The result is a set of orthogonal vectors sorted in descending order of achieved variance. The first of these vectors is that onto which the variance of the projection of the samples is maximum. In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance. To be rigorous, up to N synthetic orthogonal KPIs may be computed. However, only a small set of them, the first N^, is enough to account for most of the variance of the data.\n", "keywords": "a set of orthogonal vectors\ndata mining\ndimensionality reduction\nfinds the mutually-uncorrelated vectors\nlinear PCA\nN-dimensional vector space basis\nN^ synthetic KPIs\nPCA\nPrincipal Component Analysis method\nthe first N^\nthe projection of the samples generates the highest variances\nthe simplest version of PCA\nthe variance of the projection of the samples is maximum\nup to N synthetic orthogonal KPIs\n"}, {"id": "S0957417416303773", "text": "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen. Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002). In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances. The result is a set of orthogonal vectors sorted in descending order of achieved variance. The first of these vectors is that onto which the variance of the projection of the samples is maximum. In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance. To be rigorous, up to N synthetic orthogonal KPIs may be computed. However, only a small set of them, the first N^, is enough to account for most of the variance of the data.\n", "keywords": "a set of orthogonal vectors\ndata mining\ndimensionality reduction\nfinds the mutually-uncorrelated vectors\nlinear PCA\nN-dimensional vector space basis\nN^ synthetic KPIs\nPCA\nPrincipal Component Analysis method\nthe first N^\nthe projection of the samples generates the highest variances\nthe simplest version of PCA\nthe variance of the projection of the samples is maximum\nup to N synthetic orthogonal KPIs\n"}, {"id": "S0957417416303773", "text": "In the recent years and mainly motivated by the impulse of data mining many methods for dimensionality reduction have arisen. Within these, it is worth highlighting the Principal Component Analysis method (PCA) (Jolliffe, 2002). In an N-dimensional vector space, the simplest version of PCA (linear PCA) is a technique that finds the mutually-uncorrelated vectors onto which the projection of the samples generates the highest variances. The result is a set of orthogonal vectors sorted in descending order of achieved variance. The first of these vectors is that onto which the variance of the projection of the samples is maximum. In this sense, the original KPIs constitute the N-dimensional vector space basis, whereas the N^ synthetic KPIs represent the orthogonal vectors with the highest variance. To be rigorous, up to N synthetic orthogonal KPIs may be computed. However, only a small set of them, the first N^, is enough to account for most of the variance of the data.\n", "keywords": "a set of orthogonal vectors\ndata mining\ndimensionality reduction\nfinds the mutually-uncorrelated vectors\nlinear PCA\nN-dimensional vector space basis\nN^ synthetic KPIs\nPCA\nPrincipal Component Analysis method\nthe first N^\nthe projection of the samples generates the highest variances\nthe simplest version of PCA\nthe variance of the projection of the samples is maximum\nup to N synthetic orthogonal KPIs\n"}, {"id": "S1877750313001269", "text": "While virtualization technologies certainly reduce the complexity of using a system, and especially when working across multiple heterogeneous computing environments, they are not widely deployed in high performance computing scenarios. As its name suggest, HPC seeks to obtain maximum performance from computing platforms. Extra software layers impact detrimentally on performance, meaning that in HPC scenarios users typically run the applications as close to the \u2018bare metal\u2019 as possible. In addition to the performance degradation introduced by virtualization technologies, choosing what details to abstract in a virtualized interface is itself very important. Grid and cloud computing support different interaction models. In grid computing, the user interacts with an individual resource (or sometimes a broker) in order to launch jobs into a queuing system. In cloud computing, users interact with a virtual server, in effect putting them in control of their own complete operating system. Both of these interaction models put the onus on the user to understand very specific details of the system that they are dealing with, making life difficult for the end user, typically a scientist who wants to progress his or her scientific investigations without any specific usability hurdles obstructing the pathway.\n", "keywords": "applications\nbroker\ncloud computing\nGrid and cloud computing\ngrid computing\nhigh performance computing\nhigh performance computing scenarios\nHPC\ninteraction models\nqueuing system\nvirtualization technologies\nvirtualized interface\nvirtual server\n"}, {"id": "S0370269304009086", "text": "The ART model is a hadronic transport model that includes baryons such as N, \u0394(1232), N\u2217(1440), N\u2217(1535), \u039b, \u03a3, and mesons such as \u03c0, \u03c1, \u03c9, \u03b7, K, K\u2217. Both elastic and inelastic collisions among most of these particles are included by using the experimental data from hadron\u2013hadron collisions. The ART model has been quite successful in explaining many experimental observations, including the surprisingly large kaon antiflow [11,12] in heavy ion collisions at AGS energies. The ART model also allows us to understand whether or not the strongly interacting matter formed in these collisions reaches chemical and/or thermal equilibrium. In the present study, we extend the ART model to include perturbatively the \u039e particle as in the studies for other rare particles using the transport model [6,13,14].\n", "keywords": "AGS energies\nART model\nbaryons\ncollisions\nelastic and inelastic collisions\nexperimental data from hadron\u2013hadron collisions\nexplaining many experimental observations\nextend the ART model to include perturbatively the \u039e particle\nhadron\nhadron\u2013hadron collisions\nhadronic transport model\nheavy ion\nheavy ion collisions\nK\nK\u2217\nkaon\nkaon antiflow\nmesons\nN\u2217(1440)\nN\u2217(1535)\nN, \u0394(1232)\nrare particles\nstrongly interacting matter\nthese particles\ntransport model\nunderstand whether or not the strongly interacting matter formed in these collisions reaches chemical and/or thermal equilibrium\n\u03b7\n\u039b\n\u039e particle\n\u03c0\n\u03c1\n\u03a3\n\u03c9\n"}, {"id": "S0370269304009086", "text": "The ART model is a hadronic transport model that includes baryons such as N, \u0394(1232), N\u2217(1440), N\u2217(1535), \u039b, \u03a3, and mesons such as \u03c0, \u03c1, \u03c9, \u03b7, K, K\u2217. Both elastic and inelastic collisions among most of these particles are included by using the experimental data from hadron\u2013hadron collisions. The ART model has been quite successful in explaining many experimental observations, including the surprisingly large kaon antiflow [11,12] in heavy ion collisions at AGS energies. The ART model also allows us to understand whether or not the strongly interacting matter formed in these collisions reaches chemical and/or thermal equilibrium. In the present study, we extend the ART model to include perturbatively the \u039e particle as in the studies for other rare particles using the transport model [6,13,14].\n", "keywords": "AGS energies\nART model\nbaryons\ncollisions\nelastic and inelastic collisions\nexperimental data from hadron\u2013hadron collisions\nexplaining many experimental observations\nextend the ART model to include perturbatively the \u039e particle\nhadron\nhadron\u2013hadron collisions\nhadronic transport model\nheavy ion\nheavy ion collisions\nK\nK\u2217\nkaon\nkaon antiflow\nmesons\nN\u2217(1440)\nN\u2217(1535)\nN, \u0394(1232)\nrare particles\nstrongly interacting matter\nthese particles\ntransport model\nunderstand whether or not the strongly interacting matter formed in these collisions reaches chemical and/or thermal equilibrium\n\u03b7\n\u039b\n\u039e particle\n\u03c0\n\u03c1\n\u03a3\n\u03c9\n"}, {"id": "S2214509515300103", "text": "Another important reason for the damages incurred by the RC buildings is workmanship defects. It is understood that granulometry of the handmade concretes was not in compliance with the standards since the aggregate utilized in them was not sieved. Also the compaction process was not properly implemented in general in the installment of concrete in RC buildings. This situation resulted in the concrete to exhibit an excessively porous structure. The most fundamental rules of thumb of construction, namely concrete cover, was not taken care of in formwork workmanship. Faults in the connections of stirrups to the longitudinal bars, unstaggered formation of stirrup hooks in beams and columns, the perpendicular angles of the hooks, inadequately anchorage lengths of the stirrup hooks and longitudinal bars, and the use of cold joints were the other frequently encountered workmanship defects (Figs. 19\u201322).\n", "keywords": "beams\ncolumns\ncompaction process\nconcrete\nconcrete cover\nFaults in the connections of stirrups\ngranulometry of the handmade concretes\nhooks\ninadequately anchorage lengths\ninstallment of concrete\nlongitudinal bars\nperpendicular angles\nstirrup hooks\nunstaggered formation of stirrup\nuse of cold joints\nworkmanship defects\n"}, {"id": "S0022311514008691", "text": "The class of steels known as oxide dispersion strengthened (ODS) ferritic alloys (also known as nanostructured ferritic alloys) consist of a dispersion of ultra-fine oxide particles throughout the matrix. These oxide particles serve to improve the mechanical properties of the system, particularly at high temperatures, of the system through inhibiting dislocation motion and grain boundary sliding. In nuclear applications the oxide particles have been suggested to act as point defect sinks [10,11] to improve radiation tolerance, and as preferential sites for the formation of nano-scale He bubbles therefore reducing swelling compared to non-ODS steels [12\u201315]. The ability of the oxide particles to improve these properties depends on the structure and composition of the particles [10,11,16,17] and their stability under irradiation. Typical compositions of ODS steels include between 9 and 14at.% Cr for oxidation resistance (most commonly 14at.%); W for solid solution hardening; Y2O3 that is put into solid solution during the initial, mechanical alloying, process but then during consolidation at high temperatures forms precipitates; and Ti to inhibit significant growth of the oxide particles; the balance being made up of Fe and impurities [18]. For this reason these steels are often referred to as 14YWT, reflecting the constituent elements.\n", "keywords": "14YWT\n9 and 14at.% Cr\nbalance\nclass of steels\nconsolidation\nconstituent elements\ndispersion of ultra-fine oxide particles throughout the matrix\nFe and impurities\nferritic alloys\nimprove radiation tolerance\nimprove the mechanical properties of the system\nirradiation\nmechanical alloying\nnano-scale He bubbles\nnanostructured\nnon-ODS steels\nODS\nODS steels\noxidation\noxide dispersion strengthened\noxide particles\nprecipitates\nradiation\nreducing swelling\nsolid solution\nsteels\nstructure and composition of the particles\nswelling\nTi\nW\nY2O3\n"}, {"id": "S0045782513001473", "text": "In this paper, however, we prefer the simpler \u2018framed\u2019 cell employed by Hadjiconstantinou and Patera [16], where the shear stress is generated by constraining the velocity in a \u2018frame\u2019 rather than by modifying the shape of the box. The framed cell is periodic, but we cannot simply calculate the average stress in the whole box because the presence of an external buffer would produce spurious results. We need the local stress in the core region, but this complicates the Oij term in Eq. (3). There are other methods to calculate the stress tensor such as the method of planes [32], the volume-average approach [26,14], or the method derived from the Schweitz virial relation [25], but, in general, we must choose between a complicated computational cell (i.e. Lees\u2013Edwards cell) and simplifying the calculation of the momentum flux, or a simple cell (i.e. framed cell) and complicating the calculation of the momentum flux. The new method we propose here does not need the direct calculation of the flux, so it avoids this issue altogether: we can use the framed cell and, at the same time, avoid the calculation of the IK equation.\n", "keywords": "calculate the stress tensor\ncomplicated computational cell\nconstraining the velocity\nframed cell\nframed\u2019 cell\nIK equation\nLees\u2013Edwards cell\nmethod of planes\nSchweitz virial relation\nshear stress\nsimple cell\nvolume-average approach\n"}, {"id": "S0045782513001473", "text": "In this paper, however, we prefer the simpler \u2018framed\u2019 cell employed by Hadjiconstantinou and Patera [16], where the shear stress is generated by constraining the velocity in a \u2018frame\u2019 rather than by modifying the shape of the box. The framed cell is periodic, but we cannot simply calculate the average stress in the whole box because the presence of an external buffer would produce spurious results. We need the local stress in the core region, but this complicates the Oij term in Eq. (3). There are other methods to calculate the stress tensor such as the method of planes [32], the volume-average approach [26,14], or the method derived from the Schweitz virial relation [25], but, in general, we must choose between a complicated computational cell (i.e. Lees\u2013Edwards cell) and simplifying the calculation of the momentum flux, or a simple cell (i.e. framed cell) and complicating the calculation of the momentum flux. The new method we propose here does not need the direct calculation of the flux, so it avoids this issue altogether: we can use the framed cell and, at the same time, avoid the calculation of the IK equation.\n", "keywords": "calculate the stress tensor\ncomplicated computational cell\nconstraining the velocity\nframed cell\nframed\u2019 cell\nIK equation\nLees\u2013Edwards cell\nmethod of planes\nSchweitz virial relation\nshear stress\nsimple cell\nvolume-average approach\n"}, {"id": "S0168365912007560", "text": "We addressed the question whether carbohydrate coupling increased antigen uptake by DCs via C-type lectin receptor targeting. Therefore, the antigens were labeled with pHrodo Red dye (Invitrogen), a dye that specifically fluoresces as pH decreases from neutral to acidic, as provided in endosomes/lysosomes of cells. In vitro characterization of the cellular uptake of neoglycocomplexes using bone marrow derived dendritic cells (BMDCs) demonstrated superior ingestion of mannan-conjugates MN\u2013Ova and MN\u2013Pap (Supplementary Fig. S4A-D,F). This was confirmed in vivo by intradermal needle-injection of labeled antigen into the ear pinnae of mice. Antigen uptake and transport to the ear dLNs were measured after 24h by FACS analysis. DCs in cervical LNs were identified according to their high expression of MHC class II (Fig.\u00a03A) and additionally characterized by CD8\u03b1, CD11b, and CD11c expression and uptake of pHrodo-labeled antigen (Fig.\u00a03B, D\u2013F). The results showed significantly elevated numbers of pHrodo+MHCIIhigh DCs for mannan conjugates MN\u2013Ova and MN\u2013Pap (and to a lesser degree for MD\u2013Pap) in comparison to the unmodified antigens (Fig.\u00a03C). Both carbohydrates targeted antigen preferentially to CD8\u03b1\u2212 DCs, as indicated by an increase in CD8\u03b1\u2212/pHrodo+ DCs compared to unmodified antigens (Fig.\u00a03E and F). Nevertheless, whether the antigens were taken up in situ by dermal DCs or by LN resident APCs via the afferent lymphatics could not be fully elucidated. Histology revealed that antigen-loaded cells in the dLNs were already present 30min after intradermal injection (Supplementary Fig. S4G), suggesting both mechanisms.\n", "keywords": "antigen\nAntigen\nantigen-loaded cells\nantigens\nBMDCs\nbone marrow derived dendritic cells\ncarbohydrates\nCD11b\nCD11c\nCD8\u03b1\nCD8\u03b1\u2212 DCs\nCD8\u03b1\u2212/pHrodo+ DCs\nC-type lectin receptor targeting\nendosomes\nFACS analysis\nfluoresces\ningestion\nintradermal injection\nintradermal needle-injection\nInvitrogen\nlysosomes\nmannan-conjugates MN\u2013Ova and MN\u2013Pap\nMD\u2013Pap\nMN\u2013Ova\nMN\u2013Pap\nneoglycocomplexes\npHrodo-labeled antigen\npHrodo+MHCIIhigh DCs\npHrodo Red dye\nwhether carbohydrate coupling increased antigen uptake\n"}, {"id": "S0168365912007560", "text": "We addressed the question whether carbohydrate coupling increased antigen uptake by DCs via C-type lectin receptor targeting. Therefore, the antigens were labeled with pHrodo Red dye (Invitrogen), a dye that specifically fluoresces as pH decreases from neutral to acidic, as provided in endosomes/lysosomes of cells. In vitro characterization of the cellular uptake of neoglycocomplexes using bone marrow derived dendritic cells (BMDCs) demonstrated superior ingestion of mannan-conjugates MN\u2013Ova and MN\u2013Pap (Supplementary Fig. S4A-D,F). This was confirmed in vivo by intradermal needle-injection of labeled antigen into the ear pinnae of mice. Antigen uptake and transport to the ear dLNs were measured after 24h by FACS analysis. DCs in cervical LNs were identified according to their high expression of MHC class II (Fig.\u00a03A) and additionally characterized by CD8\u03b1, CD11b, and CD11c expression and uptake of pHrodo-labeled antigen (Fig.\u00a03B, D\u2013F). The results showed significantly elevated numbers of pHrodo+MHCIIhigh DCs for mannan conjugates MN\u2013Ova and MN\u2013Pap (and to a lesser degree for MD\u2013Pap) in comparison to the unmodified antigens (Fig.\u00a03C). Both carbohydrates targeted antigen preferentially to CD8\u03b1\u2212 DCs, as indicated by an increase in CD8\u03b1\u2212/pHrodo+ DCs compared to unmodified antigens (Fig.\u00a03E and F). Nevertheless, whether the antigens were taken up in situ by dermal DCs or by LN resident APCs via the afferent lymphatics could not be fully elucidated. Histology revealed that antigen-loaded cells in the dLNs were already present 30min after intradermal injection (Supplementary Fig. S4G), suggesting both mechanisms.\n", "keywords": "antigen\nAntigen\nantigen-loaded cells\nantigens\nBMDCs\nbone marrow derived dendritic cells\ncarbohydrates\nCD11b\nCD11c\nCD8\u03b1\nCD8\u03b1\u2212 DCs\nCD8\u03b1\u2212/pHrodo+ DCs\nC-type lectin receptor targeting\nendosomes\nFACS analysis\nfluoresces\ningestion\nintradermal injection\nintradermal needle-injection\nInvitrogen\nlysosomes\nmannan-conjugates MN\u2013Ova and MN\u2013Pap\nMD\u2013Pap\nMN\u2013Ova\nMN\u2013Pap\nneoglycocomplexes\npHrodo-labeled antigen\npHrodo+MHCIIhigh DCs\npHrodo Red dye\nwhether carbohydrate coupling increased antigen uptake\n"}, {"id": "S088523081530036X", "text": "Note that the presented architecture works at the frame level, meaning that each single frame (plus its corresponding context) is fed-forward through the network, obtaining a class posterior probability for all of the target languages. This fact makes the DNNs particularly suitable for real-time applications because, unlike other approaches (i.e. i-vectors), we can potentially make a decision about the language at each new frame. Indeed, at each frame, we can combine the evidence from past frames to get a single similarity score between the test utterance and the targetlanguages. A simple way of doing this combination is to assume that frames are independent and multiply the posterior estimates of the last layer. The score sl for language l of a given test utterance is computed by multiplying the output probabilities pl obtained for all of its frames; or equivalently, accumulating the logs as:(6)sl=1N\u2211t=1Nlogp(Ll|xt\u200b,\u2009\u03b8)where p(Ll|xt\u200b,\u2009\u03b8) represents the class probability output for the language l corresponding to the input example at time t, xt by using the DNN defined by parameters \u03b8.\n", "keywords": "accumulating the logs as:(6)sl=1N\u2211t=1Nlogp(Ll|xt\u200b,\u2009\u03b8)\nassume that frames are independent and multiply the posterior estimates of the last layer\ncombine the evidence from past frames\nDNN\nDNNs\nevidence from past frames\ni-vectors\nmultiplying the output probabilities pl obtained for all of its frames\nother approaches\ntarget languages\ntest utterance\n"}, {"id": "S0010938X15003261", "text": "The related Volta potential (\u03a8) is the potential difference between a position infinitely far away from the surface and a position just outside the surface, and is the measureable quantity characterising electrochemical behaviour of a metal [12,17]. The scanning Kelvin probe force microscopy (SKPFM) technique allows detection of local EWF (if the EWF of the tip is known), or Volta potential differences (\u0394\u03a8) between an atomic force microscopy tip (usually Pt coated) and the metal surface [14,15,19]. The lateral resolution of SKPFM can be as high as 10\u2019s of nm in ambient air, with a sensitivity up to 10\u201320meV [19]. Volta potential is a characteristic property of a metal surface and can be used to understand electrochemical processes [16] . It is sensitive to any kind of surface defects, chemical variations, and residual stress [13,17]. Volta potential differences in microstructure have been used to predict corrosion behaviour [10,15,18,20\u201322]. Regions with larger (\u0394\u03a8) indicate increased surface reactivity [11,15,18], and even a correlation between Volta potential differences measured in nominally dry air and their free corrosion potential (Ecorr) pre-determined under immersed conditions has been reported [18].\n", "keywords": "ambient air\natomic force microscopy tip\ndetection of local EWF\ndry air\nEcorr\nelectrochemical behaviour of a metal\nelectrochemical processes\nfree corrosion potential\nlarger (\u0394\u03a8) indicate increased surface reactivity\nmeasureable quantity\nmetal\nmetal surface\npotential difference\npredict corrosion behaviour\nproperty of a metal surface\nPt coated\nscanning Kelvin probe force microscopy\nSKPFM\nunderstand electrochemical processes\nVolta potential\nVolta potential differences\nVolta potential differences in microstructure\n\u0394\u03a8\n\u03a8\n"}, {"id": "S0021999113005603", "text": "After all micro elements reach a relaxed steady-state, measurements are obtained using a cumulative averaging technique to reduce noise. Each micro element is divided into spatially-oriented bins in the y-direction in order to resolve the velocity and shear-stress profiles. Velocity in each bin is measured using the Cumulative Averaging Method (CAM) [24], while the stress tensor field is measured using the Irving\u2013Kirkwood relationship [25]. A least-squares polynomial fit to the data is performed, which helps reduce noise further. The fit produces a continuous function that avoids stability issues arising from supplying highly fluctuating data to the macro solver. A least-squares fit is applied to an Nth order polynomial for the velocity profile in the core region, and an Mth order polynomial for the velocity profile in the constrained region:(16)\u3008ui,core\u3009=\u2211k=1Nbk,iyi\u2032(N\u2212k),for 0\u2a7dyi\u2032\u2a7dhcore, and(17)\u3008ui,cs\u3009=\u2211k=1Mck,iyi\u2033(M\u2212k),for 0\u2a7dyi\u2033\u2a7dhcs, where bk,i and ck,i are the coefficients of the polynomials used in the core micro region and constrained region respectively. An estimate of the new slip velocity uB for input to the macro solution (6) is taken directly from the compressed wall micro-element solution (16), at yi\u2032=0.\n", "keywords": "CAM\ncompressed wall micro-element solution\ncontinuous function\nCumulative Averaging Method\ncumulative averaging technique\ndivided into spatially-oriented bins\nIrving\u2013Kirkwood relationship\nleast-squares fit\nleast-squares polynomial fit\nmacro solution\nmacro solver\nMth order polynomial\nnew slip velocity\nNth order polynomial\npolynomials\nreduce noise\nresolve the velocity and shear-stress profiles\nshear-stress profiles\nstability issues\nstress tensor field\nsupplying highly fluctuating data to the macro solver\nvelocity\n"}, {"id": "S0377025714000317", "text": "Denier and Hewitt [12] have shown that bounded solutions to 9a, 9b and 9c subject to (10a) and (10b) exist only in the shear-thinning case for n>12. In the shear-thickening case they have shown that solutions become non-differentiable at some critical location \u03b7c, and although it transpires that this singularity can be regularised entirely within the context of the power-law model, we will not consider such flows here. Thus in this study we will consider flows with power-law index in the range 12<n\u2a7d1. They have also shown that for 12<n<1 to ensure the correct algebraic decay in the numerical solutions one must apply the Robin condition(11)(u\u00af\u2032,v\u00af\u2032)=n\u03b7(n-1)(u\u00af,v\u00af)as\u03b7\u2192\u221e,at some suitably large value of \u03b7=\u03b7\u221e\u226b1. In the Newtonian case this relationship becomes singular, this is due to the fact that when n=1 the functions u\u00af and v\u00af decay exponentially. Cochran [13] showed that in this case(12)(u\u00af\u2032,v\u00af\u2032)=w\u00af\u221e(u\u00af,v\u00af)as\u03b7\u2192\u221e,where w\u221e=-2\u222b0\u221eu\u00afd\u03b7.\n", "keywords": "correct algebraic decay\nflows\npower-law model\nRobin condition(11)\nshear-thickening\nsingularity\nsolutions\n"}, {"id": "S0377025714000317", "text": "Denier and Hewitt [12] have shown that bounded solutions to 9a, 9b and 9c subject to (10a) and (10b) exist only in the shear-thinning case for n>12. In the shear-thickening case they have shown that solutions become non-differentiable at some critical location \u03b7c, and although it transpires that this singularity can be regularised entirely within the context of the power-law model, we will not consider such flows here. Thus in this study we will consider flows with power-law index in the range 12<n\u2a7d1. They have also shown that for 12<n<1 to ensure the correct algebraic decay in the numerical solutions one must apply the Robin condition(11)(u\u00af\u2032,v\u00af\u2032)=n\u03b7(n-1)(u\u00af,v\u00af)as\u03b7\u2192\u221e,at some suitably large value of \u03b7=\u03b7\u221e\u226b1. In the Newtonian case this relationship becomes singular, this is due to the fact that when n=1 the functions u\u00af and v\u00af decay exponentially. Cochran [13] showed that in this case(12)(u\u00af\u2032,v\u00af\u2032)=w\u00af\u221e(u\u00af,v\u00af)as\u03b7\u2192\u221e,where w\u221e=-2\u222b0\u221eu\u00afd\u03b7.\n", "keywords": "correct algebraic decay\nflows\npower-law model\nRobin condition(11)\nshear-thickening\nsingularity\nsolutions\n"}, {"id": "S0377025714000317", "text": "Denier and Hewitt [12] have shown that bounded solutions to 9a, 9b and 9c subject to (10a) and (10b) exist only in the shear-thinning case for n>12. In the shear-thickening case they have shown that solutions become non-differentiable at some critical location \u03b7c, and although it transpires that this singularity can be regularised entirely within the context of the power-law model, we will not consider such flows here. Thus in this study we will consider flows with power-law index in the range 12<n\u2a7d1. They have also shown that for 12<n<1 to ensure the correct algebraic decay in the numerical solutions one must apply the Robin condition(11)(u\u00af\u2032,v\u00af\u2032)=n\u03b7(n-1)(u\u00af,v\u00af)as\u03b7\u2192\u221e,at some suitably large value of \u03b7=\u03b7\u221e\u226b1. In the Newtonian case this relationship becomes singular, this is due to the fact that when n=1 the functions u\u00af and v\u00af decay exponentially. Cochran [13] showed that in this case(12)(u\u00af\u2032,v\u00af\u2032)=w\u00af\u221e(u\u00af,v\u00af)as\u03b7\u2192\u221e,where w\u221e=-2\u222b0\u221eu\u00afd\u03b7.\n", "keywords": "correct algebraic decay\nflows\npower-law model\nRobin condition(11)\nshear-thickening\nsingularity\nsolutions\n"}, {"id": "S2212667812000937", "text": "In this paper, we present algorithms for automatic generation of logic reasoning questions. The algorithms are able to construct questions that are solvable with unique solutions. The algorithms employ AI techniques such as semantic networks to produce verbal questions. These algorithms are small in size and are able to replace traditional question databases. They are particularly suitable for implementation on the memory constrained mobile platforms. The algorithms can be applied to question generation for job interview, civil service exam, etc.", "keywords": "AI techniques\nalgorithms for automatic generation of logic reasoning questions.\nconstruct questions that are solvable with unique solutions\ngeneration of logic reasoning questions\nmemory constrained mobile platforms\nquestion generation for job interview\nsemantic networks\ntraditional question databases\nverbal questions\n"}, {"id": "S0045782514004812", "text": "We shall establish the variational format in the space\u2013time domain S=def\u03a9\u00d7I, for given spatial domain \u03a9 and time domain I=(0,T), for a quite broad class of problems involving a first order time-derivative. In particular, the coupled problem of consolidation of geomaterials falls within this class. Another interesting application is the problem of dynamics, rewritten in first-order form, i.e.\u00a0through a Hamiltonian description. It is of considerable interest to note from the outset that, due to the forward transport of information in time, it is always possible to consider a set of finite time intervals, whereby the solution at the end of any such interval will act as the initial data for the next one. To this end, we introduce a partition 0=t0<t1<\u22ef<tN=T of the considered time domain I=(0,T) into time-intervals In=(tn\u22121,tn) of length \u0394tn=tn\u2212tn\u22121.11The abbreviated notation \u0394t=\u0394tn will be used henceforth for the current time step associated with In. Hence, we define space\u2013time slabs Sn=def\u03a9\u00d7In such that the space\u2013time domain can be given as S=def\u03a9\u00d7I=S1\u222aS2\u22ef\u222aSn.\n", "keywords": "coupled problem of consolidation of geomaterials\nfirst order time-derivative\nHamiltonian description\nproblem of dynamics, rewritten in first-order form\nS=def\u03a9\u00d7I\nS=def\u03a9\u00d7I=S1\u222aS2\u22ef\u222aSn\nSn=def\u03a9\u00d7In\nspace\u2013time domain\nspace\u2013time slabs\nspatial domain \u03a9\ntime domain I=(0,T)\nvariational format in the space\u2013time domain\n"}, {"id": "S2212667812000664", "text": "According to the shortcomings of long time and big errors about the moving plate recognition system, we present the moving plate recognition algorithm based on principal component analysis(PCA) color extraction. On the basis of the analysis of moving plate recognition system's basic principles, it introduces the basic principles and calculation steps about PCA extraction algorithm, and discusses the feasibility of applying the algorithm to PRS in the paper. The experimental results show that the algorithm has the advantages of faster speed and higher accuracy of recognition. The algorithm provides a new thought for the research on the moving plate recognition algorithm.", "keywords": "analysis of moving plate recognition system's basic principles\napplying the algorithm to PRS\ncolor extraction.\nmoving plate recognition\nmoving plate recognition algorithm\nmoving plate recognition system\nPCA\nPCA extraction algorithm\nprincipal component analysis\nPRS\nrecognition\nresearch on the moving plate recognition algorithm\n"}, {"id": "S0257897213004131", "text": "Fig.\u00a07 shows the relationship between the testing time and friction coefficients of various samples under dry conditions. There exist running in and steady wear period in the wear process of uncoated AZ31 and anodizing coating without Al2O3 nanoparticles while there has a steady wear period only in the wear process of composite anodizing coating with Al2O3 nanoparticles. At the same time, the addition of nano-particles to electrolyte led to reduction of friction coefficient. The friction coefficient of composite coating is relatively lower and more stable than what has been reported in literature [24,25] for anodizing coatings. This may be caused by \u201crolling effect\u201d made by Al2O3 nanoparticles on the surface of oxide coating. Spherical nanoparticles change sliding into rolling, which reduce friction, making the friction coefficient becomes more stable. The friction coefficient of anodizing coating without Al2O3 nanoparticles has large fluctuation maybe for the damage of coating. In contrast to the uncoated AZ31 magnesium alloy, the anodizing coatings show slightly lower friction coefficient. This can be attributed to their higher load-bearing capacity for high hardness.\n", "keywords": "Al2O3 nanoparticles\nanodizing coatings\nanodizing coating without Al2O3 nanoparticles\ncoating\ncomposite anodizing coating\ncomposite coating\nelectrolyte\nfriction coefficient\nfriction coefficients\nnano-particles\nreduce friction\nreduction of friction coefficient\n\u201crolling effect\u201d\nSpherical nanoparticles\ntesting time\nuncoated AZ31\nuncoated AZ31 magnesium alloy\nwear process\n"}, {"id": "S0257897213004131", "text": "Fig.\u00a07 shows the relationship between the testing time and friction coefficients of various samples under dry conditions. There exist running in and steady wear period in the wear process of uncoated AZ31 and anodizing coating without Al2O3 nanoparticles while there has a steady wear period only in the wear process of composite anodizing coating with Al2O3 nanoparticles. At the same time, the addition of nano-particles to electrolyte led to reduction of friction coefficient. The friction coefficient of composite coating is relatively lower and more stable than what has been reported in literature [24,25] for anodizing coatings. This may be caused by \u201crolling effect\u201d made by Al2O3 nanoparticles on the surface of oxide coating. Spherical nanoparticles change sliding into rolling, which reduce friction, making the friction coefficient becomes more stable. The friction coefficient of anodizing coating without Al2O3 nanoparticles has large fluctuation maybe for the damage of coating. In contrast to the uncoated AZ31 magnesium alloy, the anodizing coatings show slightly lower friction coefficient. This can be attributed to their higher load-bearing capacity for high hardness.\n", "keywords": "Al2O3 nanoparticles\nanodizing coatings\nanodizing coating without Al2O3 nanoparticles\ncoating\ncomposite anodizing coating\ncomposite coating\nelectrolyte\nfriction coefficient\nfriction coefficients\nnano-particles\nreduce friction\nreduction of friction coefficient\n\u201crolling effect\u201d\nSpherical nanoparticles\ntesting time\nuncoated AZ31\nuncoated AZ31 magnesium alloy\nwear process\n"}, {"id": "S0010938X13002187", "text": "The results from two types of oxidation test are combined in this study. Table 1 shows the test matrix with the two approaches included. All the 100h tests and the test conducted at 650\u00b0C were performed using a thermogravimetric balance (TGA). The weight change during these tests was monitored continually and adjusted to accommodate buoyancy effects. All other tests were conducted in horizontal tube furnaces. For these latter tests, batches of specimens were placed in alumina boats and inserted into the furnaces at temperature. Intermittent weighing at room temperature was used to determine the oxidation kinetics. At selected time intervals, a specimen was removed from the batch for examination before the high temperature exposure continued for the remainder of the batch. Table 1 shows the time intervals chosen for examination. At 600\u00b0C one isothermal test, having an exposure time of 1000h, has been performed to date.\n", "keywords": "adjusted to accommodate buoyancy effects\nalumina boats\ndetermine the oxidation kinetics\nfurnaces\nhorizontal tube furnaces\noxidation test\nTable 1 shows the time intervals chosen for examination\ntest matrix\nTGA\nthermogravimetric balance\nweight change during these tests was monitored continually\n"}, {"id": "S0010938X13002187", "text": "The results from two types of oxidation test are combined in this study. Table 1 shows the test matrix with the two approaches included. All the 100h tests and the test conducted at 650\u00b0C were performed using a thermogravimetric balance (TGA). The weight change during these tests was monitored continually and adjusted to accommodate buoyancy effects. All other tests were conducted in horizontal tube furnaces. For these latter tests, batches of specimens were placed in alumina boats and inserted into the furnaces at temperature. Intermittent weighing at room temperature was used to determine the oxidation kinetics. At selected time intervals, a specimen was removed from the batch for examination before the high temperature exposure continued for the remainder of the batch. Table 1 shows the time intervals chosen for examination. At 600\u00b0C one isothermal test, having an exposure time of 1000h, has been performed to date.\n", "keywords": "adjusted to accommodate buoyancy effects\nalumina boats\ndetermine the oxidation kinetics\nfurnaces\nhorizontal tube furnaces\noxidation test\nTable 1 shows the time intervals chosen for examination\ntest matrix\nTGA\nthermogravimetric balance\nweight change during these tests was monitored continually\n"}, {"id": "S0010938X13002187", "text": "The results from two types of oxidation test are combined in this study. Table 1 shows the test matrix with the two approaches included. All the 100h tests and the test conducted at 650\u00b0C were performed using a thermogravimetric balance (TGA). The weight change during these tests was monitored continually and adjusted to accommodate buoyancy effects. All other tests were conducted in horizontal tube furnaces. For these latter tests, batches of specimens were placed in alumina boats and inserted into the furnaces at temperature. Intermittent weighing at room temperature was used to determine the oxidation kinetics. At selected time intervals, a specimen was removed from the batch for examination before the high temperature exposure continued for the remainder of the batch. Table 1 shows the time intervals chosen for examination. At 600\u00b0C one isothermal test, having an exposure time of 1000h, has been performed to date.\n", "keywords": "adjusted to accommodate buoyancy effects\nalumina boats\ndetermine the oxidation kinetics\nfurnaces\nhorizontal tube furnaces\noxidation test\nTable 1 shows the time intervals chosen for examination\ntest matrix\nTGA\nthermogravimetric balance\nweight change during these tests was monitored continually\n"}, {"id": "S0010938X13002187", "text": "The results from two types of oxidation test are combined in this study. Table 1 shows the test matrix with the two approaches included. All the 100h tests and the test conducted at 650\u00b0C were performed using a thermogravimetric balance (TGA). The weight change during these tests was monitored continually and adjusted to accommodate buoyancy effects. All other tests were conducted in horizontal tube furnaces. For these latter tests, batches of specimens were placed in alumina boats and inserted into the furnaces at temperature. Intermittent weighing at room temperature was used to determine the oxidation kinetics. At selected time intervals, a specimen was removed from the batch for examination before the high temperature exposure continued for the remainder of the batch. Table 1 shows the time intervals chosen for examination. At 600\u00b0C one isothermal test, having an exposure time of 1000h, has been performed to date.\n", "keywords": "adjusted to accommodate buoyancy effects\nalumina boats\ndetermine the oxidation kinetics\nfurnaces\nhorizontal tube furnaces\noxidation test\nTable 1 shows the time intervals chosen for examination\ntest matrix\nTGA\nthermogravimetric balance\nweight change during these tests was monitored continually\n"}, {"id": "S0010938X13002187", "text": "The results from two types of oxidation test are combined in this study. Table 1 shows the test matrix with the two approaches included. All the 100h tests and the test conducted at 650\u00b0C were performed using a thermogravimetric balance (TGA). The weight change during these tests was monitored continually and adjusted to accommodate buoyancy effects. All other tests were conducted in horizontal tube furnaces. For these latter tests, batches of specimens were placed in alumina boats and inserted into the furnaces at temperature. Intermittent weighing at room temperature was used to determine the oxidation kinetics. At selected time intervals, a specimen was removed from the batch for examination before the high temperature exposure continued for the remainder of the batch. Table 1 shows the time intervals chosen for examination. At 600\u00b0C one isothermal test, having an exposure time of 1000h, has been performed to date.\n", "keywords": "adjusted to accommodate buoyancy effects\nalumina boats\ndetermine the oxidation kinetics\nfurnaces\nhorizontal tube furnaces\noxidation test\nTable 1 shows the time intervals chosen for examination\ntest matrix\nTGA\nthermogravimetric balance\nweight change during these tests was monitored continually\n"}, {"id": "S0010938X13002187", "text": "The results from two types of oxidation test are combined in this study. Table 1 shows the test matrix with the two approaches included. All the 100h tests and the test conducted at 650\u00b0C were performed using a thermogravimetric balance (TGA). The weight change during these tests was monitored continually and adjusted to accommodate buoyancy effects. All other tests were conducted in horizontal tube furnaces. For these latter tests, batches of specimens were placed in alumina boats and inserted into the furnaces at temperature. Intermittent weighing at room temperature was used to determine the oxidation kinetics. At selected time intervals, a specimen was removed from the batch for examination before the high temperature exposure continued for the remainder of the batch. Table 1 shows the time intervals chosen for examination. At 600\u00b0C one isothermal test, having an exposure time of 1000h, has been performed to date.\n", "keywords": "adjusted to accommodate buoyancy effects\nalumina boats\ndetermine the oxidation kinetics\nfurnaces\nhorizontal tube furnaces\noxidation test\nTable 1 shows the time intervals chosen for examination\ntest matrix\nTGA\nthermogravimetric balance\nweight change during these tests was monitored continually\n"}, {"id": "S0010938X13002187", "text": "The results from two types of oxidation test are combined in this study. Table 1 shows the test matrix with the two approaches included. All the 100h tests and the test conducted at 650\u00b0C were performed using a thermogravimetric balance (TGA). The weight change during these tests was monitored continually and adjusted to accommodate buoyancy effects. All other tests were conducted in horizontal tube furnaces. For these latter tests, batches of specimens were placed in alumina boats and inserted into the furnaces at temperature. Intermittent weighing at room temperature was used to determine the oxidation kinetics. At selected time intervals, a specimen was removed from the batch for examination before the high temperature exposure continued for the remainder of the batch. Table 1 shows the time intervals chosen for examination. At 600\u00b0C one isothermal test, having an exposure time of 1000h, has been performed to date.\n", "keywords": "adjusted to accommodate buoyancy effects\nalumina boats\ndetermine the oxidation kinetics\nfurnaces\nhorizontal tube furnaces\noxidation test\nTable 1 shows the time intervals chosen for examination\ntest matrix\nTGA\nthermogravimetric balance\nweight change during these tests was monitored continually\n"}, {"id": "S0010938X13002187", "text": "The results from two types of oxidation test are combined in this study. Table 1 shows the test matrix with the two approaches included. All the 100h tests and the test conducted at 650\u00b0C were performed using a thermogravimetric balance (TGA). The weight change during these tests was monitored continually and adjusted to accommodate buoyancy effects. All other tests were conducted in horizontal tube furnaces. For these latter tests, batches of specimens were placed in alumina boats and inserted into the furnaces at temperature. Intermittent weighing at room temperature was used to determine the oxidation kinetics. At selected time intervals, a specimen was removed from the batch for examination before the high temperature exposure continued for the remainder of the batch. Table 1 shows the time intervals chosen for examination. At 600\u00b0C one isothermal test, having an exposure time of 1000h, has been performed to date.\n", "keywords": "adjusted to accommodate buoyancy effects\nalumina boats\ndetermine the oxidation kinetics\nfurnaces\nhorizontal tube furnaces\noxidation test\nTable 1 shows the time intervals chosen for examination\ntest matrix\nTGA\nthermogravimetric balance\nweight change during these tests was monitored continually\n"}, {"id": "S0963869514000863", "text": "EM sensors exploit the difference in magnetic properties, such as relative permeability, and electrical conductivity between samples with different microstructural phase balances. In ferromagnetic steels, the change in relative permeability has a significant effect. Previously, multi-frequency EM sensors have been shown to be able to measure austenite/ferrite fraction from 0% to 100% in model (HIPped austenitic/ferritc stainless steel powder) alloys [7,8]. The large difference in magnetic properties of ferrite (ferromagnetic) and austenite (paramagnetic) phases makes the change in signal large and hence relatively easy to measure. EM sensors have also measured the levels of decarburisation (variation in ferrite content with depth) in steel rod [9,10]. The approach adopted to relate the overall steel EM sensor signal to its microstructure has been to construct a finite element (FE) model for the microstructure (phase, region size and distribution). The EM properties of the individual phases are assigned to those regions to give the overall EM properties of the steel. Within the model the particular sensor geometry is included (e.g. two-dimensional axisymmetric for a cylindrical sample and tubular sensor [10]) and the interaction with the steel and any external circuits predicted. In this way different microstructures and sensor designs can be compared.\n", "keywords": "austenite\ncylindrical sample\ndecarburisation\ndifference in magnetic properties\nelectrical conductivity\nEM properties of the individual phases\nEM sensor\nEM sensors\nEM sensor signal\nexternal circuits\nFE\nferrite\nferromagnetic\nferromagnetic steels\nfinite element\nmicrostructural phase balances\nmicrostructures\nmulti-frequency EM sensors\noverall EM properties\nparamagnetic\nparticular sensor geometry\nrelative permeability\nsensor\nstainless steel powder\nsteel\nsteel rod\ntubular sensor\nvariation in ferrite content with depth\n"}, {"id": "S0963869514000863", "text": "EM sensors exploit the difference in magnetic properties, such as relative permeability, and electrical conductivity between samples with different microstructural phase balances. In ferromagnetic steels, the change in relative permeability has a significant effect. Previously, multi-frequency EM sensors have been shown to be able to measure austenite/ferrite fraction from 0% to 100% in model (HIPped austenitic/ferritc stainless steel powder) alloys [7,8]. The large difference in magnetic properties of ferrite (ferromagnetic) and austenite (paramagnetic) phases makes the change in signal large and hence relatively easy to measure. EM sensors have also measured the levels of decarburisation (variation in ferrite content with depth) in steel rod [9,10]. The approach adopted to relate the overall steel EM sensor signal to its microstructure has been to construct a finite element (FE) model for the microstructure (phase, region size and distribution). The EM properties of the individual phases are assigned to those regions to give the overall EM properties of the steel. Within the model the particular sensor geometry is included (e.g. two-dimensional axisymmetric for a cylindrical sample and tubular sensor [10]) and the interaction with the steel and any external circuits predicted. In this way different microstructures and sensor designs can be compared.\n", "keywords": "austenite\ncylindrical sample\ndecarburisation\ndifference in magnetic properties\nelectrical conductivity\nEM properties of the individual phases\nEM sensor\nEM sensors\nEM sensor signal\nexternal circuits\nFE\nferrite\nferromagnetic\nferromagnetic steels\nfinite element\nmicrostructural phase balances\nmicrostructures\nmulti-frequency EM sensors\noverall EM properties\nparamagnetic\nparticular sensor geometry\nrelative permeability\nsensor\nstainless steel powder\nsteel\nsteel rod\ntubular sensor\nvariation in ferrite content with depth\n"}, {"id": "S0370269304007695", "text": "In summary, we have shown that one can describe the experimental data of the HERMES Collaboration for hadron attenuation on nuclei without invoking any changes in the fragmentation function due to gluon radiation. In our dynamical studies, that include the most relevant FSI, we employ only the \u2018free\u2019 fragmentation function on a nucleon and attribute the hadron attenuation to the deceleration of the produced (pre-)hadrons due to FSI in the surrounding medium. We find that in particular the z-dependence of RMh is very sensitive to the interaction cross section of leading prehadrons and can be used to determine \u03c3lead. The interaction of the leading prehadrons during the formation time could be interpreted as an in-medium change of the fragmentation function, which however could not be given in a closed form. The extracted average hadron formation times of \u03c4f\u22730.3\u00a0fm/c are compatible with the analysis of antiproton attenuation in p+A reactions at AGS energies [17]. In an upcoming work we will investigate in detail the spectra for different particle species (\u03c0\u00b1,K\u00b1,p,p\u0304) to examine, if the formation times of mesons and antibaryons are about equal. In addition we will improve our model to describe the primary photon\u2013nucleon reaction below the PYTHIA threshold of W\u2a7e4\u00a0GeV.\n", "keywords": "analysis of antiproton attenuation\nantibaryons\nantiproton\ndescribe the experimental data of the HERMES Collaboration for hadron attenuation on nuclei\ndetail the spectra for different particle species (\u03c0\u00b1,K\u00b1,p,p\u0304)\ndetermine \u03c3lead\ndynamical studies\nexamine, if the formation times of mesons and antibaryons are about equal\nfragmentation function\n\u2018free\u2019 fragmentation function\nFSI\ngluon radiation\nhadron\nhadron attenuation\nhadron formation\nimprove our model to describe the primary photon\u2013nucleon reaction below the PYTHIA threshold of W\u2a7e4\u00a0GeV\nin-medium change of the fragmentation function\ninteraction of the leading prehadrons during the formation time\nK\u00b1\nleading prehadrons\nmesons\nnuclei\nnucleon\np\np\u0304\np+A reactions\nparticle species\nphoton\nprimary photon\u2013nucleon reaction\nproduced (pre-)hadrons\nRMh\nsurrounding medium\nthe z-dependence of RMh\n\u03c0\u00b1\n"}, {"id": "S0029549314001551", "text": "An increase of neutron leakage from the core region can be achieved through modifications in the core geometry (usually by adopting a pan-cake geometry of the active core region at the expense of the general neutron economy). Extensive studies determined a set of core design modifications that optimised the total sodium void reactivity (becoming less positive). Among the most efficient design solutions identified is an enlarged sodium plenum above the active core region in combination with an absorber layer above the sodium plenum (to reduce neutron backscattering from the reflector region above the plenum). Fig. 19 shows the combined effect of different upper plenum thicknesses of the absorber and boron layers. It can be observed that the sequential increase of the layer's thickness converge to an asymptotic value of reactivity reduction slightly over 800pcm. The pair of values selected was 60cm for the sodium plenum and 30cm for the boron layer. These modifications implied a considerable increase in the sub-assembly length that was compensated by reducing the upper axial reflector width (Sun et al., 2013).\n", "keywords": "absorber and boron layers\nabsorber layer\naxial reflector\nboron layer\ncore\ncore design modifications that optimised the total sodium void reactivity\ncore region\ndesign solutions\nenlarged sodium plenum\nincrease in the sub-assembly length\nincrease of the layer's thickness\nmodifications in the core geometry\nneutron\nneutron leakage\npan-cake geometry of the active core region\nplenum\nreactivity reduction\nreduce neutron backscattering\nreduce neutron backscattering from the reflector region above the plenum\nreducing the upper axial reflector width\nsodium\nsodium plenum\nupper plenum\n"}, {"id": "S2212671612000716", "text": "Based on the description model of object-orientation-based direction relation in two-dimensional space, the description mode of object-orientation-based direction relation in three-dimensional space is proposed. The basic idea is that the actual direction region is modeled as an open shape. The computation related to the world boundary of spatial direction region is eliminated, and the processing of the direction predicates is converted into the processing of topological operations between open shapes and closed geometry objects. The algorithms of topological operations between open shapes and closed geometry objects are presented and the theoretical proof for the correctness and completeness of the algorithms is performed.", "keywords": "algorithms\nalgorithms of topological operations\ncomputation\ndescription mode\ndescription model\ndescription mode of object-orientation-based direction relation in three-dimensional space is proposed\ndirection region\ngeometry objects\nobject-orientation-based direction relation\nopen shape\nopen shapes\nprocessing of the direction predicates\nprocessing of topological operations\nspatial direction region\ntheoretical proof for the correctness and completeness of the algorithms\ntwo-dimensional space\nworld boundary\n"}, {"id": "S2212667814000070", "text": "This paper suggests a design of high quality real-time rotation face detection architecture for gesture recognition of smart TV. For high performance rotated face detection, the multiple-MCT(Modified Census Transform) architecture, which is robust against lighting change, was used. The Adaboost learning algorithm was used for creating optimized learning data. The proposed hardware structure was composed of Color Space Converter, Image Resizer, Noise Filter, Memory Controller Interface, Image Rotator, Image Scaler, MCT Generator, Candidate Detector, Confidence Switch, Confidence Mapper, Position Resizer, Data Grouper, Overlay Processor and Color Overlay Processer. As a result, suggested face detection device can conduct real-time processing at speed of at least 30 frames per second.", "keywords": "Adaboost learning algorithm\nCandidate Detector,\nColor Overlay Processer\nColor Space Converter\nConfidence Mappe\nConfidence Switch\ncreating optimized learning data\nData Grouper\ndesign of high quality real-time rotation face detection architecture for gesture recognition of smart TV\nface detection\ngesture recognition\nhardware structure\nImage Resizer,\nImage Rotator\nImage Scaler\nMCT\nMCT Generator\nMemory Controller Interface\nModified Census Transform\nNoise Filter\nOverlay Processor\nPosition Resizer,\nreal-time processing\nrotation face detection architecture\n"}, {"id": "S2212667814000070", "text": "This paper suggests a design of high quality real-time rotation face detection architecture for gesture recognition of smart TV. For high performance rotated face detection, the multiple-MCT(Modified Census Transform) architecture, which is robust against lighting change, was used. The Adaboost learning algorithm was used for creating optimized learning data. The proposed hardware structure was composed of Color Space Converter, Image Resizer, Noise Filter, Memory Controller Interface, Image Rotator, Image Scaler, MCT Generator, Candidate Detector, Confidence Switch, Confidence Mapper, Position Resizer, Data Grouper, Overlay Processor and Color Overlay Processer. As a result, suggested face detection device can conduct real-time processing at speed of at least 30 frames per second.", "keywords": "Adaboost learning algorithm\nCandidate Detector,\nColor Overlay Processer\nColor Space Converter\nConfidence Mappe\nConfidence Switch\ncreating optimized learning data\nData Grouper\ndesign of high quality real-time rotation face detection architecture for gesture recognition of smart TV\nface detection\ngesture recognition\nhardware structure\nImage Resizer,\nImage Rotator\nImage Scaler\nMCT\nMCT Generator\nMemory Controller Interface\nModified Census Transform\nNoise Filter\nOverlay Processor\nPosition Resizer,\nreal-time processing\nrotation face detection architecture\n"}, {"id": "S0167931712002699", "text": "As the progression towards smaller lithographic nodes continues it has become necessary to adopt thinner resist films to mitigate problems such as pattern collapse. To address the issue of reduced etch resistance of thin photoresist films the semiconductor industry has begun to develop multilayer processes where the pattern is first transferred into an intermediate organic hardmask with higher etch selectivity before final silicon pattern transfer [25\u201327]. In this paper we demonstrate how the introduction of such a multilayer process can also benefit nanosphere lithography by increasing achievable aspect ratios of silicon nanopillars without the need for complex etch processes requiring specialised and expensive equipment, but instead needing only a standard SF6/C4F8 inductively coupled plasma (ICP) mixed mode etch process at room temperature [28]. As intermediate layer material we used polyimide, which finds widespread use as encapsulation material for IC production. It is readily patterned in oxygen plasma and has a lower etch rate than silicon in SF6 gas. Its flexibility can also be used for the fabrication of soft polymer pillars by the same process as we will show. The multilayer process slightly increases the complexity of sample preparation but allows basic ICP etching to achieve high aspect ratio structures at smaller feature sizes that previously reported without the need for complex etching equipment.\n", "keywords": "complex etch processes\ncoupled plasma\nencapsulation material\netching equipment\nfabrication of soft polymer pillars\nICP\nICP etching\nIC production\nintermediate layer material\nintermediate organic hardmask\nmultilayer process\nmultilayer processes\nnanosphere lithography\noxygen plasma\npattern collapse\npolyimide\nreduced etch resistance of thin photoresist films\nsample preparation\nSF6 gas\nsilicon\nsilicon nanopillars\nsilicon pattern transfer\nsmaller lithographic nodes\nthinner resist films\nthin photoresist films\n"}, {"id": "S0038092X15001024", "text": "The wind speed and cloud height Markov chains are produced accounting for seasonal variations. A Markov chain is used for each variable representing each of the four seasons, capturing the variability at different times of the year, totalling four chains each. The okta number Markov chains also consider the effect of season, with the inclusion of impacts from pressure and diurnal variation. Eight okta Markov chains are produced that are split by above and below average pressure for each season, and four additional morning okta Markov chains are produced to capture the diurnal variation for okta transitions between 00:00 and 05:00am for each season. The intent is to capture the variation in transition probability that occurs as a result of weather changes due to the presence of solar energy. 5am is considered the cut-off because it is a typical sunrise in the summer for the applied study locations. 5h represents 5 okta transitions and is considered an appropriate duration for the slight propensity to shift towards an increased okta to be represented, Fig. 8 demonstrates the diurnal transition differences. Fig. 2 visually demonstrates the mean okta Markov chain for the entire year, whilst the effect of season can be seen in Fig. 11.\n", "keywords": "capture the variation in transition probability\nMarkov chain\nokta Markov chain\nokta Markov chains\nokta number Markov chains\nwind speed and cloud height Markov chains\n"}, {"id": "S0021999115007238", "text": "An inherent problem of the phase-space discretisation is the spurious separation of energy into the discretised bins. This is called the \u201cGarden Sprinkler Effect\u201d and has been extensively studied in [48,49,20]. (In the Boltzmann transport community this is known as the ray effect.) To showcase this effect in the angular dimension, a large spatial domain (4000km\u00d74000km) is simulated, with a monochromatic wave propagating over a long distance in deep water (d=10000m). For the spatial discretisation a structured triangle mesh is used, with an element edge length of 67km (Fig. 11(a)). The initial wave field, located 500km from the lower and left side has a Gaussian distribution in space, with a significant wave height of Hs=2.5m and a standard deviation of 150km (Fig. 11(b)). Its mean direction is 30\u00b0 with an angular distribution of cos2\u2061(\u03b8) and a frequency of 0.1Hz. The simulation is time-dependent and runs for 5 days with a time-step of 600s.\n", "keywords": "deep water\nGarden Sprinkler Effect\nGaussian distribution in space\nmonochromatic wave\nphase-space discretisation\nray effect\nsimulation\nspatial discretisation\nspatial domain (4000km\u00d74000km) is simulated\nspurious separation of energy into the discretised bins\nstructured triangle mesh\nwave\nwave field\n"}, {"id": "S0022311515303901", "text": "The Magnox reactors represent the first generation of gas-cooled reactors in the UK that used carbon dioxide (CO2) as the primary coolant and a honeycomb network of graphite bricks to provide neutron moderation. During reactor operation significant amounts of carbon monoxide (CO) was produced from the CO2 coolant. This CO in turn can be radiolytically polymerised to form a carbonaceous deposit on free surfaces [12]. This non-graphitic carbon deposit is significantly more chemically reactive to air than the underlying graphite [12,13]. During the lifetime of some Magnox reactors, small quantities of methane gas were injected into the coolant gas to inhibit weight loss of the graphite core due to radiolytic oxidation [14]. Methane (CH4) is a precursor for carbonaceous deposits that form a sacrificial layer protecting the underlying graphite from excessive weight loss [15] and reduction in mechanical strength [16]. It is assumed nitrogen incorporation during deposit formation is the subsequent production route for the high 14C levels observed.\n", "keywords": "air\ncarbonaceous deposits\ncarbon dioxide\ncarbon monoxide\nCH4\nCO\nCO2\nCO2 coolant\ncoolant\ncoolant gas\nexcessive weight loss\nform a carbonaceous deposit on free surfaces\ngas-cooled reactors\ngraphite\ngraphite core\nhoneycomb network of graphite bricks\ninhibit weight loss of the graphite core due to radiolytic oxidation\nMagnox reactors\nMethane\nmethane gas\nneutron\nneutron moderation\nnitrogen\nnitrogen incorporation during deposit formation\nnon-graphitic carbon deposit\nradiolytically polymerised\nradiolytic oxidation\nreactor\nreduction in mechanical strength\n"}, {"id": "S0370157309002877", "text": "We start by outlining the motivation, structure and content of the review. It has long been known that cardiovascular signals contain a number of oscillatory components that are not exactly periodic. To put it differently, their periods (frequencies) fluctuate with time. For example, heart rate variability (HRV) has in itself provided a major topic of discussion. We introduce one of the statistical approaches to HRV in Section\u00a03. However, in order to understand the variability of the cardiovascular system, discussion of a single source is insufficient because the cardiovascular system is composed of many different physiological components (subsystems) and it is the effects of their mutual interaction that combine to produce HRV. This is demonstrated in Section\u00a04, revealed by results obtained using the wavelet transform. In Section\u00a05, we discuss the cardio-respiratory interaction in terms of phase synchronization. To set the scene for these later discussions, we summarize the basic principles of phase dynamics in Section\u00a02. For readers who are unfamiliar with the physiological aspects of the research, we provide Appendices\u00a0A on the cardiovascular system and B on how measurements of cardiovascular signals are conducted. Appendix\u00a0C provides details of the statistical methods used in the group data analyses.\n", "keywords": "cardio-respiratory interaction\ncardiovascular signals\ncardiovascular system\ndiscuss the cardio-respiratory interaction\nfrequencies\ngroup data analyses\nheart rate variability\nHRV\nintroduce one of the statistical approaches to HRV\nmeasurements of cardiovascular signals\noscillatory components\nperiods\nphase dynamics\nphase synchronization\nphysiological components\nstatistical approaches\nstatistical methods\nsubsystems\nsummarize the basic principles of phase dynamics\nunderstand the variability of the cardiovascular system\nwavelet transform\n"}, {"id": "S2212671612002302", "text": "Monitoring the wear condition of the tramway superstructure is one of the key points to guarantee an adequate safety level of the light rail transport system. The purpose of this paper is to suggest a new non-conventionalprocedure for measuring the transverse profile of rails in operation by means of image-processing technique. This methodological approach is based on the \u201cinformation\u201d contained in high-resolution photographic images of tracks and on specific algorithms which allow to obtain the exact geometric profile of the rails and therefore to measure the state of the rail-head extrados wear.", "keywords": "high-resolution photographic images\nimage-processing technique\ninformation\u201d\nlight rail transport system\nMonitoring the wear condition\nnew non-conventionalprocedure\nrails\nsafety level\nspecific algorithms\ntracks\ntramway superstructure\ntransverse profile\nwear condition\n"}, {"id": "S2212671612002302", "text": "Monitoring the wear condition of the tramway superstructure is one of the key points to guarantee an adequate safety level of the light rail transport system. The purpose of this paper is to suggest a new non-conventionalprocedure for measuring the transverse profile of rails in operation by means of image-processing technique. This methodological approach is based on the \u201cinformation\u201d contained in high-resolution photographic images of tracks and on specific algorithms which allow to obtain the exact geometric profile of the rails and therefore to measure the state of the rail-head extrados wear.", "keywords": "high-resolution photographic images\nimage-processing technique\ninformation\u201d\nlight rail transport system\nMonitoring the wear condition\nnew non-conventionalprocedure\nrails\nsafety level\nspecific algorithms\ntracks\ntramway superstructure\ntransverse profile\nwear condition\n"}, {"id": "S1631070514000954", "text": "Superconductivity in actinides was first observed in thorium metal in 1929 [7], then in elemental uranium in 1942 [8], and in uranium compounds in 1958 [9]. A new class of uranium superconductors emerged in the 1980's with the discovery of uranium heavy fermion superconductors [10]. Further surprises came at the beginning of the century with the discovery of ferromagnetic superconductors in uranium systems [11] and the first observation of superconductivity in plutonium [12] and neptunium [13] compounds. The actinides (or actinoids) are located at the end of the periodic table (N=89 (Ac) or 90 (Th) to 103 (Lr)). Transuranium elements (or transuranics) are the chemical elements with atomic number (Z) greater than 92 (uranium) and due to their short half-life on a geological timescale, they are essentially synthetic elements. Above Z=103 (Lr), one talks about transactinides (or superactinides) elements. These latter elements have extremely short half-lives and no macroscopic quantity is available for the study of condensed-matter properties.\n", "keywords": "Ac\nactinides\nactinoids\nferromagnetic superconductors\nLr\nneptunium\nperiodic table\nplutonium\nstudy of condensed-matter properties\nsuperactinides\nsuperconductivity\nSuperconductivity\nSuperconductivity in actinides\nsynthetic elements\nTh\nthorium metal\ntransactinides\ntransuranics\nTransuranium elements\nuranium\nuranium compounds\nuranium heavy fermion superconductors\nuranium superconductors\n"}, {"id": "S1631070514000954", "text": "Superconductivity in actinides was first observed in thorium metal in 1929 [7], then in elemental uranium in 1942 [8], and in uranium compounds in 1958 [9]. A new class of uranium superconductors emerged in the 1980's with the discovery of uranium heavy fermion superconductors [10]. Further surprises came at the beginning of the century with the discovery of ferromagnetic superconductors in uranium systems [11] and the first observation of superconductivity in plutonium [12] and neptunium [13] compounds. The actinides (or actinoids) are located at the end of the periodic table (N=89 (Ac) or 90 (Th) to 103 (Lr)). Transuranium elements (or transuranics) are the chemical elements with atomic number (Z) greater than 92 (uranium) and due to their short half-life on a geological timescale, they are essentially synthetic elements. Above Z=103 (Lr), one talks about transactinides (or superactinides) elements. These latter elements have extremely short half-lives and no macroscopic quantity is available for the study of condensed-matter properties.\n", "keywords": "Ac\nactinides\nactinoids\nferromagnetic superconductors\nLr\nneptunium\nperiodic table\nplutonium\nstudy of condensed-matter properties\nsuperactinides\nsuperconductivity\nSuperconductivity\nSuperconductivity in actinides\nsynthetic elements\nTh\nthorium metal\ntransactinides\ntransuranics\nTransuranium elements\nuranium\nuranium compounds\nuranium heavy fermion superconductors\nuranium superconductors\n"}, {"id": "S0167844214000652", "text": "A bond failure is thought of as a micro-crack nucleation, specifically as a separation between the adjacent cells in the cellular structure along their common face. Initially, the micro-cracks may be dispersed in the model reflecting the random distribution of pore sizes and the low level of interaction due to force redistribution. Interaction and coalescence may follow as the population of micro-cracks increases. These situations are illustrated in Fig. 3. The structure of the failed surface can be represented with a mathematical graph, where graph nodes represent failed faces and graph edges exist between failed faces with common triple line in the cellular structure, i.e. where two micro-cracks formed a continuous larger crack. With reference to Fig. 3, each failed face is a graph node and each pair of neighbouring failed faces is a graph edge.\n", "keywords": "bond failure\ncellular structure\ncrack\nfailed face\nfailed faces\nfailed surface\nmathematical graph\nmicro-crack nucleation\nmicro-cracks\npore\nseparation between the adjacent cells in the cellular structure along their common face\n"}, {"id": "S2212667812000895", "text": "Faced with deficient ability of autonomic learning among learners and low emotional involvement in current web-based instructional environment, here we propose a construct model that is based on inter-subjectivity fusing cognition with emotion to make up for these shortages. Further more, we\u2019ve put the construct model into practice through the online teaching reformation of the quality course apparel production and management.", "keywords": "autonomic learning\nconstruct model\nfusing cognition with emotion\nlow emotional involvement\nonline teaching reformation\npropose a construct model\nweb-based instructional environment\n"}, {"id": "S2212667812000895", "text": "Faced with deficient ability of autonomic learning among learners and low emotional involvement in current web-based instructional environment, here we propose a construct model that is based on inter-subjectivity fusing cognition with emotion to make up for these shortages. Further more, we\u2019ve put the construct model into practice through the online teaching reformation of the quality course apparel production and management.", "keywords": "autonomic learning\nconstruct model\nfusing cognition with emotion\nlow emotional involvement\nonline teaching reformation\npropose a construct model\nweb-based instructional environment\n"}, {"id": "S2212667812000895", "text": "Faced with deficient ability of autonomic learning among learners and low emotional involvement in current web-based instructional environment, here we propose a construct model that is based on inter-subjectivity fusing cognition with emotion to make up for these shortages. Further more, we\u2019ve put the construct model into practice through the online teaching reformation of the quality course apparel production and management.", "keywords": "autonomic learning\nconstruct model\nfusing cognition with emotion\nlow emotional involvement\nonline teaching reformation\npropose a construct model\nweb-based instructional environment\n"}, {"id": "S2212667812000895", "text": "Faced with deficient ability of autonomic learning among learners and low emotional involvement in current web-based instructional environment, here we propose a construct model that is based on inter-subjectivity fusing cognition with emotion to make up for these shortages. Further more, we\u2019ve put the construct model into practice through the online teaching reformation of the quality course apparel production and management.", "keywords": "autonomic learning\nconstruct model\nfusing cognition with emotion\nlow emotional involvement\nonline teaching reformation\npropose a construct model\nweb-based instructional environment\n"}, {"id": "S2212667812000895", "text": "Faced with deficient ability of autonomic learning among learners and low emotional involvement in current web-based instructional environment, here we propose a construct model that is based on inter-subjectivity fusing cognition with emotion to make up for these shortages. Further more, we\u2019ve put the construct model into practice through the online teaching reformation of the quality course apparel production and management.", "keywords": "autonomic learning\nconstruct model\nfusing cognition with emotion\nlow emotional involvement\nonline teaching reformation\npropose a construct model\nweb-based instructional environment\n"}, {"id": "S2212667812000895", "text": "Faced with deficient ability of autonomic learning among learners and low emotional involvement in current web-based instructional environment, here we propose a construct model that is based on inter-subjectivity fusing cognition with emotion to make up for these shortages. Further more, we\u2019ve put the construct model into practice through the online teaching reformation of the quality course apparel production and management.", "keywords": "autonomic learning\nconstruct model\nfusing cognition with emotion\nlow emotional involvement\nonline teaching reformation\npropose a construct model\nweb-based instructional environment\n"}, {"id": "S030439911200040X", "text": "Some methods use 1D radial profiles obtained from circular averaging of 2D experimental PSD [4,8,11] or by elliptical averaging [17]. An inadequacy of circular averaging is that it neglects astigmatism. Astigmatism distorts the circular shape of the Thon rings and thus decreases their modulation depth in the obtained 1D profile. A few algorithms that consider astigmatism involve concepts such as dividing the PSD into sectors where Thon rings are approximated by circular arcs [15,21], applying Canny edge detection to find the rings [17] prior to elliptical averaging, determining the relationship between the 1D circular averages with and without astigmatism [22], or using a brute-force scan of a database containing precalculated patterns as in ATLAS [23]. Some other approaches for estimating CTF parameters do a fully 2D PSD optimization [12,14,18,20] but they usually regulate and fit numerous parameters by an extensive search that does not guarantee convergence. Furthermore, only a few schemes that were developed for defocus estimation provide an error analysis [23,24].\n", "keywords": "1D profile\n1D radial profiles\n2D experimental PSD\n2D PSD optimization\nastigmatism\nAstigmatism\nATLAS\nCanny edge detection\ncircular averaging\nCTF\nelliptical averaging\nPSD\nThon rings\n"}, {"id": "S030439911200040X", "text": "Some methods use 1D radial profiles obtained from circular averaging of 2D experimental PSD [4,8,11] or by elliptical averaging [17]. An inadequacy of circular averaging is that it neglects astigmatism. Astigmatism distorts the circular shape of the Thon rings and thus decreases their modulation depth in the obtained 1D profile. A few algorithms that consider astigmatism involve concepts such as dividing the PSD into sectors where Thon rings are approximated by circular arcs [15,21], applying Canny edge detection to find the rings [17] prior to elliptical averaging, determining the relationship between the 1D circular averages with and without astigmatism [22], or using a brute-force scan of a database containing precalculated patterns as in ATLAS [23]. Some other approaches for estimating CTF parameters do a fully 2D PSD optimization [12,14,18,20] but they usually regulate and fit numerous parameters by an extensive search that does not guarantee convergence. Furthermore, only a few schemes that were developed for defocus estimation provide an error analysis [23,24].\n", "keywords": "1D profile\n1D radial profiles\n2D experimental PSD\n2D PSD optimization\nastigmatism\nAstigmatism\nATLAS\nCanny edge detection\ncircular averaging\nCTF\nelliptical averaging\nPSD\nThon rings\n"}, {"id": "S0010938X12001163", "text": "In situ oxidation, experiments were carried out using 3mm diameter discs with one surface ground and polished to a 1\u03bcm diamond finish. The 3mm discs were then oxidised in a Philips XL-30 FEG ESEM with a hot stage attachment. The oxidising atmosphere used was laboratory air at a pressure of 266Pa. During the experiment, the sample was observed and imaged using a primary beam energy of 20kV and an Everhart\u2013Thornley secondary electron detector. The sample was heated at a rate of 100\u00b0C/min to a temperature of 700\u00b0C and held at this temperature for 8min to stabilise the stage and the microscope. The sample was then heated to a final temperature of 900\u00b0C at the same heating rate. The total time of exposure of the sample was 120min before cooling to room temperature by turning off the heating coils. The samples were then examined in the LEO 1530VP FEGSEM with chemical information gathered using EDS. Cross-sections and Transmission Electron Microscope (TEM) samples were produced using a dual beam FEI Nova Nanolab 600 for Focused Ion Beam (FIB) milling perpendicular to the phase boundaries to determine their influence on the oxide development and imaged using a Jeol 2000FX W-filament TEM. EDS maps of the TEM samples were collected using the Nanolab 600 with a Scanning TEM (STEM) detector and an EDAX Genesis EDS system at an accelerating voltage of 30kV.\n", "keywords": "3mm diameter discs\naccelerating voltage of 30kV\ndual beam FEI Nova Nanolab 600\nEDAX Genesis EDS system\nEDS\nFIB\nFocused Ion Beam\nground and polished\nheated\nheated to a final temperature of 900\u00b0C\nheating coils\nhot stage attachment\nIn situ oxidation\nJeol 2000FX W-filament TEM\nLEO 1530VP FEGSEM\nNanolab 600\noxide\noxide development\noxidised\nPhilips XL-30 FEG ESEM\nScanning TEM\nScanning TEM (STEM) detector\nstabilise the stage and the microscope\nSTEM\nTEM\nTEM samples\nTransmission Electron Microscope\n"}, {"id": "S0370269304008809", "text": "The most ambitious goal may be stated as the one of detecting the location of, say, one missing level on an otherwise complete sequence. Dyson, in a recent review [7], uses information theory concepts and argues that correlations in a sequence may provide the necessary redundancy from which error correcting codes can be constructed. At one extreme where no correlations and therefore no redundancy are present (Poissonian sequence), there is no possibility of detecting one missing level. At the other extreme, a sequence of equally spaced levels (picket fence), there is a maximum redundancy and a missed level can be obviously detected as a hole in the spectrum. Eigenvalues of random matrices, which exhibit characteristic correlations, correspond to an intermediate situation between these two extremes. The attempts to locate in the last case a single missed level have remained unsuccessful so far. However, it should be mentioned that for two-dimensional chaotic systems where, besides correlations of the order of one mean spacing as described by random matrices, the presence and the role of long range correlations governed by the shortest periodic orbits and reflected in Weyl's law describing the average spectral density, is well understood. It is then possible to approximately locate, from the study of spectral fluctuations, a single missed level\u00a0[8].\n", "keywords": "approximately locate\na single missed level\ncorrelations in a sequence\ndetecting one missing level\ndetecting the location of, say, one missing level on an otherwise complete sequence\nerror correcting codes can be constructed\ninformation theory concepts\nlocate in the last case a single missed level\nmissed level can be obviously detected as a hole in the spectrum\nno correlations and therefore no redundancy are present\npicket fence\nPoissonian sequence\nsequence of equally spaced levels\nstudy of spectral fluctuations\ntwo-dimensional chaotic systems\nWeyl's law describing the average spectral density\n"}, {"id": "S0010938X13005945", "text": "The adhesion/cohesion of the coating was evaluated by the scratch test method, using a Revetest system (CSM Instruments SA, Switzerland) equipped with a H-270 diamond indentor (200\u03bcm diameter). Six scratch indentations were carried out under previously optimized conditions (linear progressive load mode 1\u20134N, 4Nmin\u22121). In order to aid in determination of location of spallation/delamination, an extended scratch length of 6mm was employed. The scratch tracks were subsequently observed by SEM to determine the locations of the first coating failure and to understand the nature of the coating failure. During the scratch tests, the loading force and penetration depth were recorded and their respective values were correlated with the observed failure locations. The surface roughness of the coating was evaluated using a surface roughness tester (TR200, Timegroup Inc.) according to ISO standard [29]. Due to the presence of the open porosity in the outer layer of the coating, a measurement length for determination of the roughness (Ra) of 0.8mm was used. In total, eight measurements were carried out in different directions.\n", "keywords": "adhesion/cohesion\nadhesion/cohesion of the coating\ncoating\ndetermination of location of spallation/delamination\neight measurements\nloading force and penetration depth were recorded\nloading force and penetration depth were recorded and their respective values were correlated with the observed failure locations\nouter layer\nRa\nRevetest system (CSM Instruments SA, Switzerland) equipped with a H-270 diamond indentor (200\u03bcm diameter)\nroughness\nscratch indentations were carried out\nscratch test method\nsurface roughness of the coating was evaluated\nsurface roughness tester\nunderstand the nature of the coating failure\n"}, {"id": "S0370269304009530", "text": "If signals suggesting supersymmetry (SUSY) are discovered at the LHC then it will be vital to measure the spins of the new particles to demonstrate that they are indeed the predicted super-partners. A method is discussed by which the spins of some of the SUSY particles can be determined. Angular distributions in sparticle decays lead to charge asymmetry in lepton-jet invariant mass distributions. The size of the asymmetry is proportional to the primary production asymmetry between squarks and anti-squarks. Monte Carlo simulations are performed for a particular mSUGRA model point at the LHC. The resultant asymmetry distributions are consistent with a spin-0 slepton and a spin-12\u03c7\u02dc20, but are not consistent with both particles being scalars.", "keywords": "Angular distributions\nanti-squarks\nasymmetry distributions\ncharge asymmetry\ndemonstrate that they are indeed the predicted super-partners\nlepton-jet invariant mass distributions\nmeasure the spins of the new particles\nMonte Carlo simulations\nmSUGRA model point\nparticles\nprimary production asymmetry\nsparticle decays\nspin-0 slepton\nsquarks\nsuper-partners\nsupersymmetry\nSUSY\nSUSY particles\n"}, {"id": "S2212667813001068", "text": "It is difficult in directly predicting permeability from porosity in tight sandstones due to the poor relationship between core derived porosity and permeability that caused by the extreme heterogeneity. The classical SDR (Schlumberger Doll Research) and Timur-Coates models are all unusable because not enough core samples were drilled for lab NMR experimental measurements to calibrate the involved model parameters. Based on the classification scale method (CSM), after the target tight sandstones are classified into two types, the relationship between core porosity and permeability is established for every type of formations, and the corresponding permeability estimation models are established. Field examples show that the classification scale method is effective in estimating tight sandstone permeability.", "keywords": "calibrate\ncalibrate the involved model parameters\nclassification scale method\nCSM\nexperimental measurements\nField examples\nNMR\npermeability\npermeability estimation models\nporosity\nsandstone\nsandstones\nSchlumberger Doll Research\nSDR\nTimur-Coates models\n"}, {"id": "S2212667813001068", "text": "It is difficult in directly predicting permeability from porosity in tight sandstones due to the poor relationship between core derived porosity and permeability that caused by the extreme heterogeneity. The classical SDR (Schlumberger Doll Research) and Timur-Coates models are all unusable because not enough core samples were drilled for lab NMR experimental measurements to calibrate the involved model parameters. Based on the classification scale method (CSM), after the target tight sandstones are classified into two types, the relationship between core porosity and permeability is established for every type of formations, and the corresponding permeability estimation models are established. Field examples show that the classification scale method is effective in estimating tight sandstone permeability.", "keywords": "calibrate\ncalibrate the involved model parameters\nclassification scale method\nCSM\nexperimental measurements\nField examples\nNMR\npermeability\npermeability estimation models\nporosity\nsandstone\nsandstones\nSchlumberger Doll Research\nSDR\nTimur-Coates models\n"}, {"id": "S2212667813001068", "text": "It is difficult in directly predicting permeability from porosity in tight sandstones due to the poor relationship between core derived porosity and permeability that caused by the extreme heterogeneity. The classical SDR (Schlumberger Doll Research) and Timur-Coates models are all unusable because not enough core samples were drilled for lab NMR experimental measurements to calibrate the involved model parameters. Based on the classification scale method (CSM), after the target tight sandstones are classified into two types, the relationship between core porosity and permeability is established for every type of formations, and the corresponding permeability estimation models are established. Field examples show that the classification scale method is effective in estimating tight sandstone permeability.", "keywords": "calibrate\ncalibrate the involved model parameters\nclassification scale method\nCSM\nexperimental measurements\nField examples\nNMR\npermeability\npermeability estimation models\nporosity\nsandstone\nsandstones\nSchlumberger Doll Research\nSDR\nTimur-Coates models\n"}, {"id": "S0022311515300830", "text": "Solid pieces of 23\u2013114\u00a0mg were further used to measure the enthalpy increments using a Setaram Multi-detector High Temperature Calorimeter (MDHTC-96) using a drop detector. For more details about the technique we refer to our previous studies [9,10]. The measurements were carried out under an argon atmosphere (with an oxygen content of 7\u00a0ppm), using pure platinum ingots (64\u2013144\u00a0mg) of 99.95\u00a0at % purity as a reference material. The temperature range of the experiment was from 430.3\u00a0K to 1088.8\u00a0K using steps of 50\u00a0K. Each isothermal run consisted of 2\u20134 drops of Bi2UO6 samples, each surrounded by two drops of platinum from which the sensitivity of the device was determined. The drops were separated by time intervals of 20\u00a0min, long enough to re-stabilize the monitored heat flow signal. Background subtraction and peak integration were performed using commercially available software for data processing. The reported temperatures were corrected in accordance with the calibration curve obtained prior to measurement using several high purity standard metals (Sn, Pb, Zn, Al, Ag, Ni) with various melting temperatures in order to cover the whole temperature range of the measurement. After drop calorimetric measurements at the maximum considered temperature, the material was subjected to a new XRD measurement, confirming the stability of the compound under the experimental conditions.\n", "keywords": "2\u20134 drops of Bi2UO6 samples\nAg\nAl\nargon\nBackground subtraction and peak integration\ncalorimetric measurements\ncommercially available software for data processing\ncompound\ncorrected\ndata processing\ndrop detector\nenthalpy increments\nheat flow\nhigh purity standard metals\nisothermal run\nMDHTC-96\nmeasure the enthalpy increments\nNi\noxygen\nPb\nplatinum\nplatinum ingots\nreference material\nSetaram Multi-detector High Temperature Calorimeter\nSn\nSolid pieces\nXRD\nZn\n"}, {"id": "S0022311515300830", "text": "Solid pieces of 23\u2013114\u00a0mg were further used to measure the enthalpy increments using a Setaram Multi-detector High Temperature Calorimeter (MDHTC-96) using a drop detector. For more details about the technique we refer to our previous studies [9,10]. The measurements were carried out under an argon atmosphere (with an oxygen content of 7\u00a0ppm), using pure platinum ingots (64\u2013144\u00a0mg) of 99.95\u00a0at % purity as a reference material. The temperature range of the experiment was from 430.3\u00a0K to 1088.8\u00a0K using steps of 50\u00a0K. Each isothermal run consisted of 2\u20134 drops of Bi2UO6 samples, each surrounded by two drops of platinum from which the sensitivity of the device was determined. The drops were separated by time intervals of 20\u00a0min, long enough to re-stabilize the monitored heat flow signal. Background subtraction and peak integration were performed using commercially available software for data processing. The reported temperatures were corrected in accordance with the calibration curve obtained prior to measurement using several high purity standard metals (Sn, Pb, Zn, Al, Ag, Ni) with various melting temperatures in order to cover the whole temperature range of the measurement. After drop calorimetric measurements at the maximum considered temperature, the material was subjected to a new XRD measurement, confirming the stability of the compound under the experimental conditions.\n", "keywords": "2\u20134 drops of Bi2UO6 samples\nAg\nAl\nargon\nBackground subtraction and peak integration\ncalorimetric measurements\ncommercially available software for data processing\ncompound\ncorrected\ndata processing\ndrop detector\nenthalpy increments\nheat flow\nhigh purity standard metals\nisothermal run\nMDHTC-96\nmeasure the enthalpy increments\nNi\noxygen\nPb\nplatinum\nplatinum ingots\nreference material\nSetaram Multi-detector High Temperature Calorimeter\nSn\nSolid pieces\nXRD\nZn\n"}, {"id": "S0022311515300830", "text": "Solid pieces of 23\u2013114\u00a0mg were further used to measure the enthalpy increments using a Setaram Multi-detector High Temperature Calorimeter (MDHTC-96) using a drop detector. For more details about the technique we refer to our previous studies [9,10]. The measurements were carried out under an argon atmosphere (with an oxygen content of 7\u00a0ppm), using pure platinum ingots (64\u2013144\u00a0mg) of 99.95\u00a0at % purity as a reference material. The temperature range of the experiment was from 430.3\u00a0K to 1088.8\u00a0K using steps of 50\u00a0K. Each isothermal run consisted of 2\u20134 drops of Bi2UO6 samples, each surrounded by two drops of platinum from which the sensitivity of the device was determined. The drops were separated by time intervals of 20\u00a0min, long enough to re-stabilize the monitored heat flow signal. Background subtraction and peak integration were performed using commercially available software for data processing. The reported temperatures were corrected in accordance with the calibration curve obtained prior to measurement using several high purity standard metals (Sn, Pb, Zn, Al, Ag, Ni) with various melting temperatures in order to cover the whole temperature range of the measurement. After drop calorimetric measurements at the maximum considered temperature, the material was subjected to a new XRD measurement, confirming the stability of the compound under the experimental conditions.\n", "keywords": "2\u20134 drops of Bi2UO6 samples\nAg\nAl\nargon\nBackground subtraction and peak integration\ncalorimetric measurements\ncommercially available software for data processing\ncompound\ncorrected\ndata processing\ndrop detector\nenthalpy increments\nheat flow\nhigh purity standard metals\nisothermal run\nMDHTC-96\nmeasure the enthalpy increments\nNi\noxygen\nPb\nplatinum\nplatinum ingots\nreference material\nSetaram Multi-detector High Temperature Calorimeter\nSn\nSolid pieces\nXRD\nZn\n"}, {"id": "S0378381215300674", "text": "The next phase of our current study is to use the parameters obtained from pure-component systems in a transferable manner to represent the corresponding mixtures. Mixtures of n-alkanes and H2O have been studied previously with SAFT-\u03b3 SW [82]. In general it is well known that the extreme nature of the phase separation [150] makes it challenging to model mixtures of H2O with non-polar compounds. Because of the large differences in the dielectric constant of the two phases as well as in the dipole moment of H2O and the hydrophobic molecules, it especially difficult to obtain phase-independent unlike interaction parameters [112] and thus to model simultaneously the equilibrium phases. In previous work [82], emphasis was placed on obtaining an accurate description of the alkane-rich phases (both liquid and vapour), while small absolute (but not relative) deviations for the aqueous phase composition were achieved. The systems of interest in our current work are typically aqueous mixtures containing a high proportion of H2O, alkylamine, and CO2. Consequently, in order to provide an improved overall description of the fluid-phase equilibria at the conditions of interest, refinements have been made to the unlike parameters presented in the previous study [129] relating to the interactions between H2O and the alkyl groups, CH3 and CH2, namely \u03f5CH3,H2O, \u03f5CH2,H2O and \u03bbCH3,H2O, \u03bbCH2,H2O.\n", "keywords": "alkylamine\nalkyl groups\naqueous mixtures\nCH3 and CH2\n\u03f5CH3,H2O, \u03f5CH2,H2O\nCO2\ndescription of the alkane-rich phases\nfluid\nfluid-phase equilibria\nH2O\nhydrophobic molecules\ninteractions between H2O and the alkyl groups\nliquid\nmixtures of H2O\nMixtures of n-alkanes and H2O\nnon-polar compounds\npure-component systems\nSAFT-\u03b3 SW\nvapour\n\u03bbCH3,H2O, \u03bbCH2,H2O\n"}, {"id": "S0378381215300674", "text": "The next phase of our current study is to use the parameters obtained from pure-component systems in a transferable manner to represent the corresponding mixtures. Mixtures of n-alkanes and H2O have been studied previously with SAFT-\u03b3 SW [82]. In general it is well known that the extreme nature of the phase separation [150] makes it challenging to model mixtures of H2O with non-polar compounds. Because of the large differences in the dielectric constant of the two phases as well as in the dipole moment of H2O and the hydrophobic molecules, it especially difficult to obtain phase-independent unlike interaction parameters [112] and thus to model simultaneously the equilibrium phases. In previous work [82], emphasis was placed on obtaining an accurate description of the alkane-rich phases (both liquid and vapour), while small absolute (but not relative) deviations for the aqueous phase composition were achieved. The systems of interest in our current work are typically aqueous mixtures containing a high proportion of H2O, alkylamine, and CO2. Consequently, in order to provide an improved overall description of the fluid-phase equilibria at the conditions of interest, refinements have been made to the unlike parameters presented in the previous study [129] relating to the interactions between H2O and the alkyl groups, CH3 and CH2, namely \u03f5CH3,H2O, \u03f5CH2,H2O and \u03bbCH3,H2O, \u03bbCH2,H2O.\n", "keywords": "alkylamine\nalkyl groups\naqueous mixtures\nCH3 and CH2\n\u03f5CH3,H2O, \u03f5CH2,H2O\nCO2\ndescription of the alkane-rich phases\nfluid\nfluid-phase equilibria\nH2O\nhydrophobic molecules\ninteractions between H2O and the alkyl groups\nliquid\nmixtures of H2O\nMixtures of n-alkanes and H2O\nnon-polar compounds\npure-component systems\nSAFT-\u03b3 SW\nvapour\n\u03bbCH3,H2O, \u03bbCH2,H2O\n"}, {"id": "S0022311514006849", "text": "The early theoretical work of Catlow assessed a number of Willis type clusters and found them all to be stable using potential-based methods [6]. More recently \u201csplit interstitial\u201d type clusters (Fig. 1) have emerged from computational studies as stable species following the potential based investigation of Govers et al. which found the 2:2:2 cluster in a UO2 supercell relaxed to a split di-interstitial [13] (Fig. 1(b)); a single VO with three Oi displaced approximately 1.6\u00c5 in \u3008111\u3009 directions from the VO. This result was later confirmed by the LSDA+U calculations of Geng et al. [7]. The family of split interstitial clusters was extended to include tri-interstitials [8] (a di-interstitial with the fourth Oi site occupied) and quad-interstitials [9] (two di-interstitials on adjacent sites, giving a total of two VO and six Oi) (Fig. 1(d)). Following this Andersson et al. postulated a model for U4O9 based on a UO2 supercell containing multiple split quad-interstitial clusters; following the prediction of their LSDA+U calculations that the quad-interstitial is more stable than its cuboctahedral counterpart [12].\n", "keywords": "a single VO with three Oi\ncuboctahedral\ndi-interstitial\ndi-interstitial with the fourth Oi site occupied\nLSDA+U\nmodel for U4O9\nmultiple split quad-interstitial clusters\nOi\npotential based investigation\npotential-based methods\nquad-interstitial\nquad-interstitials\nsplit interstitial clusters\nsplit interstitial\u201d type clusters\ntri-interstitials\ntwo di-interstitials on adjacent sites\nUO2 supercell\nVO\nWillis type clusters\n"}, {"id": "S0022311514006849", "text": "The early theoretical work of Catlow assessed a number of Willis type clusters and found them all to be stable using potential-based methods [6]. More recently \u201csplit interstitial\u201d type clusters (Fig. 1) have emerged from computational studies as stable species following the potential based investigation of Govers et al. which found the 2:2:2 cluster in a UO2 supercell relaxed to a split di-interstitial [13] (Fig. 1(b)); a single VO with three Oi displaced approximately 1.6\u00c5 in \u3008111\u3009 directions from the VO. This result was later confirmed by the LSDA+U calculations of Geng et al. [7]. The family of split interstitial clusters was extended to include tri-interstitials [8] (a di-interstitial with the fourth Oi site occupied) and quad-interstitials [9] (two di-interstitials on adjacent sites, giving a total of two VO and six Oi) (Fig. 1(d)). Following this Andersson et al. postulated a model for U4O9 based on a UO2 supercell containing multiple split quad-interstitial clusters; following the prediction of their LSDA+U calculations that the quad-interstitial is more stable than its cuboctahedral counterpart [12].\n", "keywords": "a single VO with three Oi\ncuboctahedral\ndi-interstitial\ndi-interstitial with the fourth Oi site occupied\nLSDA+U\nmodel for U4O9\nmultiple split quad-interstitial clusters\nOi\npotential based investigation\npotential-based methods\nquad-interstitial\nquad-interstitials\nsplit interstitial clusters\nsplit interstitial\u201d type clusters\ntri-interstitials\ntwo di-interstitials on adjacent sites\nUO2 supercell\nVO\nWillis type clusters\n"}, {"id": "S0022311513001165", "text": "Our simulations confirm experimental observations that W net erosion represents only tiny fraction (in our simulation \u223c1%) of the W gross erosion. The estimated upstream W fluxes, FWupstrem, are in good agreement with the experimentally observed values \u2a7d1019m-2s-1 [16]. Moreover, this value is not very sensitive to the divertor plasma temperature. For low temperatures the energy of D and C ions hitting to the divertor plates is too low to sputter sufficient amount of W. With increasing energy the W sputtering increases, but the potential drop in the divertor plasma increases too. As a result, most of the W atoms are ionized in the vicinity of the divertor and return back to the plates. There are two effects leading to the observed prompt redeposition of W ions: first is the \u201cnear-divertor\u201d ionization of W due to low ionization potential \u22127.86eV (for comparison the ionization potentials for D and C are 13,6 and 10.6eV), second, W+n ions have large Larmor radius \u223c2/nmm, so that they are redeposited within the distance of a Larmor radius. Important to note that a significant fraction of W ions escaping this prompt redeposition are returned back due to the friction with the main ions.\n", "keywords": "C\nD\nD and C ions\ndivertor\ndivertor plasma\ndivertor plates\nestimated upstream W fluxes\nexperimentally observed values\nfriction with the main ions\nFWupstrem\nhitting to the divertor plates\nincreasing energy\nionization\nionized\nlarge Larmor radius\nnear-divertor\u201d ionization\nobserved prompt redeposition\nplates\nreturn\nsimulations\nW\nW atoms\nW fluxes\nW gross erosion\nW ions\nW net erosion\nW+n ions\nW sputtering\n"}, {"id": "S0022311513001165", "text": "Our simulations confirm experimental observations that W net erosion represents only tiny fraction (in our simulation \u223c1%) of the W gross erosion. The estimated upstream W fluxes, FWupstrem, are in good agreement with the experimentally observed values \u2a7d1019m-2s-1 [16]. Moreover, this value is not very sensitive to the divertor plasma temperature. For low temperatures the energy of D and C ions hitting to the divertor plates is too low to sputter sufficient amount of W. With increasing energy the W sputtering increases, but the potential drop in the divertor plasma increases too. As a result, most of the W atoms are ionized in the vicinity of the divertor and return back to the plates. There are two effects leading to the observed prompt redeposition of W ions: first is the \u201cnear-divertor\u201d ionization of W due to low ionization potential \u22127.86eV (for comparison the ionization potentials for D and C are 13,6 and 10.6eV), second, W+n ions have large Larmor radius \u223c2/nmm, so that they are redeposited within the distance of a Larmor radius. Important to note that a significant fraction of W ions escaping this prompt redeposition are returned back due to the friction with the main ions.\n", "keywords": "C\nD\nD and C ions\ndivertor\ndivertor plasma\ndivertor plates\nestimated upstream W fluxes\nexperimentally observed values\nfriction with the main ions\nFWupstrem\nhitting to the divertor plates\nincreasing energy\nionization\nionized\nlarge Larmor radius\nnear-divertor\u201d ionization\nobserved prompt redeposition\nplates\nreturn\nsimulations\nW\nW atoms\nW fluxes\nW gross erosion\nW ions\nW net erosion\nW+n ions\nW sputtering\n"}, {"id": "S0022311513001165", "text": "Our simulations confirm experimental observations that W net erosion represents only tiny fraction (in our simulation \u223c1%) of the W gross erosion. The estimated upstream W fluxes, FWupstrem, are in good agreement with the experimentally observed values \u2a7d1019m-2s-1 [16]. Moreover, this value is not very sensitive to the divertor plasma temperature. For low temperatures the energy of D and C ions hitting to the divertor plates is too low to sputter sufficient amount of W. With increasing energy the W sputtering increases, but the potential drop in the divertor plasma increases too. As a result, most of the W atoms are ionized in the vicinity of the divertor and return back to the plates. There are two effects leading to the observed prompt redeposition of W ions: first is the \u201cnear-divertor\u201d ionization of W due to low ionization potential \u22127.86eV (for comparison the ionization potentials for D and C are 13,6 and 10.6eV), second, W+n ions have large Larmor radius \u223c2/nmm, so that they are redeposited within the distance of a Larmor radius. Important to note that a significant fraction of W ions escaping this prompt redeposition are returned back due to the friction with the main ions.\n", "keywords": "C\nD\nD and C ions\ndivertor\ndivertor plasma\ndivertor plates\nestimated upstream W fluxes\nexperimentally observed values\nfriction with the main ions\nFWupstrem\nhitting to the divertor plates\nincreasing energy\nionization\nionized\nlarge Larmor radius\nnear-divertor\u201d ionization\nobserved prompt redeposition\nplates\nreturn\nsimulations\nW\nW atoms\nW fluxes\nW gross erosion\nW ions\nW net erosion\nW+n ions\nW sputtering\n"}, {"id": "S0021999114008432", "text": "The validity of semi-classical boundary conditions for the WTE as introduced in [8] is a topic under vivid debate, especially after recent works which address the non-uniqueness and the symmetry properties of the Wigner function [27,28,46]. The numerical test cases presented therein are for symmetric potentials for which we cannot provide reliable, i.e. well-resolved, results due to the presence of singular terms in the steady state Wigner functions, see Section 4.3. Other recent studies demonstrate the convergence of the WTE calculations upon increasing the size of the simulation domain [44] as well as possible improvements by adapting the boundary distribution to the physical state of the active device region [47]. Despite their approximate nature we employ inflow/outflow boundary conditions here as well and demonstrate that accurate and physically valid results can be achieved for sufficiently large values of Lres. Due to the problematics with singular terms we present simulations only for non-zero bias voltages VDS\u22600\u00a0V.\n", "keywords": "accurate and physically valid results\nadapting the boundary distribution\nconvergence of the WTE calculations\nemploy inflow/outflow boundary conditions\nincreasing the size of the simulation domain\nnon-uniqueness and the symmetry properties of the Wigner function\nnumerical test cases\nOther recent studies\npresent simulations only for non-zero bias voltages\nsteady state Wigner functions\nsymmetric potentials\nvalidity of semi-classical boundary conditions\n"}, {"id": "S2212667813000610", "text": "The most important goal of the software industry is to produce successful product. During the process of production several times the product fails due to lack of proper management. This paper is exploring the role of software engineering courses in computer engineering related branches and then reasons why software developers lack project management in proper software management trainings. Our findings reflect that in majority of computer related branches like computer science, computer engineering, information system engineering there is no place for software project management course. Our findings are based on a survey of course curriculums of computer engineering, computer science and information system engineering courses taught in Turkish universities.", "keywords": "computer engineering\ncomputer science and information system engineering\nmanagement\nproduce successful product\nproduct fails\nproduction\nproject management\nproper software management trainings\nrole of software engineering courses\nsoftware industry\nsoftware management\nsoftware project management\nsoftware project management course\nsurvey of course curriculums\n"}, {"id": "S2212667813000610", "text": "The most important goal of the software industry is to produce successful product. During the process of production several times the product fails due to lack of proper management. This paper is exploring the role of software engineering courses in computer engineering related branches and then reasons why software developers lack project management in proper software management trainings. Our findings reflect that in majority of computer related branches like computer science, computer engineering, information system engineering there is no place for software project management course. Our findings are based on a survey of course curriculums of computer engineering, computer science and information system engineering courses taught in Turkish universities.", "keywords": "computer engineering\ncomputer science and information system engineering\nmanagement\nproduce successful product\nproduct fails\nproduction\nproject management\nproper software management trainings\nrole of software engineering courses\nsoftware industry\nsoftware management\nsoftware project management\nsoftware project management course\nsurvey of course curriculums\n"}, {"id": "S0045782513001448", "text": "Traditionally, the simulation of incompressible fluid flow by the SPH method has been through a weakly compressible SPH formulation (WCSPH). In this approach, the pressure is treated as a thermodynamic variable and is calculated using an artificial equation of state. The sound speed is set to be sufficiently high to limit density variations to within a small fraction of the actual fluid density. In practice, this high sound speed places a limitation on the maximum permissible time-step size through the Courant\u2013Friedrichs\u2013Lewy (CFL) constraint. A particular weakness relates to noise in the pressure field since a small perturbation in the local density will yield a large variation in the local pressure. This can make WCSPH formulations ineffective for accurate force and pressure prediction, although recent developments which create more uniform particle distributions have improved this [1,2]. A review of the SPH method can be found in [3] while a review of the classical WCSPH formulation applied to free-surface flows can be found in [4].\n", "keywords": "artificial equation of state\ncreate more uniform particle distributions\nfluid\nfluid flow\nfree-surface flows\nparticle\nsimulation of incompressible fluid flow\nSPH method\nWCSPH\nweakly compressible SPH formulation\n"}, {"id": "S0166218X1300348X", "text": "Max-linear programs have been used to describe optimisation problems for multiprocessor interactive systems. In some instances the variables used in this model are required to be integer; however, no method seems to exist for finding integer solutions to max-linear programs.For a generic class of matrices, we show that integer solutions to two-sided max-linear systems and programs can be found in polynomial time. For general matrices, we adapt the existing methods for finding real solutions to obtain algorithms for finding integer solutions.\n", "keywords": "adapt the existing methods for finding real solutions\nalgorithms for finding integer solutions\ngeneral matrices\ngeneric class of matrices\ninteger solutions\nmax-linear programs\nMax-linear programs\nmultiprocessor interactive systems\noptimisation problems\ntwo-sided max-linear systems and programs\nvariables\n"}, {"id": "S2212667814000884", "text": "Many models have been propounded for forecasting lightning. Though majority of the model had shown accuracy, the response time in detecting natural phenomenon is quite low. In this model, we used the mathematical experimentation of the micro scale plasmas to develop the macro scale atmospheric plasma which we believe is a major influence of lightning. The Schr\u00f6dinger-electrostatic algorithm was propounded to further increase both the accuracy and alacrity of detecting natural phenomena. According to our theoretical experimentation, the air density plays a major role in lightning forecast. Our guess was verified using the Davis Weather Station to track the air density both in the upper and lower atmosphere. The air density in the upper atmosphere showed prospect as a vital factor for lightning forecast.", "keywords": "air\nDavis Weather Station\ndetecting natural phenomena.\nforecasting lightning\nlightning\nmacro scale atmospheric plasma\nmathematical experimentation\nmicro scale plasmas\nmodel\nmodels\nnatural phenomenon\nSchr\u00f6dinger-electrostatic algorithm\ntheoretical experimentation\ntrack the air density\nupper atmosphere\n"}, {"id": "S2212667814000884", "text": "Many models have been propounded for forecasting lightning. Though majority of the model had shown accuracy, the response time in detecting natural phenomenon is quite low. In this model, we used the mathematical experimentation of the micro scale plasmas to develop the macro scale atmospheric plasma which we believe is a major influence of lightning. The Schr\u00f6dinger-electrostatic algorithm was propounded to further increase both the accuracy and alacrity of detecting natural phenomena. According to our theoretical experimentation, the air density plays a major role in lightning forecast. Our guess was verified using the Davis Weather Station to track the air density both in the upper and lower atmosphere. The air density in the upper atmosphere showed prospect as a vital factor for lightning forecast.", "keywords": "air\nDavis Weather Station\ndetecting natural phenomena.\nforecasting lightning\nlightning\nmacro scale atmospheric plasma\nmathematical experimentation\nmicro scale plasmas\nmodel\nmodels\nnatural phenomenon\nSchr\u00f6dinger-electrostatic algorithm\ntheoretical experimentation\ntrack the air density\nupper atmosphere\n"}, {"id": "S221266781300018X", "text": "Amodel are proposed for modeling data-centric Web services which are powered by relational databases and interact with users according to logical formulas specifying input constraints, control-flow constraints and state/output/action rules. The Linear Temporal First-Order Logic (LTL-FO) formulas over inputs, states, outputs and actions are used to express the properties to be verified.We have proven that automatic verification of LTL-FO properties of data-centric Web services under input-bounded constraints is decidable by reducing Web services to data-centric Web applications. Thus, we can verify Web service specifications using existing verifier designed for Web applications.", "keywords": "control-flow constraints\ndata-centric Web applications\ndata-centric Web services\ninteract with users\nLinear Temporal First-Order Logic\nlogical formulas\nLTL-FO\nmodeling data-centric Web services\nreducing Web services to data-centric Web applications\nrelational databases\nstate/output/action rules\nverify Web service specifications\nWeb applications\n"}, {"id": "S221266781300018X", "text": "Amodel are proposed for modeling data-centric Web services which are powered by relational databases and interact with users according to logical formulas specifying input constraints, control-flow constraints and state/output/action rules. The Linear Temporal First-Order Logic (LTL-FO) formulas over inputs, states, outputs and actions are used to express the properties to be verified.We have proven that automatic verification of LTL-FO properties of data-centric Web services under input-bounded constraints is decidable by reducing Web services to data-centric Web applications. Thus, we can verify Web service specifications using existing verifier designed for Web applications.", "keywords": "control-flow constraints\ndata-centric Web applications\ndata-centric Web services\ninteract with users\nLinear Temporal First-Order Logic\nlogical formulas\nLTL-FO\nmodeling data-centric Web services\nreducing Web services to data-centric Web applications\nrelational databases\nstate/output/action rules\nverify Web service specifications\nWeb applications\n"}, {"id": "S0032386109001463", "text": "When incompatible three component polymer chains are tethered at a junction point, the resultant star molecules of the ABC type are in a very frustrated field in bulk. That is, their junction points cannot be aligned on two-dimensional planes but on one-dimensional lines, as schematically shown in Fig.\u00a01. Furthermore, when the chain length difference is not so large, the array of junction points tends to be straight and long one. Consequently each domain with mesoscopic sizes becomes cylinders, and their cross sections could be conformed by polygons [28,29]. This is because three interfaces, A/B, B/C and C/A are likely to be flat since there exist no junction points at interfaces and therefore chain entropy contribution to the free energy of structure formation is considerably small comparing with regular block and graft copolymer systems. As a matter of fact, Dotera predicted several tiling patterns by the diagonal bond method, a new Monte Carlo Simulation [30], while Gemma and Dotera pointed out that only three regular tilings, i.e., (6.6.6), (4.8.8) and (4.6.12) are permitted for three-branched molecules proposed as the \u201ceven polygon theorem\u201d [31].\n", "keywords": "chain entropy\ncylinders\ndiagonal bond method\neven polygon theorem\nMonte Carlo Simulation\npolygons\npolymer chains\nstar molecules\nthree-branched molecules\nthree interfaces, A/B, B/C and C/A\n"}, {"id": "S0031920113000708", "text": "Geomagnetic jerks are conspicuous yet poorly understood phenomena of Earth\u2019s magnetic field, motivating investigations of their morphology and the theory behind their origins. Jerks are most commonly defined by their observed form at a single observatory as \u2018V\u2019 shapes in a single component of the geomagnetic secular variation (SV), the first time derivative of the main magnetic field (MF). The times of the gradient changes, which separate linear trends of several years, have associated step changes in the second time derivative of the MF (secular acceleration (SA)) and impulses in the third time derivative. The \u2018V\u2019 shape SV definition of jerks includes an implicit expectation of a \u2018large\u2019 magnitude step change in the gradient without definition of this scale or its threshold value other than the basic need for it to be observable in the data above the highly variable background noise. Jerks can be described by their amplitude, that is, the difference in the gradients of the two linear SV segments about a jerk, A=a2-a1, where a2 is the gradient after the jerk and a1 is the gradient before the jerk. This measure is essentially the best fit SA change across a jerk. Jerk amplitude is thus positive for a positive step in SA and negative for a negative step. Here we do not consider spatial extent in our definition and refer to individual features in one field component of a given observatory time series as a single jerk.\n", "keywords": "Earth\u2019s magnetic field\nGeomagnetic jerks\ngeomagnetic secular variation\ngradient changes\njerk\nJerk amplitude\njerks\nJerks\n\u2018large\u2019 magnitude step change\nmagnetic field\nmain magnetic field\nMF\nSA\nSA change\nsecular acceleration\nsecular variation\nSV\nSV segments\n"}, {"id": "S0370269304009268", "text": "One of the great successes of the experimental program carried out at LEP has been to put a firm lower bound on the Higgs mass, mH>114\u00a0GeV\u00a0[1], and at the same time, together with the information coming from SLD, to give a strong indirect evidence that the Higgs boson, the still missing particle of the Standard Model (SM), should be relatively light with a high probability for its mass to be below 200\u00a0GeV. The search for the Higgs boson is one of the main objective of the Tevatron and the future Large Hadron Collider (LHC), that are supposed to span all the Higgs mass regions up to 1\u00a0TeV. At hadron colliders the main Higgs production mechanism is the gluon fusion [2], a process whose knowledge is fundamental in order to put limits on the Higgs mass or, in case the Higgs is discovered, to compare the measured cross section with the SM result. Concerning the Higgs decay channels, it is quite difficult for an hadron collider to access part of the mass range favored by the LEP results, the so-called intermediate Higgs mass region 114\u2272mH\u2272160\u00a0GeV, because of the large QCD background to the dominant modes. In this region the rare decay H\u2192\u03b3\u03b3 is the most interesting alternative to the usual decay channels.\n", "keywords": "114\u2272mH\u2272160\u00a0GeV\naccess part of the mass range favored by the LEP results\ncompare the measured cross section with the SM result\ndecay channels\ngluon fusion\nhadron collider\nhadron colliders\nHiggs\nHiggs boson\nHiggs decay\nHiggs mass\nHiggs production mechanism\nH\u2192\u03b3\u03b3\nintermediate Higgs mass region\nLarge Hadron Collider\n(LHC)\nmain Higgs production mechanism\nmH>114\u00a0GeV\nparticle\npart of the mass range favored by the LEP results\nput a firm lower bound on the Higgs mass\nput limits on the Higgs mass\nQCD background\nrare decay\nSLD\nSM\nspan all the Higgs mass regions up to 1\u00a0TeV\nStandard Model\nTevatron\nThe search for the Higgs boson\n"}, {"id": "S0370269304009268", "text": "One of the great successes of the experimental program carried out at LEP has been to put a firm lower bound on the Higgs mass, mH>114\u00a0GeV\u00a0[1], and at the same time, together with the information coming from SLD, to give a strong indirect evidence that the Higgs boson, the still missing particle of the Standard Model (SM), should be relatively light with a high probability for its mass to be below 200\u00a0GeV. The search for the Higgs boson is one of the main objective of the Tevatron and the future Large Hadron Collider (LHC), that are supposed to span all the Higgs mass regions up to 1\u00a0TeV. At hadron colliders the main Higgs production mechanism is the gluon fusion [2], a process whose knowledge is fundamental in order to put limits on the Higgs mass or, in case the Higgs is discovered, to compare the measured cross section with the SM result. Concerning the Higgs decay channels, it is quite difficult for an hadron collider to access part of the mass range favored by the LEP results, the so-called intermediate Higgs mass region 114\u2272mH\u2272160\u00a0GeV, because of the large QCD background to the dominant modes. In this region the rare decay H\u2192\u03b3\u03b3 is the most interesting alternative to the usual decay channels.\n", "keywords": "114\u2272mH\u2272160\u00a0GeV\naccess part of the mass range favored by the LEP results\ncompare the measured cross section with the SM result\ndecay channels\ngluon fusion\nhadron collider\nhadron colliders\nHiggs\nHiggs boson\nHiggs decay\nHiggs mass\nHiggs production mechanism\nH\u2192\u03b3\u03b3\nintermediate Higgs mass region\nLarge Hadron Collider\n(LHC)\nmain Higgs production mechanism\nmH>114\u00a0GeV\nparticle\npart of the mass range favored by the LEP results\nput a firm lower bound on the Higgs mass\nput limits on the Higgs mass\nQCD background\nrare decay\nSLD\nSM\nspan all the Higgs mass regions up to 1\u00a0TeV\nStandard Model\nTevatron\nThe search for the Higgs boson\n"}, {"id": "S0021961413004321", "text": "The thermodynamics of copper-zinc alloys (brass) was subject of numerous investigations. Brass is characterised by an excess enthalpy and excess entropy of mixing, both of which are negative. The enthalpic data were measured by solution calorimetry e.g., [1\u20133] and based on chemical potential data calculated from phase equilibrium experiments e.g., [4\u20136], the excess entropy of mixing could be evaluated e.g., [7\u20139]. This excess entropy contains both, the vibrational and the configurational parts. The excess vibrational entropy, defined as the deviation from the entropy of a mechanical mixture of the end members A and B (i.e., Smmechmix=XASmA+XBSmB), can be determined by measuring the low temperature heat capacity (5 to 300K) versus composition behaviour. The determination of the excess configurational entropy, i.e., the excess entropy coming from non-random atomic distributions and defects, is much more difficult. Here, neutron scattering investigations together with computer simulations are normally used. If, however, reliable data of the total excess entropy (from enthalpic and chemical potential data) are available, the measurement of the excess vibrational entropy enables the determination of the excess configurational entropy simply by subtraction. Since configurational and vibrational entropies may have different temperature dependencies, it is worthwhile to separate the entropic effects. This is one aim of this study. Another aim is to deliver experimental data so that first principles studies can test their models on a disordered alloy, whose structural details (short-range order) depend on temperature.\n", "keywords": "brass\nBrass\nchemical potential data\ncomputer simulations\ncopper-zinc alloys\ndeliver experimental data\ndetermination of the excess configurational entropy\ndisordered alloy\nenthalpic and chemical potential data\nenthalpic data\nexcess configurational entropy\nexcess enthalpy and excess entropy of mixing\nexcess entropy of mixing\nexperimental data\nmeasurement of the excess vibrational entropy\nmeasuring the low temperature heat capacity (5 to 300K) versus composition behaviour\nneutron scattering investigations\nphase equilibrium experiments\nreliable data of the total excess entropy\nseparate the entropic effects\nSmmechmix=XASmA+XBSmB\nsolution calorimetry\nsubtraction\nthermodynamics of copper-zinc alloys (brass)\nvibrational and the configurational parts\n"}, {"id": "S0021961413004321", "text": "The thermodynamics of copper-zinc alloys (brass) was subject of numerous investigations. Brass is characterised by an excess enthalpy and excess entropy of mixing, both of which are negative. The enthalpic data were measured by solution calorimetry e.g., [1\u20133] and based on chemical potential data calculated from phase equilibrium experiments e.g., [4\u20136], the excess entropy of mixing could be evaluated e.g., [7\u20139]. This excess entropy contains both, the vibrational and the configurational parts. The excess vibrational entropy, defined as the deviation from the entropy of a mechanical mixture of the end members A and B (i.e., Smmechmix=XASmA+XBSmB), can be determined by measuring the low temperature heat capacity (5 to 300K) versus composition behaviour. The determination of the excess configurational entropy, i.e., the excess entropy coming from non-random atomic distributions and defects, is much more difficult. Here, neutron scattering investigations together with computer simulations are normally used. If, however, reliable data of the total excess entropy (from enthalpic and chemical potential data) are available, the measurement of the excess vibrational entropy enables the determination of the excess configurational entropy simply by subtraction. Since configurational and vibrational entropies may have different temperature dependencies, it is worthwhile to separate the entropic effects. This is one aim of this study. Another aim is to deliver experimental data so that first principles studies can test their models on a disordered alloy, whose structural details (short-range order) depend on temperature.\n", "keywords": "brass\nBrass\nchemical potential data\ncomputer simulations\ncopper-zinc alloys\ndeliver experimental data\ndetermination of the excess configurational entropy\ndisordered alloy\nenthalpic and chemical potential data\nenthalpic data\nexcess configurational entropy\nexcess enthalpy and excess entropy of mixing\nexcess entropy of mixing\nexperimental data\nmeasurement of the excess vibrational entropy\nmeasuring the low temperature heat capacity (5 to 300K) versus composition behaviour\nneutron scattering investigations\nphase equilibrium experiments\nreliable data of the total excess entropy\nseparate the entropic effects\nSmmechmix=XASmA+XBSmB\nsolution calorimetry\nsubtraction\nthermodynamics of copper-zinc alloys (brass)\nvibrational and the configurational parts\n"}, {"id": "S0021961413004321", "text": "The thermodynamics of copper-zinc alloys (brass) was subject of numerous investigations. Brass is characterised by an excess enthalpy and excess entropy of mixing, both of which are negative. The enthalpic data were measured by solution calorimetry e.g., [1\u20133] and based on chemical potential data calculated from phase equilibrium experiments e.g., [4\u20136], the excess entropy of mixing could be evaluated e.g., [7\u20139]. This excess entropy contains both, the vibrational and the configurational parts. The excess vibrational entropy, defined as the deviation from the entropy of a mechanical mixture of the end members A and B (i.e., Smmechmix=XASmA+XBSmB), can be determined by measuring the low temperature heat capacity (5 to 300K) versus composition behaviour. The determination of the excess configurational entropy, i.e., the excess entropy coming from non-random atomic distributions and defects, is much more difficult. Here, neutron scattering investigations together with computer simulations are normally used. If, however, reliable data of the total excess entropy (from enthalpic and chemical potential data) are available, the measurement of the excess vibrational entropy enables the determination of the excess configurational entropy simply by subtraction. Since configurational and vibrational entropies may have different temperature dependencies, it is worthwhile to separate the entropic effects. This is one aim of this study. Another aim is to deliver experimental data so that first principles studies can test their models on a disordered alloy, whose structural details (short-range order) depend on temperature.\n", "keywords": "brass\nBrass\nchemical potential data\ncomputer simulations\ncopper-zinc alloys\ndeliver experimental data\ndetermination of the excess configurational entropy\ndisordered alloy\nenthalpic and chemical potential data\nenthalpic data\nexcess configurational entropy\nexcess enthalpy and excess entropy of mixing\nexcess entropy of mixing\nexperimental data\nmeasurement of the excess vibrational entropy\nmeasuring the low temperature heat capacity (5 to 300K) versus composition behaviour\nneutron scattering investigations\nphase equilibrium experiments\nreliable data of the total excess entropy\nseparate the entropic effects\nSmmechmix=XASmA+XBSmB\nsolution calorimetry\nsubtraction\nthermodynamics of copper-zinc alloys (brass)\nvibrational and the configurational parts\n"}, {"id": "S037877531001949X", "text": "The Substrate Induced Coagulation (SIC) coating process provides a self assembled and almost binder free coating with small particles. Most research so far has been used to coat a variety of surfaces with highly conductive carbon blacks [34,35,36]. Layers deposited by this technique have been used in electromagnetic wave shielding, in the metallization process of through-holes in printed wiring boards, and in the manufacture of conducting polymers (such as Teflon) [37,38,39]. An advantage of this dip-coating process is that it can be used for any kind of surface, provided the substrate is stable in water and that the particles used for the coating form a meta-stable dispersion. Recently, a non-aqueous SIC coating process of carbon black was developed by investigating the stabilities of non-aqueous dispersions [36]. These dispersions were used to prepare LiCoO2-composite electrodes for Li-ion batteries with an improved conductivity while keeping the content of active battery material high [35].\n", "keywords": "carbon black\ncarbon blacks\ncoating\nconducting polymers\ndip-coating\nelectromagnetic wave shielding\nLiCoO2-composite electrodes\nLi-ion batteries\nmetallization\nmeta-stable dispersion\nprinted wiring boards\nSIC\nSIC coating\nSubstrate Induced Coagulation\nTeflon\n"}, {"id": "S037877531001949X", "text": "The Substrate Induced Coagulation (SIC) coating process provides a self assembled and almost binder free coating with small particles. Most research so far has been used to coat a variety of surfaces with highly conductive carbon blacks [34,35,36]. Layers deposited by this technique have been used in electromagnetic wave shielding, in the metallization process of through-holes in printed wiring boards, and in the manufacture of conducting polymers (such as Teflon) [37,38,39]. An advantage of this dip-coating process is that it can be used for any kind of surface, provided the substrate is stable in water and that the particles used for the coating form a meta-stable dispersion. Recently, a non-aqueous SIC coating process of carbon black was developed by investigating the stabilities of non-aqueous dispersions [36]. These dispersions were used to prepare LiCoO2-composite electrodes for Li-ion batteries with an improved conductivity while keeping the content of active battery material high [35].\n", "keywords": "carbon black\ncarbon blacks\ncoating\nconducting polymers\ndip-coating\nelectromagnetic wave shielding\nLiCoO2-composite electrodes\nLi-ion batteries\nmetallization\nmeta-stable dispersion\nprinted wiring boards\nSIC\nSIC coating\nSubstrate Induced Coagulation\nTeflon\n"}, {"id": "S0010938X15300512", "text": "Anodizing processes are widely used for protecting aluminium alloys against corrosion [1]. The resultant films are composed of amorphous alumina and consist of a relatively thick, porous, outer region and a thinner, non-porous, inner region [2,3]. The porous region contains the major pores of the film, which extend from the film surface to the barrier layer. Near the film surface, shorter, incipient pores are also present, whose growth stopped in the early stages of anodizing. The diameter of the major pores and the thickness of the inner, barrier region are dependent on the potential applied during anodizing, with typical proportionalities of \u223c1nmV\u22121 [3,4]. Studies of ionic migration in barrier-type and porous anodic alumina films have usually found a transport number of O2\u2212 ions of \u223c0.6 [5,6]. During the formation of porous films, the outward migrating Al3+ ions, constituting the remainder of the ionic current, are ejected to the electrolyte at the pore bases [7]. The electronic current in the barrier region is generally considered to be negligible. The thickness of the barrier region, which is relatively constant during the growth of a film under either a constant potential or constant current density, is maintained by a balance between growth of the barrier layer by continued oxidation of the aluminium substrate and thinning of the barrier layer by either field-assisted dissolution of the alumina at the pore bases [8] or field-assisted flow of alumina from the barrier layer to the pore walls [9\u201313]. The pores may be widened toward the film surface by chemical dissolution to an extent dependent on the anodizing conditions.\n", "keywords": "alumina\naluminium substrate\namorphous alumina\nanodic alumina films\nanodizing\nAnodizing processes\nbarrier layer\nbarrier region\nchemical dissolution\ncorrosion\nejected\nelectrolyte\nextend from the film surface\nfield-assisted flow of alumina\nfilm\nfilm surface\ngrowth of the barrier layer by continued oxidation\ngrowth stopped in the early stages of anodizing\nincipient pores\nionic current\nionic migration\nmajor pores\nO2\u2212 ions\noutward migrating Al3+ ions\noxidation\npore bases\npores\npore walls\nporous films\nprotecting aluminium alloys\nresultant films\nthinning of the barrier layer by either field-assisted dissolution\n"}, {"id": "S0010938X15300512", "text": "Anodizing processes are widely used for protecting aluminium alloys against corrosion [1]. The resultant films are composed of amorphous alumina and consist of a relatively thick, porous, outer region and a thinner, non-porous, inner region [2,3]. The porous region contains the major pores of the film, which extend from the film surface to the barrier layer. Near the film surface, shorter, incipient pores are also present, whose growth stopped in the early stages of anodizing. The diameter of the major pores and the thickness of the inner, barrier region are dependent on the potential applied during anodizing, with typical proportionalities of \u223c1nmV\u22121 [3,4]. Studies of ionic migration in barrier-type and porous anodic alumina films have usually found a transport number of O2\u2212 ions of \u223c0.6 [5,6]. During the formation of porous films, the outward migrating Al3+ ions, constituting the remainder of the ionic current, are ejected to the electrolyte at the pore bases [7]. The electronic current in the barrier region is generally considered to be negligible. The thickness of the barrier region, which is relatively constant during the growth of a film under either a constant potential or constant current density, is maintained by a balance between growth of the barrier layer by continued oxidation of the aluminium substrate and thinning of the barrier layer by either field-assisted dissolution of the alumina at the pore bases [8] or field-assisted flow of alumina from the barrier layer to the pore walls [9\u201313]. The pores may be widened toward the film surface by chemical dissolution to an extent dependent on the anodizing conditions.\n", "keywords": "alumina\naluminium substrate\namorphous alumina\nanodic alumina films\nanodizing\nAnodizing processes\nbarrier layer\nbarrier region\nchemical dissolution\ncorrosion\nejected\nelectrolyte\nextend from the film surface\nfield-assisted flow of alumina\nfilm\nfilm surface\ngrowth of the barrier layer by continued oxidation\ngrowth stopped in the early stages of anodizing\nincipient pores\nionic current\nionic migration\nmajor pores\nO2\u2212 ions\noutward migrating Al3+ ions\noxidation\npore bases\npores\npore walls\nporous films\nprotecting aluminium alloys\nresultant films\nthinning of the barrier layer by either field-assisted dissolution\n"}, {"id": "S221267161200176X", "text": "Modeling or approximating high dimensional, computationally-expensive problems faces an exponentially increasing difficulty, the \u201ccurse of dimensionality\u201d. This paper proposes a new form of high dimensional model representation (HDMR) by utilizing the support vector regression (SVR), termed as adaptive SVR-HMDR, to conquer this dilemma. The proposed model could reveal explicit correlations among different input variables of the underlying function which is unknown or expensive for computation. Taking advantage of HDMR's hierarchical structure, it could alleviate the exponential increasing difficulty, and gain satisfying accuracy with small set of samples by SVR. Numerical examples of different dimensionality are given to illustrate the principle, procedure and performance of SVR-HDMR.", "keywords": "adaptive SVR-HMDR\nalleviate the exponential increasing difficulty\napproximating\ncomputation\nconquer this dilemma\ncurse of dimensionality\nexplicit correlations\nHDMR\nhigh dimensional model representation\nillustrate the principle, procedure and performance\ninput variables\nModeling\nModeling or approximating high dimensional, computationally-expensive problems\nnew form of high dimensional model representation\nsupport vector regression\nSVR\nSVR-HDMR\n"}, {"id": "S0167931714003347", "text": "In order to study the mechanical behavior of metal films on compliant polymer substrates, fragmentation testing is often employed [8\u201312]. During fragmentation testing, the film-substrate couple is strained under uni-axial tension and observed with light microscopy (LM) or scanning electron microscopy (SEM). Brittle metals or ceramic films fracture, forming through thickness cracks (channel cracks) at low strain perpendicular to the straining direction. On the other hand, ductile metal films will first deform locally in the form of necks at low strains (Fig. 1a) and with increased strain through thickness cracks (TTC) can evolve (Fig. 1b). Fragmentation testing is best performed in-situ with LM or SEM so that the strain when the first crack forms can be observed. The initial fracture strain of the film, also known as the crack onset strain, can then be used to determine the interfacial fracture shear stress with knowledge of the crack spacing at saturation, \u03bb, film thickness, h, and the fracture stress, \u03c3f=Efilm\u03b5f, where \u03b5f is the fracture strain, using the shear lag model [8,13,14]. In-situ fragmentation testing with LM or SEM allows for the crack spacing evolution to be observed as a function of applied strain (Fig. 1c). Under tensile straining conditions, a brittle film will initially fracture at very low strains (<1%) and then with further strain continue to form cracks until the saturation crack spacing is reached. After the saturation spacing has been reached, cracks can no longer form between existing crack fragments and the film could delaminate via buckling.\n", "keywords": "brittle film\nBrittle metals\nbuckling\nceramic films\ncrack spacing evolution\ndeform locally\ndelaminate\nfilm\nfilm-substrate couple\nform cracks\nfracture\nfracture strain\nfragmentation testing\nFragmentation testing\nfunction of applied strain\nlight microscopy\nLM\nmechanical behavior\nmetal films\nperformed in-situ with LM or SEM\npolymer substrates\nscanning electron microscopy\nSEM\nshear lag model\nshear stress\nstrain\nstrained under uni-axial tension\nstudy the mechanical behavior\ntensile straining\n\u03c3f=Efilm\u03b5f\n"}, {"id": "S0032386114008428", "text": "Another choice was to graft the fluorinated groups on the copolymers with functional groups. Casazza et\u00a0al. [52] synthesized an acrylic terpolymer with pendent perfluoroether segments via grafting fluorinated groups into a poly(butyl methacrylate-co-hydroxyehtyl acrylate-co-ethyl acrylate) random copolymer through hexamethylene diisocyanate functionality. Malshe et\u00a0al. [53,54] studied the coating properties of fluorinated acrylic copolymers based on MMA, BA, and 2-hydroxyethyl methacrylate (HEMA). They partially esterified hydroxyl functionality of HEMA with tetrafluoro propanoic acid and cured the polymer with butylated melamine formaldehyde resin. Such methods were suited for the synthesis of copolymers containing complicated fluorinated groups or difficult to be provided directly by living polymerization.\n", "keywords": "2-hydroxyethyl methacrylate\nan acrylic terpolymer with pendent perfluoroether segments\nAnother choice\nBA\nbutyl methacrylate-co-hydroxyehtyl acrylate-co-ethyl acrylate\ncopolymer\ncopolymers containing complicated fluorinated groups\nfluorinated acrylic copolymers\ngrafting fluorinated groups\ngraft the fluorinated groups on the copolymers with functional groups\nHEMA\nhexamethylene diisocyanate functionality\nhydroxyl functionality of HEMA with tetrafluoro propanoic acid\nliving polymerization\nMMA\npoly\nthe coating properties of fluorinated acrylic copolymers\nthe polymer with butylated melamine formaldehyde resin\n"}, {"id": "S2212667812000883", "text": "The 21st century in the face of an aging population trend, the health status of the elderly is a hot issue of social concern, therefore, to explore the health status of the Chinese population aging and the elderly, elderly fitness exercise Misunderstanding study and formulate measures and methods of fitness of the elderly, promoting elderly fitness training towards a healthy, scientific direction, to promote a nationwide fitness activities carried out in order to achieve the exercise of scientific fitness of older persons.", "keywords": "aging population trend\nelderly fitness training\nexercise of scientific fitness\nexplore the health status\nformulate measures and methods of fitness\npromote a nationwide fitness activities\n"}, {"id": "S0045782512002678", "text": "To the best of authors\u2019 knowledge, so far there are only very few papers [12,13,16,29] which address the performance of linear algebra solvers. In Ref. [16], the authors study the performance of direct solvers which are clearly not suitable for large problems, specially in three-dimensions. In Ref. [29], the tearing and interconnecting approach of finite element methods is used in the context of isogeometric analysis, and the numerical tests (in absence of any theoretical study) suggest almost optimal (with a logarithmic factor) convergence rates of the proposed isogeometric tearing and interconnecting method. The only paper which provides rigorous theoretical study, supported by extensive numerical examples, is by Beirao et al. [12] where the authors discuss the overlapping Schwarz methods. The same authors have also proposed BDDC preconditioners for isogeometric analysis in [13].\n", "keywords": "BDDC preconditioners\nextensive numerical examples\nisogeometric analysis\nisogeometric tearing and interconnecting method\nlarge problems\nnumerical tests\nperformance of direct solvers\nperformance of linear algebra solvers\nrigorous theoretical study\nSchwarz methods\ntearing and interconnecting approach of finite element methods\ntheoretical study\n"}, {"id": "S0045782512002678", "text": "To the best of authors\u2019 knowledge, so far there are only very few papers [12,13,16,29] which address the performance of linear algebra solvers. In Ref. [16], the authors study the performance of direct solvers which are clearly not suitable for large problems, specially in three-dimensions. In Ref. [29], the tearing and interconnecting approach of finite element methods is used in the context of isogeometric analysis, and the numerical tests (in absence of any theoretical study) suggest almost optimal (with a logarithmic factor) convergence rates of the proposed isogeometric tearing and interconnecting method. The only paper which provides rigorous theoretical study, supported by extensive numerical examples, is by Beirao et al. [12] where the authors discuss the overlapping Schwarz methods. The same authors have also proposed BDDC preconditioners for isogeometric analysis in [13].\n", "keywords": "BDDC preconditioners\nextensive numerical examples\nisogeometric analysis\nisogeometric tearing and interconnecting method\nlarge problems\nnumerical tests\nperformance of direct solvers\nperformance of linear algebra solvers\nrigorous theoretical study\nSchwarz methods\ntearing and interconnecting approach of finite element methods\ntheoretical study\n"}, {"id": "S0045782512002678", "text": "To the best of authors\u2019 knowledge, so far there are only very few papers [12,13,16,29] which address the performance of linear algebra solvers. In Ref. [16], the authors study the performance of direct solvers which are clearly not suitable for large problems, specially in three-dimensions. In Ref. [29], the tearing and interconnecting approach of finite element methods is used in the context of isogeometric analysis, and the numerical tests (in absence of any theoretical study) suggest almost optimal (with a logarithmic factor) convergence rates of the proposed isogeometric tearing and interconnecting method. The only paper which provides rigorous theoretical study, supported by extensive numerical examples, is by Beirao et al. [12] where the authors discuss the overlapping Schwarz methods. The same authors have also proposed BDDC preconditioners for isogeometric analysis in [13].\n", "keywords": "BDDC preconditioners\nextensive numerical examples\nisogeometric analysis\nisogeometric tearing and interconnecting method\nlarge problems\nnumerical tests\nperformance of direct solvers\nperformance of linear algebra solvers\nrigorous theoretical study\nSchwarz methods\ntearing and interconnecting approach of finite element methods\ntheoretical study\n"}, {"id": "S0045782512002678", "text": "To the best of authors\u2019 knowledge, so far there are only very few papers [12,13,16,29] which address the performance of linear algebra solvers. In Ref. [16], the authors study the performance of direct solvers which are clearly not suitable for large problems, specially in three-dimensions. In Ref. [29], the tearing and interconnecting approach of finite element methods is used in the context of isogeometric analysis, and the numerical tests (in absence of any theoretical study) suggest almost optimal (with a logarithmic factor) convergence rates of the proposed isogeometric tearing and interconnecting method. The only paper which provides rigorous theoretical study, supported by extensive numerical examples, is by Beirao et al. [12] where the authors discuss the overlapping Schwarz methods. The same authors have also proposed BDDC preconditioners for isogeometric analysis in [13].\n", "keywords": "BDDC preconditioners\nextensive numerical examples\nisogeometric analysis\nisogeometric tearing and interconnecting method\nlarge problems\nnumerical tests\nperformance of direct solvers\nperformance of linear algebra solvers\nrigorous theoretical study\nSchwarz methods\ntearing and interconnecting approach of finite element methods\ntheoretical study\n"}, {"id": "S0010938X12002508", "text": "The study outlines a trial of transient response analysis on full-scale motorway bridge structures to obtain information concerning the steel\u2013concrete interface and is part of a larger study to assess the long-term sustained benefits offered by Impressed Current Cathodic Protection (ICCP) after the interruption of the protective current [1]. These structures had previously been protected for 5\u201316years by an ICCP system prior to the start of the study. The protective current was interrupted, in order to assess the long-term benefits provided by ICCP after it has been turned off. This paper develops and examines a simplified approach for the on-site use of transient response analysis and discusses the potential advantages of the technique as a tool for the assessment of the corrosion condition of steel in reinforced concrete structures.\n", "keywords": "assessment of the corrosion condition\nassess the long-term benefits\ndevelops and examines a simplified approach\ndiscusses the potential advantages\nICCP\nICCP system\nImpressed Current Cathodic Protection\ninterruption of the protective current\nlong-term sustained benefits\nmotorway bridge structures\nprotective current was interrupted\nreinforced concrete structures\nsteel\nsteel\u2013concrete interface\ntransient response analysis\n"}, {"id": "S2212667814000264", "text": "In this paper, we present a project aiming at integrating immersive virtual reality technologies into a three-dimensional virtual world. We use an educational platform vAcademia as a test bed for the project, and focus on improving the learning process and, subsequently \u2013 the outcomes. We aim at increasing the immersiveness of 3D virtual world experience by applying motion tracking for controlling the avatar and two technologies for natural navigation: immersive projection and head-mounted display. In addition, we propose the major types of learning scenarios for the use of the designed systems.", "keywords": "controlling the avatar\neducational platform\nhead-mounted display\nimmersive projection\nincreasing the immersiveness of 3D virtual world experience\nintegrating immersive virtual reality technologies into a three-dimensional virtual world\nlearning process\nmajor types of learning scenarios\nmotion tracking\nnatural navigation\nvAcademia\nvirtual reality technologies\n"}, {"id": "S1364815216302705", "text": "The main objective of this manuscript is to present and discuss the application of SLAMM to the New York coast. Although the base analysis considers a range of different possible SLR scenarios, the effects of various sources of uncertainties such as input parameters and driving data are not accounted for. In addition, refined and site-specific data are often not available requiring the use of regional data collected from literature and professional judgement in order to run the model. To ignore the effects of these uncertainties on predictions may make interpretation of the results and subsequent decision making misleading since the likelihood and probabilities of predicted outcomes would be unknown. A unique capability of the current version of SLAMM is the ability to aggregate multiple types of input-data uncertainty to create outputs accompanied by probability statements and confidence intervals. Uncertainty in elevation data layers have been considered by several modeling groups to various extents (Gesch, 2009; Gilmer and Ferda\u00f1a, 2012; Schmid et\u00a0al., 2014). However, to the best of our knowledge, no other marsh migration model simultaneously accounts for the combined effects of uncertainty in spatial inputs (DEM, VDATUM, etc.) and parameter choices (accretion rates, tide ranges, etc.) on landcover projections. This added feature of SLAMM allows results to be evaluated in terms of their likelihood of occurrence with respect to input-data and parameter uncertainties. Further, by assigning wide ranges of uncertainty when appropriate, it permits the production of meaningful projections in areas where high-quality local data are not available.\n", "keywords": "accretion rates\nDEM\nhigh-quality local data\nlandcover projections\nmarsh migration model\nparameter choices\npresent and discuss the application of SLAMM to the New York coast\nSLAMM\nSLR\nspatial inputs\ntide ranges\nVDATUM\n"}, {"id": "S2352179114200056", "text": "Power and particle exhaust are crucial for the viability of any future fusion power plant concept. Heat in fusion reactors must be extracted through a wall and cannot be exhausted volumetrically, which limits the allowed power density in fusion reactors [1] and is a severe technical challenge in itself [2]. In addition, structural material changes resulting from neutron irradiation cause degradation in the heat exhaust capabilities of existing designs [3] and static surfaces can suffer severely from erosion due to impinging plasma particles [4,5]. It is concluded that conventional concepts and materials for plasma facing components (PFCs) reach their limits in terms of material lifetime and power exhaust at approximately 20MW/m2, which is presumably dramatically reduced to <10MW/m2 due to neutron damage in a D-T reactor [6] or even only half that value [7].\n", "keywords": "D-T reactor\nfusion power plant\nfusion reactors\nimpinging plasma particles\nneutron\nneutron damage\nneutron irradiation\nPFCs\nplasma facing components\nPower and particle exhaust\nsevere technical challenge\nstatic surfaces\nstructural material\nstructural material changes\n"}, {"id": "S2352179114200056", "text": "Power and particle exhaust are crucial for the viability of any future fusion power plant concept. Heat in fusion reactors must be extracted through a wall and cannot be exhausted volumetrically, which limits the allowed power density in fusion reactors [1] and is a severe technical challenge in itself [2]. In addition, structural material changes resulting from neutron irradiation cause degradation in the heat exhaust capabilities of existing designs [3] and static surfaces can suffer severely from erosion due to impinging plasma particles [4,5]. It is concluded that conventional concepts and materials for plasma facing components (PFCs) reach their limits in terms of material lifetime and power exhaust at approximately 20MW/m2, which is presumably dramatically reduced to <10MW/m2 due to neutron damage in a D-T reactor [6] or even only half that value [7].\n", "keywords": "D-T reactor\nfusion power plant\nfusion reactors\nimpinging plasma particles\nneutron\nneutron damage\nneutron irradiation\nPFCs\nplasma facing components\nPower and particle exhaust\nsevere technical challenge\nstatic surfaces\nstructural material\nstructural material changes\n"}, {"id": "S2352179114200056", "text": "Power and particle exhaust are crucial for the viability of any future fusion power plant concept. Heat in fusion reactors must be extracted through a wall and cannot be exhausted volumetrically, which limits the allowed power density in fusion reactors [1] and is a severe technical challenge in itself [2]. In addition, structural material changes resulting from neutron irradiation cause degradation in the heat exhaust capabilities of existing designs [3] and static surfaces can suffer severely from erosion due to impinging plasma particles [4,5]. It is concluded that conventional concepts and materials for plasma facing components (PFCs) reach their limits in terms of material lifetime and power exhaust at approximately 20MW/m2, which is presumably dramatically reduced to <10MW/m2 due to neutron damage in a D-T reactor [6] or even only half that value [7].\n", "keywords": "D-T reactor\nfusion power plant\nfusion reactors\nimpinging plasma particles\nneutron\nneutron damage\nneutron irradiation\nPFCs\nplasma facing components\nPower and particle exhaust\nsevere technical challenge\nstatic surfaces\nstructural material\nstructural material changes\n"}, {"id": "S2352179114200056", "text": "Power and particle exhaust are crucial for the viability of any future fusion power plant concept. Heat in fusion reactors must be extracted through a wall and cannot be exhausted volumetrically, which limits the allowed power density in fusion reactors [1] and is a severe technical challenge in itself [2]. In addition, structural material changes resulting from neutron irradiation cause degradation in the heat exhaust capabilities of existing designs [3] and static surfaces can suffer severely from erosion due to impinging plasma particles [4,5]. It is concluded that conventional concepts and materials for plasma facing components (PFCs) reach their limits in terms of material lifetime and power exhaust at approximately 20MW/m2, which is presumably dramatically reduced to <10MW/m2 due to neutron damage in a D-T reactor [6] or even only half that value [7].\n", "keywords": "D-T reactor\nfusion power plant\nfusion reactors\nimpinging plasma particles\nneutron\nneutron damage\nneutron irradiation\nPFCs\nplasma facing components\nPower and particle exhaust\nsevere technical challenge\nstatic surfaces\nstructural material\nstructural material changes\n"}, {"id": "S221450951530005X", "text": "An innovative sound wall system was developed in the University of Western Ontario, and was examined to serve as a vertical extension to the existing sound walls. The wall system (denoted as flexi-wall) consists of stay-in-place poly-blocks as formwork, light polyurethane foam (LPF) reinforced with steel rebars as structural cores and polyurea as a coating of the wall surfaces (Fig. 1). Poly-blocks are interlocking light-weight blocks which are stacked up layer by layer and act as formwork for the LPF cores. The poly-block is 20\u00d720\u00d780cm3 and includes four cylindrical voids with 14cm diameter. It is made of molded low-density polyurethane and weighs approximately 1kg. The poly-blocks are fire-resistant blocks and have an excellent capability to absorb, mitigate and reflect a wide range of noises with unmatched frequency of reflective noise. Polyurea coating is an abrasion-resistant finishing layer, which is sprayed on the surfaces of the wall and sets within 2\u20133min. This layer also enhances the surface resistance of poly-blocks against stone impact, weathering, fire development, chemicals and penetration. LPF is an expanding liquid mixture which is injected into the poly-block voids and cures within 10min. Steel rebars are epoxied into holes drilled in the existing sound wall to connect the wall extension to its base.\n", "keywords": "abrasion-resistant finishing layer\nabsorb\nbase\nchemicals\ncoating\ncures\ncylindrical voids\nenhances the surface resistance\nexisting sound wall\nexisting sound walls\nexpanding liquid mixture\nfire development\nfire-resistant blocks\nflexi-wall\nholes\ninterlocking light-weight blocks\nlayer\nlight polyurethane foam\nLPF\nLPF cores\nmitigate\nmolded low-density polyurethane\npenetration\npoly-block\npoly-blocks\nPoly-blocks\npoly-block voids\npolyurea\nPolyurea coating\nreflect\nsound wall system\nsprayed\nstay-in-place poly-blocks\nsteel rebars\nSteel rebars\nstone impact\nstructural cores\nsurfaces of the wall\nwall extension\nwall surfaces\nwall system\nweathering\n"}, {"id": "S0963869514000875", "text": "There are a number of avenues to explore for future work, in particular the use of other time\u2013frequency analysis methods. The STFT spectrogram was utilised here, as it is the simplest to implement. Whilst all of the echoes could be clearly resolved in both time and frequency, the spectrogram suffers from a fixed resolution, i.e. an increase of time resolution necessarily leads to a decrease in frequency resolution. Other methods of time\u2013frequency analysis, such as discrete wavelet analysis, benefit from advantage of multi-resolution analysis, which offers improved temporal resolution of the high frequency components, and frequency resolution of the low frequency components [25,18,19]. Also, whilst the current work has utilised SH waves that are generated by EMATs, the physics that describes the pulsed array system is universal to other types of waves. Future work will include demonstrating this phenomenon with a number of other systems, for example using longitudinal ultrasonic waves or electromagnetic waves.\n", "keywords": "demonstrating this phenomenon with a number of other systems\ndiscrete wavelet analysis\nelectromagnetic waves\nEMATs\nfixed resolution\nfrequency resolution\nlongitudinal ultrasonic waves\nlow frequency components\nmulti-resolution analysis\npulsed array system\nSH waves\nspectrogram\nSTFT spectrogram\ntemporal resolution\ntime\u2013frequency analysis\ntime resolution\nuse of other time\u2013frequency analysis methods\n"}, {"id": "S0963869514000875", "text": "There are a number of avenues to explore for future work, in particular the use of other time\u2013frequency analysis methods. The STFT spectrogram was utilised here, as it is the simplest to implement. Whilst all of the echoes could be clearly resolved in both time and frequency, the spectrogram suffers from a fixed resolution, i.e. an increase of time resolution necessarily leads to a decrease in frequency resolution. Other methods of time\u2013frequency analysis, such as discrete wavelet analysis, benefit from advantage of multi-resolution analysis, which offers improved temporal resolution of the high frequency components, and frequency resolution of the low frequency components [25,18,19]. Also, whilst the current work has utilised SH waves that are generated by EMATs, the physics that describes the pulsed array system is universal to other types of waves. Future work will include demonstrating this phenomenon with a number of other systems, for example using longitudinal ultrasonic waves or electromagnetic waves.\n", "keywords": "demonstrating this phenomenon with a number of other systems\ndiscrete wavelet analysis\nelectromagnetic waves\nEMATs\nfixed resolution\nfrequency resolution\nlongitudinal ultrasonic waves\nlow frequency components\nmulti-resolution analysis\npulsed array system\nSH waves\nspectrogram\nSTFT spectrogram\ntemporal resolution\ntime\u2013frequency analysis\ntime resolution\nuse of other time\u2013frequency analysis methods\n"}, {"id": "S0022311511010014", "text": "Discovering that both the vacancy and interstitial defect migration pathways are confined to Ga-free regions suggests changes in recombination rates of isolated vacancy-interstitial pairs in comparison to pure Pu. The degree to which the rates are effected depends on the distribution of residual defects post a cascade event, in addition to the concentration and ordering of the Ga atoms. If vacancies and interstitials become greatly separated after the collision cascade, then pathways to recombination are likely to become restricted and recovery times will be extended. This is viable for cascades that created a vacancy rich core surrounded by dispersed interstitials, as found for the low energy cascades in Pu and PuGa [11,12]. This may also be the case for channelling events, where energetic atoms travel deep into the lattice through channels of low atomic density.\n", "keywords": "cascade event\nchannelling events\ncollision cascade\ndefect migration pathways\nenergetic atoms travel deep into the lattice through channels of low atomic density\nGa\nGa atoms\nlow energy cascades in Pu and PuGa\n"}, {"id": "S0022311511010014", "text": "Discovering that both the vacancy and interstitial defect migration pathways are confined to Ga-free regions suggests changes in recombination rates of isolated vacancy-interstitial pairs in comparison to pure Pu. The degree to which the rates are effected depends on the distribution of residual defects post a cascade event, in addition to the concentration and ordering of the Ga atoms. If vacancies and interstitials become greatly separated after the collision cascade, then pathways to recombination are likely to become restricted and recovery times will be extended. This is viable for cascades that created a vacancy rich core surrounded by dispersed interstitials, as found for the low energy cascades in Pu and PuGa [11,12]. This may also be the case for channelling events, where energetic atoms travel deep into the lattice through channels of low atomic density.\n", "keywords": "cascade event\nchannelling events\ncollision cascade\ndefect migration pathways\nenergetic atoms travel deep into the lattice through channels of low atomic density\nGa\nGa atoms\nlow energy cascades in Pu and PuGa\n"}, {"id": "S0022311511010014", "text": "Discovering that both the vacancy and interstitial defect migration pathways are confined to Ga-free regions suggests changes in recombination rates of isolated vacancy-interstitial pairs in comparison to pure Pu. The degree to which the rates are effected depends on the distribution of residual defects post a cascade event, in addition to the concentration and ordering of the Ga atoms. If vacancies and interstitials become greatly separated after the collision cascade, then pathways to recombination are likely to become restricted and recovery times will be extended. This is viable for cascades that created a vacancy rich core surrounded by dispersed interstitials, as found for the low energy cascades in Pu and PuGa [11,12]. This may also be the case for channelling events, where energetic atoms travel deep into the lattice through channels of low atomic density.\n", "keywords": "cascade event\nchannelling events\ncollision cascade\ndefect migration pathways\nenergetic atoms travel deep into the lattice through channels of low atomic density\nGa\nGa atoms\nlow energy cascades in Pu and PuGa\n"}, {"id": "S0022311511010014", "text": "Discovering that both the vacancy and interstitial defect migration pathways are confined to Ga-free regions suggests changes in recombination rates of isolated vacancy-interstitial pairs in comparison to pure Pu. The degree to which the rates are effected depends on the distribution of residual defects post a cascade event, in addition to the concentration and ordering of the Ga atoms. If vacancies and interstitials become greatly separated after the collision cascade, then pathways to recombination are likely to become restricted and recovery times will be extended. This is viable for cascades that created a vacancy rich core surrounded by dispersed interstitials, as found for the low energy cascades in Pu and PuGa [11,12]. This may also be the case for channelling events, where energetic atoms travel deep into the lattice through channels of low atomic density.\n", "keywords": "cascade event\nchannelling events\ncollision cascade\ndefect migration pathways\nenergetic atoms travel deep into the lattice through channels of low atomic density\nGa\nGa atoms\nlow energy cascades in Pu and PuGa\n"}, {"id": "S0009261413006738", "text": "The PESs here employed have already been tested in order to verify their validity for dynamical purposes. Such tests include studies of the nitrogen exchange reaction [14] both adiabatic by running trajectories on the lowest surfaces and non-adiabatic by using the trajectory surface hoping (TSH) method [22,23] for transitions to the excited state of same symmetry. It was concluded that nonadiabatic transitions could not make a significant impact on the rate coefficients, and therefore all trajectories here reported are independently integrated for each symmetry on the corresponding lowest adiabatic PES. In fact, we have tested the impact of running the trajectories starting on the upper sheets, and found no vibrational transition to take place, only small amounts of rotational energy is exchanged in this case. Also neglected are electronic transitions to the quartet state which are believed to be far less probable than the simple vibrational energy transfer here studied due to their spin-forbidden character. It should also be noted that the use of quasiclassical trajectories is justified by the large masses of the atoms involved [24].\n", "keywords": "atoms\nelectronic transitions to the quartet state\nindependently integrated for each symmetry\nrate coefficients\nrotational energy\nstudies of the nitrogen exchange reaction\ntrajectory surface hoping\ntrajectory surface hoping (TSH) method\nTSH\nuse of quasiclassical trajectories\nverify their validity for dynamical purposes\nvibrational transition\n"}, {"id": "S1877750315300119", "text": "Generalized polynomial chaos expansions. One approach to model densities with stochastically dependent components numerically, is to reformulate the uncertainty problem as a set of independent components through generalised polynomial chaos expansion [34]. As described in detail in Section 3.1, a Rosenblatt transformation allows for the mapping between any domain and the unit hypercube [0, 1]D. With a double transformation we can reformulate the response function f asf(x,t,Q)=f(x,t,TQ\u22121(TR(R)))\u2248f\u02c6(x,t,R)=\u2211n\u2208INcn(x,t)\u03a6n(R),where R is any random variable drawn from pR, which for simplicity is chosen to consists of independent components. Also, {\u03a6n}n\u2208IN is constructed to be orthogonal with respect to LR, not LQ. In any case, R is either selected from the Askey-Wilson scheme, or calculated using the discretized Stieltjes procedure. We remark that the accuracy of the approximation deteriorate if the transformation composition TQ\u22121\u2218TR is not smooth [34]. Dakota, Turns, and Chaospy all support generalized polynomial chaos expansions for independent stochastic variables and the Normal/Nataf copula listed in Table 2. Since Chaospy has the Rosenblatt transformation underlying the computational framework, generalized polynomial chaos expansions are in fact available for all densities.\n", "keywords": "Askey-Wilson scheme\ncomputational framework\ndiscretized Stieltjes procedure\ngeneralised polynomial chaos expansion\ngeneralized polynomial chaos expansions\nGeneralized polynomial chaos expansions\nmodel densities\nRosenblatt transformation\nuncertainty problem\n"}, {"id": "S0010482516301810", "text": "Three-dimensional digital subtraction angiographic (3D-DSA) images from diagnostic cerebral angiography were obtained at least one day prior to embolization in all patients. The raw data of 3D-DSA in a DICOM file were used for creating a 3D model of the target vessel segment. These data were converted to standard triangulation language (STL) surface data as an aggregation of fine triangular meshes using 3D visualization and measurement software (Amira version X, FEI, Burlington, MA, USA). An unstructured computational volumetric mesh was constructed from the triangulated surface. Smoothing and remeshing followed as next steps. The STL file was then transferred to a 3D printer (OBJET30 Pro; Stratasys Ltd., Eden Prairie, MN, USA). The resolution of the build layer was 0.028mm, and the 3D printed vessel model was produced using acrylic resin (Vero). Following immersion in water for a few hours, the surface of the 3D printed model was smoothed by manually removing spicule.\n", "keywords": "3D-DSA\n3D model\n3D printed model\n3D printer\n3D visualization\n3D visualization and measurement software\nacrylic resin\nAmira version X\ncerebral angiography\ncomputational volumetric mesh\ncreating a 3D model of the target vessel segment\nDICOM file\nembolization\nimmersion in water\nmeasurement\nOBJET30 Pro\nremeshing\nremoving spicule.\nsmoothed\nSmoothing\nstandard triangulation language\nSTL\nSTL file\nThree-dimensional digital subtraction angiographic\ntriangular meshes\ntriangulated surface\nVero\nwater\n"}, {"id": "S0370269302011838", "text": "First results from RHIC on charged multiplicities, evolution of multiplicities with centrality, particle ratios and transverse momentum distributions in central and minimum bias collisions, are analyzed in a string model which includes hard collisions, collectivity in the initial state considered as string fusion, and rescattering of the produced secondaries. Multiplicities and their evolution with centrality are successfully reproduced. Transverse momentum distributions in the model show a larger pT-tail than experimental data, disagreement which grows with increasing centrality. Discrepancies with particle ratios appear and are examined comparing with previous features of the model at SPS.First results from RHIC on charged multiplicities, evolution of multiplicities with centrality, particle ratios and transverse momentum distributions in central and minimum bias collisions, are analyzed in a string model which includes hard collisions, collectivity in the initial state considered as string fusion, and rescattering of the produced secondaries. Multiplicities and their evolution with centrality are successfully reproduced. Transverse momentum distributions in the model show a larger pT-tail than experimental data, disagreement which grows with increasing centrality. Discrepancies with particle ratios appear and are examined comparing with previous features of the model at SPS.\n", "keywords": "charged multiplicities\ncollectivity in the initial state\nDiscrepancies with particle ratios\nevolution of multiplicities with centrality\nhard collisions\nMultiplicities\nparticle\nparticle ratios\nrescattering of the produced secondaries\nstring model\ntheir evolution\ntransverse momentum distributions\nTransverse momentum distributions\n"}, {"id": "S0167273811005091", "text": "Owing to widespread availability, the most extensively adopted tomography technique utilizes the milling power of a focused ion beam (FIB) in conjunction with the imaging capabilities of high resolution FE-SEM, to provide a sequence of 2D images that can be effectively re-combined in 3D space. However, because this technique is destructive, studies of microstructural evolution are influenced by inherent sample variability. Non-destructive X-ray nano-computed tomography (CT) [9\u201311] provides a platform for exploring dynamic microstructural change in the absence of these possible complications and is compatible with both laboratory and synchrotron radiation. The authors have previously demonstrated a technique for preparation of optimal sample geometries for X-ray nano-CT [12], while this FIB sample preparation route will involve the selective removal of portions of the fuel cell electrode microstructure (and therefore may be destructive to the working fuel cell), the non-destructive X-ray characterization technique allows repeated, non-destructive characterization of the selected sample which facilitates the study of microstructural evolution processes in response to various environmental changes.\n", "keywords": "2D images\nCT\nexploring dynamic microstructural change\nFIB\nFIB sample\nFIB sample preparation route\nfocused ion beam\nfuel cell electrode microstructure\nhigh resolution FE-SEM\nlaboratory and synchrotron radiation\nmicrostructural evolution processes\nnon-destructive X-ray characterization technique\nNon-destructive X-ray nano-computed tomography\npreparation of optimal sample geometries for X-ray nano-CT\nselected sample\nstudies of microstructural evolution\ntomography technique\nworking fuel cell\nX-ray nano-CT\n"}, {"id": "S0167273811005091", "text": "Owing to widespread availability, the most extensively adopted tomography technique utilizes the milling power of a focused ion beam (FIB) in conjunction with the imaging capabilities of high resolution FE-SEM, to provide a sequence of 2D images that can be effectively re-combined in 3D space. However, because this technique is destructive, studies of microstructural evolution are influenced by inherent sample variability. Non-destructive X-ray nano-computed tomography (CT) [9\u201311] provides a platform for exploring dynamic microstructural change in the absence of these possible complications and is compatible with both laboratory and synchrotron radiation. The authors have previously demonstrated a technique for preparation of optimal sample geometries for X-ray nano-CT [12], while this FIB sample preparation route will involve the selective removal of portions of the fuel cell electrode microstructure (and therefore may be destructive to the working fuel cell), the non-destructive X-ray characterization technique allows repeated, non-destructive characterization of the selected sample which facilitates the study of microstructural evolution processes in response to various environmental changes.\n", "keywords": "2D images\nCT\nexploring dynamic microstructural change\nFIB\nFIB sample\nFIB sample preparation route\nfocused ion beam\nfuel cell electrode microstructure\nhigh resolution FE-SEM\nlaboratory and synchrotron radiation\nmicrostructural evolution processes\nnon-destructive X-ray characterization technique\nNon-destructive X-ray nano-computed tomography\npreparation of optimal sample geometries for X-ray nano-CT\nselected sample\nstudies of microstructural evolution\ntomography technique\nworking fuel cell\nX-ray nano-CT\n"}, {"id": "S0370269304009335", "text": "In these frameworks, however, the physical spacetime dimension is an input rather than a prediction of the theory. In fact, in standard theories whose gravitational sector is described by the Einstein\u2013Hilbert action, there is no obstruction to perform dimensional reductions to spacetimes of dimensions d\u22604. Then the question arises, since eleven-dimensional Minkowski space is a maximally (super)symmetric state, and the theory is well-behaved around it, why the theory does not select this configuration as the vacuum, but instead, it chooses a particular compactified space with less symmetry. An ideal situation, instead, would be that the eleven-dimensional theory dynamically predicted a low energy regime which could only be a four-dimensional effective theory. In such a scenario, a background solution with an effective spacetime dimension d>4 should be expected to be a false vacuum where the propagators for the dynamical fields are ill-defined, lest a low energy effective theory could exist in dimensions higher than four.\n", "keywords": "background solution\ncompactified space\nd>4\ndimensional reductions\neffective spacetime dimension\nEinstein\u2013Hilbert action\neleven-dimensional theory\nfalse vacuum\nfour-dimensional effective theory\nlow energy effective theory\nlow energy regime\nMinkowski space\nphysical spacetime dimension\nprediction of the theory\npropagators for the dynamical fields\nselect this configuration\nstandard theories\n(super)symmetric state\n"}, {"id": "S0079642514000887", "text": "The exquisite manipulation and exact measurement of properties of individual nanomaterials, compared with notable progress in their preparation, have not been thoroughly addressed albeit being of prime importance for the sustained development of new devices [58\u201361]. To date, several instruments have been designed for such goals, namely, scanning electron microscopes (SEM), atomic force microscopes (AFM) and transmission electron microscopes (TEM) [62,63]. Compared with the first two setups, which have no direct access to the material internal structure and atomic bonding information [64\u201367], the state-of-the-art in situ high-resolution TEM technique allows one to not only manipulate with an individual object at the nano-scale precision but to also get deep insights into its physical, chemical, and microstructural statuses [68\u201371]. Combining the capabilities of a conventional high-resolution TEM and AFM or STM probes produces advanced and dedicated TEM holders, which are becoming the powerful tools in nanomaterials manipulation and properties analysis. Such holders have been commercialized, for instance, by \u201cNanofactory Instruments AB\u2019\u2019, Goteborg, Sweden [72]. The full usefulness of these advanced in-situ TEM techniques is apparent with respect to mechanical and thermal property analysis of individual nanostructures, e.g., elasticity, plasticity and strength data while employing direct bent or tensile tests [73\u201375], probing electrical characteristics, e.g., field emission [27,76,77], electrical transport tracing [78\u201380], soldering [81,82], and doping [83], etc.\n", "keywords": "AFM\natomic bonding information\natomic force microscopes\ndirect bent or tensile tests\ndoping\nelectrical transport tracing\nexact measurement of properties\nfield emission\nholders\ninstruments\nmanipulate with an individual object\nmaterial internal structure\nmechanical and thermal property analysis\nnanomaterials\nnanomaterials manipulation\nnanostructures\nprobing electrical characteristics\nproperties analysis\nscanning electron microscopes\nSEM\nsoldering\nSTM probes\nsustained development of new devices\nTEM\nTEM holders\nTEM techniques\ntransmission electron microscopes\n"}, {"id": "S0079642514000887", "text": "The exquisite manipulation and exact measurement of properties of individual nanomaterials, compared with notable progress in their preparation, have not been thoroughly addressed albeit being of prime importance for the sustained development of new devices [58\u201361]. To date, several instruments have been designed for such goals, namely, scanning electron microscopes (SEM), atomic force microscopes (AFM) and transmission electron microscopes (TEM) [62,63]. Compared with the first two setups, which have no direct access to the material internal structure and atomic bonding information [64\u201367], the state-of-the-art in situ high-resolution TEM technique allows one to not only manipulate with an individual object at the nano-scale precision but to also get deep insights into its physical, chemical, and microstructural statuses [68\u201371]. Combining the capabilities of a conventional high-resolution TEM and AFM or STM probes produces advanced and dedicated TEM holders, which are becoming the powerful tools in nanomaterials manipulation and properties analysis. Such holders have been commercialized, for instance, by \u201cNanofactory Instruments AB\u2019\u2019, Goteborg, Sweden [72]. The full usefulness of these advanced in-situ TEM techniques is apparent with respect to mechanical and thermal property analysis of individual nanostructures, e.g., elasticity, plasticity and strength data while employing direct bent or tensile tests [73\u201375], probing electrical characteristics, e.g., field emission [27,76,77], electrical transport tracing [78\u201380], soldering [81,82], and doping [83], etc.\n", "keywords": "AFM\natomic bonding information\natomic force microscopes\ndirect bent or tensile tests\ndoping\nelectrical transport tracing\nexact measurement of properties\nfield emission\nholders\ninstruments\nmanipulate with an individual object\nmaterial internal structure\nmechanical and thermal property analysis\nnanomaterials\nnanomaterials manipulation\nnanostructures\nprobing electrical characteristics\nproperties analysis\nscanning electron microscopes\nSEM\nsoldering\nSTM probes\nsustained development of new devices\nTEM\nTEM holders\nTEM techniques\ntransmission electron microscopes\n"}, {"id": "S0079642514000887", "text": "The exquisite manipulation and exact measurement of properties of individual nanomaterials, compared with notable progress in their preparation, have not been thoroughly addressed albeit being of prime importance for the sustained development of new devices [58\u201361]. To date, several instruments have been designed for such goals, namely, scanning electron microscopes (SEM), atomic force microscopes (AFM) and transmission electron microscopes (TEM) [62,63]. Compared with the first two setups, which have no direct access to the material internal structure and atomic bonding information [64\u201367], the state-of-the-art in situ high-resolution TEM technique allows one to not only manipulate with an individual object at the nano-scale precision but to also get deep insights into its physical, chemical, and microstructural statuses [68\u201371]. Combining the capabilities of a conventional high-resolution TEM and AFM or STM probes produces advanced and dedicated TEM holders, which are becoming the powerful tools in nanomaterials manipulation and properties analysis. Such holders have been commercialized, for instance, by \u201cNanofactory Instruments AB\u2019\u2019, Goteborg, Sweden [72]. The full usefulness of these advanced in-situ TEM techniques is apparent with respect to mechanical and thermal property analysis of individual nanostructures, e.g., elasticity, plasticity and strength data while employing direct bent or tensile tests [73\u201375], probing electrical characteristics, e.g., field emission [27,76,77], electrical transport tracing [78\u201380], soldering [81,82], and doping [83], etc.\n", "keywords": "AFM\natomic bonding information\natomic force microscopes\ndirect bent or tensile tests\ndoping\nelectrical transport tracing\nexact measurement of properties\nfield emission\nholders\ninstruments\nmanipulate with an individual object\nmaterial internal structure\nmechanical and thermal property analysis\nnanomaterials\nnanomaterials manipulation\nnanostructures\nprobing electrical characteristics\nproperties analysis\nscanning electron microscopes\nSEM\nsoldering\nSTM probes\nsustained development of new devices\nTEM\nTEM holders\nTEM techniques\ntransmission electron microscopes\n"}, {"id": "S037026930400930X", "text": "Recent publications\u00a0[31] employ a variety of methods for calculating upper limits and there is no universally accepted procedure\u00a0[27,32,33]. We choose an approach similar to that first advocated by Feldman and Cousins\u00a0[27]. This method has been since extended by Conrad\u00a0et\u00a0al.\u00a0[34] to incorporate uncertainties in detector sensitivity and the background estimate based on an approach described by Cousins and Highland\u00a0[35]. A\u00a0further refinement of the Conrad\u00a0et\u00a0al. method by Hill\u00a0[36] results in more appropriate behavior of the upper limit when the observed number of events is less than the estimated background, as is the case for the present measurement. We have adopted this method but note that Table\u00a02 contains all of the numbers needed to calculate an upper limit using any of the methods in the papers cited above. We assume that the probability density functions of Fsens and background estimates are Gaussian-distributed.\n", "keywords": "background estimate\nbackground estimates\ncalculate an upper limit\ncalculating upper limits\ndetector sensitivity\nfurther refinement of the Conrad\u00a0et\u00a0al. method\nGaussian-distributed\nincorporate uncertainties\nmeasurement\nmore appropriate behavior\nprobability density functions\n"}, {"id": "S0010938X1530161X", "text": "A key part of this problem is that an inspector only has access to data from a small inspected area. In this area, there is only one minimum thickness, which does not provide enough information to build a model of the smallest thicknesses. An inspector can generate a sample of the smallest thickness measurements by partitioning the inspection data into a number of equally sized blocks. In each block the minimum thickness is recorded. This set forms a sample of the smallest thickness measurements. From this sample, one can build a model which takes into account the variations of the smallest thickness measurements. Extreme value analysis (EVA) provides a limiting form for this model. It states that, if the underlying thickness measurements in each block are taken from independent and identical distributions, then the sample of minimum thickness measurements will follow a generalized extreme value distribution (GEVD).\n", "keywords": "block\nbuild a model of the smallest thicknesses\nequally sized blocks\nEVA\nExtreme value analysis\ngeneralized extreme value distribution\nGEVD\ninspection data\nmodel of the smallest thicknesses\nmodel which takes into account the variations of the smallest thickness measurements\npartitioning the inspection data\nsample\n"}, {"id": "S0010938X1530161X", "text": "A key part of this problem is that an inspector only has access to data from a small inspected area. In this area, there is only one minimum thickness, which does not provide enough information to build a model of the smallest thicknesses. An inspector can generate a sample of the smallest thickness measurements by partitioning the inspection data into a number of equally sized blocks. In each block the minimum thickness is recorded. This set forms a sample of the smallest thickness measurements. From this sample, one can build a model which takes into account the variations of the smallest thickness measurements. Extreme value analysis (EVA) provides a limiting form for this model. It states that, if the underlying thickness measurements in each block are taken from independent and identical distributions, then the sample of minimum thickness measurements will follow a generalized extreme value distribution (GEVD).\n", "keywords": "block\nbuild a model of the smallest thicknesses\nequally sized blocks\nEVA\nExtreme value analysis\ngeneralized extreme value distribution\nGEVD\ninspection data\nmodel of the smallest thicknesses\nmodel which takes into account the variations of the smallest thickness measurements\npartitioning the inspection data\nsample\n"}, {"id": "S0254058414000662", "text": "HOMO\u2013LUMO energy band gaps between ylides and their pyrene adducts propose that the 1,3-DC of second pyridinium ylides to ylidepyrene adducts are HOMOylide\u2013LUMOylide\u2013pyrene controlled since the energy band gap is smaller than HOMOylide\u2013pyrene\u2013LUMOylide. Regioselectivity of second cycloaddition was predicted using the atomic orbital coefficients corresponding to HOMOylide\u2013LUMOylide\u2013pyrene. According to Fukui [33], reactions can be favorable in the direction of maximal HOMO\u2013LUMO overlapping of larger coefficients at the reactive sites. The most favorable interactions between corresponding ylides and ylidepyrene adducts to form the most favorable regioisomer conformation are given in Fig.\u00a03. Second ylide addition to ylidepyrene structure is therefore anticipated to proceed via ylideC2/C6\u2013ylidepyrene-C3 and ylideC7\u2013ylidepyrene/C2 interactions to produce the same regioisomer conformations. Considering the theoretical calculations performed for pyrrolidine attached pyrene structure, it is also expected that formation of the same type of regioisomers are favorable for SWNTs after the 1,3-DC of pyridinium ylides, Fig.\u00a03.\n", "keywords": "1,3-DC\n1,3-DC of pyridinium ylides\natomic orbital coefficients\nformation of the same type of regioisomers\nHOMO\u2013LUMO\nHOMOylide\u2013LUMOylide\u2013pyrene\nHOMOylide\u2013pyrene\u2013LUMOylide\nmaximal HOMO\u2013LUMO overlapping\nmost favorable regioisomer conformation\npyrene adducts\npyrrolidine attached pyrene\nregioisomer conformations\nregioisomers\nRegioselectivity of second cycloaddition\nsecond pyridinium ylides\nSWNTs\ntheoretical calculations\nylide\nylideC2/C6\u2013ylidepyrene-C3\nylideC7\u2013ylidepyrene/C2\nylidepyrene\nylidepyrene adducts\nylides\nylides and ylidepyrene adducts\n"}, {"id": "S0301679X14003272", "text": "The lateral force, Q, is measured and recorded throughout the entire test by a piezoelectric load cell which is connected to the quasi-stationary LSMB. The LSMB is mounted on flexures which provide flexibility in the horizontal direction so that the majority of the lateral force is transmitted though the much stiffer load path which contains the load cell as shown in Fig. 2. Both displacement and load sensors have been calibrated (both externally and in-situ) in static conditions. The load and displacement signals are sampled at a rate of two hundred measurements per fretting cycle at all fretting frequencies, with these data being used to generate fretting loops. The loops were used to derive the contact slip amplitude and the energy coefficient of friction in each cycle according to the method suggested by Fouvry et al. [17]. Average values for these were calculated for each test (the average coefficient of friction included values associated with the initial transients in the tests as suggested by Hirsch and Neu [18]).\n", "keywords": "displacement and load sensors\nflexures\nfretting loops.\nlateral force, Q, is measured and recorded\nload cell\nLSMB\npiezoelectric load cell\n"}, {"id": "S0021999115008256", "text": "This section is devoted to the discretization of the advection\u2013diffusion equation and to the analysis of dispersion and diffusion eigencurves for different polynomial orders. The spectral/hp continuous Galerkin method considered closely resembles the formulation presented in [7]. Sec. 2.1 describes in detail the derivation of the semi-discrete advection\u2013diffusion problem as applied to wave-like solutions, from which the relevant eigencurves can be obtained. The inviscid case (linear advection) is then addressed in Sec. 2.2, where the role of primary and secondary eigencurves is discussed from the perspective introduced in [9]. The viscous case is subsequently considered in Sec. 2.3, where eigencurves are shown to feature irregular oscillations for problems strongly dominated by either convection or diffusion.\n", "keywords": "derivation of the semi-discrete advection\u2013diffusion problem\ndiscretization of the advection\u2013diffusion equation\ndispersion and diffusion eigencurves\nGalerkin method\ninviscid case\nlinear advection\nviscous case\nwave-like solutions\n"}, {"id": "S0021999115008256", "text": "This section is devoted to the discretization of the advection\u2013diffusion equation and to the analysis of dispersion and diffusion eigencurves for different polynomial orders. The spectral/hp continuous Galerkin method considered closely resembles the formulation presented in [7]. Sec. 2.1 describes in detail the derivation of the semi-discrete advection\u2013diffusion problem as applied to wave-like solutions, from which the relevant eigencurves can be obtained. The inviscid case (linear advection) is then addressed in Sec. 2.2, where the role of primary and secondary eigencurves is discussed from the perspective introduced in [9]. The viscous case is subsequently considered in Sec. 2.3, where eigencurves are shown to feature irregular oscillations for problems strongly dominated by either convection or diffusion.\n", "keywords": "derivation of the semi-discrete advection\u2013diffusion problem\ndiscretization of the advection\u2013diffusion equation\ndispersion and diffusion eigencurves\nGalerkin method\ninviscid case\nlinear advection\nviscous case\nwave-like solutions\n"}, {"id": "S0997754612001318", "text": "Many applications in fluid mechanics have shown that surface suction can be used as an effective flow-control mechanism. For example, Gregory and Walker\u00a0[1] discuss how the introduction of suction extends the laminar-flow region over a swept wing by reducing the thickness of the boundary layer and the magnitude of crossflow velocity. Conclusions for the swept-wing flow arose from equivalent studies of the von K\u00e1rm\u00e1n (rotating disk) flow (see Gregory and Walker\u00a0[2], Stuart\u00a0[3]) and work has since continued into this and related flows using numerical and asymptotic approaches (see Ockendon\u00a0[4], Dhanak\u00a0[5], Bassom and Seddougui\u00a0[6], Lingwood\u00a0[7], Turkyilmazoglu\u00a0[8], Lingwood and Garrett\u00a0[9], for example). The literature shows that increasing suction has a stabilising effect on the general class of \u201cB\u00f6dewadt, Ekman and von K\u00e1rm\u00e1n\u201d (BEK) flows which results in an increase in critical Reynolds numbers for the onset of convective and absolute instabilities, a narrowing in the range of unstable parameters and a decrease in amplification rates of the unstable convective modes. The convective instability results are interpreted in terms of a delay in the onset of spiral vortices, and the absolute instability results in terms of the onset of laminar-turbulent transition (Lingwood\u00a0[7,10,11]).\n", "keywords": "absolute instability\n\u201cB\u00f6dewadt, Ekman and von K\u00e1rm\u00e1n\u201d (BEK) flows\nconvective instability\ndecrease in amplification rates\nflow-control mechanism\nfluid mechanics\nincrease in critical Reynolds numbers\nincreasing suction\nlaminar-turbulent transition\nmagnitude of crossflow velocity\nnarrowing in the range of unstable parameters\nnumerical and asymptotic approaches\nreducing the thickness of the boundary layer\nreducing the thickness of the boundary layer and the magnitude of crossflow velocity\nrotating disk\nspiral vortices\nsurface suction\nswept wing\nswept-wing\nswept-wing flow\n"}, {"id": "S2212671612001783", "text": "Metal\u2013intermetallic laminated (MIL) composites are fabricated upon reaction sintering of titanium and aluminum foils of various thicknesses. The intermetallic phase of Al3Ti forming during the above processing gives high hardness and stiffness to the composite, while unreacted titanium provides the necessary high strength and ductility. Some results of studies of microstructure and some mechanical properties of layered composites are presented on the example of Ti-Al system. Static and dynamic tests results are discussed for the case when the intermetallic reaction was interrupted in the course of intermetallic sintering and also for the case when it was completed.", "keywords": "Al3Ti\nAl3Ti forming\ncomposite\nfabricated upon reaction sintering\nintermetallic phase\nintermetallic reaction\nintermetallic sintering\ninterrupted\nlayered composites\nMetal\u2013intermetallic laminated\nMIL\npresented on the example of Ti-Al system\nprocessing\nreaction sintering\nStatic and dynamic tests\nStatic and dynamic tests results are discussed\nstudies of microstructure\nTi-Al system\ntitanium and aluminum foils\nunreacted titanium\n"}, {"id": "S0021999115003459", "text": "The boundary element method (BEM) has clear advantages when applied to shape optimisation of high-voltage devices, see [4\u20138] for an introduction to BEM. First of all, BEM relies only on a surface discretisation so that there is no need to maintain an analysis-suitable volume discretisation during the shape optimisation process. Moreover, BEM is ideal for solving problems in unbounded domains that occur in electrostatic field analysis. In gradient-based shape optimisation the shape derivative of the cost functional with respect to geometry perturbations is needed [9\u201311]. To this purpose, we use the adjoint approach and solve the primary and the adjoint boundary value problems with BEM. The associated linear systems of equations are dense and an acceleration technique, such as the fast multipole method [12,13], is necessary for their efficient solution. For some recent applications of fast BEM in shape optimisation and Bernoulli-type free-boundary problems we refer to [14\u201316].\n", "keywords": "adjoint approach\nBEM\nBernoulli-type free-boundary problems\nboundary element method\nelectrostatic field analysis\nfast BEM in shape optimisation\nfast multipole method\ngradient-based shape optimisation\nprimary and the adjoint boundary value problems\nshape derivative\nshape optimisation\nshape optimisation of high-voltage devices\nsolving problems in unbounded domains\nsurface discretisation\n"}, {"id": "S0021999115003459", "text": "The boundary element method (BEM) has clear advantages when applied to shape optimisation of high-voltage devices, see [4\u20138] for an introduction to BEM. First of all, BEM relies only on a surface discretisation so that there is no need to maintain an analysis-suitable volume discretisation during the shape optimisation process. Moreover, BEM is ideal for solving problems in unbounded domains that occur in electrostatic field analysis. In gradient-based shape optimisation the shape derivative of the cost functional with respect to geometry perturbations is needed [9\u201311]. To this purpose, we use the adjoint approach and solve the primary and the adjoint boundary value problems with BEM. The associated linear systems of equations are dense and an acceleration technique, such as the fast multipole method [12,13], is necessary for their efficient solution. For some recent applications of fast BEM in shape optimisation and Bernoulli-type free-boundary problems we refer to [14\u201316].\n", "keywords": "adjoint approach\nBEM\nBernoulli-type free-boundary problems\nboundary element method\nelectrostatic field analysis\nfast BEM in shape optimisation\nfast multipole method\ngradient-based shape optimisation\nprimary and the adjoint boundary value problems\nshape derivative\nshape optimisation\nshape optimisation of high-voltage devices\nsolving problems in unbounded domains\nsurface discretisation\n"}, {"id": "S221266781400121X", "text": "In the paper we present an extended version of the graph-based unsupervised Word Sense Disambiguation algorithm. The algorithm is based on the spreading activation scheme applied to the graphs dynamically built on the basis of the text words and a large wordnet. The algorithm, originally proposed for English and Princeton WordNet, was adapted to Polish and plWordNet. An extension based on the knowledge acquired from the corpus-derived Measure of Semantic Relatedness was proposed. The extended algorithm was evaluated against the manually disambiguated corpus. We observed improvement in the case of the disambiguation performed for shorter text contexts. In addition the algorithm application expressed improvement in document clustering task.", "keywords": "algorithm\nbuilt on the basis of the text words and a large wordne\ncorpus-derived Measure of Semantic Relatedness\ndisambiguation\ndocument clustering task\nEnglish and Princeton WordNet\nextended algorithm\nextended version of the graph-based unsupervised Word Sense Disambiguation algorithm\nextension\nlarge wordnet\nmanually disambiguated corpus\nPolish and plWordNet\nspreading activation scheme\ntext words\nWord Sense Disambiguation algorithm\n"}, {"id": "S0167931713004061", "text": "We have demonstrated a new approach to the manufacture of self-folding hydrogel scaffolds by the use of readily available and fast throughput methods. The process shows effective pattern transfer by first embossing a sacrificial layer and using it as a soluble mould in the fabrication process. The use of a sacrificial layer of PAA imparts environmental sensitivity to the hydrogel film on only one surface. The subsequent swelling of the PAA inter-penetrating network (IPN) in elevated pH causes a swelling differential across the film, causing it to roll to accommodate the difference in surface area between the two surfaces. The surface functionalization and patterning stages are thus combined into one photolithographic operation. The net result is a method of producing environmentally triggered self-folding all hydrogel scaffolds by a, to the authors\u2019 knowledge, novel use of sacrificial layer embossing. The patterned hydrogel films can be triggered consecutively allowing for successive rolling and unrolling depending on the aqueous pH. The choice of PEGDMA hydrogel provides a versatile platform for creating a variety of hydrogel scaffolds, and while being non-fouling and nontoxic it is permeable to proteins. Furthermore PEGDMA can be modified to produce biodegradable and cell adhesive hydrogels for a variety of biomedical applications.\n", "keywords": "accommodate the difference in surface area between the two surfaces\naqueous pH\nbiomedical applications\neffective pattern transfer\nelevated pH\nembossing a sacrificial layer\nfabrication process\nfilm\nhydrogel film\nhydrogels\nhydrogel scaffolds\nIPN\nmanufacture of self-folding hydrogel scaffolds\nnovel use of sacrificial layer embossing\nPAA inter-penetrating network\npatterned hydrogel films\nPEGDMA\nPEGDMA hydrogel\nphotolithographic operation\nproduce biodegradable and cell adhesive hydrogels\nproducing environmentally triggered self-folding all hydrogel scaffolds\nproteins\nreadily available and fast throughput methods\nroll\nrolling\nsacrificial layer of PAA\nself-folding hydrogel scaffolds\nsoluble mould\nsurface\nsurfaces\nswelling differential\nswelling of the PAA inter-penetrating network\ntriggered consecutively\nunrolling\nusing it as a soluble mould in the fabrication process\nversatile platform for creating a variety of hydrogel scaffolds\n"}, {"id": "S0167931713002438", "text": "There have been suggestions that electrons can be trapped in the bulk and at surfaces of silica [15] but new models of electron trapping centres started to appear only recently. It has been suggested by Bersuker et al., who used molecular models, that electrons can be trapped by Si\u2013O bonds in a-SiO2 leading to their weakening and thus facilitating Si\u2013O bond dissociation [16]. Further calculations by Camellone et al. have shown that electrons can spontaneously trap in non-defective continuum random network model of a-SiO2 [17]. Recent calculations have also demonstrated that the two dominant neutral paramagnetic defects at surfaces of a-SiO2, the non-bridging oxygen centre and the silicon dangling bond, are deep electron traps and can form the corresponding negatively charged defects [18]. However, these theoretical predictions have not yet been confirmed experimentally, emphasising the challenges for identifying defect centres.\n", "keywords": "a-SiO2\na-SiO2 leading\nbulk\ndeep electron traps\nelectrons\nelectron trapping centres\nidentifying defect centres\nmolecular models\nnegatively charged defects\nneutral paramagnetic defects\nnon-bridging oxygen centre\nnon-defective continuum random network model\nsilica\nsilicon dangling bond\nSi\u2013O bond\nSi\u2013O bond dissociation\nSi\u2013O bonds\nspontaneously trap\nsurfaces of a-SiO2\ntrapped\ntrapped in the bulk and at surfaces of silica\nweakening\n"}, {"id": "S0167931713002438", "text": "There have been suggestions that electrons can be trapped in the bulk and at surfaces of silica [15] but new models of electron trapping centres started to appear only recently. It has been suggested by Bersuker et al., who used molecular models, that electrons can be trapped by Si\u2013O bonds in a-SiO2 leading to their weakening and thus facilitating Si\u2013O bond dissociation [16]. Further calculations by Camellone et al. have shown that electrons can spontaneously trap in non-defective continuum random network model of a-SiO2 [17]. Recent calculations have also demonstrated that the two dominant neutral paramagnetic defects at surfaces of a-SiO2, the non-bridging oxygen centre and the silicon dangling bond, are deep electron traps and can form the corresponding negatively charged defects [18]. However, these theoretical predictions have not yet been confirmed experimentally, emphasising the challenges for identifying defect centres.\n", "keywords": "a-SiO2\na-SiO2 leading\nbulk\ndeep electron traps\nelectrons\nelectron trapping centres\nidentifying defect centres\nmolecular models\nnegatively charged defects\nneutral paramagnetic defects\nnon-bridging oxygen centre\nnon-defective continuum random network model\nsilica\nsilicon dangling bond\nSi\u2013O bond\nSi\u2013O bond dissociation\nSi\u2013O bonds\nspontaneously trap\nsurfaces of a-SiO2\ntrapped\ntrapped in the bulk and at surfaces of silica\nweakening\n"}, {"id": "S0167931713002438", "text": "There have been suggestions that electrons can be trapped in the bulk and at surfaces of silica [15] but new models of electron trapping centres started to appear only recently. It has been suggested by Bersuker et al., who used molecular models, that electrons can be trapped by Si\u2013O bonds in a-SiO2 leading to their weakening and thus facilitating Si\u2013O bond dissociation [16]. Further calculations by Camellone et al. have shown that electrons can spontaneously trap in non-defective continuum random network model of a-SiO2 [17]. Recent calculations have also demonstrated that the two dominant neutral paramagnetic defects at surfaces of a-SiO2, the non-bridging oxygen centre and the silicon dangling bond, are deep electron traps and can form the corresponding negatively charged defects [18]. However, these theoretical predictions have not yet been confirmed experimentally, emphasising the challenges for identifying defect centres.\n", "keywords": "a-SiO2\na-SiO2 leading\nbulk\ndeep electron traps\nelectrons\nelectron trapping centres\nidentifying defect centres\nmolecular models\nnegatively charged defects\nneutral paramagnetic defects\nnon-bridging oxygen centre\nnon-defective continuum random network model\nsilica\nsilicon dangling bond\nSi\u2013O bond\nSi\u2013O bond dissociation\nSi\u2013O bonds\nspontaneously trap\nsurfaces of a-SiO2\ntrapped\ntrapped in the bulk and at surfaces of silica\nweakening\n"}, {"id": "S0167931713002438", "text": "There have been suggestions that electrons can be trapped in the bulk and at surfaces of silica [15] but new models of electron trapping centres started to appear only recently. It has been suggested by Bersuker et al., who used molecular models, that electrons can be trapped by Si\u2013O bonds in a-SiO2 leading to their weakening and thus facilitating Si\u2013O bond dissociation [16]. Further calculations by Camellone et al. have shown that electrons can spontaneously trap in non-defective continuum random network model of a-SiO2 [17]. Recent calculations have also demonstrated that the two dominant neutral paramagnetic defects at surfaces of a-SiO2, the non-bridging oxygen centre and the silicon dangling bond, are deep electron traps and can form the corresponding negatively charged defects [18]. However, these theoretical predictions have not yet been confirmed experimentally, emphasising the challenges for identifying defect centres.\n", "keywords": "a-SiO2\na-SiO2 leading\nbulk\ndeep electron traps\nelectrons\nelectron trapping centres\nidentifying defect centres\nmolecular models\nnegatively charged defects\nneutral paramagnetic defects\nnon-bridging oxygen centre\nnon-defective continuum random network model\nsilica\nsilicon dangling bond\nSi\u2013O bond\nSi\u2013O bond dissociation\nSi\u2013O bonds\nspontaneously trap\nsurfaces of a-SiO2\ntrapped\ntrapped in the bulk and at surfaces of silica\nweakening\n"}, {"id": "S0167931712002936", "text": "A 3D finite element based (FEM) COMSOL capacitance analysis is combined with Monte Carlo single-electron circuit simulations to model device operations during single electron detection. The 3D structural data (Fig. 1b) of the nanoscale DQD pair and multiple gate electrodes are precisely input into COMSOL\u2019s FEM-based electrostatics simulator. Capacitances between different device components are then extracted and fed into the well-tested single electron circuit simulator SETSPICE [11], based on the orthodox theory of single electron tunnelling [12]. For our target d1 of 60nm, simulation results (Fig. 1c) showed that as we sweep the voltage applied on gate G1, VG1, single electron tunnelling into the turnstile\u2019s two QDs should generate shifts in the electrometer current, IDS, of tens of pA. This is well within the charge sensitivity of DQD electrometer [6] and consistent to the same order of magnitude with previous work in single electron detection [13]. In addition, the gate to QD capacitive coupling appear to be sufficient for the control of QD occupations down to the single electron limit, allowing for future manipulation of single electron spins in qubit research.\n", "keywords": "3D finite element based (FEM) COMSOL capacitance analysis\n3D structural data\nCOMSOL\u2019s FEM-based electrostatics simulator\ncontrol of QD occupations\ndevice components\nDQD electrometer\nelectrometer current\ngate\ngate G1\nIDS\nmanipulation of single electron spins\nmodel device operations during single electron detection\nMonte Carlo single-electron circuit simulations\nmultiple gate electrodes\nnanoscale DQD pair\northodox theory\nQD capacitive coupling\nqubit research\nSETSPICE\nshifts in the electrometer current\nsimulation\nsingle electron detection\nsingle electron spins\nsingle electron tunnelling\ntarget d1\nturnstile\u2019s two QDs\nVG1\nwell-tested single electron circuit simulator\n"}, {"id": "S0167931712002936", "text": "A 3D finite element based (FEM) COMSOL capacitance analysis is combined with Monte Carlo single-electron circuit simulations to model device operations during single electron detection. The 3D structural data (Fig. 1b) of the nanoscale DQD pair and multiple gate electrodes are precisely input into COMSOL\u2019s FEM-based electrostatics simulator. Capacitances between different device components are then extracted and fed into the well-tested single electron circuit simulator SETSPICE [11], based on the orthodox theory of single electron tunnelling [12]. For our target d1 of 60nm, simulation results (Fig. 1c) showed that as we sweep the voltage applied on gate G1, VG1, single electron tunnelling into the turnstile\u2019s two QDs should generate shifts in the electrometer current, IDS, of tens of pA. This is well within the charge sensitivity of DQD electrometer [6] and consistent to the same order of magnitude with previous work in single electron detection [13]. In addition, the gate to QD capacitive coupling appear to be sufficient for the control of QD occupations down to the single electron limit, allowing for future manipulation of single electron spins in qubit research.\n", "keywords": "3D finite element based (FEM) COMSOL capacitance analysis\n3D structural data\nCOMSOL\u2019s FEM-based electrostatics simulator\ncontrol of QD occupations\ndevice components\nDQD electrometer\nelectrometer current\ngate\ngate G1\nIDS\nmanipulation of single electron spins\nmodel device operations during single electron detection\nMonte Carlo single-electron circuit simulations\nmultiple gate electrodes\nnanoscale DQD pair\northodox theory\nQD capacitive coupling\nqubit research\nSETSPICE\nshifts in the electrometer current\nsimulation\nsingle electron detection\nsingle electron spins\nsingle electron tunnelling\ntarget d1\nturnstile\u2019s two QDs\nVG1\nwell-tested single electron circuit simulator\n"}, {"id": "S0167931712002936", "text": "A 3D finite element based (FEM) COMSOL capacitance analysis is combined with Monte Carlo single-electron circuit simulations to model device operations during single electron detection. The 3D structural data (Fig. 1b) of the nanoscale DQD pair and multiple gate electrodes are precisely input into COMSOL\u2019s FEM-based electrostatics simulator. Capacitances between different device components are then extracted and fed into the well-tested single electron circuit simulator SETSPICE [11], based on the orthodox theory of single electron tunnelling [12]. For our target d1 of 60nm, simulation results (Fig. 1c) showed that as we sweep the voltage applied on gate G1, VG1, single electron tunnelling into the turnstile\u2019s two QDs should generate shifts in the electrometer current, IDS, of tens of pA. This is well within the charge sensitivity of DQD electrometer [6] and consistent to the same order of magnitude with previous work in single electron detection [13]. In addition, the gate to QD capacitive coupling appear to be sufficient for the control of QD occupations down to the single electron limit, allowing for future manipulation of single electron spins in qubit research.\n", "keywords": "3D finite element based (FEM) COMSOL capacitance analysis\n3D structural data\nCOMSOL\u2019s FEM-based electrostatics simulator\ncontrol of QD occupations\ndevice components\nDQD electrometer\nelectrometer current\ngate\ngate G1\nIDS\nmanipulation of single electron spins\nmodel device operations during single electron detection\nMonte Carlo single-electron circuit simulations\nmultiple gate electrodes\nnanoscale DQD pair\northodox theory\nQD capacitive coupling\nqubit research\nSETSPICE\nshifts in the electrometer current\nsimulation\nsingle electron detection\nsingle electron spins\nsingle electron tunnelling\ntarget d1\nturnstile\u2019s two QDs\nVG1\nwell-tested single electron circuit simulator\n"}, {"id": "S2212671612000686", "text": "This paper make the explained variables our financial stress index consist of the synchronous variables financial systemic risk, and make the explanatory variables the macroeconomic variable, currency credit variable, asset price variable and the macroeconomic variable of correlative economic powers, then use stepwise regression method to establish the financial systemic risk best predict equation, thus set up the reasonable and practical financial systemic risk early-warning index system; besides, use the best prediction equations predicts the financial systemic risk status in 2011. The predicted results show that Chinese financial systemic risk is on the rise in the first three quarters and higher than the peak of 2008; financial systemic risk start to decline since the fourth quarter.", "keywords": "asset price variable\nbest prediction equations\ncurrency credit variable\nestablish the financial systemic risk best predict equation\nfinancial stress index\nfinancial systemic risk best predict equation\nfinancial systemic risk early-warning index system\nmacroeconomic variable\nmacroeconomic variable of correlative economic powers\nset up the reasonable and practical financial systemic risk early-warning index system\nstepwise regression method\nsynchronous variables financial systemic risk\n"}, {"id": "S0306437913000768", "text": "Modeling collaboration processes is a challenging task. Existing modeling approaches are not capable of expressing the unpredictable, non-routine nature of human collaboration, which is influenced by the social context of involved collaborators. We propose a modeling approach which considers collaboration processes as the evolution of a network of collaborative documents along with a social network of collaborators. Our modeling approach, accompanied by a graphical notation and formalization, allows to capture the influence of complex social structures formed by collaborators, and therefore facilitates such activities as the discovery of socially coherent teams, social hubs, or unbiased experts. We demonstrate the applicability and expressiveness of our approach and notation, and discuss their strengths and weaknesses.\n", "keywords": "collaboration processes\ndiscovery of socially coherent teams\nevolution of a network of collaborative documents along with a social network of collaborators\ngraphical notation and formalization\nhuman collaboration\ninfluence of complex social structures\nmodeling approach\nmodeling approaches\nModeling collaboration processes\n"}, {"id": "S0022311513011422", "text": "There is still some debate about the crystal structure and composition of the fine oxides found in ODS steels and a number of different phases have been both proposed and identified. A complete characterisation of the oxide particles, including crystal structure and composition, is needed as different phases and chemical variants of a single structure have been shown to respond differently to high temperatures and irradiation. Ribis and de Carlan [6] have studied the coarsening characteristics of Y2O3 and Y2Ti2O7 oxides at high temperatures. They show that the increase in particle size is greater for the non-Ti containing phase. Similarly, Ratti et al. [9], although they do not allude to specific oxide phases, have shown that small Ti additions to an 18%Cr ODS alloy dramatically reduces the coarsening rates of dispersoids when compared to an equivalent alloy without titanium. For example, Ribis indicates that coarsening rates may be controlled by interfacial energy between the secondary phase particles and the matrix; he points out that the resistance to coarsening observed in the Y, Ti, O system is probably the result of a very low interface energy and this would differ from one phase to another. Whittle et al. [10] have shown that pyrochlore and structures closely related to the pyrochlore structure respond in different ways to irradiation. They revealed that oxide structure and variations in composition can affect their ability to withstand and recover from irradiation induced damage.\n", "keywords": "18%Cr ODS alloy\nalloy\ncoarsening rates\ncomplete characterisation of the oxide particles\ncrystal\ndispersoids\nfine oxides\nirradiation\nmatrix\nO\nODS steels\noxide\noxide particles\nparticle\nparticles\npyrochlore\nTi\ntitanium\nY\nY2O3\nY2Ti2O7 oxides\n"}, {"id": "S0022311513011422", "text": "There is still some debate about the crystal structure and composition of the fine oxides found in ODS steels and a number of different phases have been both proposed and identified. A complete characterisation of the oxide particles, including crystal structure and composition, is needed as different phases and chemical variants of a single structure have been shown to respond differently to high temperatures and irradiation. Ribis and de Carlan [6] have studied the coarsening characteristics of Y2O3 and Y2Ti2O7 oxides at high temperatures. They show that the increase in particle size is greater for the non-Ti containing phase. Similarly, Ratti et al. [9], although they do not allude to specific oxide phases, have shown that small Ti additions to an 18%Cr ODS alloy dramatically reduces the coarsening rates of dispersoids when compared to an equivalent alloy without titanium. For example, Ribis indicates that coarsening rates may be controlled by interfacial energy between the secondary phase particles and the matrix; he points out that the resistance to coarsening observed in the Y, Ti, O system is probably the result of a very low interface energy and this would differ from one phase to another. Whittle et al. [10] have shown that pyrochlore and structures closely related to the pyrochlore structure respond in different ways to irradiation. They revealed that oxide structure and variations in composition can affect their ability to withstand and recover from irradiation induced damage.\n", "keywords": "18%Cr ODS alloy\nalloy\ncoarsening rates\ncomplete characterisation of the oxide particles\ncrystal\ndispersoids\nfine oxides\nirradiation\nmatrix\nO\nODS steels\noxide\noxide particles\nparticle\nparticles\npyrochlore\nTi\ntitanium\nY\nY2O3\nY2Ti2O7 oxides\n"}, {"id": "S0022311513011422", "text": "There is still some debate about the crystal structure and composition of the fine oxides found in ODS steels and a number of different phases have been both proposed and identified. A complete characterisation of the oxide particles, including crystal structure and composition, is needed as different phases and chemical variants of a single structure have been shown to respond differently to high temperatures and irradiation. Ribis and de Carlan [6] have studied the coarsening characteristics of Y2O3 and Y2Ti2O7 oxides at high temperatures. They show that the increase in particle size is greater for the non-Ti containing phase. Similarly, Ratti et al. [9], although they do not allude to specific oxide phases, have shown that small Ti additions to an 18%Cr ODS alloy dramatically reduces the coarsening rates of dispersoids when compared to an equivalent alloy without titanium. For example, Ribis indicates that coarsening rates may be controlled by interfacial energy between the secondary phase particles and the matrix; he points out that the resistance to coarsening observed in the Y, Ti, O system is probably the result of a very low interface energy and this would differ from one phase to another. Whittle et al. [10] have shown that pyrochlore and structures closely related to the pyrochlore structure respond in different ways to irradiation. They revealed that oxide structure and variations in composition can affect their ability to withstand and recover from irradiation induced damage.\n", "keywords": "18%Cr ODS alloy\nalloy\ncoarsening rates\ncomplete characterisation of the oxide particles\ncrystal\ndispersoids\nfine oxides\nirradiation\nmatrix\nO\nODS steels\noxide\noxide particles\nparticle\nparticles\npyrochlore\nTi\ntitanium\nY\nY2O3\nY2Ti2O7 oxides\n"}, {"id": "S0022311513011422", "text": "There is still some debate about the crystal structure and composition of the fine oxides found in ODS steels and a number of different phases have been both proposed and identified. A complete characterisation of the oxide particles, including crystal structure and composition, is needed as different phases and chemical variants of a single structure have been shown to respond differently to high temperatures and irradiation. Ribis and de Carlan [6] have studied the coarsening characteristics of Y2O3 and Y2Ti2O7 oxides at high temperatures. They show that the increase in particle size is greater for the non-Ti containing phase. Similarly, Ratti et al. [9], although they do not allude to specific oxide phases, have shown that small Ti additions to an 18%Cr ODS alloy dramatically reduces the coarsening rates of dispersoids when compared to an equivalent alloy without titanium. For example, Ribis indicates that coarsening rates may be controlled by interfacial energy between the secondary phase particles and the matrix; he points out that the resistance to coarsening observed in the Y, Ti, O system is probably the result of a very low interface energy and this would differ from one phase to another. Whittle et al. [10] have shown that pyrochlore and structures closely related to the pyrochlore structure respond in different ways to irradiation. They revealed that oxide structure and variations in composition can affect their ability to withstand and recover from irradiation induced damage.\n", "keywords": "18%Cr ODS alloy\nalloy\ncoarsening rates\ncomplete characterisation of the oxide particles\ncrystal\ndispersoids\nfine oxides\nirradiation\nmatrix\nO\nODS steels\noxide\noxide particles\nparticle\nparticles\npyrochlore\nTi\ntitanium\nY\nY2O3\nY2Ti2O7 oxides\n"}, {"id": "S0168583X14003929", "text": "The most widely used ion source in FIB instruments is a gallium (Ga) liquid metal ion source (LMIS) [1]. Gallium is attractive as an ion source because of its low melting temperature (29.8\u00b0C at standard atmospheric pressure [4]) and its low volatility [1]. However, some materials show sensitivity to the Ga ion beam. This sensitivity is manifested as changes in the structure and chemical composition of the starting material upon exposure to the Ga ion beam [5]. Group III\u2013V compound semiconductors are one class of materials that show such sensitivity. Cryo-FIB milling has recently been reported to suppress the reactions between the Ga ion beam and III\u2013V materials [6]. The suggested advantage of cryo-FIB milling over room temperature milling of Group III\u2013V materials is appealing, given the variety of present and potential future applications for these materials (e.g., as electronic or photonic devices given the favorable electron transport and direct band gap properties associated with several III\u2013V semiconductor systems).\n", "keywords": "cryo-FIB milling\nCryo-FIB milling\nelectronic or photonic device\nFIB instruments\nGa\nGa ion beam\ngallium\nGallium\nGroup III\u2013V materials\nIII\u2013V materials\nsemiconductor\nsemiconductors\n"}, {"id": "S1566253516300069", "text": "The above discussion also lays bare the difference of perspectives between the fusion of hard constraints and knowledge-base merging: the idea of Konieczny and Pino-Perez is to explain the fusion of plain epistemic states, understood as a set of plausible worlds, by the existence of underlying partial orderings or numerical plausibility degrees (obtained by distances), based on axioms that only use plausible sets attached to these orderings. In [67] the same authors use both hard (integrity) constraints and belief sets referring to plausible worlds, and try to extend both the AGM revision and knowledge-based merging. However, they do not envisage the merging of integrity constraints discussed in the previous section. The belief revision and merging literature takes an external point of view on cognitive processes under study. The underlying ordered structures are here a consequence of the merging postulates, but they do not appear explicitly in the axioms and they are not observable from the outside. On the contrary, our approach is to construct fusion rules that only rely on what is explicitly supplied by sources. In the sequel we consider the counterpart of our fusion postulates for ranked models, that can be expressed by means of total orders of possible worlds or by their encodings on a plausibility scale.\n", "keywords": "AGM revision\nbelief revision and merging\nbelief sets\nconstruct fusion rules\nfusion of plain epistemic states\nhard constraints\nhard (integrity) constraints\nknowledge-based merging\nknowledge-base merging\nmerging of integrity constraints\nnumerical plausibility degrees\nordered structures\npartial orderings\nranked models\n"}, {"id": "S0266352X16301550", "text": "To address the vertical displacement estimation of conventional pile groups subjected to mechanical loads, various numerical and analytical methods have been proposed. These methods include the finite element method [e.g., 2,3], the boundary element method [e.g., 4,5], the finite difference method [e.g., 6], the interaction factor method [e.g., 7,8\u201311], the equivalent pier and raft methods [e.g., 12\u201314], and the settlement ratio method [e.g., 15]. The finite element method, while providing the most rigorous and exhaustive representation of the pile group-related problem, is generally computationally expensive and considered mainly a research tool rather than a design tool. Conversely, the versatility of simplified (approximate) methods, such as the interaction factor approach that allows capturing the (e.g., vertical) displacements of any general pile group by the analysis of the displacement interaction between two identical piles and by the use of the elastic principle of superposition of effects, makes them attractive as design tools because they allow for the use of expedient parametric studies under various design conditions.\n", "keywords": "analysis of the displacement interaction between two identical piles\nboundary element method\ncapturing the (e.g., vertical) displacements of any general pile group\ndesign tool\ndesign tools\nelastic principle of superposition of effects\nequivalent pier and raft methods\nexpedient parametric studies\nfinite difference method\nfinite element method\ninteraction factor approach\ninteraction factor method\nnumerical and analytical methods\npile group-related problem\nresearch tool\nsettlement ratio method\nversatility of simplified (approximate) methods\nvertical displacement estimation of conventional pile groups subjected to mechanical loads\n"}, {"id": "S0266352X16301550", "text": "To address the vertical displacement estimation of conventional pile groups subjected to mechanical loads, various numerical and analytical methods have been proposed. These methods include the finite element method [e.g., 2,3], the boundary element method [e.g., 4,5], the finite difference method [e.g., 6], the interaction factor method [e.g., 7,8\u201311], the equivalent pier and raft methods [e.g., 12\u201314], and the settlement ratio method [e.g., 15]. The finite element method, while providing the most rigorous and exhaustive representation of the pile group-related problem, is generally computationally expensive and considered mainly a research tool rather than a design tool. Conversely, the versatility of simplified (approximate) methods, such as the interaction factor approach that allows capturing the (e.g., vertical) displacements of any general pile group by the analysis of the displacement interaction between two identical piles and by the use of the elastic principle of superposition of effects, makes them attractive as design tools because they allow for the use of expedient parametric studies under various design conditions.\n", "keywords": "analysis of the displacement interaction between two identical piles\nboundary element method\ncapturing the (e.g., vertical) displacements of any general pile group\ndesign tool\ndesign tools\nelastic principle of superposition of effects\nequivalent pier and raft methods\nexpedient parametric studies\nfinite difference method\nfinite element method\ninteraction factor approach\ninteraction factor method\nnumerical and analytical methods\npile group-related problem\nresearch tool\nsettlement ratio method\nversatility of simplified (approximate) methods\nvertical displacement estimation of conventional pile groups subjected to mechanical loads\n"}, {"id": "S221266781200007X", "text": "In this paper, the design of a varnish plant at Crocodile Matchet Limited, Tema, Ghana was considered and modification made to eliminate blooming and rusting of its product at the final processing plant when there is high moisture content in the atmosphere. The proposed design included pipelines or ductsand hot air receiving chambers for the Varnish Plant.Heat from the exhaust gas which would have otherwise, gone wasted, was utilised by redesigning the varnish plant and this yielded 6.74kW of heat energy which was transferred into the air chambers to aid the drying ofmatchets at the hardening plant. Consequently, the absorption of the moisture on the steel and the dryness of the product were improved. Further studies were done to ensure constant supply of hot air into the air chambers.", "keywords": "absorption of the moisture\nair chambers\nblooming\nconstant supply of hot air into the air chambers\ndesign of a varnish plant\ndrying ofmatchets\nductsand hot air receiving chambers\nexhaust gas\nFurther studies\nhardening plant\nheat energy\nmodification made to eliminate blooming and rusting\npipelines\nprocessing plant\nredesigning the varnish plant\nrusting\nsteel\ntransferred into the air chambers\nvarnish plant\nVarnish Plant\n"}, {"id": "S221266781200007X", "text": "In this paper, the design of a varnish plant at Crocodile Matchet Limited, Tema, Ghana was considered and modification made to eliminate blooming and rusting of its product at the final processing plant when there is high moisture content in the atmosphere. The proposed design included pipelines or ductsand hot air receiving chambers for the Varnish Plant.Heat from the exhaust gas which would have otherwise, gone wasted, was utilised by redesigning the varnish plant and this yielded 6.74kW of heat energy which was transferred into the air chambers to aid the drying ofmatchets at the hardening plant. Consequently, the absorption of the moisture on the steel and the dryness of the product were improved. Further studies were done to ensure constant supply of hot air into the air chambers.", "keywords": "absorption of the moisture\nair chambers\nblooming\nconstant supply of hot air into the air chambers\ndesign of a varnish plant\ndrying ofmatchets\nductsand hot air receiving chambers\nexhaust gas\nFurther studies\nhardening plant\nheat energy\nmodification made to eliminate blooming and rusting\npipelines\nprocessing plant\nredesigning the varnish plant\nrusting\nsteel\ntransferred into the air chambers\nvarnish plant\nVarnish Plant\n"}, {"id": "S221266781200007X", "text": "In this paper, the design of a varnish plant at Crocodile Matchet Limited, Tema, Ghana was considered and modification made to eliminate blooming and rusting of its product at the final processing plant when there is high moisture content in the atmosphere. The proposed design included pipelines or ductsand hot air receiving chambers for the Varnish Plant.Heat from the exhaust gas which would have otherwise, gone wasted, was utilised by redesigning the varnish plant and this yielded 6.74kW of heat energy which was transferred into the air chambers to aid the drying ofmatchets at the hardening plant. Consequently, the absorption of the moisture on the steel and the dryness of the product were improved. Further studies were done to ensure constant supply of hot air into the air chambers.", "keywords": "absorption of the moisture\nair chambers\nblooming\nconstant supply of hot air into the air chambers\ndesign of a varnish plant\ndrying ofmatchets\nductsand hot air receiving chambers\nexhaust gas\nFurther studies\nhardening plant\nheat energy\nmodification made to eliminate blooming and rusting\npipelines\nprocessing plant\nredesigning the varnish plant\nrusting\nsteel\ntransferred into the air chambers\nvarnish plant\nVarnish Plant\n"}, {"id": "S221266781200007X", "text": "In this paper, the design of a varnish plant at Crocodile Matchet Limited, Tema, Ghana was considered and modification made to eliminate blooming and rusting of its product at the final processing plant when there is high moisture content in the atmosphere. The proposed design included pipelines or ductsand hot air receiving chambers for the Varnish Plant.Heat from the exhaust gas which would have otherwise, gone wasted, was utilised by redesigning the varnish plant and this yielded 6.74kW of heat energy which was transferred into the air chambers to aid the drying ofmatchets at the hardening plant. Consequently, the absorption of the moisture on the steel and the dryness of the product were improved. Further studies were done to ensure constant supply of hot air into the air chambers.", "keywords": "absorption of the moisture\nair chambers\nblooming\nconstant supply of hot air into the air chambers\ndesign of a varnish plant\ndrying ofmatchets\nductsand hot air receiving chambers\nexhaust gas\nFurther studies\nhardening plant\nheat energy\nmodification made to eliminate blooming and rusting\npipelines\nprocessing plant\nredesigning the varnish plant\nrusting\nsteel\ntransferred into the air chambers\nvarnish plant\nVarnish Plant\n"}, {"id": "S221266781200007X", "text": "In this paper, the design of a varnish plant at Crocodile Matchet Limited, Tema, Ghana was considered and modification made to eliminate blooming and rusting of its product at the final processing plant when there is high moisture content in the atmosphere. The proposed design included pipelines or ductsand hot air receiving chambers for the Varnish Plant.Heat from the exhaust gas which would have otherwise, gone wasted, was utilised by redesigning the varnish plant and this yielded 6.74kW of heat energy which was transferred into the air chambers to aid the drying ofmatchets at the hardening plant. Consequently, the absorption of the moisture on the steel and the dryness of the product were improved. Further studies were done to ensure constant supply of hot air into the air chambers.", "keywords": "absorption of the moisture\nair chambers\nblooming\nconstant supply of hot air into the air chambers\ndesign of a varnish plant\ndrying ofmatchets\nductsand hot air receiving chambers\nexhaust gas\nFurther studies\nhardening plant\nheat energy\nmodification made to eliminate blooming and rusting\npipelines\nprocessing plant\nredesigning the varnish plant\nrusting\nsteel\ntransferred into the air chambers\nvarnish plant\nVarnish Plant\n"}, {"id": "S221266781200007X", "text": "In this paper, the design of a varnish plant at Crocodile Matchet Limited, Tema, Ghana was considered and modification made to eliminate blooming and rusting of its product at the final processing plant when there is high moisture content in the atmosphere. The proposed design included pipelines or ductsand hot air receiving chambers for the Varnish Plant.Heat from the exhaust gas which would have otherwise, gone wasted, was utilised by redesigning the varnish plant and this yielded 6.74kW of heat energy which was transferred into the air chambers to aid the drying ofmatchets at the hardening plant. Consequently, the absorption of the moisture on the steel and the dryness of the product were improved. Further studies were done to ensure constant supply of hot air into the air chambers.", "keywords": "absorption of the moisture\nair chambers\nblooming\nconstant supply of hot air into the air chambers\ndesign of a varnish plant\ndrying ofmatchets\nductsand hot air receiving chambers\nexhaust gas\nFurther studies\nhardening plant\nheat energy\nmodification made to eliminate blooming and rusting\npipelines\nprocessing plant\nredesigning the varnish plant\nrusting\nsteel\ntransferred into the air chambers\nvarnish plant\nVarnish Plant\n"}, {"id": "S0370269304008998", "text": "Including the\u00a0O(\u03b1s) corrections, all the operators listed in\u00a0(9) and\u00a0(10) have to be included. A\u00a0convenient framework to carry out these calculations is the QCD factorization framework\u00a0[14] which allows to express the hadronic matrix elements in the schematic form: (11)\u3008V\u03b3|Oi|B\u3009=FB\u2192VTiI+\u222bdk+2\u03c0\u222b01du\u03c6B,+(k+)TiII(k+,u)\u03c6V\u22a5(u), where FB\u2192V are the transition form factors defined through the matrix elements of the operator\u00a0O7, \u03c6B,+(k+) is the leading-twist B-meson wave-function with k+ being a light-cone component of the spectator quark momentum, \u03c6\u22a5V(u) is the leading-twist light-cone distribution amplitude (LCDA) of the transversely-polarized vector meson\u00a0V, and\u00a0u is the fractional momentum of the vector meson carried by one of the two partons. The quantities\u00a0TiI and\u00a0TiII are the hard-perturbative kernels calculated to order\u00a0\u03b1s, with the latter containing the so-called hard-spectator contributions. The factorization formula\u00a0(11) holds in the heavy quark limit, i.e., to order\u00a0\u039bQCD/MB. This factorization framework has been used to calculate the branching fractions and related quantities for the decays B\u2192K\u2217\u03b3\u00a0[15\u201317] and B\u2192\u03c1\u03b3\u00a0[15,17]. The isospin violation in the B\u2192K\u2217\u03b3 decays in this framework have also been studied\u00a0[18]. (For applications to B\u2192K\u2217\u03b3\u2217, see Refs.\u00a0[16,19,20].) Very recently, the hard-spectator contribution arising from the chromomagnetic operator\u00a0O8 have also been calculated in next-to-next-to-leading order (NNLO) in \u03b1s showing that the spectator interactions factorize in the heavy quark limit\u00a0[21]. However, the numerical effect of the resummed NNLO contributions is marginal and we shall not include this in our update.\n", "keywords": "express the hadronic matrix elements in the schematic form:\nFB\u2192V\nfractional momentum of the vector meson carried by one of the two partons\nhard-spectator contribution arising from the chromomagnetic operator\u00a0O8\nheavy quark\nisospin violation in the B\u2192K\u2217\u03b3 decays\n+(k+)\nLCDA\nleading-twist B-meson wave-function with k+ being a light-cone component of the spectator quark momentum\nleading-twist light-cone distribution amplitude\nnext-to-next-to-leading orde\nNNLO\noperators listed in\u00a0(9) and\u00a0(10) have to be included\nQCD factorization framework\nschematic form\nspectator interactions\nThe factorization formula\nThis factorization framework\ntransition form factors defined through the matrix elements of the operator\u00a0O7, \u03c6B\nu\n\u3008V\u03b3|Oi|B\u3009=FB\u2192VTiI+\u222bdk+2\u03c0\u222b01du\u03c6B,+(k+)TiII(k+,u)\u03c6V\u22a5(u)\n\u03c6\u22a5V(u\n"}, {"id": "S0167273814004548", "text": "Two different micro-contact set-ups were used in the experiments. The asymmetrically heated measurement set-up (Fig.\u00a02a) allows to change the contacted electrode within seconds and thereby to gain statistical information over a large number of different microelectrodes on one and the same sample in a relatively short time. It also enables monitoring of optical changes during the measurement in real time. However, the asymmetrical heating from the bottom side and local cooling (e.g. by convection, radiation, and the contacting tip acting as a heat sink) is known to cause temperature gradients within the sample [11]. Such temperature gradients are responsible for thermo-voltages, which can lead to measurement artifacts in electrochemical experiments [24]. Moreover, in this set-up temperature cycles can hardly be performed on single microelectrodes but require subsequent contacting and de-contacting of different microelectrodes.\n", "keywords": "asymmetrical heating\nasymmetrically heated measurement set-up\nchange the contacted electrode\ncontacting and de-contacting of different microelectrodes\ncontacting tip\nconvection\nelectrochemical experiments\nelectrode\ngain statistical information\nheat sink\nlocal cooling\nmicro-contact set-ups\nmicroelectrodes\nmonitoring of optical changes\nradiation\ntemperature cycles\ntemperature gradients\n"}, {"id": "S0167931712003012", "text": "We evaluated three spin-on carbon hardmasks from Irresistible Materials [12]. The spin-on carbon compositions were dissolved in a suitable solvent such as chloroform or anisole with a concentration in the range 5\u201350g/l. In this report, film thickness measurements were made for IM-HM11-01 and IM-HM11-02 films, whilst IM-HM11-03 was used for etching; further investigations to compare the performance of the different compositions across tasks are underway. Films of the SoC were prepared by spin coating on hydrogen-terminated silicon substrates with a speed varying between 800 and 2000 RPM for 60s. After spin coating the film was baked for 2min at temperatures of up to 330\u00b0C. In order to enable further processing, the SoC should be rendered insoluble in typical solvents for resist and spin-on-hardmask to enable further processing. The elution behavior of films of IM-HM11-01 and IM-HM11-02 for thicknesses between 30 and 325nm was tested as a function of the baking temperature. Fig. 1 shows the normalized film thickness of two formulations of the SoC (IM-HM11-01 and IM-HM11-02), before and after dipping in monochlorobenzene (MCB):IPA 1:1 solution. Prior to baking the thickness of IM-HM11-01 was \u223c320nm, and the thickness of IM-HM11-02 was \u223c250nm. For temperatures above 190\u00b0C the IM-HM11-02 film was rendered insoluble, whilst a temperature of 260\u00b0C was required to achieve the same for IM-HM11-01. Film thickness did not affect the elution results.\n", "keywords": "aked for 2min at temperatures of up to 330\u00b0C\nanisole\nbaking\nbefore and after dipping\nchloroform\ncompare the performance of the different compositions across tasks\nelution behavior\netching\nevaluated three spin-on carbon hardmasks from Irresistible Materials\nfilm\nFilm\nFilms of the SoC\nfilm thickness measurements\nhydrogen-terminated silicon substrates\nIM-HM11-01\nIM-HM11-01 and IM-HM11-02\nIM-HM11-02\nIM-HM11-02 film\nIM-HM11-02 films\nIM-HM11-03\nMCB\nmonochlorobenzene\nSoC\nSoC (\nsolvent\nspin coating\nspin-on carbon compositions\nspin-on-hardmask\nthree spin-on carbon hardmasks\n"}, {"id": "S0167931712003012", "text": "We evaluated three spin-on carbon hardmasks from Irresistible Materials [12]. The spin-on carbon compositions were dissolved in a suitable solvent such as chloroform or anisole with a concentration in the range 5\u201350g/l. In this report, film thickness measurements were made for IM-HM11-01 and IM-HM11-02 films, whilst IM-HM11-03 was used for etching; further investigations to compare the performance of the different compositions across tasks are underway. Films of the SoC were prepared by spin coating on hydrogen-terminated silicon substrates with a speed varying between 800 and 2000 RPM for 60s. After spin coating the film was baked for 2min at temperatures of up to 330\u00b0C. In order to enable further processing, the SoC should be rendered insoluble in typical solvents for resist and spin-on-hardmask to enable further processing. The elution behavior of films of IM-HM11-01 and IM-HM11-02 for thicknesses between 30 and 325nm was tested as a function of the baking temperature. Fig. 1 shows the normalized film thickness of two formulations of the SoC (IM-HM11-01 and IM-HM11-02), before and after dipping in monochlorobenzene (MCB):IPA 1:1 solution. Prior to baking the thickness of IM-HM11-01 was \u223c320nm, and the thickness of IM-HM11-02 was \u223c250nm. For temperatures above 190\u00b0C the IM-HM11-02 film was rendered insoluble, whilst a temperature of 260\u00b0C was required to achieve the same for IM-HM11-01. Film thickness did not affect the elution results.\n", "keywords": "aked for 2min at temperatures of up to 330\u00b0C\nanisole\nbaking\nbefore and after dipping\nchloroform\ncompare the performance of the different compositions across tasks\nelution behavior\netching\nevaluated three spin-on carbon hardmasks from Irresistible Materials\nfilm\nFilm\nFilms of the SoC\nfilm thickness measurements\nhydrogen-terminated silicon substrates\nIM-HM11-01\nIM-HM11-01 and IM-HM11-02\nIM-HM11-02\nIM-HM11-02 film\nIM-HM11-02 films\nIM-HM11-03\nMCB\nmonochlorobenzene\nSoC\nSoC (\nsolvent\nspin coating\nspin-on carbon compositions\nspin-on-hardmask\nthree spin-on carbon hardmasks\n"}, {"id": "S2212671612001163", "text": "The paper deals with the computation of distribution network components reliability parameters. Knowledge of the component reliability parameters in power networks is necessary for the reliability computation and also for reliability-centered maintenance system. Component reliability parameters are possible to retrieve only with accurate databases of distribution companies. Such a database includes records of outages and interruptions in power networks. It is impossible to retrieve reliability parameters from this data in a direct way because of heterogeneity. In this paper, we introduce some results of databases calculations. We apply this framework for the retrieving of parameters from outage data in the Czech and Slovak republics. There are also actual results.", "keywords": "accurate databases of distribution companies\ncomponent reliability parameters\nComponent reliability parameters\ncomputation\ncomputation of distribution network components reliability parameters\ndata\ndatabase\nframework for the retrieving of parameters from outage data\nheterogeneity\nintroduce some results of databases calculations\npower networks\nrecords of outages and interruptions\nreliability-centered maintenance system\nreliability computation\nretrieve\nretrieve reliability parameters\n"}, {"id": "S2212671612001163", "text": "The paper deals with the computation of distribution network components reliability parameters. Knowledge of the component reliability parameters in power networks is necessary for the reliability computation and also for reliability-centered maintenance system. Component reliability parameters are possible to retrieve only with accurate databases of distribution companies. Such a database includes records of outages and interruptions in power networks. It is impossible to retrieve reliability parameters from this data in a direct way because of heterogeneity. In this paper, we introduce some results of databases calculations. We apply this framework for the retrieving of parameters from outage data in the Czech and Slovak republics. There are also actual results.", "keywords": "accurate databases of distribution companies\ncomponent reliability parameters\nComponent reliability parameters\ncomputation\ncomputation of distribution network components reliability parameters\ndata\ndatabase\nframework for the retrieving of parameters from outage data\nheterogeneity\nintroduce some results of databases calculations\npower networks\nrecords of outages and interruptions\nreliability-centered maintenance system\nreliability computation\nretrieve\nretrieve reliability parameters\n"}, {"id": "S0009261415000974", "text": "Within the range of temperatures chosen, alanine dipeptide exhibits very simple behaviour. This result is due to the relatively small number of physically relevant minima (seven were characterised using this force field and solvent model) and the larger potential energy spacing between the global minimum and higher energy minima. Indeed, cross-overs in the approximate global free energy minimum for this system (where the free energy of the second-lowest potential energy minimum becomes lower than that of the global potential energy minimum) in the harmonic approximation would occur at 1170K. In general, the harmonic prediction for the crossover temperature between two minima is(4)kBTxo=V1\u2212V2ln((o2\u03bd\u00af2\u03ba)/(o1\u03bd\u00af1\u03ba)),from Eq. (3), which clearly illustrates the balance between potential energy and well entropy.\n", "keywords": "alanine dipeptide\nbalance between potential energy and well entropy\ncross-overs in the approximate global free energy\nforce field and solvent model\nlarger potential energy spacing\nphysically relevant minima\nrange of temperatures chosen\n"}, {"id": "S0009261415000974", "text": "Within the range of temperatures chosen, alanine dipeptide exhibits very simple behaviour. This result is due to the relatively small number of physically relevant minima (seven were characterised using this force field and solvent model) and the larger potential energy spacing between the global minimum and higher energy minima. Indeed, cross-overs in the approximate global free energy minimum for this system (where the free energy of the second-lowest potential energy minimum becomes lower than that of the global potential energy minimum) in the harmonic approximation would occur at 1170K. In general, the harmonic prediction for the crossover temperature between two minima is(4)kBTxo=V1\u2212V2ln((o2\u03bd\u00af2\u03ba)/(o1\u03bd\u00af1\u03ba)),from Eq. (3), which clearly illustrates the balance between potential energy and well entropy.\n", "keywords": "alanine dipeptide\nbalance between potential energy and well entropy\ncross-overs in the approximate global free energy\nforce field and solvent model\nlarger potential energy spacing\nphysically relevant minima\nrange of temperatures chosen\n"}, {"id": "S0736585316300661", "text": "To calculate hedonic price indices in the linear model, the initial or reference price has to be calculated (Triplett, 2006). The present study adopts the approach of de Haan and Diewert (2013): a price index is constructed using the price generated by the estimated coefficients of a base period regression model, and it is calculated based on the based period average values of a given cell phone plan characteristic z\u00af for each operator (Supplementary Table S5). For continuous characteristics, direct averages are used; for binary characteristics, the proportions of cell phone plans containing the feature are used. The resulting prices for this average cell phone plan are converted to an index by applying previously calculated pure price changes (\u03b4s). Finally, the overall hedonic price index is calculated as the weighted average of firm-level indices. Weights correspond to the relative proportion of cell phone plans by operator in the sample (0.3534 for HT, 0.3212 for Vip, and 0.3254 for Tele2).\n", "keywords": "applying previously calculated pure price changes\nbase period regression model\ncalculate hedonic price indices\nlinear model\n"}, {"id": "S0736585316300661", "text": "To calculate hedonic price indices in the linear model, the initial or reference price has to be calculated (Triplett, 2006). The present study adopts the approach of de Haan and Diewert (2013): a price index is constructed using the price generated by the estimated coefficients of a base period regression model, and it is calculated based on the based period average values of a given cell phone plan characteristic z\u00af for each operator (Supplementary Table S5). For continuous characteristics, direct averages are used; for binary characteristics, the proportions of cell phone plans containing the feature are used. The resulting prices for this average cell phone plan are converted to an index by applying previously calculated pure price changes (\u03b4s). Finally, the overall hedonic price index is calculated as the weighted average of firm-level indices. Weights correspond to the relative proportion of cell phone plans by operator in the sample (0.3534 for HT, 0.3212 for Vip, and 0.3254 for Tele2).\n", "keywords": "applying previously calculated pure price changes\nbase period regression model\ncalculate hedonic price indices\nlinear model\n"}, {"id": "S0736585316300661", "text": "To calculate hedonic price indices in the linear model, the initial or reference price has to be calculated (Triplett, 2006). The present study adopts the approach of de Haan and Diewert (2013): a price index is constructed using the price generated by the estimated coefficients of a base period regression model, and it is calculated based on the based period average values of a given cell phone plan characteristic z\u00af for each operator (Supplementary Table S5). For continuous characteristics, direct averages are used; for binary characteristics, the proportions of cell phone plans containing the feature are used. The resulting prices for this average cell phone plan are converted to an index by applying previously calculated pure price changes (\u03b4s). Finally, the overall hedonic price index is calculated as the weighted average of firm-level indices. Weights correspond to the relative proportion of cell phone plans by operator in the sample (0.3534 for HT, 0.3212 for Vip, and 0.3254 for Tele2).\n", "keywords": "applying previously calculated pure price changes\nbase period regression model\ncalculate hedonic price indices\nlinear model\n"}, {"id": "S0736585316300661", "text": "To calculate hedonic price indices in the linear model, the initial or reference price has to be calculated (Triplett, 2006). The present study adopts the approach of de Haan and Diewert (2013): a price index is constructed using the price generated by the estimated coefficients of a base period regression model, and it is calculated based on the based period average values of a given cell phone plan characteristic z\u00af for each operator (Supplementary Table S5). For continuous characteristics, direct averages are used; for binary characteristics, the proportions of cell phone plans containing the feature are used. The resulting prices for this average cell phone plan are converted to an index by applying previously calculated pure price changes (\u03b4s). Finally, the overall hedonic price index is calculated as the weighted average of firm-level indices. Weights correspond to the relative proportion of cell phone plans by operator in the sample (0.3534 for HT, 0.3212 for Vip, and 0.3254 for Tele2).\n", "keywords": "applying previously calculated pure price changes\nbase period regression model\ncalculate hedonic price indices\nlinear model\n"}, {"id": "S0045782515001899", "text": "In recent years, the Discontinuous Galerkin (DG) method has emerged as a more thorough alternative for locally solving conservation laws of the shallow water equations with higher accuracy\u00a0 [21\u201327]. The DG method further involves finite element weak formulation to\u2013inherently from conservation principles\u2013shape a piecewise-polynomial solution over each local discrete cell, via local basis functions. On this basis, the DG polynomial accuracy is spanned by a set of coefficients, describing accuracy information, which are all locally evolved in time from conservation principles at the discrete level, with an arbitrary order of accuracy. A DG-based shallow water model appeals in providing higher quality solutions on very coarse meshes than a traditional finite volume counterpart, but is comparatively expensive to run and imposes a more restrictive stability condition for the CFL number\u00a0 [28,29].\n", "keywords": "CFL number\nconservation principles\nDG\nDG-based shallow water model\nDG method\nDG polynomial accuracy\nDiscontinuous Galerkin\nfinite element weak formulation\nlocal basis functions\nshallow water equations\nshape a piecewise-polynomial solution over each local discrete cell\nsolving conservation laws of the shallow water equations\nvery coarse meshes\n"}, {"id": "S0045782515001899", "text": "In recent years, the Discontinuous Galerkin (DG) method has emerged as a more thorough alternative for locally solving conservation laws of the shallow water equations with higher accuracy\u00a0 [21\u201327]. The DG method further involves finite element weak formulation to\u2013inherently from conservation principles\u2013shape a piecewise-polynomial solution over each local discrete cell, via local basis functions. On this basis, the DG polynomial accuracy is spanned by a set of coefficients, describing accuracy information, which are all locally evolved in time from conservation principles at the discrete level, with an arbitrary order of accuracy. A DG-based shallow water model appeals in providing higher quality solutions on very coarse meshes than a traditional finite volume counterpart, but is comparatively expensive to run and imposes a more restrictive stability condition for the CFL number\u00a0 [28,29].\n", "keywords": "CFL number\nconservation principles\nDG\nDG-based shallow water model\nDG method\nDG polynomial accuracy\nDiscontinuous Galerkin\nfinite element weak formulation\nlocal basis functions\nshallow water equations\nshape a piecewise-polynomial solution over each local discrete cell\nsolving conservation laws of the shallow water equations\nvery coarse meshes\n"}, {"id": "S2212667814001348", "text": "Some nonlinear wave equations are more difficult to investigate mathematically, as no general analytical method for their solutions exists. The Exponential Time Differencing (ETD) technique requires minimum stages to obtain the requiredaccurateness, which suggests an efficient technique relatingto computational duration thatensures remarkable stability characteristicsupon resolving nonlinear wave equations. This article solves the diagonal example of Kawahara equation via the ETD Runge-Kutta 4 technique. Implementation of this technique is proposed by short Matlab programs.", "keywords": "analytical method\ncomputational duration\nETD\nETD Runge-Kutta 4 technique\nExponential Time Differencing\ninvestigate mathematically\nMatlab programs\nminimum stages\nnonlinear wave equations\nresolving nonlinear wave equations\nsolves the diagonal example of Kawahara equation\nstability characteristicsupon\n"}, {"id": "S0021999115003939", "text": "The extrapolation of the upwind value required for TVD differencing is a particular hurdle for the application on unstructured meshes. As discussed in Section 3.2, two methods to extrapolate the value at the virtual upwind node, using data readily available on unstructured meshes, are considered. Given how the virtual upwind node is incorporated in the gradient ratio rf, the extrapolation method of Darwish and Moukalled [13] is referred to as implicit extrapolation and the method introduced by Ubbink and Issa [12] as explicit extrapolation. Both methods precisely reconstruct the upwind value for equidistant, rectilinear meshes but fail to do so on non-equidistant or non-rectilinear meshes, as discussed in Section 3.2. Using the explicit extrapolation method this issue can be rectified by imposing appropriate limits on the extrapolated upwind value.\n", "keywords": "Darwish and Moukalled\ndata readily available on unstructured meshes\nequidistant, rectilinear meshes\nexplicit extrapolation\nextrapolation of the upwind value\nimplicit extrapolation\nmethod introduced by Ubbink and Issa\nnon-equidistant\nnon-rectilinear meshes\nTVD differencing\nunstructured meshes\nvirtual upwind node is incorporated in the gradient ratio rf\n"}, {"id": "S0021999114007876", "text": "In this work, light propagation in a scattering medium with piece-wise constant refractive index using the radiative transport equation was studied. Light propagation in each sub-domain with a constant refractive index was modeled using the RTE and the equations were coupled using boundary conditions describing Fresnel reflection and transmission phenomenas on the interfaces between the sub-domains. The resulting coupled system of RTEs was numerically solved using the FEM. The proposed model was tested using simulations and was compared with the solution of the Monte Carlo method. The results show that the coupled RTE model describes light propagation accurately in comparison with the Monte Carlo method. In addition, results show that neglecting internal refractive index changes can lead to erroneous boundary measurements of scattered light. This indicates that the quality of the DOT reconstructions could possible be increased by incorporating a model for internal refractive index changes in the image reconstruction procedure.\n", "keywords": "coupled system of RTEs\nDOT reconstructions\nFEM\nFresnel reflection and transmission phenomenas\nimage reconstruction procedure\nlight propagation in a scattering medium with piece-wise constant refractive index\nMonte Carlo method\nradiative transport equation\nrefractive index changes\nRTE\nsimulations\nsolution of the Monte Carlo method\n"}, {"id": "S2214657115000052", "text": "The exponential relationships reported in the plots may be used to convert the dielectric values to air void values as prescribed in previous studies [1\u20133]. The AC pavement composite permittivity reduces, along with the reflection coefficient, as the volumetric proportion of air increases as compared to the remaining components. However, the method relies on an empirical fit, determined on a case-by-case basis, since the permittivity of the remaining components depends on the mix design (aggregate type, binder content, etc.). Long term studies in Finland concluded that this empirical fit is an exponential relationship [1]. The exponential fits, using a sufficient amount of cores, can be used to map the air void content variation in a similar manner to the dielectric maps shown in Fig. 4. Only 4 cores were feasible due to various factors involved with testing the final lift of an in-service pavement. More cores are needed for more stable exponential coefficients, although the limited cores show that the predicted relationships are similar for the measured dielectric range in this case-study. Since both regressions predict air void content at a maximum difference of 0.56%, which is within the uncertainty of the core measurement precision of 0.7%, use of either the initial or repeat run regression predictions are appropriate.\n", "keywords": "AC pavement composite permittivity\nair void content\nconvert the dielectric values to air void values\ncore measurement precision\ndielectric maps\nexponential relationship\nexponential relationships\ninitial or repeat run regression predictions\nLong term studies in Finland\nmap the air void content variation\nmeasured dielectric range\nmix design\nmore stable exponential coefficients\npredicted relationships\nregressions\nsufficient amount of cores\ntesting the final lift\nvolumetric proportion of air\n"}, {"id": "S2212667814001361", "text": "Contractor selection for a project is an important decision, one for the project time and cost, next for the quality obtained by the project. Although the project managers can easily determine the project time and cost, the quality is usually undefined especially for un-experienced managers. With a learnable property, an approach is first introduced in this paper to quantify the quality obtained for a gas well drilling project. Then, based on these three objectives (time, cost, and quality), a contractor selection problem is converted to an optimization problem. Next, the NSGA-II algorithm is utilized for solution. At the end, a sensitivity analysis is performed to select the parameters of the algorithm.", "keywords": "Contractor selection\ncontractor selection problem\ncost\ndetermine the project time and cost\ngas well drilling project\nlearnable property\nNSGA-II algorithm\nobjectives\noptimization problem\nproject managers\nproject time and cost\nquality\nquantify the quality obtained for a gas well drilling project\nsensitivity analysis\n(time\nun-experienced managers\n"}, {"id": "S2212667814001361", "text": "Contractor selection for a project is an important decision, one for the project time and cost, next for the quality obtained by the project. Although the project managers can easily determine the project time and cost, the quality is usually undefined especially for un-experienced managers. With a learnable property, an approach is first introduced in this paper to quantify the quality obtained for a gas well drilling project. Then, based on these three objectives (time, cost, and quality), a contractor selection problem is converted to an optimization problem. Next, the NSGA-II algorithm is utilized for solution. At the end, a sensitivity analysis is performed to select the parameters of the algorithm.", "keywords": "Contractor selection\ncontractor selection problem\ncost\ndetermine the project time and cost\ngas well drilling project\nlearnable property\nNSGA-II algorithm\nobjectives\noptimization problem\nproject managers\nproject time and cost\nquality\nquantify the quality obtained for a gas well drilling project\nsensitivity analysis\n(time\nun-experienced managers\n"}, {"id": "S2212667814001361", "text": "Contractor selection for a project is an important decision, one for the project time and cost, next for the quality obtained by the project. Although the project managers can easily determine the project time and cost, the quality is usually undefined especially for un-experienced managers. With a learnable property, an approach is first introduced in this paper to quantify the quality obtained for a gas well drilling project. Then, based on these three objectives (time, cost, and quality), a contractor selection problem is converted to an optimization problem. Next, the NSGA-II algorithm is utilized for solution. At the end, a sensitivity analysis is performed to select the parameters of the algorithm.", "keywords": "Contractor selection\ncontractor selection problem\ncost\ndetermine the project time and cost\ngas well drilling project\nlearnable property\nNSGA-II algorithm\nobjectives\noptimization problem\nproject managers\nproject time and cost\nquality\nquantify the quality obtained for a gas well drilling project\nsensitivity analysis\n(time\nun-experienced managers\n"}, {"id": "S2212667814001361", "text": "Contractor selection for a project is an important decision, one for the project time and cost, next for the quality obtained by the project. Although the project managers can easily determine the project time and cost, the quality is usually undefined especially for un-experienced managers. With a learnable property, an approach is first introduced in this paper to quantify the quality obtained for a gas well drilling project. Then, based on these three objectives (time, cost, and quality), a contractor selection problem is converted to an optimization problem. Next, the NSGA-II algorithm is utilized for solution. At the end, a sensitivity analysis is performed to select the parameters of the algorithm.", "keywords": "Contractor selection\ncontractor selection problem\ncost\ndetermine the project time and cost\ngas well drilling project\nlearnable property\nNSGA-II algorithm\nobjectives\noptimization problem\nproject managers\nproject time and cost\nquality\nquantify the quality obtained for a gas well drilling project\nsensitivity analysis\n(time\nun-experienced managers\n"}, {"id": "S2212667814001361", "text": "Contractor selection for a project is an important decision, one for the project time and cost, next for the quality obtained by the project. Although the project managers can easily determine the project time and cost, the quality is usually undefined especially for un-experienced managers. With a learnable property, an approach is first introduced in this paper to quantify the quality obtained for a gas well drilling project. Then, based on these three objectives (time, cost, and quality), a contractor selection problem is converted to an optimization problem. Next, the NSGA-II algorithm is utilized for solution. At the end, a sensitivity analysis is performed to select the parameters of the algorithm.", "keywords": "Contractor selection\ncontractor selection problem\ncost\ndetermine the project time and cost\ngas well drilling project\nlearnable property\nNSGA-II algorithm\nobjectives\noptimization problem\nproject managers\nproject time and cost\nquality\nquantify the quality obtained for a gas well drilling project\nsensitivity analysis\n(time\nun-experienced managers\n"}, {"id": "S2212667814001361", "text": "Contractor selection for a project is an important decision, one for the project time and cost, next for the quality obtained by the project. Although the project managers can easily determine the project time and cost, the quality is usually undefined especially for un-experienced managers. With a learnable property, an approach is first introduced in this paper to quantify the quality obtained for a gas well drilling project. Then, based on these three objectives (time, cost, and quality), a contractor selection problem is converted to an optimization problem. Next, the NSGA-II algorithm is utilized for solution. At the end, a sensitivity analysis is performed to select the parameters of the algorithm.", "keywords": "Contractor selection\ncontractor selection problem\ncost\ndetermine the project time and cost\ngas well drilling project\nlearnable property\nNSGA-II algorithm\nobjectives\noptimization problem\nproject managers\nproject time and cost\nquality\nquantify the quality obtained for a gas well drilling project\nsensitivity analysis\n(time\nun-experienced managers\n"}, {"id": "S0022311515002470", "text": "To conclude, the electrochemical reduction of uranium dioxide to uranium metal has been studied in a lithium chloride\u2013potassium chloride eutectic molten salt at 450\u00b0C. Both electrochemical and synchrotron X-ray techniques have been utilised to deduce the electrochemical reduction potential, mechanism and reduction pathway. The electrochemical reduction potential of the UO2|U couple is dependent on the activity of oxide ions existing within the melt. The electrochemical reduction of uranium dioxide to uranium metal seems to occur in a single, 4-electron-step, process; indicated by a single reduction peak (C1) in the cyclic voltammograms and also by the exclusion of any other phases in the EDXD data. The electrochemical reduction may be impeded by an increase in oxo-acidity of the molten salt. That is, O2\u2212 ions that are liberated by the electroreduction may not react at the counter electrode and, thus, not be removed from the molten salt. This could be due to the electrode geometry and/or the inherent microstructure of the working electrode: a high tortuosity, for example, would impede the diffusion of O2\u2212 ions out of the working electrode. This could then cause an increase in the activity of oxide ions existing within the melt and hence inhibit the electrochemical reduction \u2013 exploration of the microstructure of working electrodes will be the focus of future work.\n", "keywords": "4-electron-step\ncounter electrode\ncyclic voltammograms\ndeduce the electrochemical reduction potential, mechanism and reduction pathway\nEDXD data\nelectrochemical\nelectrochemical reduction\nelectrochemical reduction of uranium dioxide\nelectrode\nelectrodes\nelectron\nelectroreduction\nexploration of the microstructure of working electrodes\nlithium chloride\u2013potassium chloride eutectic molten salt\nmelt\nmolten salt\nO2\u2212 ions\noxide ions\nsynchrotron X-ray\nUO2|U couple\nuranium dioxide\nuranium metal\n"}, {"id": "S0021999112003579", "text": "We order the discrete unknowns so that the vector of unknowns, xPS=[X,L], contains the nx unknown nodal coordinates, followed by the nb unknown discrete Lagrange multipliers. The linear systems to be solved in the course of the Newton-based solution of Eq. (10), subject to the displacement constraint (9), then have saddle-point structure,(15)where E is the tangent stiffness matrix of the unconstrained pseudo-solid problem, and the two off-diagonal blocks Cxl and Clx=CxlT arise through the imposition of the displacement constraint by the Lagrange multipliers. We refer to [34] for the proof of the LBB stability of this discretisation; see also [35,36] for a discussion of the LBB stability of the Lagrange-multiplier-based imposition of Dirichlet boundary conditions in related problems. We note that during the first step of the Newton iteration, E is symmetric positive definite since it represents the tangent stiffness matrix relative to the system\u2019s equilibrium configuration.\n", "keywords": "Clx=CxlT\nCxl\nDirichlet boundary conditions\ndiscretisation\ndisplacement constraint\nequilibrium configuration\nLagrange-multiplier-based imposition\nLagrange multipliers\nLBB\nNewton-based solution\nNewton iteration\nnodal coordinates\norder the discrete unknowns\nsaddle-point structure\ntangent stiffness matrix\ntwo off-diagonal blocks\nunconstrained pseudo-solid problem\nvector of unknowns\nxPS=[X,L]\n"}, {"id": "S0021999112003579", "text": "We order the discrete unknowns so that the vector of unknowns, xPS=[X,L], contains the nx unknown nodal coordinates, followed by the nb unknown discrete Lagrange multipliers. The linear systems to be solved in the course of the Newton-based solution of Eq. (10), subject to the displacement constraint (9), then have saddle-point structure,(15)where E is the tangent stiffness matrix of the unconstrained pseudo-solid problem, and the two off-diagonal blocks Cxl and Clx=CxlT arise through the imposition of the displacement constraint by the Lagrange multipliers. We refer to [34] for the proof of the LBB stability of this discretisation; see also [35,36] for a discussion of the LBB stability of the Lagrange-multiplier-based imposition of Dirichlet boundary conditions in related problems. We note that during the first step of the Newton iteration, E is symmetric positive definite since it represents the tangent stiffness matrix relative to the system\u2019s equilibrium configuration.\n", "keywords": "Clx=CxlT\nCxl\nDirichlet boundary conditions\ndiscretisation\ndisplacement constraint\nequilibrium configuration\nLagrange-multiplier-based imposition\nLagrange multipliers\nLBB\nNewton-based solution\nNewton iteration\nnodal coordinates\norder the discrete unknowns\nsaddle-point structure\ntangent stiffness matrix\ntwo off-diagonal blocks\nunconstrained pseudo-solid problem\nvector of unknowns\nxPS=[X,L]\n"}, {"id": "S0021999112003579", "text": "We order the discrete unknowns so that the vector of unknowns, xPS=[X,L], contains the nx unknown nodal coordinates, followed by the nb unknown discrete Lagrange multipliers. The linear systems to be solved in the course of the Newton-based solution of Eq. (10), subject to the displacement constraint (9), then have saddle-point structure,(15)where E is the tangent stiffness matrix of the unconstrained pseudo-solid problem, and the two off-diagonal blocks Cxl and Clx=CxlT arise through the imposition of the displacement constraint by the Lagrange multipliers. We refer to [34] for the proof of the LBB stability of this discretisation; see also [35,36] for a discussion of the LBB stability of the Lagrange-multiplier-based imposition of Dirichlet boundary conditions in related problems. We note that during the first step of the Newton iteration, E is symmetric positive definite since it represents the tangent stiffness matrix relative to the system\u2019s equilibrium configuration.\n", "keywords": "Clx=CxlT\nCxl\nDirichlet boundary conditions\ndiscretisation\ndisplacement constraint\nequilibrium configuration\nLagrange-multiplier-based imposition\nLagrange multipliers\nLBB\nNewton-based solution\nNewton iteration\nnodal coordinates\norder the discrete unknowns\nsaddle-point structure\ntangent stiffness matrix\ntwo off-diagonal blocks\nunconstrained pseudo-solid problem\nvector of unknowns\nxPS=[X,L]\n"}, {"id": "S0021999112003579", "text": "We order the discrete unknowns so that the vector of unknowns, xPS=[X,L], contains the nx unknown nodal coordinates, followed by the nb unknown discrete Lagrange multipliers. The linear systems to be solved in the course of the Newton-based solution of Eq. (10), subject to the displacement constraint (9), then have saddle-point structure,(15)where E is the tangent stiffness matrix of the unconstrained pseudo-solid problem, and the two off-diagonal blocks Cxl and Clx=CxlT arise through the imposition of the displacement constraint by the Lagrange multipliers. We refer to [34] for the proof of the LBB stability of this discretisation; see also [35,36] for a discussion of the LBB stability of the Lagrange-multiplier-based imposition of Dirichlet boundary conditions in related problems. We note that during the first step of the Newton iteration, E is symmetric positive definite since it represents the tangent stiffness matrix relative to the system\u2019s equilibrium configuration.\n", "keywords": "Clx=CxlT\nCxl\nDirichlet boundary conditions\ndiscretisation\ndisplacement constraint\nequilibrium configuration\nLagrange-multiplier-based imposition\nLagrange multipliers\nLBB\nNewton-based solution\nNewton iteration\nnodal coordinates\norder the discrete unknowns\nsaddle-point structure\ntangent stiffness matrix\ntwo off-diagonal blocks\nunconstrained pseudo-solid problem\nvector of unknowns\nxPS=[X,L]\n"}, {"id": "S0021999112003579", "text": "We order the discrete unknowns so that the vector of unknowns, xPS=[X,L], contains the nx unknown nodal coordinates, followed by the nb unknown discrete Lagrange multipliers. The linear systems to be solved in the course of the Newton-based solution of Eq. (10), subject to the displacement constraint (9), then have saddle-point structure,(15)where E is the tangent stiffness matrix of the unconstrained pseudo-solid problem, and the two off-diagonal blocks Cxl and Clx=CxlT arise through the imposition of the displacement constraint by the Lagrange multipliers. We refer to [34] for the proof of the LBB stability of this discretisation; see also [35,36] for a discussion of the LBB stability of the Lagrange-multiplier-based imposition of Dirichlet boundary conditions in related problems. We note that during the first step of the Newton iteration, E is symmetric positive definite since it represents the tangent stiffness matrix relative to the system\u2019s equilibrium configuration.\n", "keywords": "Clx=CxlT\nCxl\nDirichlet boundary conditions\ndiscretisation\ndisplacement constraint\nequilibrium configuration\nLagrange-multiplier-based imposition\nLagrange multipliers\nLBB\nNewton-based solution\nNewton iteration\nnodal coordinates\norder the discrete unknowns\nsaddle-point structure\ntangent stiffness matrix\ntwo off-diagonal blocks\nunconstrained pseudo-solid problem\nvector of unknowns\nxPS=[X,L]\n"}, {"id": "S0021999112003579", "text": "We order the discrete unknowns so that the vector of unknowns, xPS=[X,L], contains the nx unknown nodal coordinates, followed by the nb unknown discrete Lagrange multipliers. The linear systems to be solved in the course of the Newton-based solution of Eq. (10), subject to the displacement constraint (9), then have saddle-point structure,(15)where E is the tangent stiffness matrix of the unconstrained pseudo-solid problem, and the two off-diagonal blocks Cxl and Clx=CxlT arise through the imposition of the displacement constraint by the Lagrange multipliers. We refer to [34] for the proof of the LBB stability of this discretisation; see also [35,36] for a discussion of the LBB stability of the Lagrange-multiplier-based imposition of Dirichlet boundary conditions in related problems. We note that during the first step of the Newton iteration, E is symmetric positive definite since it represents the tangent stiffness matrix relative to the system\u2019s equilibrium configuration.\n", "keywords": "Clx=CxlT\nCxl\nDirichlet boundary conditions\ndiscretisation\ndisplacement constraint\nequilibrium configuration\nLagrange-multiplier-based imposition\nLagrange multipliers\nLBB\nNewton-based solution\nNewton iteration\nnodal coordinates\norder the discrete unknowns\nsaddle-point structure\ntangent stiffness matrix\ntwo off-diagonal blocks\nunconstrained pseudo-solid problem\nvector of unknowns\nxPS=[X,L]\n"}, {"id": "S0021999112003579", "text": "We order the discrete unknowns so that the vector of unknowns, xPS=[X,L], contains the nx unknown nodal coordinates, followed by the nb unknown discrete Lagrange multipliers. The linear systems to be solved in the course of the Newton-based solution of Eq. (10), subject to the displacement constraint (9), then have saddle-point structure,(15)where E is the tangent stiffness matrix of the unconstrained pseudo-solid problem, and the two off-diagonal blocks Cxl and Clx=CxlT arise through the imposition of the displacement constraint by the Lagrange multipliers. We refer to [34] for the proof of the LBB stability of this discretisation; see also [35,36] for a discussion of the LBB stability of the Lagrange-multiplier-based imposition of Dirichlet boundary conditions in related problems. We note that during the first step of the Newton iteration, E is symmetric positive definite since it represents the tangent stiffness matrix relative to the system\u2019s equilibrium configuration.\n", "keywords": "Clx=CxlT\nCxl\nDirichlet boundary conditions\ndiscretisation\ndisplacement constraint\nequilibrium configuration\nLagrange-multiplier-based imposition\nLagrange multipliers\nLBB\nNewton-based solution\nNewton iteration\nnodal coordinates\norder the discrete unknowns\nsaddle-point structure\ntangent stiffness matrix\ntwo off-diagonal blocks\nunconstrained pseudo-solid problem\nvector of unknowns\nxPS=[X,L]\n"}, {"id": "S0021999112003579", "text": "We order the discrete unknowns so that the vector of unknowns, xPS=[X,L], contains the nx unknown nodal coordinates, followed by the nb unknown discrete Lagrange multipliers. The linear systems to be solved in the course of the Newton-based solution of Eq. (10), subject to the displacement constraint (9), then have saddle-point structure,(15)where E is the tangent stiffness matrix of the unconstrained pseudo-solid problem, and the two off-diagonal blocks Cxl and Clx=CxlT arise through the imposition of the displacement constraint by the Lagrange multipliers. We refer to [34] for the proof of the LBB stability of this discretisation; see also [35,36] for a discussion of the LBB stability of the Lagrange-multiplier-based imposition of Dirichlet boundary conditions in related problems. We note that during the first step of the Newton iteration, E is symmetric positive definite since it represents the tangent stiffness matrix relative to the system\u2019s equilibrium configuration.\n", "keywords": "Clx=CxlT\nCxl\nDirichlet boundary conditions\ndiscretisation\ndisplacement constraint\nequilibrium configuration\nLagrange-multiplier-based imposition\nLagrange multipliers\nLBB\nNewton-based solution\nNewton iteration\nnodal coordinates\norder the discrete unknowns\nsaddle-point structure\ntangent stiffness matrix\ntwo off-diagonal blocks\nunconstrained pseudo-solid problem\nvector of unknowns\nxPS=[X,L]\n"}, {"id": "S0888327016300048", "text": "The research work in this paper elaborates on the theoretical effectiveness of the proposed method based on the multivariate EMD. It also clearly indicates through numerical simulations and applications to bearing monitoring that the expansion from standard EMD to multivariate EMD is a successful exploration. Using multiple sensors to collect signal from different locations of the machine and using the multivariate EMD to analyze multivariate signal can contribute to comprehensively collect all the frequency components related to any bearing fault, and is beneficial to extract fault information, especially for early weak fault characteristics. Both the characteristic frequencies of simulated signal and the fault frequencies of practical rolling bearing signal can be extracted from the same order of IMF groups, thus showing that multivariate EMD is an effective signal decomposition algorithm and can be competently applied to fault diagnosis of rolling bearings when combined with a multiscale reduction method and fault correlation factor analysis. In signal acquisition and processing, given the circumstance that there is a trend toward the use of multiple sensors, multivariate EMD appears to be very useful and meaningful as a kind of multivariate data processing algorithm. By analyzing the simulated signal and two different practical multivariate signals, the results demonstrate the significance of the proposed method in the field of fault diagnosis of rolling bearing.\n", "keywords": "applications to bearing monitoring\nbearing monitoring\nelaborates on the theoretical effectiveness of the proposed method based on the multivariate EMD\nexpansion from standard EMD to multivariate EMD\nextract fault information\nfault correlation factor analysis\nfault diagnosis\nfault diagnosis of rolling bearing\nfrequency components\nmultiscale reduction method\nmultivariate data processing algorithm\nmultivariate EMD\nmultivariate signal\nnumerical simulations\npractical multivariate signals\npractical rolling bearing signal\nrolling bearing\nrolling bearings\nsensors\nsignal acquisition and processing\nsignal decomposition algorithm\nsimulated signal\nstandard EMD\n"}, {"id": "S0888327016300048", "text": "The research work in this paper elaborates on the theoretical effectiveness of the proposed method based on the multivariate EMD. It also clearly indicates through numerical simulations and applications to bearing monitoring that the expansion from standard EMD to multivariate EMD is a successful exploration. Using multiple sensors to collect signal from different locations of the machine and using the multivariate EMD to analyze multivariate signal can contribute to comprehensively collect all the frequency components related to any bearing fault, and is beneficial to extract fault information, especially for early weak fault characteristics. Both the characteristic frequencies of simulated signal and the fault frequencies of practical rolling bearing signal can be extracted from the same order of IMF groups, thus showing that multivariate EMD is an effective signal decomposition algorithm and can be competently applied to fault diagnosis of rolling bearings when combined with a multiscale reduction method and fault correlation factor analysis. In signal acquisition and processing, given the circumstance that there is a trend toward the use of multiple sensors, multivariate EMD appears to be very useful and meaningful as a kind of multivariate data processing algorithm. By analyzing the simulated signal and two different practical multivariate signals, the results demonstrate the significance of the proposed method in the field of fault diagnosis of rolling bearing.\n", "keywords": "applications to bearing monitoring\nbearing monitoring\nelaborates on the theoretical effectiveness of the proposed method based on the multivariate EMD\nexpansion from standard EMD to multivariate EMD\nextract fault information\nfault correlation factor analysis\nfault diagnosis\nfault diagnosis of rolling bearing\nfrequency components\nmultiscale reduction method\nmultivariate data processing algorithm\nmultivariate EMD\nmultivariate signal\nnumerical simulations\npractical multivariate signals\npractical rolling bearing signal\nrolling bearing\nrolling bearings\nsensors\nsignal acquisition and processing\nsignal decomposition algorithm\nsimulated signal\nstandard EMD\n"}, {"id": "S0888327016300048", "text": "The research work in this paper elaborates on the theoretical effectiveness of the proposed method based on the multivariate EMD. It also clearly indicates through numerical simulations and applications to bearing monitoring that the expansion from standard EMD to multivariate EMD is a successful exploration. Using multiple sensors to collect signal from different locations of the machine and using the multivariate EMD to analyze multivariate signal can contribute to comprehensively collect all the frequency components related to any bearing fault, and is beneficial to extract fault information, especially for early weak fault characteristics. Both the characteristic frequencies of simulated signal and the fault frequencies of practical rolling bearing signal can be extracted from the same order of IMF groups, thus showing that multivariate EMD is an effective signal decomposition algorithm and can be competently applied to fault diagnosis of rolling bearings when combined with a multiscale reduction method and fault correlation factor analysis. In signal acquisition and processing, given the circumstance that there is a trend toward the use of multiple sensors, multivariate EMD appears to be very useful and meaningful as a kind of multivariate data processing algorithm. By analyzing the simulated signal and two different practical multivariate signals, the results demonstrate the significance of the proposed method in the field of fault diagnosis of rolling bearing.\n", "keywords": "applications to bearing monitoring\nbearing monitoring\nelaborates on the theoretical effectiveness of the proposed method based on the multivariate EMD\nexpansion from standard EMD to multivariate EMD\nextract fault information\nfault correlation factor analysis\nfault diagnosis\nfault diagnosis of rolling bearing\nfrequency components\nmultiscale reduction method\nmultivariate data processing algorithm\nmultivariate EMD\nmultivariate signal\nnumerical simulations\npractical multivariate signals\npractical rolling bearing signal\nrolling bearing\nrolling bearings\nsensors\nsignal acquisition and processing\nsignal decomposition algorithm\nsimulated signal\nstandard EMD\n"}, {"id": "S0888327016300048", "text": "The research work in this paper elaborates on the theoretical effectiveness of the proposed method based on the multivariate EMD. It also clearly indicates through numerical simulations and applications to bearing monitoring that the expansion from standard EMD to multivariate EMD is a successful exploration. Using multiple sensors to collect signal from different locations of the machine and using the multivariate EMD to analyze multivariate signal can contribute to comprehensively collect all the frequency components related to any bearing fault, and is beneficial to extract fault information, especially for early weak fault characteristics. Both the characteristic frequencies of simulated signal and the fault frequencies of practical rolling bearing signal can be extracted from the same order of IMF groups, thus showing that multivariate EMD is an effective signal decomposition algorithm and can be competently applied to fault diagnosis of rolling bearings when combined with a multiscale reduction method and fault correlation factor analysis. In signal acquisition and processing, given the circumstance that there is a trend toward the use of multiple sensors, multivariate EMD appears to be very useful and meaningful as a kind of multivariate data processing algorithm. By analyzing the simulated signal and two different practical multivariate signals, the results demonstrate the significance of the proposed method in the field of fault diagnosis of rolling bearing.\n", "keywords": "applications to bearing monitoring\nbearing monitoring\nelaborates on the theoretical effectiveness of the proposed method based on the multivariate EMD\nexpansion from standard EMD to multivariate EMD\nextract fault information\nfault correlation factor analysis\nfault diagnosis\nfault diagnosis of rolling bearing\nfrequency components\nmultiscale reduction method\nmultivariate data processing algorithm\nmultivariate EMD\nmultivariate signal\nnumerical simulations\npractical multivariate signals\npractical rolling bearing signal\nrolling bearing\nrolling bearings\nsensors\nsignal acquisition and processing\nsignal decomposition algorithm\nsimulated signal\nstandard EMD\n"}, {"id": "S0168365913008766", "text": "Immunopotentiators activate innate immunity directly (for example, cytokines) or through pattern-recognition receptors (PRRs, such as those for bacterial components). The Toll-like receptors (TLRs) are a family of PRRs that are an important link between innate and adaptive immunity. Some studies have shown that TLR ligands have adjuvant activity and enhance antigen-specific antibody and cell-mediated immune responses, especially when they are combined with delivery systems that promote their uptake and delivery into antigen-presenting cells [22\u201324]. For clinical studies, TLR9 is generally stimulated with synthetic oligodeoxynucleotides containing one or more unmethylated CpG dinucleotides. In humans, CpG has been used as an adjuvant for infectious disease vaccination [25,26] and in the development of cancer therapy [27]. In a mouse model, CpG has also been shown to induce T helper 1 (Th1) immune responses, which are characterized by the production of IFN-\u03b3 and the generation of IgG2a [28,29]. Moreover, a previous study had demonstrated that different liposomes with CpG ODN significantly increased Th1-biased cytokines and augmented cell mediated immune response [30].\n", "keywords": "activate innate immunity\nadjuvant\nantigen-presenting cells\naugmented cell mediated immune response\nclinical studies\nCpG\ne development of cancer therapy\nImmunopotentiators\ninduce T helper 1\ninfectious disease vaccination\nliposomes\nmouse model\npattern-recognition receptors\nproduction of IFN-\u03b3 and the generation of IgG2a\nPRRs\nstimulated\nsynthetic oligodeoxynucleotides\nTh1-biased cytokines\nTLR9\nTLR ligands\nTLRs\nToll-like receptors\nunmethylated CpG dinucleotides\n"}, {"id": "S000926141500651X", "text": "Previous studies have shown that there are two main mechanisms for the development of radiation-induced DSBs [16,17]. For \u03b3-ray radiation, single step is the main process to cause DSBs (see Figure 3b), which is attributed to the generation of number of ROS upon the incident of individual photon of \u03b3-ray. Whereas photo-radiation causes DSBs through two step mechanism (Figure 3a) by reflecting that each single photon causes mostly single ROS and thus induces only single strand break. Then, when a second single strand break occurs where near the existing single strand break, DBS is caused, i.e., the two step mechanism. Summarizing the results and discussion we may conclude as that: (1) The significant protective effect of AA against photo-induced damage may reflect the effective diminish of ROS by AA. (2) For the \u03b3-ray induced DSB, the protective effect by AA is a little bit weaker than the case of photo irradiation. This may be due to the generation of numbers of ROS by single photon of \u03b3-ray. Surviving oxygen species against the diminishment effect by AA may cause DSBs. (3) As for the DSBs by ultrasound, damage is caused by the shockwave through the generation of cavitations [18]. Thus, the chemical effect of AA to diminish ROS is considered to be negligibly small for the protection of DSBs.\n", "keywords": "AA\ncavitations\nDBS\ndevelopment of radiation-induced DSBs\ndiminish ROS\nDSBs\ngeneration of number of ROS\nphoto\nphoto-induced damage\nphoto irradiation\nphoton\nphoton of \u03b3-ray\nphoto-radiation\nradiation-induced DSBs\nROS\nshockwave\nsingle photon of \u03b3-ray\nsingle step\nsingle strand break\nstrand\ntwo step mechanism\nultrasound\n\u03b3-ray induced DSB\n\u03b3-ray radiation\n"}, {"id": "S000926141500651X", "text": "Previous studies have shown that there are two main mechanisms for the development of radiation-induced DSBs [16,17]. For \u03b3-ray radiation, single step is the main process to cause DSBs (see Figure 3b), which is attributed to the generation of number of ROS upon the incident of individual photon of \u03b3-ray. Whereas photo-radiation causes DSBs through two step mechanism (Figure 3a) by reflecting that each single photon causes mostly single ROS and thus induces only single strand break. Then, when a second single strand break occurs where near the existing single strand break, DBS is caused, i.e., the two step mechanism. Summarizing the results and discussion we may conclude as that: (1) The significant protective effect of AA against photo-induced damage may reflect the effective diminish of ROS by AA. (2) For the \u03b3-ray induced DSB, the protective effect by AA is a little bit weaker than the case of photo irradiation. This may be due to the generation of numbers of ROS by single photon of \u03b3-ray. Surviving oxygen species against the diminishment effect by AA may cause DSBs. (3) As for the DSBs by ultrasound, damage is caused by the shockwave through the generation of cavitations [18]. Thus, the chemical effect of AA to diminish ROS is considered to be negligibly small for the protection of DSBs.\n", "keywords": "AA\ncavitations\nDBS\ndevelopment of radiation-induced DSBs\ndiminish ROS\nDSBs\ngeneration of number of ROS\nphoto\nphoto-induced damage\nphoto irradiation\nphoton\nphoton of \u03b3-ray\nphoto-radiation\nradiation-induced DSBs\nROS\nshockwave\nsingle photon of \u03b3-ray\nsingle step\nsingle strand break\nstrand\ntwo step mechanism\nultrasound\n\u03b3-ray induced DSB\n\u03b3-ray radiation\n"}, {"id": "S0377221716301904", "text": "We propose an equilibrium model that allows to analyze the long-run impact of the electricity market design on transmission line expansion by the regulator and investment in generation capacity by private firms in liberalized electricity markets. The model incorporates investment decisions of the transmission system operator and private firms in expectation of an energy-only market and cost-based redispatch. In different specifications we consider the cases of one vs. multiple price zones (market splitting) and analyze different approaches to recover network cost\u2014in particular lump sum, generation capacity based, and energy based fees. In order to compare the outcomes of our multilevel market model with a first best benchmark, we also solve the corresponding integrated planner problem. Using two test networks we illustrate that energy-only markets can lead to suboptimal locational decisions for generation capacity and thus imply excessive network expansion. Market splitting heals these problems only partially. These results are valid for all considered types of network tariffs, although investment slightly differs across those regimes.\n", "keywords": "analyze different approaches to recover network cost\nanalyze the long-run impact of the electricity market design on transmission line expansion\nenergy based fees\nequilibrium model\nexcessive network expansion\nfirst best benchmark\ngeneration capacity based\nintegrated planner problem\nlump sum\nmarket splitting\nMarket splitting\nmultilevel market model\nrecover network cost\nsolve the corresponding integrated planner problem\n"}, {"id": "S002199911200068X", "text": "Myocardial electrical propagation can be simulated using the monodomain or bidomain PDEs [5,6]. Due to its capacity to represent complex geometries with ease, approximations are often obtained using the finite element method (FEM) to discretise the PDEs in space on realistic cardiac geometry meshes; this results in very large (up to forty-million degrees of freedom (DOF) for human heart geometries) systems of linear equations which must be solved many thousands of times over the course of even a short simulation. Thus, they are extremely computationally demanding, presenting taxing problems even to high-end supercomputing resources. This computational demand means that effort has been invested in developing efficient solution techniques, including work on preconditioning, parallelisation and adaptivity in space and time [7\u201312]. In this study, we investigate the potential of reducing the number of DOF by using a high-order polynomial FEM [13\u201315] to approximate the monodomain PDE in space, with the goal of significantly improving simulation efficiency over the piecewise-linear FEM approach commonly used in the field [16\u201319]. For schemes where the polynomial degree p of the elements is adjusted according to the error in the approximation, this is known as the finite element p-version. In the work presented here, we work with schemes which keep p fixed.\n", "keywords": "adaptivity in space and time\ncardiac geometry meshes\ndeveloping efficient solution techniques\nFEM\nfinite element method\nfinite element p-version\ngeometries\nhigh-end supercomputing resources\nhigh-order polynomial FEM\nhuman heart geometries\ninvestigate the potential of reducing the number of DOF\nmonodomain or bidomain PDEs\nmonodomain PDE\nMyocardial electrical propagation\nparallelisation\nPDEs\npiecewise-linear FEM\npreconditioning\nsignificantly improving simulation efficiency\nsimulation\nsimulation efficiency\nsystems of linear equations\n"}, {"id": "S0022311514001640", "text": "The vapour phase consists of a number of different gases with silicon exhibiting a far higher partial pressure than all carbon containing species over the full temperature range. As an immediate result the vapour contains a higher amount of silicon leaving the solid phase with excess carbon. This carbon is likely to precipitate on the surface of the SiC grains, a process that becomes very rapid as the temperature approaches 2100K [24]. Within the TRISO particle the SiC layer is sandwiched between two coatings of dense carbon. The partial pressure in thermodynamic equilibrium of gaseous carbon forming above graphite was calculated using data taken from JANAF tables and added to Fig. 1 [25], which showed that in the whole temperature range relevant for this study the vapour pressure of carbon is several magnitudes smaller than that of the dominant gas phases above SiC.\n", "keywords": "carbon\ndata taken from JANAF tables\ngas\ngaseous carbon\ngraphite\nnumber of different gases\npartial pressure\nprecipitate on the surface of the SiC grains\nSiC\nSiC grains\nsilicon\nsolid phase\nTRISO particle\nvapour\nvapour phase\nvapour pressure of carbon\n"}, {"id": "S0022311514001640", "text": "The vapour phase consists of a number of different gases with silicon exhibiting a far higher partial pressure than all carbon containing species over the full temperature range. As an immediate result the vapour contains a higher amount of silicon leaving the solid phase with excess carbon. This carbon is likely to precipitate on the surface of the SiC grains, a process that becomes very rapid as the temperature approaches 2100K [24]. Within the TRISO particle the SiC layer is sandwiched between two coatings of dense carbon. The partial pressure in thermodynamic equilibrium of gaseous carbon forming above graphite was calculated using data taken from JANAF tables and added to Fig. 1 [25], which showed that in the whole temperature range relevant for this study the vapour pressure of carbon is several magnitudes smaller than that of the dominant gas phases above SiC.\n", "keywords": "carbon\ndata taken from JANAF tables\ngas\ngaseous carbon\ngraphite\nnumber of different gases\npartial pressure\nprecipitate on the surface of the SiC grains\nSiC\nSiC grains\nsilicon\nsolid phase\nTRISO particle\nvapour\nvapour phase\nvapour pressure of carbon\n"}, {"id": "S0022311514001640", "text": "The vapour phase consists of a number of different gases with silicon exhibiting a far higher partial pressure than all carbon containing species over the full temperature range. As an immediate result the vapour contains a higher amount of silicon leaving the solid phase with excess carbon. This carbon is likely to precipitate on the surface of the SiC grains, a process that becomes very rapid as the temperature approaches 2100K [24]. Within the TRISO particle the SiC layer is sandwiched between two coatings of dense carbon. The partial pressure in thermodynamic equilibrium of gaseous carbon forming above graphite was calculated using data taken from JANAF tables and added to Fig. 1 [25], which showed that in the whole temperature range relevant for this study the vapour pressure of carbon is several magnitudes smaller than that of the dominant gas phases above SiC.\n", "keywords": "carbon\ndata taken from JANAF tables\ngas\ngaseous carbon\ngraphite\nnumber of different gases\npartial pressure\nprecipitate on the surface of the SiC grains\nSiC\nSiC grains\nsilicon\nsolid phase\nTRISO particle\nvapour\nvapour phase\nvapour pressure of carbon\n"}, {"id": "S0022311514001640", "text": "The vapour phase consists of a number of different gases with silicon exhibiting a far higher partial pressure than all carbon containing species over the full temperature range. As an immediate result the vapour contains a higher amount of silicon leaving the solid phase with excess carbon. This carbon is likely to precipitate on the surface of the SiC grains, a process that becomes very rapid as the temperature approaches 2100K [24]. Within the TRISO particle the SiC layer is sandwiched between two coatings of dense carbon. The partial pressure in thermodynamic equilibrium of gaseous carbon forming above graphite was calculated using data taken from JANAF tables and added to Fig. 1 [25], which showed that in the whole temperature range relevant for this study the vapour pressure of carbon is several magnitudes smaller than that of the dominant gas phases above SiC.\n", "keywords": "carbon\ndata taken from JANAF tables\ngas\ngaseous carbon\ngraphite\nnumber of different gases\npartial pressure\nprecipitate on the surface of the SiC grains\nSiC\nSiC grains\nsilicon\nsolid phase\nTRISO particle\nvapour\nvapour phase\nvapour pressure of carbon\n"}, {"id": "S0022311514001640", "text": "The vapour phase consists of a number of different gases with silicon exhibiting a far higher partial pressure than all carbon containing species over the full temperature range. As an immediate result the vapour contains a higher amount of silicon leaving the solid phase with excess carbon. This carbon is likely to precipitate on the surface of the SiC grains, a process that becomes very rapid as the temperature approaches 2100K [24]. Within the TRISO particle the SiC layer is sandwiched between two coatings of dense carbon. The partial pressure in thermodynamic equilibrium of gaseous carbon forming above graphite was calculated using data taken from JANAF tables and added to Fig. 1 [25], which showed that in the whole temperature range relevant for this study the vapour pressure of carbon is several magnitudes smaller than that of the dominant gas phases above SiC.\n", "keywords": "carbon\ndata taken from JANAF tables\ngas\ngaseous carbon\ngraphite\nnumber of different gases\npartial pressure\nprecipitate on the surface of the SiC grains\nSiC\nSiC grains\nsilicon\nsolid phase\nTRISO particle\nvapour\nvapour phase\nvapour pressure of carbon\n"}, {"id": "S0022311514001640", "text": "The vapour phase consists of a number of different gases with silicon exhibiting a far higher partial pressure than all carbon containing species over the full temperature range. As an immediate result the vapour contains a higher amount of silicon leaving the solid phase with excess carbon. This carbon is likely to precipitate on the surface of the SiC grains, a process that becomes very rapid as the temperature approaches 2100K [24]. Within the TRISO particle the SiC layer is sandwiched between two coatings of dense carbon. The partial pressure in thermodynamic equilibrium of gaseous carbon forming above graphite was calculated using data taken from JANAF tables and added to Fig. 1 [25], which showed that in the whole temperature range relevant for this study the vapour pressure of carbon is several magnitudes smaller than that of the dominant gas phases above SiC.\n", "keywords": "carbon\ndata taken from JANAF tables\ngas\ngaseous carbon\ngraphite\nnumber of different gases\npartial pressure\nprecipitate on the surface of the SiC grains\nSiC\nSiC grains\nsilicon\nsolid phase\nTRISO particle\nvapour\nvapour phase\nvapour pressure of carbon\n"}, {"id": "S0022311514001640", "text": "The vapour phase consists of a number of different gases with silicon exhibiting a far higher partial pressure than all carbon containing species over the full temperature range. As an immediate result the vapour contains a higher amount of silicon leaving the solid phase with excess carbon. This carbon is likely to precipitate on the surface of the SiC grains, a process that becomes very rapid as the temperature approaches 2100K [24]. Within the TRISO particle the SiC layer is sandwiched between two coatings of dense carbon. The partial pressure in thermodynamic equilibrium of gaseous carbon forming above graphite was calculated using data taken from JANAF tables and added to Fig. 1 [25], which showed that in the whole temperature range relevant for this study the vapour pressure of carbon is several magnitudes smaller than that of the dominant gas phases above SiC.\n", "keywords": "carbon\ndata taken from JANAF tables\ngas\ngaseous carbon\ngraphite\nnumber of different gases\npartial pressure\nprecipitate on the surface of the SiC grains\nSiC\nSiC grains\nsilicon\nsolid phase\nTRISO particle\nvapour\nvapour phase\nvapour pressure of carbon\n"}, {"id": "S2212667814000690", "text": "The paper presents the results of studies of the effect of multiwalled carbon nanotubes 18-20nm in concentrations of 1 and 10mg / ml for diatoms Pseudo-nitzschia pungens (clone PP-07) and golden alga Isochrysis galbana (clone TISO). The toxic effects of multiwalled nanotubes on both types of algae is revealed, which results in a decrease of the linear dimensions of cells, chloroplasts, and a reduced number of cells when incubated over 24h (Pseudo-nitzschia pungens) and 36hours (Isochrysis galbana).", "keywords": "algae\ncarbon nanotubes\ncells\nchloroplasts\nclone PP-07\nclone TISO\ngolden alga Isochrysis galbana\nincubated\nIsochrysis galbana\nnanotubes\npresents the results of studies of the effect of multiwalled carbon nanotubes\nPseudo-nitzschia pungens\nstudies\ntoxic effects\n"}, {"id": "S0301932214001931", "text": "The aim of this paper is to investigate the influence of the particle shape on interacting particles flowing in a horizontal turbulent channel flow, for particles with a significant Stokes number. To achieve this, large eddy simulations (LES) of a horizontal turbulent channel flow laden with five different particle shapes, incorporating the drag, lift and toque model derived in Zastawny et al. (2012), are performed. The well-documented horizontal channel flow case described in Kussin and Sommerfeld (2002), who study spherical particles, is used as a reference case. The measurements in their work was done with phase Doppler anemometry (PDA), to measure the fluid and particle velocity simultaneously. The numerical framework applied in this paper has been previously validated for spherical particles in Mallouppas and van Wachem (2013). In that paper, it is shown that the comprehensive discrete element model (DEM) is more accurate in determining the behaviour of the particles in this horizontal gas\u2013solid channel flow that the hard-sphere model. Moreover, this paper showed that the fluid mechanics are accurately modelled using the LES framework. In the current paper, this framework is extended to account for non-spherical particles.\n", "keywords": "comprehensive discrete element model\nDEM\ndetermining the behaviour of the particles in this horizontal gas\u2013solid channel flow\ndrag, lift and toque model\nfluid\ngas\ngas\u2013solid channel flow\nhard-sphere model\nhorizontal channel flow\nhorizontal turbulent channel flow\ninvestigate the influence of the particle shape on interacting particles flowing in a horizontal turbulent channel flow\nlarge eddy simulations\nLES\nLES framework\nnon-spherical particles\nnumerical framework\nparticle\nparticles\nPDA\nphase Doppler anemometry\nspherical particles\n"}, {"id": "S000926141301539X", "text": "In this Letter we revisit the Chesnavich model Hamiltonian [37] in the light of recent developments in TST. For barrierless systems such as ion\u2013molecule reactions, the concepts of OTS and TTS can be clearly formulated in terms of well defined phase space geometrical objects. (For work on the phase space description of OTS, see Refs. [38\u201340].) The first goal of the present article is the identification of these notions with well defined phase space dividing surfaces attached to NHIMs. The second and main goal is an elucidation of the roaming phenomenon in the context of the Chesnavich model Hamiltonian. The associated potential function, possessing many features associated with a realistic molecular PES, leads to dynamics which clearly reveal the origins of the roaming effect. Based on our trajectory simulations, we show how the identification of the TTS and OTS DSs with periodic orbit dividing surfaces (PODS) provides the natural framework for analysis of the roaming mechanism.\n", "keywords": "associated potential function\nChesnavich model Hamiltonian\ndynamics which clearly reveal the origins of the roaming effect\nidentification\nion\u2013molecule reactions\nNHIMs\nOTS\nperiodic orbit dividing surfaces\nPODS\nspace geometrical objects\ntrajectory simulations\nTST\nTTS\nwell defined phase space dividing surfaces attached to NHIMs\n"}, {"id": "S2212667812000536", "text": "According to the situation that the IT students can not meet the software industry demand for qualified personnel, a \u201ctriple-driven\u201d three-dimensional software development practical teaching system was proposed, aiming to improve the software development capabilities and innovation sense of students. This system can effectively improve students the interest of software development and the practical skills and sense of innovation, laying a solid foundation for student after graduation to rapidly integrate into the software development process, meeting the needs of software industry.", "keywords": "improve the software development capabilities and innovation sense of students\npractical skills\nsoftware development\nsoftware development process\nsoftware industry\n\u201ctriple-driven\u201d three-dimensional software development practical teaching system\n"}, {"id": "S0370269304008421", "text": "The expression for Pc is also easily found in the same basis, where it becomes apparent that the dynamics of conversion in matter depends only on the relative orientation of the eigenstates of the vacuum and matter Hamiltonians. This allows to directly apply the known analytical solutions for Pc, and, upon rotating back, obtain a generalization of these results to the NSI case. For example, the answer for the infinite exponential profile [18,19] A\u221dexp(\u2212r/r0) becomes Pc=exp[\u03b3(1\u2212cos2\u03b8rel)/2]\u22121exp(\u03b3)\u22121, where \u03b3\u22614\u03c0r0\u0394=\u03c0r0\u0394m2/E\u03bd. We further observe that since \u03b3\u2aa21 the adiabaticity violation occurs only when |\u03b8\u2212\u03b1|\u2aa11 and \u03c6\u2243\u03c0/2, which is the analogue of the small-angle MSW [10,20] effect in the rotated basis. The \u201cresonant\u201d region in the Sun where level jumping can take place is narrow, defined by A\u2243\u0394 [21]. A neutrino produced at a lower density evolves adiabatically, while a neutrino produced at a higher density may undergo level crossing. The probability Pc in the latter case is given to a very good accuracy by the formula for the linear profile, with an appropriate gradient taken along the neutrino trajectory, (12)Pc\u2243\u0398(A\u2212\u0394)e\u2212\u03b3(cos2\u03b8rel+1)/2, where \u0398(x) is the step function, \u0398(x)=1 for x>0 and \u0398(x)=0 otherwise. We emphasize that our results differ from the similar ones given in [5,22] in three important respects: (i) they are valid for all, not just small values of \u03b1 (which is essential for our application), (ii) they include the angle \u03c6, and (iii) the argument of the \u0398 function does not contain cos2\u03b8, as follows from [21]. We stress that for large values of \u03b1 and \u03c6\u2243\u03c0/2 adiabaticity is violated for large values of\u00a0\u03b8.\n", "keywords": "adiabaticity violation\nanalytical solutions for Pc\nconversion in matter\ngeneralization of these results to the NSI case\nmatter Hamiltonians\nneutrino\nNSI\nPc\nrelative orientation of the eigenstates\nvacuum\n"}, {"id": "S0370269304008421", "text": "The expression for Pc is also easily found in the same basis, where it becomes apparent that the dynamics of conversion in matter depends only on the relative orientation of the eigenstates of the vacuum and matter Hamiltonians. This allows to directly apply the known analytical solutions for Pc, and, upon rotating back, obtain a generalization of these results to the NSI case. For example, the answer for the infinite exponential profile [18,19] A\u221dexp(\u2212r/r0) becomes Pc=exp[\u03b3(1\u2212cos2\u03b8rel)/2]\u22121exp(\u03b3)\u22121, where \u03b3\u22614\u03c0r0\u0394=\u03c0r0\u0394m2/E\u03bd. We further observe that since \u03b3\u2aa21 the adiabaticity violation occurs only when |\u03b8\u2212\u03b1|\u2aa11 and \u03c6\u2243\u03c0/2, which is the analogue of the small-angle MSW [10,20] effect in the rotated basis. The \u201cresonant\u201d region in the Sun where level jumping can take place is narrow, defined by A\u2243\u0394 [21]. A neutrino produced at a lower density evolves adiabatically, while a neutrino produced at a higher density may undergo level crossing. The probability Pc in the latter case is given to a very good accuracy by the formula for the linear profile, with an appropriate gradient taken along the neutrino trajectory, (12)Pc\u2243\u0398(A\u2212\u0394)e\u2212\u03b3(cos2\u03b8rel+1)/2, where \u0398(x) is the step function, \u0398(x)=1 for x>0 and \u0398(x)=0 otherwise. We emphasize that our results differ from the similar ones given in [5,22] in three important respects: (i) they are valid for all, not just small values of \u03b1 (which is essential for our application), (ii) they include the angle \u03c6, and (iii) the argument of the \u0398 function does not contain cos2\u03b8, as follows from [21]. We stress that for large values of \u03b1 and \u03c6\u2243\u03c0/2 adiabaticity is violated for large values of\u00a0\u03b8.\n", "keywords": "adiabaticity violation\nanalytical solutions for Pc\nconversion in matter\ngeneralization of these results to the NSI case\nmatter Hamiltonians\nneutrino\nNSI\nPc\nrelative orientation of the eigenstates\nvacuum\n"}, {"id": "S0301932213000487", "text": "As already discussed, in dilute flows the choice between the hard sphere and soft sphere models largely depends on the computational time spent to solve the particle equation of motion. For very dilute flows, the hard sphere model is the most natural choice. However, when the collisions can no longer be assumed as binary and instantaneous, the soft sphere model is the only realistic option. It is interesting to know whether the choice of the collision model affects the statistics. Fig. 14 compares the mean velocity obtained from both models with the experimental data. The same comparison is performed for the smooth walls. The differences between the hard and soft sphere models for the smooth walls are almost negligible. However, the differences between the hard and soft sphere models for the rough walls are minor. This is because the rough wall treatment in the soft sphere implementation adds extra virtual walls during the collision of a particle with a wall, which is a more realistic representation of a rough wall compared to the hard sphere rough wall treatment where one random wall is considered. This is because, a soft sphere collision is not instantaneous and occurs over a finite amount of time. Similarly, the same effects are observed on the fluid statistics. However, Fig. 15, which compares the particle velocity fluctuations, shows that the differences are somewhat larger. Additionally, the differences in both particle mean and RMS velocity profiles are because the hard sphere collisions are unfortunately heavily dependent on the tangential coefficient of restitution (\u03c8); the effects by varying this quantity are shown in Figs. 16 and 17.\n", "keywords": "collision model\ndilute flows\nfluid\nhard and soft sphere models\nhard sphere and soft sphere models\nhard sphere collisions\nhard sphere model\nhard sphere rough wall treatment\nparticle\nrandom wall\nrough wall\nrough walls\nrough wall treatment\nsmooth walls\nsoft sphere collision\nsoft sphere implementation\nsoft sphere model\nsolve the particle equation of motion\nvirtual walls\nwall\n"}, {"id": "S0375960113004568", "text": "The length effect is always important in nanodevices. So we investigate the length dependence of electronic transport properties in M3 by increasing the number of carbon unit cells in the scattering region. Here we present the transport results when the numbers of carbon unit cells in the scattering region are 10 and 12, which are called M4 and M5, respectively. The current\u2013voltage characteristics shown in Fig. 8. We can see that the large rectifying ratio still can be observed irrespective of the length of heterojunctions. This is due to the fact that the electronic transport properties for M3 are mainly determined by the parity of the \u03c0 and \u03c0\u204e subbands of left and right electrodes. Thees results indicate that the lengths of the two parts in the scattering regions have no affects on the qualitative charge transport in M3.\n", "keywords": "carbon unit cells\ncurrent\u2013voltage characteristics\nelectronic transport properties\nincreasing the number of carbon unit cells\nlarge rectifying ratio\nlength dependence of electronic transport properties\nlength effect\nnanodevices\nqualitative charge transport\n\u03c0 and \u03c0\u204e subbands\n"}, {"id": "S0375960113004568", "text": "The length effect is always important in nanodevices. So we investigate the length dependence of electronic transport properties in M3 by increasing the number of carbon unit cells in the scattering region. Here we present the transport results when the numbers of carbon unit cells in the scattering region are 10 and 12, which are called M4 and M5, respectively. The current\u2013voltage characteristics shown in Fig. 8. We can see that the large rectifying ratio still can be observed irrespective of the length of heterojunctions. This is due to the fact that the electronic transport properties for M3 are mainly determined by the parity of the \u03c0 and \u03c0\u204e subbands of left and right electrodes. Thees results indicate that the lengths of the two parts in the scattering regions have no affects on the qualitative charge transport in M3.\n", "keywords": "carbon unit cells\ncurrent\u2013voltage characteristics\nelectronic transport properties\nincreasing the number of carbon unit cells\nlarge rectifying ratio\nlength dependence of electronic transport properties\nlength effect\nnanodevices\nqualitative charge transport\n\u03c0 and \u03c0\u204e subbands\n"}, {"id": "S0375960113004568", "text": "The length effect is always important in nanodevices. So we investigate the length dependence of electronic transport properties in M3 by increasing the number of carbon unit cells in the scattering region. Here we present the transport results when the numbers of carbon unit cells in the scattering region are 10 and 12, which are called M4 and M5, respectively. The current\u2013voltage characteristics shown in Fig. 8. We can see that the large rectifying ratio still can be observed irrespective of the length of heterojunctions. This is due to the fact that the electronic transport properties for M3 are mainly determined by the parity of the \u03c0 and \u03c0\u204e subbands of left and right electrodes. Thees results indicate that the lengths of the two parts in the scattering regions have no affects on the qualitative charge transport in M3.\n", "keywords": "carbon unit cells\ncurrent\u2013voltage characteristics\nelectronic transport properties\nincreasing the number of carbon unit cells\nlarge rectifying ratio\nlength dependence of electronic transport properties\nlength effect\nnanodevices\nqualitative charge transport\n\u03c0 and \u03c0\u204e subbands\n"}, {"id": "S0375960113004568", "text": "The length effect is always important in nanodevices. So we investigate the length dependence of electronic transport properties in M3 by increasing the number of carbon unit cells in the scattering region. Here we present the transport results when the numbers of carbon unit cells in the scattering region are 10 and 12, which are called M4 and M5, respectively. The current\u2013voltage characteristics shown in Fig. 8. We can see that the large rectifying ratio still can be observed irrespective of the length of heterojunctions. This is due to the fact that the electronic transport properties for M3 are mainly determined by the parity of the \u03c0 and \u03c0\u204e subbands of left and right electrodes. Thees results indicate that the lengths of the two parts in the scattering regions have no affects on the qualitative charge transport in M3.\n", "keywords": "carbon unit cells\ncurrent\u2013voltage characteristics\nelectronic transport properties\nincreasing the number of carbon unit cells\nlarge rectifying ratio\nlength dependence of electronic transport properties\nlength effect\nnanodevices\nqualitative charge transport\n\u03c0 and \u03c0\u204e subbands\n"}, {"id": "S0375960113004568", "text": "The length effect is always important in nanodevices. So we investigate the length dependence of electronic transport properties in M3 by increasing the number of carbon unit cells in the scattering region. Here we present the transport results when the numbers of carbon unit cells in the scattering region are 10 and 12, which are called M4 and M5, respectively. The current\u2013voltage characteristics shown in Fig. 8. We can see that the large rectifying ratio still can be observed irrespective of the length of heterojunctions. This is due to the fact that the electronic transport properties for M3 are mainly determined by the parity of the \u03c0 and \u03c0\u204e subbands of left and right electrodes. Thees results indicate that the lengths of the two parts in the scattering regions have no affects on the qualitative charge transport in M3.\n", "keywords": "carbon unit cells\ncurrent\u2013voltage characteristics\nelectronic transport properties\nincreasing the number of carbon unit cells\nlarge rectifying ratio\nlength dependence of electronic transport properties\nlength effect\nnanodevices\nqualitative charge transport\n\u03c0 and \u03c0\u204e subbands\n"}, {"id": "S0021999113002362", "text": "The algorithm allows the modelling of plasmas of arbitrary degeneracy under the binary collision approximation. It uses a numerical interpolation of the inverse cumulative density function of the Fermi\u2013Dirac distribution to initialise simulation particles, and collisions are subject to Pauli blocking. It is not appropriate in the limit of very strong coupling because the plasma theory which the Monte Carlo code is based on breaks down. The strong coupling limit corresponds to ln\u039b\u22723, with ln\u039b the Coulomb logarithm [10]. The code is designed for ln\u039b>3 in collisional plasmas with a non-negligible level of degeneracy. It is noted that Monte Carlo techniques with degenerate capabilities have been developed for studying transport in semi-conductors [11] but no such method exists for fully-ionised plasmas. Some of the techniques described are potentially applicable to other types of codes, for example, Particle-In-Cell (PIC) codes.\n", "keywords": "binary collision approximation\ncollisional plasmas\ncollisions\nCoulomb logarithm\ncoupling\nFermi\u2013Dirac distribution\nfully-ionised plasmas\nmodelling of plasmas of arbitrary degeneracy\nMonte Carlo code\nMonte Carlo techniques\nnumerical interpolation\nPauli blocking\nplasmas\nplasma theory\nsimulation particles\nstrong coupling limit\ntransport in semi-conductors\n"}, {"id": "S0021999113002362", "text": "The algorithm allows the modelling of plasmas of arbitrary degeneracy under the binary collision approximation. It uses a numerical interpolation of the inverse cumulative density function of the Fermi\u2013Dirac distribution to initialise simulation particles, and collisions are subject to Pauli blocking. It is not appropriate in the limit of very strong coupling because the plasma theory which the Monte Carlo code is based on breaks down. The strong coupling limit corresponds to ln\u039b\u22723, with ln\u039b the Coulomb logarithm [10]. The code is designed for ln\u039b>3 in collisional plasmas with a non-negligible level of degeneracy. It is noted that Monte Carlo techniques with degenerate capabilities have been developed for studying transport in semi-conductors [11] but no such method exists for fully-ionised plasmas. Some of the techniques described are potentially applicable to other types of codes, for example, Particle-In-Cell (PIC) codes.\n", "keywords": "binary collision approximation\ncollisional plasmas\ncollisions\nCoulomb logarithm\ncoupling\nFermi\u2013Dirac distribution\nfully-ionised plasmas\nmodelling of plasmas of arbitrary degeneracy\nMonte Carlo code\nMonte Carlo techniques\nnumerical interpolation\nPauli blocking\nplasmas\nplasma theory\nsimulation particles\nstrong coupling limit\ntransport in semi-conductors\n"}, {"id": "S0021999113002362", "text": "The algorithm allows the modelling of plasmas of arbitrary degeneracy under the binary collision approximation. It uses a numerical interpolation of the inverse cumulative density function of the Fermi\u2013Dirac distribution to initialise simulation particles, and collisions are subject to Pauli blocking. It is not appropriate in the limit of very strong coupling because the plasma theory which the Monte Carlo code is based on breaks down. The strong coupling limit corresponds to ln\u039b\u22723, with ln\u039b the Coulomb logarithm [10]. The code is designed for ln\u039b>3 in collisional plasmas with a non-negligible level of degeneracy. It is noted that Monte Carlo techniques with degenerate capabilities have been developed for studying transport in semi-conductors [11] but no such method exists for fully-ionised plasmas. Some of the techniques described are potentially applicable to other types of codes, for example, Particle-In-Cell (PIC) codes.\n", "keywords": "binary collision approximation\ncollisional plasmas\ncollisions\nCoulomb logarithm\ncoupling\nFermi\u2013Dirac distribution\nfully-ionised plasmas\nmodelling of plasmas of arbitrary degeneracy\nMonte Carlo code\nMonte Carlo techniques\nnumerical interpolation\nPauli blocking\nplasmas\nplasma theory\nsimulation particles\nstrong coupling limit\ntransport in semi-conductors\n"}, {"id": "S0021999113002362", "text": "The algorithm allows the modelling of plasmas of arbitrary degeneracy under the binary collision approximation. It uses a numerical interpolation of the inverse cumulative density function of the Fermi\u2013Dirac distribution to initialise simulation particles, and collisions are subject to Pauli blocking. It is not appropriate in the limit of very strong coupling because the plasma theory which the Monte Carlo code is based on breaks down. The strong coupling limit corresponds to ln\u039b\u22723, with ln\u039b the Coulomb logarithm [10]. The code is designed for ln\u039b>3 in collisional plasmas with a non-negligible level of degeneracy. It is noted that Monte Carlo techniques with degenerate capabilities have been developed for studying transport in semi-conductors [11] but no such method exists for fully-ionised plasmas. Some of the techniques described are potentially applicable to other types of codes, for example, Particle-In-Cell (PIC) codes.\n", "keywords": "binary collision approximation\ncollisional plasmas\ncollisions\nCoulomb logarithm\ncoupling\nFermi\u2013Dirac distribution\nfully-ionised plasmas\nmodelling of plasmas of arbitrary degeneracy\nMonte Carlo code\nMonte Carlo techniques\nnumerical interpolation\nPauli blocking\nplasmas\nplasma theory\nsimulation particles\nstrong coupling limit\ntransport in semi-conductors\n"}, {"id": "S0022311513010313", "text": "The four bounding PCM wastes, given in Table 1, were simulated using the most appropriate materials and geometries. \u201cMock up\u201d PCM drums were assembled using the following components: PCM drums were simulated using mild steel paint cans and lids (Fenton Packaging Ltd.); PVC bags were replicated using identical PVC sheeting (Romar Workwear Ltd.); the metallic waste was simulated using commercial grade 18/8 stainless steel, aluminium and copper (Avus Metals & Plastics Ltd.), and lead shot (Aldrich); the inorganic waste was simulated using waste Pyrex labware, crushed masonry, concrete and window glass; CeO2 (from Acros Organics, >99.9%; dried 15h at 600\u00b0C) was used as a PuO2 surrogate. Commercially available ground, granulated blast-furnace slag \u201cCalumite\u201d was used as an additive [27]. The analysed chemical composition is given in Table 3. Calumite is a powdered material, with a typical particle size distribution between limits of ca. 40 to ca. 400\u03bcm.\n", "keywords": "aluminium\nblast-furnace slag \u201cCalumite\u201d\nbounding PCM wastes\nCalumite\nCeO2\ncommercial grade 18/8 stainless steel\nconcrete\ncopper\nidentical PVC sheeting\ninorganic waste\nlead shot\nmasonry\nmetallic waste\nmild steel paint cans and lids\n\u201cMock up\u201d PCM drums\nparticle\nPCM drums\npowdered material\nPuO2 surrogate\nPVC bags\nwaste Pyrex labware\nwindow glass\n"}, {"id": "S0022311513010313", "text": "The four bounding PCM wastes, given in Table 1, were simulated using the most appropriate materials and geometries. \u201cMock up\u201d PCM drums were assembled using the following components: PCM drums were simulated using mild steel paint cans and lids (Fenton Packaging Ltd.); PVC bags were replicated using identical PVC sheeting (Romar Workwear Ltd.); the metallic waste was simulated using commercial grade 18/8 stainless steel, aluminium and copper (Avus Metals & Plastics Ltd.), and lead shot (Aldrich); the inorganic waste was simulated using waste Pyrex labware, crushed masonry, concrete and window glass; CeO2 (from Acros Organics, >99.9%; dried 15h at 600\u00b0C) was used as a PuO2 surrogate. Commercially available ground, granulated blast-furnace slag \u201cCalumite\u201d was used as an additive [27]. The analysed chemical composition is given in Table 3. Calumite is a powdered material, with a typical particle size distribution between limits of ca. 40 to ca. 400\u03bcm.\n", "keywords": "aluminium\nblast-furnace slag \u201cCalumite\u201d\nbounding PCM wastes\nCalumite\nCeO2\ncommercial grade 18/8 stainless steel\nconcrete\ncopper\nidentical PVC sheeting\ninorganic waste\nlead shot\nmasonry\nmetallic waste\nmild steel paint cans and lids\n\u201cMock up\u201d PCM drums\nparticle\nPCM drums\npowdered material\nPuO2 surrogate\nPVC bags\nwaste Pyrex labware\nwindow glass\n"}, {"id": "S0038092X11004129", "text": "Assuming a constant cell electrical conversion efficiency of 15%, a constant fraction of the incident solar radiation would be dissipated by the solar cell for each solar radiation intensity level. From Table 2, it can be seen that for the worst scenario when the ambient temperature was 50\u00b0C with natural convection only, the predicted cell electrical conversion efficiency would have reduced to approximately 8% rather than the 15% assumed. The energy dissipated as heat from the cell would thus be 7% higher. To correct for this effect the apparent insolation level should be modified using the following formula:(3)Iact=I\u00d70.850.85+d\u03b7where Iact is the actual incident solar radiation intensity, d\u03b7 is solar cell efficiency difference between the initially assumed 15% and final calculated cell efficiency based on measured cell temperature.\n", "keywords": "(3)Iact=I\u00d70.850.85+d\u03b7\napparent insolation level\ncell electrical conversion\nnatural convection\npredicted cell electrical conversion efficiency\nsolar radiation\n"}, {"id": "S0370269304008780", "text": "In the NJL model studied here, we find no stable stars with either CFL or normal quark matter cores. This is the opposite of the prediction of Ref.\u00a0[15] where it was argued that there is no 2SC phase in compact stars. Let us be more precise: performing a Taylor expansion in the strange quark mass, the authors of Ref.\u00a0[15] found that in beta-equilibrated electrically and color neutral quark matter the 2SC phase is always less favored than the CFL phase or normal quark matter. From this observation they concluded that the 2SC phase is absent in compact stars. In contrast to this result, it was shown in Ref.\u00a0[16] in the framework of the NJL model that neutral 2SC matter could be the most favored quark phase in a certain regime. However, the authors argued that this interval might disappear if the hadronic phase is included more properly. This is indeed what we found for parameter set RKH, while for parameter set HK the 2SC phase survives only in a tiny window. Nevertheless, if Nature chooses to be similar to this equation of state, it will be this tiny window which gives rise to hybrid stars, whereas the CFL phase would be never present in compact stars.\n", "keywords": "2SC phase\nbeta-equilibrated electrically\nCFL or normal quark matter cores\nCFL phase\ncolor neutral quark matter\ncompact stars\nhadronic phase\nhybrid stars\nneutral 2SC matter\nNJL model\nNJL model studied\nnormal quark matter\nquark phase\nstable stars\nstrange quark mass\nTaylor expansion\n"}, {"id": "S0370269304008780", "text": "In the NJL model studied here, we find no stable stars with either CFL or normal quark matter cores. This is the opposite of the prediction of Ref.\u00a0[15] where it was argued that there is no 2SC phase in compact stars. Let us be more precise: performing a Taylor expansion in the strange quark mass, the authors of Ref.\u00a0[15] found that in beta-equilibrated electrically and color neutral quark matter the 2SC phase is always less favored than the CFL phase or normal quark matter. From this observation they concluded that the 2SC phase is absent in compact stars. In contrast to this result, it was shown in Ref.\u00a0[16] in the framework of the NJL model that neutral 2SC matter could be the most favored quark phase in a certain regime. However, the authors argued that this interval might disappear if the hadronic phase is included more properly. This is indeed what we found for parameter set RKH, while for parameter set HK the 2SC phase survives only in a tiny window. Nevertheless, if Nature chooses to be similar to this equation of state, it will be this tiny window which gives rise to hybrid stars, whereas the CFL phase would be never present in compact stars.\n", "keywords": "2SC phase\nbeta-equilibrated electrically\nCFL or normal quark matter cores\nCFL phase\ncolor neutral quark matter\ncompact stars\nhadronic phase\nhybrid stars\nneutral 2SC matter\nNJL model\nNJL model studied\nnormal quark matter\nquark phase\nstable stars\nstrange quark mass\nTaylor expansion\n"}, {"id": "S0370269304008780", "text": "In the NJL model studied here, we find no stable stars with either CFL or normal quark matter cores. This is the opposite of the prediction of Ref.\u00a0[15] where it was argued that there is no 2SC phase in compact stars. Let us be more precise: performing a Taylor expansion in the strange quark mass, the authors of Ref.\u00a0[15] found that in beta-equilibrated electrically and color neutral quark matter the 2SC phase is always less favored than the CFL phase or normal quark matter. From this observation they concluded that the 2SC phase is absent in compact stars. In contrast to this result, it was shown in Ref.\u00a0[16] in the framework of the NJL model that neutral 2SC matter could be the most favored quark phase in a certain regime. However, the authors argued that this interval might disappear if the hadronic phase is included more properly. This is indeed what we found for parameter set RKH, while for parameter set HK the 2SC phase survives only in a tiny window. Nevertheless, if Nature chooses to be similar to this equation of state, it will be this tiny window which gives rise to hybrid stars, whereas the CFL phase would be never present in compact stars.\n", "keywords": "2SC phase\nbeta-equilibrated electrically\nCFL or normal quark matter cores\nCFL phase\ncolor neutral quark matter\ncompact stars\nhadronic phase\nhybrid stars\nneutral 2SC matter\nNJL model\nNJL model studied\nnormal quark matter\nquark phase\nstable stars\nstrange quark mass\nTaylor expansion\n"}, {"id": "S0370269304008780", "text": "In the NJL model studied here, we find no stable stars with either CFL or normal quark matter cores. This is the opposite of the prediction of Ref.\u00a0[15] where it was argued that there is no 2SC phase in compact stars. Let us be more precise: performing a Taylor expansion in the strange quark mass, the authors of Ref.\u00a0[15] found that in beta-equilibrated electrically and color neutral quark matter the 2SC phase is always less favored than the CFL phase or normal quark matter. From this observation they concluded that the 2SC phase is absent in compact stars. In contrast to this result, it was shown in Ref.\u00a0[16] in the framework of the NJL model that neutral 2SC matter could be the most favored quark phase in a certain regime. However, the authors argued that this interval might disappear if the hadronic phase is included more properly. This is indeed what we found for parameter set RKH, while for parameter set HK the 2SC phase survives only in a tiny window. Nevertheless, if Nature chooses to be similar to this equation of state, it will be this tiny window which gives rise to hybrid stars, whereas the CFL phase would be never present in compact stars.\n", "keywords": "2SC phase\nbeta-equilibrated electrically\nCFL or normal quark matter cores\nCFL phase\ncolor neutral quark matter\ncompact stars\nhadronic phase\nhybrid stars\nneutral 2SC matter\nNJL model\nNJL model studied\nnormal quark matter\nquark phase\nstable stars\nstrange quark mass\nTaylor expansion\n"}, {"id": "S0370269304008780", "text": "In the NJL model studied here, we find no stable stars with either CFL or normal quark matter cores. This is the opposite of the prediction of Ref.\u00a0[15] where it was argued that there is no 2SC phase in compact stars. Let us be more precise: performing a Taylor expansion in the strange quark mass, the authors of Ref.\u00a0[15] found that in beta-equilibrated electrically and color neutral quark matter the 2SC phase is always less favored than the CFL phase or normal quark matter. From this observation they concluded that the 2SC phase is absent in compact stars. In contrast to this result, it was shown in Ref.\u00a0[16] in the framework of the NJL model that neutral 2SC matter could be the most favored quark phase in a certain regime. However, the authors argued that this interval might disappear if the hadronic phase is included more properly. This is indeed what we found for parameter set RKH, while for parameter set HK the 2SC phase survives only in a tiny window. Nevertheless, if Nature chooses to be similar to this equation of state, it will be this tiny window which gives rise to hybrid stars, whereas the CFL phase would be never present in compact stars.\n", "keywords": "2SC phase\nbeta-equilibrated electrically\nCFL or normal quark matter cores\nCFL phase\ncolor neutral quark matter\ncompact stars\nhadronic phase\nhybrid stars\nneutral 2SC matter\nNJL model\nNJL model studied\nnormal quark matter\nquark phase\nstable stars\nstrange quark mass\nTaylor expansion\n"}, {"id": "S0925838814009669", "text": "SPS has been utilized in several studies to retain the nanostructure of aluminum alloy powders during consolidation. Ye et al. investigated the effect of processing of cryomilled Al 5083 powder via SPS [13]. X-ray Diffraction (XRD) grain size calculations before and after SPS showed that the average grain size of the alloy only increased from 25nm to 50nm (from powder to bulk state). Subsequently, the hardness values obtained through nanoindentation for specimens of AA5083 produced via SPS were highly improved in comparison to conventional sintering methods were grain coarsening takes place on a larger scale. In another study the combination of cryomilling and SPS of AA-5356/B4C nanocomposites powder was found to largely improve the microhardness and flexural strengths of the bulk nanocomposite. Rana et al. [14] investigated the effect of SPS on mechanically milled AA6061 (Al\u2013Mg\u2013Si) micro-alloy powder. The average grain size after 20h of milling was \u223c35nm and increased to only \u223c85nm after processing with SPS at 500\u00b0C. Microhardness and compressive tests were carried out on the consolidated near full density specimens of both unmilled and milled powders and the results showed significant increase in both hardness and compressive strengths for the milled nanocrystalline powders as a result of the very fine grain size.\n", "keywords": "AA5083\nAA-5356/B4C nanocomposites powder\nalloy\nAl\u2013Mg\u2013Si\naluminum alloy powders\nbulk nanocomposite\ncombination of cryomilling and SPS of AA-5356/B4C nanocomposites powder\nconsolidation\ncryomilled Al 5083 powder\ncryomilling\ninvestigated the effect of processing of cryomilled Al 5083 powder\ninvestigated the effect of SPS on mechanically milled AA6061 (Al\u2013Mg\u2013Si) micro-alloy powder\nmechanically milled AA6061\nmechanically milled AA6061 (Al\u2013Mg\u2013Si) micro-alloy powder\nmilled nanocrystalline powders\nnanoindentation\nnanostructure of aluminum alloy powders\nprocessing of cryomilled Al 5083 powder\nretain the nanostructure of aluminum alloy powders during consolidation\nSPS\nunmilled and milled powders\nX-ray Diffraction\nX-ray Diffraction (XRD) grain size calculations\nXRD\n"}, {"id": "S0925838814009669", "text": "SPS has been utilized in several studies to retain the nanostructure of aluminum alloy powders during consolidation. Ye et al. investigated the effect of processing of cryomilled Al 5083 powder via SPS [13]. X-ray Diffraction (XRD) grain size calculations before and after SPS showed that the average grain size of the alloy only increased from 25nm to 50nm (from powder to bulk state). Subsequently, the hardness values obtained through nanoindentation for specimens of AA5083 produced via SPS were highly improved in comparison to conventional sintering methods were grain coarsening takes place on a larger scale. In another study the combination of cryomilling and SPS of AA-5356/B4C nanocomposites powder was found to largely improve the microhardness and flexural strengths of the bulk nanocomposite. Rana et al. [14] investigated the effect of SPS on mechanically milled AA6061 (Al\u2013Mg\u2013Si) micro-alloy powder. The average grain size after 20h of milling was \u223c35nm and increased to only \u223c85nm after processing with SPS at 500\u00b0C. Microhardness and compressive tests were carried out on the consolidated near full density specimens of both unmilled and milled powders and the results showed significant increase in both hardness and compressive strengths for the milled nanocrystalline powders as a result of the very fine grain size.\n", "keywords": "AA5083\nAA-5356/B4C nanocomposites powder\nalloy\nAl\u2013Mg\u2013Si\naluminum alloy powders\nbulk nanocomposite\ncombination of cryomilling and SPS of AA-5356/B4C nanocomposites powder\nconsolidation\ncryomilled Al 5083 powder\ncryomilling\ninvestigated the effect of processing of cryomilled Al 5083 powder\ninvestigated the effect of SPS on mechanically milled AA6061 (Al\u2013Mg\u2013Si) micro-alloy powder\nmechanically milled AA6061\nmechanically milled AA6061 (Al\u2013Mg\u2013Si) micro-alloy powder\nmilled nanocrystalline powders\nnanoindentation\nnanostructure of aluminum alloy powders\nprocessing of cryomilled Al 5083 powder\nretain the nanostructure of aluminum alloy powders during consolidation\nSPS\nunmilled and milled powders\nX-ray Diffraction\nX-ray Diffraction (XRD) grain size calculations\nXRD\n"}, {"id": "S187775031300077X", "text": "Although mean-field models have been used in all these settings, little analysis has been done on their behaviour as spatially extended dynamical systems. In part, this is due to their staggering complexity. The Liley model [15] considered here, for instance, consists of fourteen coupled Partial Differential Equations (PDEs) with strong nonlinearities, imposed by coupling between the mean membrane potentials and the mean synaptic inputs. The model can be reduced to a system of Ordinary Differential Equations (ODEs) by considering only spatially homogeneous solutions, and the resulting system has been examined in detail using numerical bifurcation analysis (see [16] and references therein). In order to compute equilibria, periodic orbits and such objects for the PDE model, we need a flexible, stable simulation code for the model and its linearization that can run in parallel to scale up to a domain size of about 2500cm2, the size of a full-grown human cortex. We also need efficient, iterative solvers for linear problems with large, sparse matrices. In this paper, we will show that all this can be accomplished in the open-source software package PETSc [17]. Our implementation consists of a number of functions in C that are available publicly [18].\n", "keywords": "efficient, iterative solvers\nefficient, iterative solvers for linear problems with large, sparse matrices\nlarge, sparse matrices\nLiley model\nmean-field models\nmembrane potentials\nnumerical bifurcation analysis\nODEs\nOrdinary Differential Equations\nPartial Differential Equations\nPDE model\nPDEs\nPETSc\nspatially extended dynamical systems\nsynaptic inputs\n"}, {"id": "S187775031300077X", "text": "Although mean-field models have been used in all these settings, little analysis has been done on their behaviour as spatially extended dynamical systems. In part, this is due to their staggering complexity. The Liley model [15] considered here, for instance, consists of fourteen coupled Partial Differential Equations (PDEs) with strong nonlinearities, imposed by coupling between the mean membrane potentials and the mean synaptic inputs. The model can be reduced to a system of Ordinary Differential Equations (ODEs) by considering only spatially homogeneous solutions, and the resulting system has been examined in detail using numerical bifurcation analysis (see [16] and references therein). In order to compute equilibria, periodic orbits and such objects for the PDE model, we need a flexible, stable simulation code for the model and its linearization that can run in parallel to scale up to a domain size of about 2500cm2, the size of a full-grown human cortex. We also need efficient, iterative solvers for linear problems with large, sparse matrices. In this paper, we will show that all this can be accomplished in the open-source software package PETSc [17]. Our implementation consists of a number of functions in C that are available publicly [18].\n", "keywords": "efficient, iterative solvers\nefficient, iterative solvers for linear problems with large, sparse matrices\nlarge, sparse matrices\nLiley model\nmean-field models\nmembrane potentials\nnumerical bifurcation analysis\nODEs\nOrdinary Differential Equations\nPartial Differential Equations\nPDE model\nPDEs\nPETSc\nspatially extended dynamical systems\nsynaptic inputs\n"}, {"id": "S2212667814000069", "text": "In the present paper, a hypergraph model for the structural system modeling and reconfigurability analysis has been presented. At first, we represent each system equation by a hyperedge, and then we extend the modeling hypergraph with others colored hyperedges (red and blue) which allows us to perform the analysis task. Based on the bottom up analysis hypergraph model, it's very easy to check the system reconfigurability in the presence of fault by verifying the existence of paths from the affected hyperedge to specifics blue hyperedges passing through specifics red hyperedges. The method is illustrated through a pedagogical example.", "keywords": "analysis task\nbottom up analysis hypergraph model\nhypergraph model\nmodeling hypergraph\npedagogical example\nreconfigurability analysis\nstructural system modeling\nsystem reconfigurability\nverifying the existence of paths from the affected hyperedge\n"}, {"id": "S2212667814000069", "text": "In the present paper, a hypergraph model for the structural system modeling and reconfigurability analysis has been presented. At first, we represent each system equation by a hyperedge, and then we extend the modeling hypergraph with others colored hyperedges (red and blue) which allows us to perform the analysis task. Based on the bottom up analysis hypergraph model, it's very easy to check the system reconfigurability in the presence of fault by verifying the existence of paths from the affected hyperedge to specifics blue hyperedges passing through specifics red hyperedges. The method is illustrated through a pedagogical example.", "keywords": "analysis task\nbottom up analysis hypergraph model\nhypergraph model\nmodeling hypergraph\npedagogical example\nreconfigurability analysis\nstructural system modeling\nsystem reconfigurability\nverifying the existence of paths from the affected hyperedge\n"}, {"id": "S0022311515002664", "text": "The primary benefit of using a 3D model is that it allows the application of anisotropic material properties. As a hexagonal close packed lattice structure, a single zirconium grain is plastically anisotropic due to the difficulty of activating slip with a \u3008c\u3009 component [23\u201326]. Abaqus allows this to be represented by setting plasticity potential ratios. The anisotropic elastic and plastic constants are shown in Table 1. Zirconium alloys can often have a bimodal basal pole distribution, with a tilt on the basal normal or c direction of \u00b130\u00b0 in the normal direction being quoted for recrystallized Zircaloy-4 [27,28]. However, for simplicity the basal normal or c direction has been taken as being parallel to the normal direction. As such the 1, 2 and 3 directions in Table 1 correlate with the X, Y and Z global coordinate system for the 3D simulations, with the 3 direction correlating to the c direction of a zirconium unit lattice. Table 1 also shows the elastic properties incorporated into the simulations. The oxide layer has been simulated as a purely elastic material. Although it is known that the oxide is strongly textured [29], it is still simulated as a homogenous solid therefore isotropic material properties have been used for the oxide in all simulations.\n", "keywords": "3D model\n3D simulations\nAbaqus\nanisotropic elastic and plastic constants\nanisotropic material properties\nbimodal basal pole distribution\nisotropic material properties\noxide\noxide layer\nplasticity potential ratios\npurely elastic material\nrecrystallized Zircaloy-4\nsimulations\nZirconium\nzirconium grain\nzirconium unit lattice\n"}, {"id": "S0377221716300984", "text": "In this paper, we propose a general agent-based distributed framework where each agent is implementing a different metaheuristic/local search combination. Moreover, an agent continuously adapts itself during the search process using a direct cooperation protocol based on reinforcement learning and pattern matching. Good patterns that make up improving solutions are identified and shared by the agents. This agent-based system aims to provide a modular flexible framework to deal with a variety of different problem domains. We have evaluated the performance of this approach using the proposed framework which embodies a set of well known metaheuristics with different configurations as agents on two problem domains, Permutation Flow-shop Scheduling and Capacitated Vehicle Routing. The results show the success of the approach yielding three new best known results of the Capacitated Vehicle Routing benchmarks tested, whilst the results for Permutation Flow-shop Scheduling are commensurate with the best known values for all the benchmarks tested.\n", "keywords": "adapts\na direct cooperation protocol\nagent-based system\nagents\napproach\nCapacitated Vehicle Routing benchmarks\ndeal with a variety of different problem domains\neach agent is implementing a different metaheuristic/local search combination\nembodies a set of well known metaheuristics with different configurations\nframework\nGood patterns\npattern matching\nPermutation Flow-shop Scheduling\nPermutation Flow-shop Scheduling and Capacitated Vehicle Routing\nprovide a modular flexible framework\nreinforcement learning\nthis approach\ntwo problem domains\n"}, {"id": "S2212667814000732", "text": "The retrospective assessment of environmental carrying capacity aims to obtain the historical development situation of reclamation domain, it's an essential tool for improving the managed level and guiding the environmental management of reclamation. In this paper, a synthetic assessment method based on cloud theory is applied to evaluate the single factor and multiple factors environmental carrying capacity in Caofeidian marine district, Tangshan Bay, China. With the field data of five assessment indexes in recent six years, the assessment results are obtained which show that the marine reclamation has a certain impact for the marine environment.", "keywords": "cloud theory\nenvironmental carrying capacity\nenvironmental management of reclamation\nfield data\nguiding the environmental management of reclamation\nimproving the managed level\nmarine reclamation\nobtain the historical development situation of reclamation domain\nretrospective assessment of environmental carrying capacity\nsynthetic assessment method\nsynthetic assessment method based on cloud theory is applied\n"}, {"id": "S2212667814000732", "text": "The retrospective assessment of environmental carrying capacity aims to obtain the historical development situation of reclamation domain, it's an essential tool for improving the managed level and guiding the environmental management of reclamation. In this paper, a synthetic assessment method based on cloud theory is applied to evaluate the single factor and multiple factors environmental carrying capacity in Caofeidian marine district, Tangshan Bay, China. With the field data of five assessment indexes in recent six years, the assessment results are obtained which show that the marine reclamation has a certain impact for the marine environment.", "keywords": "cloud theory\nenvironmental carrying capacity\nenvironmental management of reclamation\nfield data\nguiding the environmental management of reclamation\nimproving the managed level\nmarine reclamation\nobtain the historical development situation of reclamation domain\nretrospective assessment of environmental carrying capacity\nsynthetic assessment method\nsynthetic assessment method based on cloud theory is applied\n"}, {"id": "S0370269304007129", "text": "We analyze the diagonal and transition magnetic and electric dipole moments of charged leptons in extended technicolor (ETC) models, taking account of the multiscale nature of the ETC gauge symmetry breaking, conformal (walking) behavior of the technicolor theory, and mixing in the charged-lepton mass matrix. We show that mixing effects dominate the ETC contributions to charged lepton electric dipole moments and that these can yield a value of |de| comparable to the current limit. The rate for \u03bc\u2192e\u03b3 can also be close to its limit. From these and other processes we derive constraints on the charged lepton mixing angles. The constraints are such that the ETC contribution to the muon anomalous magnetic moment, which includes a significant lepton mixing term, can approach, but does not exceed, the current sensitivity level.", "keywords": "charged lepton electric dipole moments\ncharged-lepton mass matrix\ncharged lepton mixing angles\nconformal (walking) behavior of the technicolor theory\ndiagonal and transition magnetic and electric dipole moments of charged leptons\nETC\nextended technicolor\ngauge symmetry breaking\nlepton mixing term\nmixing effects\nmixing in the charged-lepton mass matrix\nmuon anomalous magnetic moment\nrate for \u03bc\u2192e\u03b3\nvalue of |de|\n"}, {"id": "S0370269304009104", "text": "Though, in this Letter we have constructed the Born\u2013Infeld black holes in the presence of a cosmological constant and discussed their thermodynamical properties, many issues however still remain to be investigated. We know that Reissner\u2013Nordstr\u00f6m AdS black holes undergo Hawking\u2013Page phase transition. This transition gets modified as we include Born\u2013Infeld corrections into account. We hope to carry out a detail study on this issue in the future. Furthermore, in the context of brane world cosmology, it was found that a brane moving in a Reissner\u2013Nordstr\u00f6m AdS background generates non-singular cosmology\u00a0[14]. However, as shown in\u00a0[15], the brane always crosses the inner horizon of the bulk geometry, creating an instability. It would be interesting to study cosmology on the brane when it is moving in the charged black hole backgrounds that we have constructed. Note that since these charged holes does not have inner horizon for certain range of parameters, we may generate non-singular cosmology without creating the instabilities that we have just mentioned.\n", "keywords": "Born\u2013Infeld black holes\nBorn\u2013Infeld corrections\nbrane\nbrane world cosmology\ncharged black hole\ncharged holes\nconstructed the Born\u2013Infeld black holes in the presence of a cosmological constant\ncosmology\ncrosses the inner horizon of the bulk geometry\ndiscussed their thermodynamical properties\ngenerate non-singular cosmology without creating the instabilities\nHawking\u2013Page phase transition\ninstability\nnon-singular cosmology\nReissner\u2013Nordstr\u00f6m AdS background\nReissner\u2013Nordstr\u00f6m AdS black holes\nstudy cosmology on the brane when it is moving in the charged black hole backgrounds\n"}, {"id": "S030193221400144X", "text": "In the present work, a LIF technique is applied for investigation of gas-sheared film flow in horizontal rectangular duct. The technique makes it possible to perform field measurements of local film thickness, resolved in both space and time, similar to the work of Alekseenko et al. (2009). The flat shape and large transverse size of the duct allow us to resolve the film thickness in transverse coordinate as well. Alekseenko et al. (2012) attempted to do this in annular downward flow, but, for technical reasons, the sampling frequency was not high enough in their experiments. More recently Alekseenko et al. (2014a) showed that the LIF technique can also detect entrained droplets. The technique allows the simultaneous study of three-dimensional wavy structures and liquid entrainment, and can improve understanding of the entrainment phenomenon.\n", "keywords": "annular downward flow\ndetect entrained droplets\nduct\nfield measurements of local film thickness\nfilm\nfilm flow\ngas-sheared film flow\nimprove understanding of the entrainment phenomenon\nLIF technique\nliquid entrainment\nresolve the film thickness\nsampling frequency\nthree-dimensional wavy structures\n"}, {"id": "S030193221400144X", "text": "In the present work, a LIF technique is applied for investigation of gas-sheared film flow in horizontal rectangular duct. The technique makes it possible to perform field measurements of local film thickness, resolved in both space and time, similar to the work of Alekseenko et al. (2009). The flat shape and large transverse size of the duct allow us to resolve the film thickness in transverse coordinate as well. Alekseenko et al. (2012) attempted to do this in annular downward flow, but, for technical reasons, the sampling frequency was not high enough in their experiments. More recently Alekseenko et al. (2014a) showed that the LIF technique can also detect entrained droplets. The technique allows the simultaneous study of three-dimensional wavy structures and liquid entrainment, and can improve understanding of the entrainment phenomenon.\n", "keywords": "annular downward flow\ndetect entrained droplets\nduct\nfield measurements of local film thickness\nfilm\nfilm flow\ngas-sheared film flow\nimprove understanding of the entrainment phenomenon\nLIF technique\nliquid entrainment\nresolve the film thickness\nsampling frequency\nthree-dimensional wavy structures\n"}, {"id": "S0378381215300297", "text": "The Statistical Associating Fluid Theory (SAFT) is a well-developed perturbation theory used to describe quantitatively the volumetric properties of fluids. The reader is referred to several reviews on the topic which describe the various stages of its development and the multiple versions available [50\u201353]. The fundamental difference between the versions is in the underlying intermolecular potential employed to describe the unbounded constituent particles. Hard spheres, square well fluids, LJ fluids, argon, alkanes have all been employed as reference fluids in the different incarnations of SAFT. For the purpose of this work we will center on a particular version of the SAFT EoS, i.e. the SAFT-VR Mie recently proposed by Laffitte et al. [54] and expanded into a group contribution approach, SAFT-\u03b3, by Papaioannou et al. [55]. This particular version of SAFT provides a closed form EoS that describes the macroscopical properties of the Mie potential [56], also known as the (m,n) potential; a generalized form of the LJ potential (albeit predating it by decades). The Mie potential has the form(1)\u03d5(r)=C\u03b5\u03c3r\u03bbr\u2212\u03c3r\u03bbawhere C is an analytical function of the repulsive and attractive exponents, \u03bba and \u03bbr, respectively, \u03c3 is a parameter that defines the length scale and is loosely related to the average diameter of a Mie bead; \u025b defines the energy scale and corresponds to the minimum potential energy between two isolated beads; expressed here as a ratio to the Boltzmann constant, kB. The Mie function, as written above, deceivingly suggests that four parameters are needed to characterize the behaviour of an isotropic molecule, however the exponents \u03bba and \u03bbr are intimately related, and for fluid phase equilibria, one needs not consider them as independent parameters [57]. Accordingly, we choose herein to fix the attractive exponent to \u03bba=6 which would be expected to be representative of the dispersion scaling of most simple fluids and refer from here on to the repulsive parameter as \u03bb=\u03bbr. The potential simplifies to(2)\u03d5(r)=\u03bb\u03bb\u22126\u03bb66/(\u03bb\u22126)\u03b5\u03c3r\u03bb\u2212\u03c3r6\n", "keywords": "alkanes\nargon\nbead\nbeads\nclosed form EoS\nfluid\nfluids\ngroup contribution approach\nHard spheres\nintermolecular potential\nisotropic molecule\nLJ fluids\nLJ potential\nMie potential\n(m,n) potential\nperturbation theory\nreference fluids\nSAFT\nSAFT EoS\nSAFT-VR Mie\nSAFT-\u03b3\nsimple fluids\nsquare well fluids\nStatistical Associating Fluid Theory\nunbounded constituent particles\n"}, {"id": "S0378381215300297", "text": "The Statistical Associating Fluid Theory (SAFT) is a well-developed perturbation theory used to describe quantitatively the volumetric properties of fluids. The reader is referred to several reviews on the topic which describe the various stages of its development and the multiple versions available [50\u201353]. The fundamental difference between the versions is in the underlying intermolecular potential employed to describe the unbounded constituent particles. Hard spheres, square well fluids, LJ fluids, argon, alkanes have all been employed as reference fluids in the different incarnations of SAFT. For the purpose of this work we will center on a particular version of the SAFT EoS, i.e. the SAFT-VR Mie recently proposed by Laffitte et al. [54] and expanded into a group contribution approach, SAFT-\u03b3, by Papaioannou et al. [55]. This particular version of SAFT provides a closed form EoS that describes the macroscopical properties of the Mie potential [56], also known as the (m,n) potential; a generalized form of the LJ potential (albeit predating it by decades). The Mie potential has the form(1)\u03d5(r)=C\u03b5\u03c3r\u03bbr\u2212\u03c3r\u03bbawhere C is an analytical function of the repulsive and attractive exponents, \u03bba and \u03bbr, respectively, \u03c3 is a parameter that defines the length scale and is loosely related to the average diameter of a Mie bead; \u025b defines the energy scale and corresponds to the minimum potential energy between two isolated beads; expressed here as a ratio to the Boltzmann constant, kB. The Mie function, as written above, deceivingly suggests that four parameters are needed to characterize the behaviour of an isotropic molecule, however the exponents \u03bba and \u03bbr are intimately related, and for fluid phase equilibria, one needs not consider them as independent parameters [57]. Accordingly, we choose herein to fix the attractive exponent to \u03bba=6 which would be expected to be representative of the dispersion scaling of most simple fluids and refer from here on to the repulsive parameter as \u03bb=\u03bbr. The potential simplifies to(2)\u03d5(r)=\u03bb\u03bb\u22126\u03bb66/(\u03bb\u22126)\u03b5\u03c3r\u03bb\u2212\u03c3r6\n", "keywords": "alkanes\nargon\nbead\nbeads\nclosed form EoS\nfluid\nfluids\ngroup contribution approach\nHard spheres\nintermolecular potential\nisotropic molecule\nLJ fluids\nLJ potential\nMie potential\n(m,n) potential\nperturbation theory\nreference fluids\nSAFT\nSAFT EoS\nSAFT-VR Mie\nSAFT-\u03b3\nsimple fluids\nsquare well fluids\nStatistical Associating Fluid Theory\nunbounded constituent particles\n"}, {"id": "S0031920113001222", "text": "Seismic tomography is a powerful tool to investigate the deep structure under the volcanoes. With the recently rapid development of Chinese provincial seismic networks (Zheng et al., 2009, 2010) and some portable seismic arrays (Hetland et al., 2004; Duan et al., 2009; Lei et al., 2012b) around the volcanoes, it has become possible to image the detailed 3-D velocity structure under some of these volcanoes, where seismic stations are densely spaced. In this overview, we synthesize the results from the deep seismic images of the upper mantle under the Changbaishan, Tengchong, Hainan volcanoes as well as the Datong volcano (Fig. 1). We also evaluate the advantages of recently updated seismic tomographic techniques for deriving potential information. This work updates a previous review of Zhao and Liu (2010) on this topic, with more detailed synthesis of all the available information.\n", "keywords": "3-D velocity structure\nadvantages of recently updated seismic tomographic techniques\nChangbaishan, Tengchong, Hainan volcanoes as well as the Datong\ndeep seismic images\nimage the detailed 3-D velocity structure\ninvestigate the deep structure under the volcanoes\nportable seismic arrays\nprovincial seismic networks\nseismic stations\nseismic tomographic techniques\nSeismic tomography\nsynthesize the results from the deep seismic images of the upper mantle\n"}, {"id": "S0098300413002185", "text": "A number of model parameters can change regionally or seasonally, in particular the inherent optical properties of water constituents [ai\u204e(\u03bb), aY\u204e(\u03bb), aD\u204e(\u03bb), bX(\u03bb), bb,X\u204e, bb,Mie\u204e] and the apparent optical properties of the bottom [Rib(\u03bb), Bi] and the atmosphere. The database provided with WASI has been derived from in-situ measurements from lakes in Southern Germany (Gege, 1998; Heege, 2000; Pinnel, 2007). If no site-specific information is available, it can be used as a first approximation for other ecosystems as well. The variability within an ecosystem can be as large as between different ecosystem, i.e. ecosystem-specific sets of optical properties do not exist. However, region or season specific information should be used whenever available. Ideally, the optical properties should be measured at the test site close to the airplane or satellite overpass. This is however not always possible. A valuable source of information is the IOCCG webpage (IOCCG, 2013b). It maintains a list of links to publicly available data sets, for example the IOCCG (2006) data bank, the NASA bio-Optical Marine Algorithm Data set (NOMAD) and the SeaWiFS Bio-Optical Archive and Storage System (SeaBASS).\n", "keywords": "aD\u204e(\u03bb)\nai\u204e(\u03bb\naY\u204e(\u03bb)\nbb\nbb,X\u204e\nbX(\u03bb)\ndatabase\nin-situ measurements\nIOCCG\nIOCCG (2006) data bank\nMie\u204e\nNASA bio-Optical Marine Algorithm Data set\nNOMAD\noptical properties\noptical properties of the bottom [Rib(\u03bb), Bi] and the atmosphere\nregion or season specific information\nSeaBASS\nSeaWiFS Bio-Optical Archive and Storage System\nWASI\nwater constituents\n"}, {"id": "S0098300413002185", "text": "A number of model parameters can change regionally or seasonally, in particular the inherent optical properties of water constituents [ai\u204e(\u03bb), aY\u204e(\u03bb), aD\u204e(\u03bb), bX(\u03bb), bb,X\u204e, bb,Mie\u204e] and the apparent optical properties of the bottom [Rib(\u03bb), Bi] and the atmosphere. The database provided with WASI has been derived from in-situ measurements from lakes in Southern Germany (Gege, 1998; Heege, 2000; Pinnel, 2007). If no site-specific information is available, it can be used as a first approximation for other ecosystems as well. The variability within an ecosystem can be as large as between different ecosystem, i.e. ecosystem-specific sets of optical properties do not exist. However, region or season specific information should be used whenever available. Ideally, the optical properties should be measured at the test site close to the airplane or satellite overpass. This is however not always possible. A valuable source of information is the IOCCG webpage (IOCCG, 2013b). It maintains a list of links to publicly available data sets, for example the IOCCG (2006) data bank, the NASA bio-Optical Marine Algorithm Data set (NOMAD) and the SeaWiFS Bio-Optical Archive and Storage System (SeaBASS).\n", "keywords": "aD\u204e(\u03bb)\nai\u204e(\u03bb\naY\u204e(\u03bb)\nbb\nbb,X\u204e\nbX(\u03bb)\ndatabase\nin-situ measurements\nIOCCG\nIOCCG (2006) data bank\nMie\u204e\nNASA bio-Optical Marine Algorithm Data set\nNOMAD\noptical properties\noptical properties of the bottom [Rib(\u03bb), Bi] and the atmosphere\nregion or season specific information\nSeaBASS\nSeaWiFS Bio-Optical Archive and Storage System\nWASI\nwater constituents\n"}, {"id": "S0098300413002185", "text": "A number of model parameters can change regionally or seasonally, in particular the inherent optical properties of water constituents [ai\u204e(\u03bb), aY\u204e(\u03bb), aD\u204e(\u03bb), bX(\u03bb), bb,X\u204e, bb,Mie\u204e] and the apparent optical properties of the bottom [Rib(\u03bb), Bi] and the atmosphere. The database provided with WASI has been derived from in-situ measurements from lakes in Southern Germany (Gege, 1998; Heege, 2000; Pinnel, 2007). If no site-specific information is available, it can be used as a first approximation for other ecosystems as well. The variability within an ecosystem can be as large as between different ecosystem, i.e. ecosystem-specific sets of optical properties do not exist. However, region or season specific information should be used whenever available. Ideally, the optical properties should be measured at the test site close to the airplane or satellite overpass. This is however not always possible. A valuable source of information is the IOCCG webpage (IOCCG, 2013b). It maintains a list of links to publicly available data sets, for example the IOCCG (2006) data bank, the NASA bio-Optical Marine Algorithm Data set (NOMAD) and the SeaWiFS Bio-Optical Archive and Storage System (SeaBASS).\n", "keywords": "aD\u204e(\u03bb)\nai\u204e(\u03bb\naY\u204e(\u03bb)\nbb\nbb,X\u204e\nbX(\u03bb)\ndatabase\nin-situ measurements\nIOCCG\nIOCCG (2006) data bank\nMie\u204e\nNASA bio-Optical Marine Algorithm Data set\nNOMAD\noptical properties\noptical properties of the bottom [Rib(\u03bb), Bi] and the atmosphere\nregion or season specific information\nSeaBASS\nSeaWiFS Bio-Optical Archive and Storage System\nWASI\nwater constituents\n"}, {"id": "S0098300413002185", "text": "A number of model parameters can change regionally or seasonally, in particular the inherent optical properties of water constituents [ai\u204e(\u03bb), aY\u204e(\u03bb), aD\u204e(\u03bb), bX(\u03bb), bb,X\u204e, bb,Mie\u204e] and the apparent optical properties of the bottom [Rib(\u03bb), Bi] and the atmosphere. The database provided with WASI has been derived from in-situ measurements from lakes in Southern Germany (Gege, 1998; Heege, 2000; Pinnel, 2007). If no site-specific information is available, it can be used as a first approximation for other ecosystems as well. The variability within an ecosystem can be as large as between different ecosystem, i.e. ecosystem-specific sets of optical properties do not exist. However, region or season specific information should be used whenever available. Ideally, the optical properties should be measured at the test site close to the airplane or satellite overpass. This is however not always possible. A valuable source of information is the IOCCG webpage (IOCCG, 2013b). It maintains a list of links to publicly available data sets, for example the IOCCG (2006) data bank, the NASA bio-Optical Marine Algorithm Data set (NOMAD) and the SeaWiFS Bio-Optical Archive and Storage System (SeaBASS).\n", "keywords": "aD\u204e(\u03bb)\nai\u204e(\u03bb\naY\u204e(\u03bb)\nbb\nbb,X\u204e\nbX(\u03bb)\ndatabase\nin-situ measurements\nIOCCG\nIOCCG (2006) data bank\nMie\u204e\nNASA bio-Optical Marine Algorithm Data set\nNOMAD\noptical properties\noptical properties of the bottom [Rib(\u03bb), Bi] and the atmosphere\nregion or season specific information\nSeaBASS\nSeaWiFS Bio-Optical Archive and Storage System\nWASI\nwater constituents\n"}, {"id": "S0098300413002185", "text": "A number of model parameters can change regionally or seasonally, in particular the inherent optical properties of water constituents [ai\u204e(\u03bb), aY\u204e(\u03bb), aD\u204e(\u03bb), bX(\u03bb), bb,X\u204e, bb,Mie\u204e] and the apparent optical properties of the bottom [Rib(\u03bb), Bi] and the atmosphere. The database provided with WASI has been derived from in-situ measurements from lakes in Southern Germany (Gege, 1998; Heege, 2000; Pinnel, 2007). If no site-specific information is available, it can be used as a first approximation for other ecosystems as well. The variability within an ecosystem can be as large as between different ecosystem, i.e. ecosystem-specific sets of optical properties do not exist. However, region or season specific information should be used whenever available. Ideally, the optical properties should be measured at the test site close to the airplane or satellite overpass. This is however not always possible. A valuable source of information is the IOCCG webpage (IOCCG, 2013b). It maintains a list of links to publicly available data sets, for example the IOCCG (2006) data bank, the NASA bio-Optical Marine Algorithm Data set (NOMAD) and the SeaWiFS Bio-Optical Archive and Storage System (SeaBASS).\n", "keywords": "aD\u204e(\u03bb)\nai\u204e(\u03bb\naY\u204e(\u03bb)\nbb\nbb,X\u204e\nbX(\u03bb)\ndatabase\nin-situ measurements\nIOCCG\nIOCCG (2006) data bank\nMie\u204e\nNASA bio-Optical Marine Algorithm Data set\nNOMAD\noptical properties\noptical properties of the bottom [Rib(\u03bb), Bi] and the atmosphere\nregion or season specific information\nSeaBASS\nSeaWiFS Bio-Optical Archive and Storage System\nWASI\nwater constituents\n"}, {"id": "S0098300413002185", "text": "A number of model parameters can change regionally or seasonally, in particular the inherent optical properties of water constituents [ai\u204e(\u03bb), aY\u204e(\u03bb), aD\u204e(\u03bb), bX(\u03bb), bb,X\u204e, bb,Mie\u204e] and the apparent optical properties of the bottom [Rib(\u03bb), Bi] and the atmosphere. The database provided with WASI has been derived from in-situ measurements from lakes in Southern Germany (Gege, 1998; Heege, 2000; Pinnel, 2007). If no site-specific information is available, it can be used as a first approximation for other ecosystems as well. The variability within an ecosystem can be as large as between different ecosystem, i.e. ecosystem-specific sets of optical properties do not exist. However, region or season specific information should be used whenever available. Ideally, the optical properties should be measured at the test site close to the airplane or satellite overpass. This is however not always possible. A valuable source of information is the IOCCG webpage (IOCCG, 2013b). It maintains a list of links to publicly available data sets, for example the IOCCG (2006) data bank, the NASA bio-Optical Marine Algorithm Data set (NOMAD) and the SeaWiFS Bio-Optical Archive and Storage System (SeaBASS).\n", "keywords": "aD\u204e(\u03bb)\nai\u204e(\u03bb\naY\u204e(\u03bb)\nbb\nbb,X\u204e\nbX(\u03bb)\ndatabase\nin-situ measurements\nIOCCG\nIOCCG (2006) data bank\nMie\u204e\nNASA bio-Optical Marine Algorithm Data set\nNOMAD\noptical properties\noptical properties of the bottom [Rib(\u03bb), Bi] and the atmosphere\nregion or season specific information\nSeaBASS\nSeaWiFS Bio-Optical Archive and Storage System\nWASI\nwater constituents\n"}, {"id": "S167420011300196X", "text": "PV cells are one of the most promising technologies for conversion of incident solar radiation into electric power. However, this technology is still far from being able to compete with fossil fuel-based energy conversion technologies because of its relatively low efficiency and energy density. Theoretically, there are three unavoidable losses that limit the solar conversion efficiency of a device with a single absorption threshold or band gap Eg: (1) incomplete absorption, where photons with energies below Eg are not absorbed; (2) thermalization or carrier cooling, where solar photons with sufficient energy generate electron-hole pairs and then immediately lose almost all energy in excess of Eg in the form of heat; and (3) radiative recombination, where a small fraction of the excited states radioactively recombine with the ground state at the maximum power output (Hanna & Nozik, 2006; Henry, 1980). Taking an air mass of 1.5 as an example, for different band gap Eg these three losses can be calculated and the results are indicated by areas S1, S2, and S3 in Fig. 1. Note that the area under the outer curve is the solar power per unit area, and that only S4 can be delivered to the load.\n", "keywords": "absorption\ncarrier cooling\nconversion of incident solar radiation into electric power\nelectric power\nelectron-hole pairs\nfossil fuel-based energy conversion\nincident solar radiation\nincomplete absorption\nphotons\nPV cells\nradiative recombination\nsolar conversion\nsolar photons\nsolar power\nthermalization\n"}, {"id": "S167420011300196X", "text": "PV cells are one of the most promising technologies for conversion of incident solar radiation into electric power. However, this technology is still far from being able to compete with fossil fuel-based energy conversion technologies because of its relatively low efficiency and energy density. Theoretically, there are three unavoidable losses that limit the solar conversion efficiency of a device with a single absorption threshold or band gap Eg: (1) incomplete absorption, where photons with energies below Eg are not absorbed; (2) thermalization or carrier cooling, where solar photons with sufficient energy generate electron-hole pairs and then immediately lose almost all energy in excess of Eg in the form of heat; and (3) radiative recombination, where a small fraction of the excited states radioactively recombine with the ground state at the maximum power output (Hanna & Nozik, 2006; Henry, 1980). Taking an air mass of 1.5 as an example, for different band gap Eg these three losses can be calculated and the results are indicated by areas S1, S2, and S3 in Fig. 1. Note that the area under the outer curve is the solar power per unit area, and that only S4 can be delivered to the load.\n", "keywords": "absorption\ncarrier cooling\nconversion of incident solar radiation into electric power\nelectric power\nelectron-hole pairs\nfossil fuel-based energy conversion\nincident solar radiation\nincomplete absorption\nphotons\nPV cells\nradiative recombination\nsolar conversion\nsolar photons\nsolar power\nthermalization\n"}, {"id": "S167420011300196X", "text": "PV cells are one of the most promising technologies for conversion of incident solar radiation into electric power. However, this technology is still far from being able to compete with fossil fuel-based energy conversion technologies because of its relatively low efficiency and energy density. Theoretically, there are three unavoidable losses that limit the solar conversion efficiency of a device with a single absorption threshold or band gap Eg: (1) incomplete absorption, where photons with energies below Eg are not absorbed; (2) thermalization or carrier cooling, where solar photons with sufficient energy generate electron-hole pairs and then immediately lose almost all energy in excess of Eg in the form of heat; and (3) radiative recombination, where a small fraction of the excited states radioactively recombine with the ground state at the maximum power output (Hanna & Nozik, 2006; Henry, 1980). Taking an air mass of 1.5 as an example, for different band gap Eg these three losses can be calculated and the results are indicated by areas S1, S2, and S3 in Fig. 1. Note that the area under the outer curve is the solar power per unit area, and that only S4 can be delivered to the load.\n", "keywords": "absorption\ncarrier cooling\nconversion of incident solar radiation into electric power\nelectric power\nelectron-hole pairs\nfossil fuel-based energy conversion\nincident solar radiation\nincomplete absorption\nphotons\nPV cells\nradiative recombination\nsolar conversion\nsolar photons\nsolar power\nthermalization\n"}, {"id": "S0032386110001254", "text": "Despite the loss of directed, self-complementary hydrogen bonding through alkylation of the imidazole ring, electrostatic aggregation of imidazolium salts is a tunable, self-assembly process, which is instrumental to several applications. Imidazolium salts are used to extract metal ions from aqueous solutions and coat metal nanoparticles [15], dissolve carbohydrates [16], and create polyelectrolyte brushes on surfaces [17]. For example, atom transfer radical\u00a0polymerization (ATRP) was used to graft poly(1-ethyl 3-(2-methacryloyloxy ethyl) imidazolium chloride) brushes onto gold surfaces [17]. One of the imidazolium salt\u2019s most promising attributes is its antimicrobial action [12,18] and molecular self-assembly into liquid crystals [19,20]. 1-Alkyl-3-methylimidazolium chlorides and bromides, 1-alkyl-2-methyl-3-hydroxyethylimidazolium chlorides, and N-alkyl-N-hydroxyethylpyrrolidinonium, for example, all exhibit strong biocidal activity [18]. Hydrogels form from polymerized methylimidazolium-based ionic liquids with acryloyl groups; the polymer self-assembles into organized lamellae with unique swelling properties, leading to bioactive applications [19]. Other bioactive applications for imidazolium salts include antiarrhythmics [21], anti-metastic agents [22,23], and imidazolium-based steroids [24].\u00a0Separation applications include efficient absorption of CO2 [25]. Imidazolium salts enhance vesicle formation as imidazolium surfactants [26], and they also find application in polymeric actuators [27].\n", "keywords": "1-alkyl-2-methyl-3-hydroxyethylimidazolium chlorides\n1-Alkyl-3-methylimidazolium chlorides\nacryloyl groups\nalkylation\nantiarrhythmics\nanti-metastic agents\nantimicrobial action\naqueous solutions\natom transfer radical\u00a0polymerization\nATRP\nbioactive applications\nbiocidal activity\nbromides\ncarbohydrates\nCO2\nefficient absorption of CO2\nelectrostatic aggregation\ngold surfaces\nHydrogels form\nhydrogen bonding\nimidazolium-based steroids\nimidazolium salt\nimidazolium salts\nImidazolium salts\nimidazolium surfactants\nliquid crystals\nmetal ions\nmetal nanoparticles\nN-alkyl-N-hydroxyethylpyrrolidinonium\norganized lamellae\npoly(1-ethyl 3-(2-methacryloyloxy ethyl) imidazolium chloride\npolyelectrolyte brushes\npolymer\npolymeric actuators\npolymerized methylimidazolium-based ionic liquids\nSeparation applications\nvesicle formation\n"}, {"id": "S2212671612000121", "text": "In this paper, three different approaches for implementing a quantum search algorithm by adiabatic evolution are shown. As expected, either one of them can provide a quadratic speed up as opposed to the classical search algorithm. This implies that adiabatic evolution based quantum computation gives more feasibilities than the quantum circuit model, although the equivalence between them has already been proven in the corresponding literature.", "keywords": "adiabatic evolution\nclassical search algorithm\nfeasibilities\nimplementing a quantum search algorithm by adiabatic evolution\nquadratic speed up\nquantum circuit model\nquantum computation\nquantum search algorithm\nthree different approaches\n"}, {"id": "S2212671612000121", "text": "In this paper, three different approaches for implementing a quantum search algorithm by adiabatic evolution are shown. As expected, either one of them can provide a quadratic speed up as opposed to the classical search algorithm. This implies that adiabatic evolution based quantum computation gives more feasibilities than the quantum circuit model, although the equivalence between them has already been proven in the corresponding literature.", "keywords": "adiabatic evolution\nclassical search algorithm\nfeasibilities\nimplementing a quantum search algorithm by adiabatic evolution\nquadratic speed up\nquantum circuit model\nquantum computation\nquantum search algorithm\nthree different approaches\n"}, {"id": "S221266781300083X", "text": "In this paper, coordination problem of agricultural products supply chain with stochastic yield is studied based on prices compensation strategy. The agricultural producing is influenced by the natural conditions, and the yield is uncertain. While agricultural products is rigid demand goods, the fluctuations of yield cause greater volatility of prices. The two- echelon supply chain with one supplier and one retailor is studied, and the mathematical model is constructed. The model showed that prices compensation strategy is Pareto improvement for agricultural products supply chain with stochastic yield, and it also incentive agricultural products supplier to rise the production plan and balance the profit allocation of supply chain.", "keywords": "agricultural producing\nagricultural products\nagricultural products supply chain\ncoordination problem\ncoordination problem of agricultural products supply chain\nfluctuations of yield\nmathematical model\nmathematical model is constructed\nprices compensation strategy\nproduction plan\nprofit allocation\nstochastic yield\nsupply chain\ntwo- echelon supply chain\nyield\n"}, {"id": "S221266781300083X", "text": "In this paper, coordination problem of agricultural products supply chain with stochastic yield is studied based on prices compensation strategy. The agricultural producing is influenced by the natural conditions, and the yield is uncertain. While agricultural products is rigid demand goods, the fluctuations of yield cause greater volatility of prices. The two- echelon supply chain with one supplier and one retailor is studied, and the mathematical model is constructed. The model showed that prices compensation strategy is Pareto improvement for agricultural products supply chain with stochastic yield, and it also incentive agricultural products supplier to rise the production plan and balance the profit allocation of supply chain.", "keywords": "agricultural producing\nagricultural products\nagricultural products supply chain\ncoordination problem\ncoordination problem of agricultural products supply chain\nfluctuations of yield\nmathematical model\nmathematical model is constructed\nprices compensation strategy\nproduction plan\nprofit allocation\nstochastic yield\nsupply chain\ntwo- echelon supply chain\nyield\n"}, {"id": "S221266781300083X", "text": "In this paper, coordination problem of agricultural products supply chain with stochastic yield is studied based on prices compensation strategy. The agricultural producing is influenced by the natural conditions, and the yield is uncertain. While agricultural products is rigid demand goods, the fluctuations of yield cause greater volatility of prices. The two- echelon supply chain with one supplier and one retailor is studied, and the mathematical model is constructed. The model showed that prices compensation strategy is Pareto improvement for agricultural products supply chain with stochastic yield, and it also incentive agricultural products supplier to rise the production plan and balance the profit allocation of supply chain.", "keywords": "agricultural producing\nagricultural products\nagricultural products supply chain\ncoordination problem\ncoordination problem of agricultural products supply chain\nfluctuations of yield\nmathematical model\nmathematical model is constructed\nprices compensation strategy\nproduction plan\nprofit allocation\nstochastic yield\nsupply chain\ntwo- echelon supply chain\nyield\n"}, {"id": "S221266781300083X", "text": "In this paper, coordination problem of agricultural products supply chain with stochastic yield is studied based on prices compensation strategy. The agricultural producing is influenced by the natural conditions, and the yield is uncertain. While agricultural products is rigid demand goods, the fluctuations of yield cause greater volatility of prices. The two- echelon supply chain with one supplier and one retailor is studied, and the mathematical model is constructed. The model showed that prices compensation strategy is Pareto improvement for agricultural products supply chain with stochastic yield, and it also incentive agricultural products supplier to rise the production plan and balance the profit allocation of supply chain.", "keywords": "agricultural producing\nagricultural products\nagricultural products supply chain\ncoordination problem\ncoordination problem of agricultural products supply chain\nfluctuations of yield\nmathematical model\nmathematical model is constructed\nprices compensation strategy\nproduction plan\nprofit allocation\nstochastic yield\nsupply chain\ntwo- echelon supply chain\nyield\n"}, {"id": "S0098300414002532", "text": "The threshold values for removing large caters were determined by examining the craters within the study area, referencing previous studies (Molloy and Stepinski, 2007), and some trial and error. After the parameter values are determined, the rest of the process is automated. However, we do anticipate some minimum manual editing may be needed in some complicated terrains when apply it to all of Mars. To minimize the distortion resulted from map projection on global datasets, we will choose an equal area projection by evaluating the options suggested in Steinwand et al. (1995) or conduct geodesic area calculation using software such as \u201cTools for Graphics and Shapes\u201d (http://www.jennessent.com/arcgis/shapes_graphics.htm)Although post-formational modification to the valleys may be minimum (Williams and Phillips, 2001), there may nonetheless be modifications such as eolian fill and mass wasting (e.g., Grant et al., 2008). Thus the volume estimates derived with PBTH method represents a lower bound. Comparing the estimates from MOLA and HRSC data reveals that MOLA estimate is about 91% of HRSC value. However, MOLA has global coverage whereas HRSC does not. Therefore, for areas where there is only MOLA coverage, the estimate may be scaled upward by 1.1 times. The algorithm has been tested on DEMs with various resolutions (2 m for simulated DEM, 75m for HRSC, and 463m for MOLA). It can certainly be applied to higher resolution DEMs for Mars when they become available, but the threshold values will need to be adjusted.\n", "keywords": "DEM\nDEMs\nHRSC\nmanual editing\nMOLA\nPBTH method\nThe threshold values for removing large caters were determined\nTools for Graphics and Shapes\n"}, {"id": "S0098300414002532", "text": "The threshold values for removing large caters were determined by examining the craters within the study area, referencing previous studies (Molloy and Stepinski, 2007), and some trial and error. After the parameter values are determined, the rest of the process is automated. However, we do anticipate some minimum manual editing may be needed in some complicated terrains when apply it to all of Mars. To minimize the distortion resulted from map projection on global datasets, we will choose an equal area projection by evaluating the options suggested in Steinwand et al. (1995) or conduct geodesic area calculation using software such as \u201cTools for Graphics and Shapes\u201d (http://www.jennessent.com/arcgis/shapes_graphics.htm)Although post-formational modification to the valleys may be minimum (Williams and Phillips, 2001), there may nonetheless be modifications such as eolian fill and mass wasting (e.g., Grant et al., 2008). Thus the volume estimates derived with PBTH method represents a lower bound. Comparing the estimates from MOLA and HRSC data reveals that MOLA estimate is about 91% of HRSC value. However, MOLA has global coverage whereas HRSC does not. Therefore, for areas where there is only MOLA coverage, the estimate may be scaled upward by 1.1 times. The algorithm has been tested on DEMs with various resolutions (2 m for simulated DEM, 75m for HRSC, and 463m for MOLA). It can certainly be applied to higher resolution DEMs for Mars when they become available, but the threshold values will need to be adjusted.\n", "keywords": "DEM\nDEMs\nHRSC\nmanual editing\nMOLA\nPBTH method\nThe threshold values for removing large caters were determined\nTools for Graphics and Shapes\n"}, {"id": "S0098300414002532", "text": "The threshold values for removing large caters were determined by examining the craters within the study area, referencing previous studies (Molloy and Stepinski, 2007), and some trial and error. After the parameter values are determined, the rest of the process is automated. However, we do anticipate some minimum manual editing may be needed in some complicated terrains when apply it to all of Mars. To minimize the distortion resulted from map projection on global datasets, we will choose an equal area projection by evaluating the options suggested in Steinwand et al. (1995) or conduct geodesic area calculation using software such as \u201cTools for Graphics and Shapes\u201d (http://www.jennessent.com/arcgis/shapes_graphics.htm)Although post-formational modification to the valleys may be minimum (Williams and Phillips, 2001), there may nonetheless be modifications such as eolian fill and mass wasting (e.g., Grant et al., 2008). Thus the volume estimates derived with PBTH method represents a lower bound. Comparing the estimates from MOLA and HRSC data reveals that MOLA estimate is about 91% of HRSC value. However, MOLA has global coverage whereas HRSC does not. Therefore, for areas where there is only MOLA coverage, the estimate may be scaled upward by 1.1 times. The algorithm has been tested on DEMs with various resolutions (2 m for simulated DEM, 75m for HRSC, and 463m for MOLA). It can certainly be applied to higher resolution DEMs for Mars when they become available, but the threshold values will need to be adjusted.\n", "keywords": "DEM\nDEMs\nHRSC\nmanual editing\nMOLA\nPBTH method\nThe threshold values for removing large caters were determined\nTools for Graphics and Shapes\n"}, {"id": "S025405841530136X", "text": "From this study where a commercial Al\u201312Si alloy was inoculated with different level of Nb+B addition to assess the grain refining potency of Nb+B inoculation it can be concluded that in-situ formed Nb-based intermetallics compounds are potent heterogeneous nucleation substrates with high potency for the refinement of Al\u2013Si cast alloys. The primary \u03b1-Al dendritic grain size varies with the addition level of Nb and B. Moreover, significant grain refinement over a wide range of cooling rates is obtained via enhanced heterogeneous nucleation making the grain size of the material less sensitive to the cooling rate. Nb+B inoculants are characterised by some fading which is still acceptable after 4\u00a0h of contact time. Moreover, alloys refined by means of Nb+B inoculants can be recycled obtaining a fine grain structure with small addition or no further addition of inoculants after the first initial addition. Concluding, Nb+B inoculation is a promising candidate for the refinement of cast Al alloy which could lead to their wider employment in the automotive industry with the resultant intrinsic advantages of lighter structural component from an environmental point of view.\n", "keywords": "Al\u201312Si alloy\nalloys\nAl\u2013Si cast alloys\nB\ncast Al alloy\nenhanced heterogeneous nucleation\nfading\nfine grain structure\ngrain\ngrain refinement\ngrain refining potency of Nb+B inoculation\nheterogeneous nucleation substrates\ninoculants\ninoculated with different level of Nb+B\nlighter structural component\nNb\nNb+B\nNb-based intermetallics compounds\nNb+B inoculants\nNb+B inoculation\nrefinement of Al\u2013Si cast alloys\nrefinement of cast Al alloy\nwide range of cooling rates\n\u03b1-Al dendritic grain\n"}, {"id": "S0370269304009074", "text": "In this Letter, we extend the McVittie's solution into charged black holes. We first deduce the metric for a Reissner\u2013Nordstr\u00f6m black hole in the expanding universe; several special cases of our solution are exactly the same as some solutions discovered previously. In the previous work\u00a0[6] we have applied the asymptotic conditions to derive the Schwarzschild metric in the expanding universe, which is exactly the same as that derived by McVittie by solving the full Einstein equations. That demonstrates the power of this simple and straight-forward approach. In this Letter we follow the same procedure to derive the metric for the Reissner\u2013Nordstr\u00f6m black holes in Friedman\u2013Robertson\u2013Walker universe. We then study the influences of the evolution of the universe on the size of the black hole. Finally, in order to study the motion of the planet, we rewrite the metric from the cosmic coordinates system to the Schwarzschild coordinates system.\n", "keywords": "applied the asymptotic conditions to derive the Schwarzschild metric in the expanding universe\nasymptotic conditions to derive the Schwarzschild metric\nblack hole\ncharged black holes\ndeduce the metric for a Reissner\u2013Nordstr\u00f6m black hole in the expanding universe\nextend the McVittie's solution into charged black holes\nfollow the same procedure to derive the metric\nMcVittie's solution\nReissner\u2013Nordstr\u00f6m black hole\nReissner\u2013Nordstr\u00f6m black holes in Friedman\u2013Robertson\u2013Walker universe\nrewrite the metric from the cosmic coordinates system to the Schwarzschild coordinates system\nsolving the full Einstein equations\nstudy the influences of the evolution of the universe on the size of the black hole\nstudy the motion of the planet\n"}, {"id": "S0370269304009074", "text": "In this Letter, we extend the McVittie's solution into charged black holes. We first deduce the metric for a Reissner\u2013Nordstr\u00f6m black hole in the expanding universe; several special cases of our solution are exactly the same as some solutions discovered previously. In the previous work\u00a0[6] we have applied the asymptotic conditions to derive the Schwarzschild metric in the expanding universe, which is exactly the same as that derived by McVittie by solving the full Einstein equations. That demonstrates the power of this simple and straight-forward approach. In this Letter we follow the same procedure to derive the metric for the Reissner\u2013Nordstr\u00f6m black holes in Friedman\u2013Robertson\u2013Walker universe. We then study the influences of the evolution of the universe on the size of the black hole. Finally, in order to study the motion of the planet, we rewrite the metric from the cosmic coordinates system to the Schwarzschild coordinates system.\n", "keywords": "applied the asymptotic conditions to derive the Schwarzschild metric in the expanding universe\nasymptotic conditions to derive the Schwarzschild metric\nblack hole\ncharged black holes\ndeduce the metric for a Reissner\u2013Nordstr\u00f6m black hole in the expanding universe\nextend the McVittie's solution into charged black holes\nfollow the same procedure to derive the metric\nMcVittie's solution\nReissner\u2013Nordstr\u00f6m black hole\nReissner\u2013Nordstr\u00f6m black holes in Friedman\u2013Robertson\u2013Walker universe\nrewrite the metric from the cosmic coordinates system to the Schwarzschild coordinates system\nsolving the full Einstein equations\nstudy the influences of the evolution of the universe on the size of the black hole\nstudy the motion of the planet\n"}, {"id": "S0370269304009074", "text": "In this Letter, we extend the McVittie's solution into charged black holes. We first deduce the metric for a Reissner\u2013Nordstr\u00f6m black hole in the expanding universe; several special cases of our solution are exactly the same as some solutions discovered previously. In the previous work\u00a0[6] we have applied the asymptotic conditions to derive the Schwarzschild metric in the expanding universe, which is exactly the same as that derived by McVittie by solving the full Einstein equations. That demonstrates the power of this simple and straight-forward approach. In this Letter we follow the same procedure to derive the metric for the Reissner\u2013Nordstr\u00f6m black holes in Friedman\u2013Robertson\u2013Walker universe. We then study the influences of the evolution of the universe on the size of the black hole. Finally, in order to study the motion of the planet, we rewrite the metric from the cosmic coordinates system to the Schwarzschild coordinates system.\n", "keywords": "applied the asymptotic conditions to derive the Schwarzschild metric in the expanding universe\nasymptotic conditions to derive the Schwarzschild metric\nblack hole\ncharged black holes\ndeduce the metric for a Reissner\u2013Nordstr\u00f6m black hole in the expanding universe\nextend the McVittie's solution into charged black holes\nfollow the same procedure to derive the metric\nMcVittie's solution\nReissner\u2013Nordstr\u00f6m black hole\nReissner\u2013Nordstr\u00f6m black holes in Friedman\u2013Robertson\u2013Walker universe\nrewrite the metric from the cosmic coordinates system to the Schwarzschild coordinates system\nsolving the full Einstein equations\nstudy the influences of the evolution of the universe on the size of the black hole\nstudy the motion of the planet\n"}, {"id": "S0370269304009074", "text": "In this Letter, we extend the McVittie's solution into charged black holes. We first deduce the metric for a Reissner\u2013Nordstr\u00f6m black hole in the expanding universe; several special cases of our solution are exactly the same as some solutions discovered previously. In the previous work\u00a0[6] we have applied the asymptotic conditions to derive the Schwarzschild metric in the expanding universe, which is exactly the same as that derived by McVittie by solving the full Einstein equations. That demonstrates the power of this simple and straight-forward approach. In this Letter we follow the same procedure to derive the metric for the Reissner\u2013Nordstr\u00f6m black holes in Friedman\u2013Robertson\u2013Walker universe. We then study the influences of the evolution of the universe on the size of the black hole. Finally, in order to study the motion of the planet, we rewrite the metric from the cosmic coordinates system to the Schwarzschild coordinates system.\n", "keywords": "applied the asymptotic conditions to derive the Schwarzschild metric in the expanding universe\nasymptotic conditions to derive the Schwarzschild metric\nblack hole\ncharged black holes\ndeduce the metric for a Reissner\u2013Nordstr\u00f6m black hole in the expanding universe\nextend the McVittie's solution into charged black holes\nfollow the same procedure to derive the metric\nMcVittie's solution\nReissner\u2013Nordstr\u00f6m black hole\nReissner\u2013Nordstr\u00f6m black holes in Friedman\u2013Robertson\u2013Walker universe\nrewrite the metric from the cosmic coordinates system to the Schwarzschild coordinates system\nsolving the full Einstein equations\nstudy the influences of the evolution of the universe on the size of the black hole\nstudy the motion of the planet\n"}, {"id": "S0370269304009074", "text": "In this Letter, we extend the McVittie's solution into charged black holes. We first deduce the metric for a Reissner\u2013Nordstr\u00f6m black hole in the expanding universe; several special cases of our solution are exactly the same as some solutions discovered previously. In the previous work\u00a0[6] we have applied the asymptotic conditions to derive the Schwarzschild metric in the expanding universe, which is exactly the same as that derived by McVittie by solving the full Einstein equations. That demonstrates the power of this simple and straight-forward approach. In this Letter we follow the same procedure to derive the metric for the Reissner\u2013Nordstr\u00f6m black holes in Friedman\u2013Robertson\u2013Walker universe. We then study the influences of the evolution of the universe on the size of the black hole. Finally, in order to study the motion of the planet, we rewrite the metric from the cosmic coordinates system to the Schwarzschild coordinates system.\n", "keywords": "applied the asymptotic conditions to derive the Schwarzschild metric in the expanding universe\nasymptotic conditions to derive the Schwarzschild metric\nblack hole\ncharged black holes\ndeduce the metric for a Reissner\u2013Nordstr\u00f6m black hole in the expanding universe\nextend the McVittie's solution into charged black holes\nfollow the same procedure to derive the metric\nMcVittie's solution\nReissner\u2013Nordstr\u00f6m black hole\nReissner\u2013Nordstr\u00f6m black holes in Friedman\u2013Robertson\u2013Walker universe\nrewrite the metric from the cosmic coordinates system to the Schwarzschild coordinates system\nsolving the full Einstein equations\nstudy the influences of the evolution of the universe on the size of the black hole\nstudy the motion of the planet\n"}, {"id": "S0009261413011111", "text": "The optical properties of charged excitations are important for understanding organic semiconductor photophysics. The injection of electric charge into organic materials polarizes the surroundings and changes the bond lengths around it, such an excitation is defined as a charged polaron. Absorption of light and fluorescence quenching by polarons are important issues in the operation of organic optoelectronic devices. It is particularly relevant to the development of electrically pumped lasers. With recent advances in materials properties and optical design the lasing threshold of organic structures under optical pumping is now low enough to enable pumping by inorganic laser diodes [1\u20133] and LEDs [4] which is promising for fabrication of very sensitive low-cost devices for biosensing and chemosensing [5,6]. However, light absorption by injected charges has been reported to be the major obstacle to electrically pumped lasing [7]. Injected charges can also quench luminescence as they accept energy from excitons by resonant dipole\u2013dipole interactions and this is an important loss mechanism in organic LEDs as well as in lasers. Absorption cross-sections of polarons are not known to the desired accuracy because of the difficulty of quantifying the charge density injected into the film. Previous studies used controlled electrical injection of charges in unipolar devices through contacting electrodes and field-dependent charge mobility measurements to estimate the charge densities which were compared with the values obtained by capacitance\u2013voltage analysis and the two results differed by a factor of three [8,9].\n", "keywords": "accept energy from excitons\nbiosensing\ncapacitance\u2013voltage analysis\ncharge density\ncharged polaron\nchemosensing\ncontacting electrodes\ncontrolled electrical injection of charges\ndevelopment of electrically pumped lasers\nelectrically pumped lasers\nelectrically pumped lasing\nelectric charge\nfield-dependent charge mobility measurements\ninorganic laser diodes\nlasers\nlasing threshold of organic structures under optical pumping\nlight absorption\noperation of organic optoelectronic devices\noptical pumping\norganic LEDs\norganic materials\norganic semiconductor photophysics\norganic structures\nresonant dipole\u2013dipole interactions\nunipolar devices\nvery sensitive low-cost devices\n"}, {"id": "S2212667814000124", "text": "Based on expectation-maximization algorithm, parameter estimation was proposed for data-driven nonlinear models in this work. On this basis, particle filters were used to approximately calculate integrals, deriving EM algorithm based on particle filter. And the effectiveness of using the proposed algorithm for the soft sensor of COx content in tail gas of PX oxidation side reactions was verified through simulation results.", "keywords": "COx\ndata-driven nonlinear models\nEM algorithm\nexpectation-maximization algorithm\nparameter estimation\nparticle filter\nparticle filters\nPX\nPX oxidation\nsimulation\nsoft sensor\ntail gas\n"}, {"id": "S0021999115004301", "text": "A popular choice is to couple a set of quadrature points with an equal number of nodal Lagrange polynomials defined at the same points, leading to a collocation method. There are many examples of this throughout the literature, both in terms of the more traditionally utilised continuous Galerkin (CG) and discontinuous Galerkin (DG) formulations, as well as newer extensions such as the flux reconstruction (FR) technique as presented by Huynh [23]. In collocation methods, while most linear operators can be exactly integrated in this setting depending on the choice of quadrature, integrals of nonlinear terms typically incur numerical error. However, the computational efficiencies that can be attained through the use of a collocation formulation, especially given the presence of a diagonal mass matrix, often outweigh the numerical error that is incurred.\n", "keywords": "CG\ncollocation formulation, especially given the presence of a diagonal mass matrix\ncollocation method\ncollocation methods\ncouple a set of quadrature points with an equal number of nodal Lagrange polynomials\nDG\ndiscontinuous Galerkin\nflux reconstruction\nFR\nGalerkin\nlinear operators\nnewer extensions\n"}, {"id": "S0021999115004301", "text": "A popular choice is to couple a set of quadrature points with an equal number of nodal Lagrange polynomials defined at the same points, leading to a collocation method. There are many examples of this throughout the literature, both in terms of the more traditionally utilised continuous Galerkin (CG) and discontinuous Galerkin (DG) formulations, as well as newer extensions such as the flux reconstruction (FR) technique as presented by Huynh [23]. In collocation methods, while most linear operators can be exactly integrated in this setting depending on the choice of quadrature, integrals of nonlinear terms typically incur numerical error. However, the computational efficiencies that can be attained through the use of a collocation formulation, especially given the presence of a diagonal mass matrix, often outweigh the numerical error that is incurred.\n", "keywords": "CG\ncollocation formulation, especially given the presence of a diagonal mass matrix\ncollocation method\ncollocation methods\ncouple a set of quadrature points with an equal number of nodal Lagrange polynomials\nDG\ndiscontinuous Galerkin\nflux reconstruction\nFR\nGalerkin\nlinear operators\nnewer extensions\n"}, {"id": "S0009261415001517", "text": "Since the receptors in human biology mostly consist of chiral molecules, drug action mostly involves a specified enantiomeric form. This has spurred the development, especially in the pharmaceutical industry, of a host of techniques to secure enantiopure products. Such methods, mostly multi-step and time-consuming, can typically be cast in one of two distinct categories: synthetic mechanisms designed to produce a single stereoisomer, or separation techniques to isolate distinct enantiomers from a racemic mixture. A significant drawback, for either approach, is a dependence on a supply of enantiopure reagents or substrates \u2013 synthesis routes generally utilise chiral building blocks or enantioselective catalysts [7,8], while enantiomer separation techniques typically incorporate chiral selector molecules to form chemically distinct and distinguishable diastereomeric complexes [8,9]. A key requirement in aiming to achieve enantiopure products, irrespective of the synthetic method, is therefore a means to measure, and duly quantitate the enantiomeric excess \u2013 signifying the degree of chirality within molecular products. Chiral discrimination through optical means is well-known to offer direct, non-contact ways to distinguish between molecules of different handedness, based on observations such as the subtle differences in absorption of left- and right-handed circularly polarised light, or indeed the twisting of polarisation in optical rotation. Other optical methods, under more recent development, also show some promise to achieve enantiomer separation, as will be introduced later.\n", "keywords": "absorption\nachieve enantiopure products, irrespective of the synthetic method\nchiral building blocks\nChiral discrimination\nchiral molecules\nchiral selector molecules\ndegree of chirality\ndiastereomeric complexes\nechniques to secure enantiopure products\nenantiomeric excess\nenantiomers\nenantiomer separation\nenantiomer separation techniques\nenantiopure products\nenantiopure reagents or substrates\nenantioselective catalysts\nmeans to measure, and duly quantitate the enantiomeric excess\nmethods, mostly multi-step and time-consuming\nmolecular products\nmolecules\noptical means\noptical methods\nseparation techniques\nstereoisomer\nsynthesis routes\nsynthetic mechanisms\nsynthetic method\ntwisting of polarisation in optical rotation\n"}, {"id": "S0009261415001517", "text": "Since the receptors in human biology mostly consist of chiral molecules, drug action mostly involves a specified enantiomeric form. This has spurred the development, especially in the pharmaceutical industry, of a host of techniques to secure enantiopure products. Such methods, mostly multi-step and time-consuming, can typically be cast in one of two distinct categories: synthetic mechanisms designed to produce a single stereoisomer, or separation techniques to isolate distinct enantiomers from a racemic mixture. A significant drawback, for either approach, is a dependence on a supply of enantiopure reagents or substrates \u2013 synthesis routes generally utilise chiral building blocks or enantioselective catalysts [7,8], while enantiomer separation techniques typically incorporate chiral selector molecules to form chemically distinct and distinguishable diastereomeric complexes [8,9]. A key requirement in aiming to achieve enantiopure products, irrespective of the synthetic method, is therefore a means to measure, and duly quantitate the enantiomeric excess \u2013 signifying the degree of chirality within molecular products. Chiral discrimination through optical means is well-known to offer direct, non-contact ways to distinguish between molecules of different handedness, based on observations such as the subtle differences in absorption of left- and right-handed circularly polarised light, or indeed the twisting of polarisation in optical rotation. Other optical methods, under more recent development, also show some promise to achieve enantiomer separation, as will be introduced later.\n", "keywords": "absorption\nachieve enantiopure products, irrespective of the synthetic method\nchiral building blocks\nChiral discrimination\nchiral molecules\nchiral selector molecules\ndegree of chirality\ndiastereomeric complexes\nechniques to secure enantiopure products\nenantiomeric excess\nenantiomers\nenantiomer separation\nenantiomer separation techniques\nenantiopure products\nenantiopure reagents or substrates\nenantioselective catalysts\nmeans to measure, and duly quantitate the enantiomeric excess\nmethods, mostly multi-step and time-consuming\nmolecular products\nmolecules\noptical means\noptical methods\nseparation techniques\nstereoisomer\nsynthesis routes\nsynthetic mechanisms\nsynthetic method\ntwisting of polarisation in optical rotation\n"}, {"id": "S0142061516308079", "text": "The above discussion summarizes the state of the art related to impacts and interpretations of communication latency between RT simulators. However, research is focused primarily on the effect of the data loss during the communication and how to mitigate it [34]. In the thermo-electric co-simulation example in [35], the time constant is larger in the thermal simulation than that of power system simulation. Thus the communication latency will not significantly affect the accuracy of co-simulation. In [36], the co-simulation is performed using resources at the same location without synthetically introduced delays, which means the communication latency between RT simulators is ignored. In [37], the authors have mentioned the communication latency as an important factor in the distributed simulation and that its effect on simulation stability will be studied as future work. An in-depth research about the role of communication latency and mitigation measure for geographically distributed RT simulations is identified as a technical gap and addressed in this paper.\n", "keywords": "communication latency\nco-simulation\ndistributed simulation\nrole of communication latency\nRT simulators\nthermo-electric co-simulation\n"}, {"id": "S0142061516308079", "text": "The above discussion summarizes the state of the art related to impacts and interpretations of communication latency between RT simulators. However, research is focused primarily on the effect of the data loss during the communication and how to mitigate it [34]. In the thermo-electric co-simulation example in [35], the time constant is larger in the thermal simulation than that of power system simulation. Thus the communication latency will not significantly affect the accuracy of co-simulation. In [36], the co-simulation is performed using resources at the same location without synthetically introduced delays, which means the communication latency between RT simulators is ignored. In [37], the authors have mentioned the communication latency as an important factor in the distributed simulation and that its effect on simulation stability will be studied as future work. An in-depth research about the role of communication latency and mitigation measure for geographically distributed RT simulations is identified as a technical gap and addressed in this paper.\n", "keywords": "communication latency\nco-simulation\ndistributed simulation\nrole of communication latency\nRT simulators\nthermo-electric co-simulation\n"}, {"id": "S0142061516308079", "text": "The above discussion summarizes the state of the art related to impacts and interpretations of communication latency between RT simulators. However, research is focused primarily on the effect of the data loss during the communication and how to mitigate it [34]. In the thermo-electric co-simulation example in [35], the time constant is larger in the thermal simulation than that of power system simulation. Thus the communication latency will not significantly affect the accuracy of co-simulation. In [36], the co-simulation is performed using resources at the same location without synthetically introduced delays, which means the communication latency between RT simulators is ignored. In [37], the authors have mentioned the communication latency as an important factor in the distributed simulation and that its effect on simulation stability will be studied as future work. An in-depth research about the role of communication latency and mitigation measure for geographically distributed RT simulations is identified as a technical gap and addressed in this paper.\n", "keywords": "communication latency\nco-simulation\ndistributed simulation\nrole of communication latency\nRT simulators\nthermo-electric co-simulation\n"}, {"id": "S0032386109005357", "text": "A hydroxyl-functionalized poly(butylene succinate) based polyester was prepared by conventional polycondensation of benzyl-protected dimethyl malonate and 1,4-butanediol (Scheme 2(a)) [24a]. Yao et\u00a0al. reported on the direct polycondensation of l-lactic acid and citric acid with the formation of poly[(l-lactic acid)-co-(citric acid)], obtaining a polyester oligomer with both pendant carboxylic and hydroxyl groups [24b]. This PLCA oligomer was reacted with dihydroxylated PLLA as a macromonomer, yielding a PLCA\u2013PLLA multiblock copolymer as shown in Scheme 2(b). While lipases have been investigated for the ring-opening polymerization (ROP) of cyclic ester monomers [25,26], they have also been used for the preparation of polyesters by polycondensation reactions. The advantage of this technique is that these enzyme-catalyzed reactions proceed without protection of the pendant functional groups. In this field, hydroxyl-bearing polyesters have been synthesized by the copolymerization of divinyl adipate with various triols (e.g. glycerol, 1,2,4-butanetriol) as represented in Scheme 2(c) [27] and by copolymerizations of 1,8-octanediol with adipic acid and several alditols [28]. Very recently, several \u03b1-hydroxy acids derived from amino acids were homo- and copolymerized with lactic acid by polycondensation in bulk without protected monomers (Scheme 2(d)) [29]. Biodegradable polyesters with various pendant groups were obtained, although the molecular weights remained low (1000\u20133000gmol\u22121).\n", "keywords": "1,2,4-butanetriol\n1,4-butanediol\n1,8-octanediol\nadipic acid\nalditols\namino acids\nBiodegradable polyesters\ncitric acid\ncopolymerization\ncopolymerizations\ncyclic ester monomers\ndihydroxylated PLLA\ndimethyl malonate\ndivinyl adipate\nenzyme-catalyzed\nglycerol\nhydroxyl-bearing polyesters\nlactic acid\nlipases\nl-lactic acid\nmonomers\nPLCA oligomer\nPLCA\u2013PLLA multiblock copolymer\npolycondensation\npolyester\npolyester oligomer\npolyesters\npoly[(l-lactic acid)-co-(citric acid)]\nring-opening polymerization\nROP\ntriols\n\u03b1-hydroxy acids\n"}, {"id": "S0032386109005357", "text": "A hydroxyl-functionalized poly(butylene succinate) based polyester was prepared by conventional polycondensation of benzyl-protected dimethyl malonate and 1,4-butanediol (Scheme 2(a)) [24a]. Yao et\u00a0al. reported on the direct polycondensation of l-lactic acid and citric acid with the formation of poly[(l-lactic acid)-co-(citric acid)], obtaining a polyester oligomer with both pendant carboxylic and hydroxyl groups [24b]. This PLCA oligomer was reacted with dihydroxylated PLLA as a macromonomer, yielding a PLCA\u2013PLLA multiblock copolymer as shown in Scheme 2(b). While lipases have been investigated for the ring-opening polymerization (ROP) of cyclic ester monomers [25,26], they have also been used for the preparation of polyesters by polycondensation reactions. The advantage of this technique is that these enzyme-catalyzed reactions proceed without protection of the pendant functional groups. In this field, hydroxyl-bearing polyesters have been synthesized by the copolymerization of divinyl adipate with various triols (e.g. glycerol, 1,2,4-butanetriol) as represented in Scheme 2(c) [27] and by copolymerizations of 1,8-octanediol with adipic acid and several alditols [28]. Very recently, several \u03b1-hydroxy acids derived from amino acids were homo- and copolymerized with lactic acid by polycondensation in bulk without protected monomers (Scheme 2(d)) [29]. Biodegradable polyesters with various pendant groups were obtained, although the molecular weights remained low (1000\u20133000gmol\u22121).\n", "keywords": "1,2,4-butanetriol\n1,4-butanediol\n1,8-octanediol\nadipic acid\nalditols\namino acids\nBiodegradable polyesters\ncitric acid\ncopolymerization\ncopolymerizations\ncyclic ester monomers\ndihydroxylated PLLA\ndimethyl malonate\ndivinyl adipate\nenzyme-catalyzed\nglycerol\nhydroxyl-bearing polyesters\nlactic acid\nlipases\nl-lactic acid\nmonomers\nPLCA oligomer\nPLCA\u2013PLLA multiblock copolymer\npolycondensation\npolyester\npolyester oligomer\npolyesters\npoly[(l-lactic acid)-co-(citric acid)]\nring-opening polymerization\nROP\ntriols\n\u03b1-hydroxy acids\n"}, {"id": "S0032386109005357", "text": "A hydroxyl-functionalized poly(butylene succinate) based polyester was prepared by conventional polycondensation of benzyl-protected dimethyl malonate and 1,4-butanediol (Scheme 2(a)) [24a]. Yao et\u00a0al. reported on the direct polycondensation of l-lactic acid and citric acid with the formation of poly[(l-lactic acid)-co-(citric acid)], obtaining a polyester oligomer with both pendant carboxylic and hydroxyl groups [24b]. This PLCA oligomer was reacted with dihydroxylated PLLA as a macromonomer, yielding a PLCA\u2013PLLA multiblock copolymer as shown in Scheme 2(b). While lipases have been investigated for the ring-opening polymerization (ROP) of cyclic ester monomers [25,26], they have also been used for the preparation of polyesters by polycondensation reactions. The advantage of this technique is that these enzyme-catalyzed reactions proceed without protection of the pendant functional groups. In this field, hydroxyl-bearing polyesters have been synthesized by the copolymerization of divinyl adipate with various triols (e.g. glycerol, 1,2,4-butanetriol) as represented in Scheme 2(c) [27] and by copolymerizations of 1,8-octanediol with adipic acid and several alditols [28]. Very recently, several \u03b1-hydroxy acids derived from amino acids were homo- and copolymerized with lactic acid by polycondensation in bulk without protected monomers (Scheme 2(d)) [29]. Biodegradable polyesters with various pendant groups were obtained, although the molecular weights remained low (1000\u20133000gmol\u22121).\n", "keywords": "1,2,4-butanetriol\n1,4-butanediol\n1,8-octanediol\nadipic acid\nalditols\namino acids\nBiodegradable polyesters\ncitric acid\ncopolymerization\ncopolymerizations\ncyclic ester monomers\ndihydroxylated PLLA\ndimethyl malonate\ndivinyl adipate\nenzyme-catalyzed\nglycerol\nhydroxyl-bearing polyesters\nlactic acid\nlipases\nl-lactic acid\nmonomers\nPLCA oligomer\nPLCA\u2013PLLA multiblock copolymer\npolycondensation\npolyester\npolyester oligomer\npolyesters\npoly[(l-lactic acid)-co-(citric acid)]\nring-opening polymerization\nROP\ntriols\n\u03b1-hydroxy acids\n"}, {"id": "S0032386109005357", "text": "A hydroxyl-functionalized poly(butylene succinate) based polyester was prepared by conventional polycondensation of benzyl-protected dimethyl malonate and 1,4-butanediol (Scheme 2(a)) [24a]. Yao et\u00a0al. reported on the direct polycondensation of l-lactic acid and citric acid with the formation of poly[(l-lactic acid)-co-(citric acid)], obtaining a polyester oligomer with both pendant carboxylic and hydroxyl groups [24b]. This PLCA oligomer was reacted with dihydroxylated PLLA as a macromonomer, yielding a PLCA\u2013PLLA multiblock copolymer as shown in Scheme 2(b). While lipases have been investigated for the ring-opening polymerization (ROP) of cyclic ester monomers [25,26], they have also been used for the preparation of polyesters by polycondensation reactions. The advantage of this technique is that these enzyme-catalyzed reactions proceed without protection of the pendant functional groups. In this field, hydroxyl-bearing polyesters have been synthesized by the copolymerization of divinyl adipate with various triols (e.g. glycerol, 1,2,4-butanetriol) as represented in Scheme 2(c) [27] and by copolymerizations of 1,8-octanediol with adipic acid and several alditols [28]. Very recently, several \u03b1-hydroxy acids derived from amino acids were homo- and copolymerized with lactic acid by polycondensation in bulk without protected monomers (Scheme 2(d)) [29]. Biodegradable polyesters with various pendant groups were obtained, although the molecular weights remained low (1000\u20133000gmol\u22121).\n", "keywords": "1,2,4-butanetriol\n1,4-butanediol\n1,8-octanediol\nadipic acid\nalditols\namino acids\nBiodegradable polyesters\ncitric acid\ncopolymerization\ncopolymerizations\ncyclic ester monomers\ndihydroxylated PLLA\ndimethyl malonate\ndivinyl adipate\nenzyme-catalyzed\nglycerol\nhydroxyl-bearing polyesters\nlactic acid\nlipases\nl-lactic acid\nmonomers\nPLCA oligomer\nPLCA\u2013PLLA multiblock copolymer\npolycondensation\npolyester\npolyester oligomer\npolyesters\npoly[(l-lactic acid)-co-(citric acid)]\nring-opening polymerization\nROP\ntriols\n\u03b1-hydroxy acids\n"}, {"id": "S0032386109005357", "text": "A hydroxyl-functionalized poly(butylene succinate) based polyester was prepared by conventional polycondensation of benzyl-protected dimethyl malonate and 1,4-butanediol (Scheme 2(a)) [24a]. Yao et\u00a0al. reported on the direct polycondensation of l-lactic acid and citric acid with the formation of poly[(l-lactic acid)-co-(citric acid)], obtaining a polyester oligomer with both pendant carboxylic and hydroxyl groups [24b]. This PLCA oligomer was reacted with dihydroxylated PLLA as a macromonomer, yielding a PLCA\u2013PLLA multiblock copolymer as shown in Scheme 2(b). While lipases have been investigated for the ring-opening polymerization (ROP) of cyclic ester monomers [25,26], they have also been used for the preparation of polyesters by polycondensation reactions. The advantage of this technique is that these enzyme-catalyzed reactions proceed without protection of the pendant functional groups. In this field, hydroxyl-bearing polyesters have been synthesized by the copolymerization of divinyl adipate with various triols (e.g. glycerol, 1,2,4-butanetriol) as represented in Scheme 2(c) [27] and by copolymerizations of 1,8-octanediol with adipic acid and several alditols [28]. Very recently, several \u03b1-hydroxy acids derived from amino acids were homo- and copolymerized with lactic acid by polycondensation in bulk without protected monomers (Scheme 2(d)) [29]. Biodegradable polyesters with various pendant groups were obtained, although the molecular weights remained low (1000\u20133000gmol\u22121).\n", "keywords": "1,2,4-butanetriol\n1,4-butanediol\n1,8-octanediol\nadipic acid\nalditols\namino acids\nBiodegradable polyesters\ncitric acid\ncopolymerization\ncopolymerizations\ncyclic ester monomers\ndihydroxylated PLLA\ndimethyl malonate\ndivinyl adipate\nenzyme-catalyzed\nglycerol\nhydroxyl-bearing polyesters\nlactic acid\nlipases\nl-lactic acid\nmonomers\nPLCA oligomer\nPLCA\u2013PLLA multiblock copolymer\npolycondensation\npolyester\npolyester oligomer\npolyesters\npoly[(l-lactic acid)-co-(citric acid)]\nring-opening polymerization\nROP\ntriols\n\u03b1-hydroxy acids\n"}, {"id": "S0032386109005357", "text": "A hydroxyl-functionalized poly(butylene succinate) based polyester was prepared by conventional polycondensation of benzyl-protected dimethyl malonate and 1,4-butanediol (Scheme 2(a)) [24a]. Yao et\u00a0al. reported on the direct polycondensation of l-lactic acid and citric acid with the formation of poly[(l-lactic acid)-co-(citric acid)], obtaining a polyester oligomer with both pendant carboxylic and hydroxyl groups [24b]. This PLCA oligomer was reacted with dihydroxylated PLLA as a macromonomer, yielding a PLCA\u2013PLLA multiblock copolymer as shown in Scheme 2(b). While lipases have been investigated for the ring-opening polymerization (ROP) of cyclic ester monomers [25,26], they have also been used for the preparation of polyesters by polycondensation reactions. The advantage of this technique is that these enzyme-catalyzed reactions proceed without protection of the pendant functional groups. In this field, hydroxyl-bearing polyesters have been synthesized by the copolymerization of divinyl adipate with various triols (e.g. glycerol, 1,2,4-butanetriol) as represented in Scheme 2(c) [27] and by copolymerizations of 1,8-octanediol with adipic acid and several alditols [28]. Very recently, several \u03b1-hydroxy acids derived from amino acids were homo- and copolymerized with lactic acid by polycondensation in bulk without protected monomers (Scheme 2(d)) [29]. Biodegradable polyesters with various pendant groups were obtained, although the molecular weights remained low (1000\u20133000gmol\u22121).\n", "keywords": "1,2,4-butanetriol\n1,4-butanediol\n1,8-octanediol\nadipic acid\nalditols\namino acids\nBiodegradable polyesters\ncitric acid\ncopolymerization\ncopolymerizations\ncyclic ester monomers\ndihydroxylated PLLA\ndimethyl malonate\ndivinyl adipate\nenzyme-catalyzed\nglycerol\nhydroxyl-bearing polyesters\nlactic acid\nlipases\nl-lactic acid\nmonomers\nPLCA oligomer\nPLCA\u2013PLLA multiblock copolymer\npolycondensation\npolyester\npolyester oligomer\npolyesters\npoly[(l-lactic acid)-co-(citric acid)]\nring-opening polymerization\nROP\ntriols\n\u03b1-hydroxy acids\n"}, {"id": "S0021999112002847", "text": "In this work we develop a new approach to DEA suitable for modelling three-dimensional problems. The present DEA methods rely on the fact that one can easily parametrise the boundary of the region being modelled, and then apply an orthonormal basis approximation over the resulting boundary phase space coordinate system. In two dimensions this is simple as the boundary may be parametrised along its arc-length and the associated momentum (or direction) coordinate taken tangential to the boundary. The basis can be any suitable (scaled) univariate basis in both position and momentum, such as a Fourier basis [8] or Chebyshev polynomials [9]. Defining a suitable parametrisation for the spatial coordinate in three-dimensions becomes much more difficult. In momentum space spherical polar coordinates may be employed and so these problems do not arise.\n", "keywords": "arc-length\nboundary phase space coordinate system\nChebyshev polynomials\nDEA\nDEA methods\ndevelop a new approach to DEA suitable for modelling three-dimensional problems\nFourier basis\nmodelling three-dimensional problems\nmomentum (or direction) coordinate\northonormal basis approximation\nparametrise the boundary of the region\nspherical polar coordinates\nunivariate basis\n"}, {"id": "S0021999112002847", "text": "In this work we develop a new approach to DEA suitable for modelling three-dimensional problems. The present DEA methods rely on the fact that one can easily parametrise the boundary of the region being modelled, and then apply an orthonormal basis approximation over the resulting boundary phase space coordinate system. In two dimensions this is simple as the boundary may be parametrised along its arc-length and the associated momentum (or direction) coordinate taken tangential to the boundary. The basis can be any suitable (scaled) univariate basis in both position and momentum, such as a Fourier basis [8] or Chebyshev polynomials [9]. Defining a suitable parametrisation for the spatial coordinate in three-dimensions becomes much more difficult. In momentum space spherical polar coordinates may be employed and so these problems do not arise.\n", "keywords": "arc-length\nboundary phase space coordinate system\nChebyshev polynomials\nDEA\nDEA methods\ndevelop a new approach to DEA suitable for modelling three-dimensional problems\nFourier basis\nmodelling three-dimensional problems\nmomentum (or direction) coordinate\northonormal basis approximation\nparametrise the boundary of the region\nspherical polar coordinates\nunivariate basis\n"}, {"id": "S092583881302834X", "text": "Plastically deformed MGs develop inhomogeneity and show harder and softer regions [16]. While this could in principle be associated with a BE according to the composite model, a MG provides no basis for a dislocation-based theory. The search for a BE in plastic flow is hindered by the softening of MGs associated with shear-banding (in contrast to the work-hardening familiar in conventional alloys). Anelastic deformation is, however, of interest as its time-dependence must relate to relaxation processes in the MG structure that in turn should be connected to the onset of plasticity. In particular, anelasticity may offer a way to study the operation of the shear transformation zones (STZs [17]) often used to interpret the deformation of MGs. Fujita et al. have used torsion tests to observe anelasticity in MGs loaded (at maximum, on the cylindrical sample surface) to 30%, 16% and just 4% of the shear yield stress \u03c4y [18]. In the present work we apply torsion to MG samples to reach stresses up to 24% of \u03c4y, and for the first time in the elastic regime investigate the effects of torque reversal.\n", "keywords": "Anelastic deformation\ninvestigate the effects of torque reversal.\nMG\nMGs\nobserve anelasticity in MGs\nsearch for a BE in plastic flow\nshear-banding\nshear transformation zones\nSTZs\n"}, {"id": "S092583881302834X", "text": "Plastically deformed MGs develop inhomogeneity and show harder and softer regions [16]. While this could in principle be associated with a BE according to the composite model, a MG provides no basis for a dislocation-based theory. The search for a BE in plastic flow is hindered by the softening of MGs associated with shear-banding (in contrast to the work-hardening familiar in conventional alloys). Anelastic deformation is, however, of interest as its time-dependence must relate to relaxation processes in the MG structure that in turn should be connected to the onset of plasticity. In particular, anelasticity may offer a way to study the operation of the shear transformation zones (STZs [17]) often used to interpret the deformation of MGs. Fujita et al. have used torsion tests to observe anelasticity in MGs loaded (at maximum, on the cylindrical sample surface) to 30%, 16% and just 4% of the shear yield stress \u03c4y [18]. In the present work we apply torsion to MG samples to reach stresses up to 24% of \u03c4y, and for the first time in the elastic regime investigate the effects of torque reversal.\n", "keywords": "Anelastic deformation\ninvestigate the effects of torque reversal.\nMG\nMGs\nobserve anelasticity in MGs\nsearch for a BE in plastic flow\nshear-banding\nshear transformation zones\nSTZs\n"}, {"id": "S092583881302834X", "text": "Plastically deformed MGs develop inhomogeneity and show harder and softer regions [16]. While this could in principle be associated with a BE according to the composite model, a MG provides no basis for a dislocation-based theory. The search for a BE in plastic flow is hindered by the softening of MGs associated with shear-banding (in contrast to the work-hardening familiar in conventional alloys). Anelastic deformation is, however, of interest as its time-dependence must relate to relaxation processes in the MG structure that in turn should be connected to the onset of plasticity. In particular, anelasticity may offer a way to study the operation of the shear transformation zones (STZs [17]) often used to interpret the deformation of MGs. Fujita et al. have used torsion tests to observe anelasticity in MGs loaded (at maximum, on the cylindrical sample surface) to 30%, 16% and just 4% of the shear yield stress \u03c4y [18]. In the present work we apply torsion to MG samples to reach stresses up to 24% of \u03c4y, and for the first time in the elastic regime investigate the effects of torque reversal.\n", "keywords": "Anelastic deformation\ninvestigate the effects of torque reversal.\nMG\nMGs\nobserve anelasticity in MGs\nsearch for a BE in plastic flow\nshear-banding\nshear transformation zones\nSTZs\n"}, {"id": "S092583881302834X", "text": "Plastically deformed MGs develop inhomogeneity and show harder and softer regions [16]. While this could in principle be associated with a BE according to the composite model, a MG provides no basis for a dislocation-based theory. The search for a BE in plastic flow is hindered by the softening of MGs associated with shear-banding (in contrast to the work-hardening familiar in conventional alloys). Anelastic deformation is, however, of interest as its time-dependence must relate to relaxation processes in the MG structure that in turn should be connected to the onset of plasticity. In particular, anelasticity may offer a way to study the operation of the shear transformation zones (STZs [17]) often used to interpret the deformation of MGs. Fujita et al. have used torsion tests to observe anelasticity in MGs loaded (at maximum, on the cylindrical sample surface) to 30%, 16% and just 4% of the shear yield stress \u03c4y [18]. In the present work we apply torsion to MG samples to reach stresses up to 24% of \u03c4y, and for the first time in the elastic regime investigate the effects of torque reversal.\n", "keywords": "Anelastic deformation\ninvestigate the effects of torque reversal.\nMG\nMGs\nobserve anelasticity in MGs\nsearch for a BE in plastic flow\nshear-banding\nshear transformation zones\nSTZs\n"}, {"id": "S092583881302834X", "text": "Plastically deformed MGs develop inhomogeneity and show harder and softer regions [16]. While this could in principle be associated with a BE according to the composite model, a MG provides no basis for a dislocation-based theory. The search for a BE in plastic flow is hindered by the softening of MGs associated with shear-banding (in contrast to the work-hardening familiar in conventional alloys). Anelastic deformation is, however, of interest as its time-dependence must relate to relaxation processes in the MG structure that in turn should be connected to the onset of plasticity. In particular, anelasticity may offer a way to study the operation of the shear transformation zones (STZs [17]) often used to interpret the deformation of MGs. Fujita et al. have used torsion tests to observe anelasticity in MGs loaded (at maximum, on the cylindrical sample surface) to 30%, 16% and just 4% of the shear yield stress \u03c4y [18]. In the present work we apply torsion to MG samples to reach stresses up to 24% of \u03c4y, and for the first time in the elastic regime investigate the effects of torque reversal.\n", "keywords": "Anelastic deformation\ninvestigate the effects of torque reversal.\nMG\nMGs\nobserve anelasticity in MGs\nsearch for a BE in plastic flow\nshear-banding\nshear transformation zones\nSTZs\n"}, {"id": "S0377221716304258", "text": "Two-state models are often insufficient to fit complex traces, therefore we also study the approximate fitting of large M3PPs. In the single class setting, a known limitation of MMPPs is the inability to simultaneously fit many statistical descriptors due to the non-linearity of their underlying equations (Bodrog, Heindl, Horv\u00e1th, & Telek, 2008; Heindl, Horv\u00e1th, & Gross, 2006; Horv\u00e1th & Telek, 2009). This has led to the definition of several approaches to fit complex traces by composing multiple small-sized MMPPs or MAPs using Kronecker operators (Andersen & Nielsen, 1998; Casale, Zhang, & Smirni, 2010; Horv\u00e1th & Telek, 2002). These methods employ composition operators for moment fitting, offering a different trade-off between computational cost and fitting accuracy compared to fitting methods based on the EM algorithm (Breuer, 2002; Horv\u00e1th & Okamura, 2013; Klemm, Lindemann, & Lohmann, 2003). In particular, the superposition operator allows one to describe a trace by the statistical multiplexing of several MMPPs, at the expense of an exponential growth of the number of states in the resulting process (Sriram & Whitt, 1986). This state space explosion is an obstacle for the application of MMPPs and MAPs to modeling real systems; for example it considerably slows down, or even renders infeasible, the numerical evaluation of queueing models by matrix geometric methods (Bini, Meini, Steff\u00e9, P\u00e9rez, & Houdt, 2012; P\u00e9rez, Velthoven, & Houdt, 2008).\n", "keywords": "composing multiple small-sized MMPPs or MAPs\ncomposition operators\nEM algorithm\nfitting methods\nKronecker operators\nM3PPs\nmatrix geometric methods\nMMPPs\nMMPPs and MAPs\nmodeling real systems\nnumerical evaluation of queueing models\nstate space explosion\nstatistical descriptors\nstatistical multiplexing of several MMPPs\nstudy the approximate fitting of large M3PPs\nsuperposition operator\nTwo-state models\nunderlying equations\n"}, {"id": "S002199911300346X", "text": "Contact methods have been developed and used in Lagrangian staggered-grid hydrodynamic (SGH) calculations for many years. Early examples of contact methods are discussed in Wilkins [37] and Cherry et al. [7]. Hallquist et al. [17] provides an overview of multiple contact algorithms used in various Lagrangian SGH codes dating back to HEMP [37]. Of particular interest, Hallquist et al. [17] describes the contact surface scheme used in TOODY [31] and later implemented in DYNA2D [36]. The contact method of TOODY uses a master\u2013slave approach. The goal of this approach is to treat the nodes on the contact surface in a manner similar to an internal node. The physical properties of the slave surface are interpolated to a ghost mesh (termed phony elements in [17]) that overlays the slave zones. The physical properties are interpolated from the slave surface to the ghost zones using surface area weights. The surface area weights are equal to the ratio of the ghost zone surface area to the surface area of the master surface. The contact surface method for nodal-based Lagrangian cell-centered hydrodynamics (CCH) presented in this paper will use surface area weights similar in concept to those in TOODY. Following the area fraction approach of TOODY may seem retrospective; however, using surface area weights naturally extends to the new CCH methods that solve a Riemann-like problem at the node of a zone [10,24,25,3].\n", "keywords": "area fraction approach\nCCH\nCCH methods\ncell-centered hydrodynamics\ncontact algorithms\ncontact method\ncontact methods\nContact methods\ncontact surface\ncontact surface method\ncontact surface scheme\nDYNA2D\nghost mesh\nghost zones\nghost zone surface area\nHEMP\ninternal node\nLagrangian SGH\nmaster\u2013slave approach\nmaster surface\nnodal-based Lagrangian cell-centered hydrodynamics\nnodes\nphony elements\nSGH\nslave surface\nslave zones\nsolve a Riemann-like problem at the node of a zone\nstaggered-grid hydrodynamic\nsurface area\nTOODY\n"}, {"id": "S2212671612002120", "text": "A design method for network attack and defense simulation platform is discussed in this paper. Firstly the component and function of the platform are analyzed. Then Visio second development method is used to construct the virtual network topology. The parsing of virtual network topology is also researched and the relative flow sheet is described. Lastly an example is carried out to test performance of the platform. Simulation results show the effectiveness of the proposed method.", "keywords": "construct the virtual network topology\ndefense simulation platform\ndesign method\nnetwork attack\nparsing of virtual network topology\nplatform\nrelative flow sheet\ntest performance\ntest performance of the platform\nvirtual network\nVisio second development method\n"}, {"id": "S2212671612002120", "text": "A design method for network attack and defense simulation platform is discussed in this paper. Firstly the component and function of the platform are analyzed. Then Visio second development method is used to construct the virtual network topology. The parsing of virtual network topology is also researched and the relative flow sheet is described. Lastly an example is carried out to test performance of the platform. Simulation results show the effectiveness of the proposed method.", "keywords": "construct the virtual network topology\ndefense simulation platform\ndesign method\nnetwork attack\nparsing of virtual network topology\nplatform\nrelative flow sheet\ntest performance\ntest performance of the platform\nvirtual network\nVisio second development method\n"}, {"id": "S2212671612002120", "text": "A design method for network attack and defense simulation platform is discussed in this paper. Firstly the component and function of the platform are analyzed. Then Visio second development method is used to construct the virtual network topology. The parsing of virtual network topology is also researched and the relative flow sheet is described. Lastly an example is carried out to test performance of the platform. Simulation results show the effectiveness of the proposed method.", "keywords": "construct the virtual network topology\ndefense simulation platform\ndesign method\nnetwork attack\nparsing of virtual network topology\nplatform\nrelative flow sheet\ntest performance\ntest performance of the platform\nvirtual network\nVisio second development method\n"}, {"id": "S1877750311000676", "text": "One way to enforce this ratio is to use a probabilistic, \u2018roulette wheel\u2019 style lane selection policy. VISSIM, along with most simulation toolkits, offers methods to specify probabilistic routing whereby a defined percentage of vehicles are sent down unique routes. This is a piecewise technique that can be reapplied at various locations around a simulation. While these methods are attractive from a calibration perspective as exact representations of existing statistics can be ensured, the process is an unrealistic one as it assumes that drivers make probabilistic decisions at precise locations. So in this case when a vehicle arrives at a point prior to the weighbridges it is allocated one of the lanes based on the respective probabilities. It turns out that this method leads to significant variations in trip times depending on the initial random number seed, this can be seen in a graphic of the key areas of the simulation for the 2 different runs (Fig. 7). One of the benefits of graphical microsimulation is that the 2D and 3D simulations help the researcher to visualise a new scheme and its potential benefits but also to highlight unrealistic behaviour. Fig. 7 shows the congestion at the decision point for 2 different runs. Using probabilistic routing to enforce correct routing percentages is a clear case of overcalibration affecting simulation brittleness.\n", "keywords": "2D and 3D simulations\ndrivers\ngraphical microsimulation\nlane selection policy\npiecewise technique\nprobabilistic, \u2018roulette wheel\u2019 style\nprobabilistic routing\nsimulation\nsimulation toolkits\nvehicle\nVISSIM\nweighbridges\n"}, {"id": "S2212667814000045", "text": "The opportunity offered by digital technologies to make deep rationalization in purchase of supplies is becoming indispensable in competition between enterprises, considering positive effects in reducing the costs of the companies that have adopted the E-Procurement. As it has been confirmed by numerous case studies, automation of procedures for the purchase through e-procurement technology enables companies to achieve a reduction in costs (average 8-12%) of total purchases. So web-based models are playing a critical role within companies, especially in the generation of value of supply chain. This article focuses on the role of e-procurement within a supply chain showing, through simulations, the advantages and difficulties of implementing a systematic use of the Internet and defining the basic structure of an e-supply chain.", "keywords": "automation of procedures\ncase studies\nE-Procurement\ne-procurement technology\ne-supply chain\ngeneration of value of supply chain\nrationalization\nreducing the costs\nrole of e-procurement\nsimulations\nsupplies\nsupply chain\nweb-based models\n"}, {"id": "S2212667814000045", "text": "The opportunity offered by digital technologies to make deep rationalization in purchase of supplies is becoming indispensable in competition between enterprises, considering positive effects in reducing the costs of the companies that have adopted the E-Procurement. As it has been confirmed by numerous case studies, automation of procedures for the purchase through e-procurement technology enables companies to achieve a reduction in costs (average 8-12%) of total purchases. So web-based models are playing a critical role within companies, especially in the generation of value of supply chain. This article focuses on the role of e-procurement within a supply chain showing, through simulations, the advantages and difficulties of implementing a systematic use of the Internet and defining the basic structure of an e-supply chain.", "keywords": "automation of procedures\ncase studies\nE-Procurement\ne-procurement technology\ne-supply chain\ngeneration of value of supply chain\nrationalization\nreducing the costs\nrole of e-procurement\nsimulations\nsupplies\nsupply chain\nweb-based models\n"}, {"id": "S0045782512000266", "text": "In this work, a numerical strategy for designing an optimal maintenance scheduling for a structure, accounting explicitly for the effects of uncertainty is suggested. This contribution, which can be regarded as an extension of the methods developed in [23], presents several novel aspects over similar approaches proposed in the literature. Firstly, the initiation and propagation of fatigue crack is modeled efficiently by means of cohesive zone elements [24\u201326]. The application of this class of elements allows modeling the crack initiation and propagation within a unified framework. It should be noted that cohesive zone elements have already been used for uncertainty quantification of the crack propagation phenomenon [27,28]. However its application within the context of maintenance scheduling constitutes a novelty. The second innovative aspect of this contribution refers to the assessment of the reliability sensitivity with respect to the variables that define the maintenance scheduling. The estimation of this sensitivity, which is required in order to determine the optimal maintenance schedule within the proposed framework, can be quite demanding as the model characterizing repair of a cracked structure leads to a discontinuous performance function associated with the failure probability. A new approach for modeling this function is proposed herein. The continuous and discontinuous parts respectively of the function are considered separately to estimate accurately the gradients of the failure events.\n", "keywords": "accounting explicitly for the effects of uncertainty\nassessment of the reliability sensitivity with respect to the variables that define the maintenance scheduling\nclass of elements\ncohesive zone elements\ncontinuous and discontinuous parts\ndiscontinuous performance function associated with the failure probability\nestimate accurately the gradients of the failure events\nestimation of this sensitivity\ninitiation and propagation of fatigue crack\nmaintenance scheduling\nmodel characterizing repair of a cracked structure\nmodeling the crack initiation and propagation within a unified framework\nnumerical strategy\noptimal maintenance schedule within the proposed framework\noptimal maintenance scheduling for a structure\nuncertainty quantification of the crack propagation phenomenon\nunified framework\n"}, {"id": "S0021999113002945", "text": "Inequality (22) indicates that the maximum-norm is the loosest among all p-norms. Fortunately, this loosest constraint would not seriously affect the accuracy since the value of ||y||\u221e is comparable to that of the 2-norm and 1-norm. The maximum-norm provides us with the largest number of possible solutions under a given error limitation [24]. This would greatly enhance the possibility of finding a group of optimized coefficients when scanning a vast solution set. On the other hand, checking the maximum deviation sounds more reasonable than checking the \u201cdistance\u201d between the accurate and approximated wave numbers since it is not working in the space domain. Therefore, we chose the maximum-norm as our criterion for designing the objective functions to extend the accurate wave number coverage as widely as possible.\n", "keywords": "1-norm\n2-norm\nextend the accurate wave number coverage\nInequality\nmaximum deviation\nmaximum-norm\nobjective functions\noptimized coefficients\np-norms\nvast solution set\n||y||\u221e\n"}, {"id": "S0045782515001322", "text": "One of the most important outcomes of the comparative analysis is the fact that in all tested cases the use of FM is associated with a dramatic reduction in computational time when compared with FE, generally being in the order of seconds for FM and in the order of hours for FE. Table\u00a01 reports the timings of the simulations for both methods. Free expansion is the fastest case, where FM reaches the load-free configuration in just 2\u00a0s, while simulations inside the vessels with the diameter of around 30\u00a0mm take approximately 30\u00a0s. Most of the execution time of the FM deployment algorithm is dedicated to the contact check and calculations of the implications the vessel wall has on the stent structure. Interestingly, in both methods, the highest computational time (i.e.,\u00a0curved vessels) is not associated with the most complex geometry (i.e.,\u00a0patient-specific case of aortic dissection). Another fact worth mentioning is the relation of the computational time to the diameter of the vessel in both methods. While the computational time of FM appeared to be directly related to the diameter of the vessel, no immediate relation was found for the FE simulations. Such outcome is probably related to the simplified contact model used by FM, which makes the stent-graft expansion terminate once the nodes come in contact with the vessel wall. On the contrary, it is well known that the contact algorithm used in the FE analyses increases the computational cost of the simulations.\n", "keywords": "comparative analysis\ncontact algorithm\ncurved vessels\nFE\nFE analyses\nFE simulations\nFM\nFM deployment algorithm\nnodes\nsimplified contact model\nsimulations\nstent-graft expansion\nvessel\nvessels\nvessel wall\n"}, {"id": "S0370269304008974", "text": "Thus, the extension to the charmed analogue \u0398c(3099) provides an interesting test for the SDO sum rule and lattice calculations [17]. Here, the charm quark is quite heavy so that the constituent-quark picture may fit well and the JW prediction for the parity is expected to be reproduced from QCD. In fact, quenched lattice calculation finds the parity of \u0398c(3099) to be positive\u00a0[28]. In the extension to the \u0398c(3099) sum rules, there are two important aspects, which make this sum rule different from the SDO sum rule. First of all, since the charm quark is too heavy to form quark condensate, it gives non-perturbative effects only by radiating gluons. The quark\u2013gluon mixed condensate \u3008s\u0304gs\u03c3\u00b7Gs\u3009, which was the important contribution in the \u0398+ sum rule, is replaced by gluonic operators in the heavy quark expansion that are normally suppressed. Secondly, the charm quark mass has to be kept finite in the OPE, which can be done by using the momentum space expression for the charm-quark propagator. This is different from the light-quark sum rule where the calculation is performed in the coordinate space and all the quark propagators are obtained based on the expansion with the small quark mass. Keeping these two aspects in mind, we construct QCD sum rules for \u0398c(3099) and see how they are different from the \u0398+(1540) sum rule.\n", "keywords": "charm quark\ncharm quark mass has to be kept finite in the OPE\ncharm-quark propagator\nconstituent-quark picture\nextension to the charmed analogue \u0398c(3099)\ngluonic operators\nheavy quark\nJW prediction\nmomentum space expression\nOPE\nQCD\nQCD sum rules\nquark condensate\nquark\u2013gluon mixed condensate\nquark propagators\nquenched lattice calculation\nradiating gluons\nreplaced by gluonic operators in the heavy quark expansion\nSDO sum rule\nSDO sum rule and lattice calculations\nsee how they are different from the \u0398+(1540) sum rule\ns\u0304gs\u03c3\u00b7Gs\nsmall quark\nsum rule\nthe light-quark sum rule\n\u0398+ sum rule\n"}, {"id": "S0167931711005120", "text": "A nanocomposite system consisting of a semiconducting matrix and embedded ferromagnetic nanostructures has been fabricated. The ferromagnetic characteristics as coercivity, remanence and magnetic anisotropy of the nanocomposite can be adjusted by the electrochemical parameters. Furthermore the spatial distribution of the metal structures within the pores can be varied which means that the magnetic interactions between the particles can be influenced. In the case of densely packed particles within the pores dipolar coupling between them occurs and results in quasi magnetic chains which offer a much larger magnetic anisotropy than non-interacting particles. By modifying the current density small Ni-particles (3\u20136nm) can be deposited. If the packing density of these particles is sufficiently close, Ni-tubes of a few nanometer in thickness are covering the pore walls. The presented nanocomposite is an interesting system for magnetic applications as magnetic sensor technology. Silicon as substrate renders this composite a good candidate for the integration in existing process technology.\n", "keywords": "a good candidate for the integration\na much larger magnetic anisotropy\nA nanocomposite system\na semiconducting matrix and embedded ferromagnetic nanostructures\ncoercivity, remanence and magnetic anisotropy of the nanocomposite\ndensely packed particles\ndistribution\nmodifying the current density\nNi-tubes\nnon-interacting particles\nSilicon\nsmall Ni-particles (3\u20136nm)\nthe electrochemical parameters\nThe ferromagnetic characteristics\nThe presented nanocomposite\nthese particles\n"}, {"id": "S0167931711005120", "text": "A nanocomposite system consisting of a semiconducting matrix and embedded ferromagnetic nanostructures has been fabricated. The ferromagnetic characteristics as coercivity, remanence and magnetic anisotropy of the nanocomposite can be adjusted by the electrochemical parameters. Furthermore the spatial distribution of the metal structures within the pores can be varied which means that the magnetic interactions between the particles can be influenced. In the case of densely packed particles within the pores dipolar coupling between them occurs and results in quasi magnetic chains which offer a much larger magnetic anisotropy than non-interacting particles. By modifying the current density small Ni-particles (3\u20136nm) can be deposited. If the packing density of these particles is sufficiently close, Ni-tubes of a few nanometer in thickness are covering the pore walls. The presented nanocomposite is an interesting system for magnetic applications as magnetic sensor technology. Silicon as substrate renders this composite a good candidate for the integration in existing process technology.\n", "keywords": "a good candidate for the integration\na much larger magnetic anisotropy\nA nanocomposite system\na semiconducting matrix and embedded ferromagnetic nanostructures\ncoercivity, remanence and magnetic anisotropy of the nanocomposite\ndensely packed particles\ndistribution\nmodifying the current density\nNi-tubes\nnon-interacting particles\nSilicon\nsmall Ni-particles (3\u20136nm)\nthe electrochemical parameters\nThe ferromagnetic characteristics\nThe presented nanocomposite\nthese particles\n"}, {"id": "S0167931711005120", "text": "A nanocomposite system consisting of a semiconducting matrix and embedded ferromagnetic nanostructures has been fabricated. The ferromagnetic characteristics as coercivity, remanence and magnetic anisotropy of the nanocomposite can be adjusted by the electrochemical parameters. Furthermore the spatial distribution of the metal structures within the pores can be varied which means that the magnetic interactions between the particles can be influenced. In the case of densely packed particles within the pores dipolar coupling between them occurs and results in quasi magnetic chains which offer a much larger magnetic anisotropy than non-interacting particles. By modifying the current density small Ni-particles (3\u20136nm) can be deposited. If the packing density of these particles is sufficiently close, Ni-tubes of a few nanometer in thickness are covering the pore walls. The presented nanocomposite is an interesting system for magnetic applications as magnetic sensor technology. Silicon as substrate renders this composite a good candidate for the integration in existing process technology.\n", "keywords": "a good candidate for the integration\na much larger magnetic anisotropy\nA nanocomposite system\na semiconducting matrix and embedded ferromagnetic nanostructures\ncoercivity, remanence and magnetic anisotropy of the nanocomposite\ndensely packed particles\ndistribution\nmodifying the current density\nNi-tubes\nnon-interacting particles\nSilicon\nsmall Ni-particles (3\u20136nm)\nthe electrochemical parameters\nThe ferromagnetic characteristics\nThe presented nanocomposite\nthese particles\n"}, {"id": "S0167931711005120", "text": "A nanocomposite system consisting of a semiconducting matrix and embedded ferromagnetic nanostructures has been fabricated. The ferromagnetic characteristics as coercivity, remanence and magnetic anisotropy of the nanocomposite can be adjusted by the electrochemical parameters. Furthermore the spatial distribution of the metal structures within the pores can be varied which means that the magnetic interactions between the particles can be influenced. In the case of densely packed particles within the pores dipolar coupling between them occurs and results in quasi magnetic chains which offer a much larger magnetic anisotropy than non-interacting particles. By modifying the current density small Ni-particles (3\u20136nm) can be deposited. If the packing density of these particles is sufficiently close, Ni-tubes of a few nanometer in thickness are covering the pore walls. The presented nanocomposite is an interesting system for magnetic applications as magnetic sensor technology. Silicon as substrate renders this composite a good candidate for the integration in existing process technology.\n", "keywords": "a good candidate for the integration\na much larger magnetic anisotropy\nA nanocomposite system\na semiconducting matrix and embedded ferromagnetic nanostructures\ncoercivity, remanence and magnetic anisotropy of the nanocomposite\ndensely packed particles\ndistribution\nmodifying the current density\nNi-tubes\nnon-interacting particles\nSilicon\nsmall Ni-particles (3\u20136nm)\nthe electrochemical parameters\nThe ferromagnetic characteristics\nThe presented nanocomposite\nthese particles\n"}, {"id": "S0370269304006756", "text": "We propose a method for the lattice QCD computation of nucleon\u2013nucleon low-energy interactions. It consists in simulating QCD in the background of a \u201celectromagnetic\u201d field whose potential is non-vanishing, but whose field strength is zero. By tuning the background field, phase-shifts at any (but small) momenta can be determined by measuring the shift of the ground state energy. Lattice sizes as small as 5 Fermi can be sufficient for the calculation of phase shifts up to momenta of order of m\u03c0/2.", "keywords": "calculation of phase shifts\nelectromagnetic\u201d field\nfield strength is zero\nlattice QCD computation\nlattice QCD computation of nucleon\u2013nucleon low-energy interactions\nLattice sizes\nmeasuring the shift of the ground state energy\nnucleon\u2013nucleon low-energy interactions\nphase-shifts at any (but small) momenta\nsimulating QCD\ntuning the background field\n"}, {"id": "S0370269304006756", "text": "We propose a method for the lattice QCD computation of nucleon\u2013nucleon low-energy interactions. It consists in simulating QCD in the background of a \u201celectromagnetic\u201d field whose potential is non-vanishing, but whose field strength is zero. By tuning the background field, phase-shifts at any (but small) momenta can be determined by measuring the shift of the ground state energy. Lattice sizes as small as 5 Fermi can be sufficient for the calculation of phase shifts up to momenta of order of m\u03c0/2.", "keywords": "calculation of phase shifts\nelectromagnetic\u201d field\nfield strength is zero\nlattice QCD computation\nlattice QCD computation of nucleon\u2013nucleon low-energy interactions\nLattice sizes\nmeasuring the shift of the ground state energy\nnucleon\u2013nucleon low-energy interactions\nphase-shifts at any (but small) momenta\nsimulating QCD\ntuning the background field\n"}, {"id": "S0370269304006756", "text": "We propose a method for the lattice QCD computation of nucleon\u2013nucleon low-energy interactions. It consists in simulating QCD in the background of a \u201celectromagnetic\u201d field whose potential is non-vanishing, but whose field strength is zero. By tuning the background field, phase-shifts at any (but small) momenta can be determined by measuring the shift of the ground state energy. Lattice sizes as small as 5 Fermi can be sufficient for the calculation of phase shifts up to momenta of order of m\u03c0/2.", "keywords": "calculation of phase shifts\nelectromagnetic\u201d field\nfield strength is zero\nlattice QCD computation\nlattice QCD computation of nucleon\u2013nucleon low-energy interactions\nLattice sizes\nmeasuring the shift of the ground state energy\nnucleon\u2013nucleon low-energy interactions\nphase-shifts at any (but small) momenta\nsimulating QCD\ntuning the background field\n"}, {"id": "S0370269304006756", "text": "We propose a method for the lattice QCD computation of nucleon\u2013nucleon low-energy interactions. It consists in simulating QCD in the background of a \u201celectromagnetic\u201d field whose potential is non-vanishing, but whose field strength is zero. By tuning the background field, phase-shifts at any (but small) momenta can be determined by measuring the shift of the ground state energy. Lattice sizes as small as 5 Fermi can be sufficient for the calculation of phase shifts up to momenta of order of m\u03c0/2.", "keywords": "calculation of phase shifts\nelectromagnetic\u201d field\nfield strength is zero\nlattice QCD computation\nlattice QCD computation of nucleon\u2013nucleon low-energy interactions\nLattice sizes\nmeasuring the shift of the ground state energy\nnucleon\u2013nucleon low-energy interactions\nphase-shifts at any (but small) momenta\nsimulating QCD\ntuning the background field\n"}, {"id": "S0370269304006756", "text": "We propose a method for the lattice QCD computation of nucleon\u2013nucleon low-energy interactions. It consists in simulating QCD in the background of a \u201celectromagnetic\u201d field whose potential is non-vanishing, but whose field strength is zero. By tuning the background field, phase-shifts at any (but small) momenta can be determined by measuring the shift of the ground state energy. Lattice sizes as small as 5 Fermi can be sufficient for the calculation of phase shifts up to momenta of order of m\u03c0/2.", "keywords": "calculation of phase shifts\nelectromagnetic\u201d field\nfield strength is zero\nlattice QCD computation\nlattice QCD computation of nucleon\u2013nucleon low-energy interactions\nLattice sizes\nmeasuring the shift of the ground state energy\nnucleon\u2013nucleon low-energy interactions\nphase-shifts at any (but small) momenta\nsimulating QCD\ntuning the background field\n"}, {"id": "S0370269304006756", "text": "We propose a method for the lattice QCD computation of nucleon\u2013nucleon low-energy interactions. It consists in simulating QCD in the background of a \u201celectromagnetic\u201d field whose potential is non-vanishing, but whose field strength is zero. By tuning the background field, phase-shifts at any (but small) momenta can be determined by measuring the shift of the ground state energy. Lattice sizes as small as 5 Fermi can be sufficient for the calculation of phase shifts up to momenta of order of m\u03c0/2.", "keywords": "calculation of phase shifts\nelectromagnetic\u201d field\nfield strength is zero\nlattice QCD computation\nlattice QCD computation of nucleon\u2013nucleon low-energy interactions\nLattice sizes\nmeasuring the shift of the ground state energy\nnucleon\u2013nucleon low-energy interactions\nphase-shifts at any (but small) momenta\nsimulating QCD\ntuning the background field\n"}, {"id": "S0370269304006756", "text": "We propose a method for the lattice QCD computation of nucleon\u2013nucleon low-energy interactions. It consists in simulating QCD in the background of a \u201celectromagnetic\u201d field whose potential is non-vanishing, but whose field strength is zero. By tuning the background field, phase-shifts at any (but small) momenta can be determined by measuring the shift of the ground state energy. Lattice sizes as small as 5 Fermi can be sufficient for the calculation of phase shifts up to momenta of order of m\u03c0/2.", "keywords": "calculation of phase shifts\nelectromagnetic\u201d field\nfield strength is zero\nlattice QCD computation\nlattice QCD computation of nucleon\u2013nucleon low-energy interactions\nLattice sizes\nmeasuring the shift of the ground state energy\nnucleon\u2013nucleon low-energy interactions\nphase-shifts at any (but small) momenta\nsimulating QCD\ntuning the background field\n"}, {"id": "S002231151500032X", "text": "Zirconium alloys are used as cladding to encapsulate fuel pellets in pressurised and boiling water nuclear reactors. Research into oxidation of these alloys has been significant since the introduction of the material. However, the microstructure and electro-chemical processes during oxidation are complex and many questions still remain unanswered. One such issue is the formation of lateral cracks near the metal-oxide interface. Small cracks have been seen to form continuously during oxidation, with large scale networks of lateral cracks forming cyclically every \u223c2\u03bcm of oxide growth. These networks of cracks can be correlated with acceleration in the corrosion kinetics [1\u20137]. These lateral cracks might enable the link up of nano pores along grain boundaries perpendicular to the metal/oxide interface as reported in [8,9]. Experiments using Synchrotron X-Ray Diffraction (S-XRD) by both Polatidis et al. and Petigny et al., have separately shown that oxides formed on Zircaloy-4 are composed of monoclinic and stabilised tetragonal phases, with an \u223c7% reduction in the tetragonal phase fraction from 1 to 3\u03bcm oxide growth [4,10]. One theory is that the lateral cracks may destabilise the tetragonal phase close to the metal-oxide interface. The phase transformation has an \u223c6% expansion associated with it, which could lead to fracture perpendicular to the metal-oxide interface, thereby generating fast ingress routes for oxygen containing species [11,12].\n", "keywords": "\u223c7% reduction in the tetragonal phase\nalloys\ncladding\ncorrosion kinetics\nfracture\nfuel pellets\ngrain\ningress\nlink up of nano pores\nmetal-oxide interface\nmetal/oxide interface\nmicrostructure and electro-chemical processes\nmonoclinic and stabilised tetragonal phases\noxidation\noxidation of these alloys\noxides\noxygen containing species\nphase transformation\nS-XRD\nSynchrotron X-Ray Diffraction\ntetragonal phase\nwater nuclear reactors\nZircaloy-4\nZirconium alloys\n"}, {"id": "S0167273812003025", "text": "In conclusion, a new approach to the \u201cgrind-free\u201d nanoprecursor route to direct combinatorial solid state synthesis of several \u201cdifficult to make\u201d and hitherto unknown phase-pure heterometallic Ruddlesden Popper type La4Ni3\u2212xFexO10 materials has been described. The new approach used a high-throughput reactor and robotic automation (RAMSI) to rapidly synthesise a range of nanoparticle co-precipitate precursors in cloned libraries at a rate of 7.5 samples an hour. Each library could then be heat-treated at a different temperature and an initial powder XRD screen was used to locate and approximate phase boundary. A more focussed second synthesis and XRD characterisation of selected larger heat-treated powders was then performed to reconfirm the locations of the phase boundaries with the highest dopant level being achieved for La4Ni2FeO10 which is significantly greater Fe doping than has been achieved by anyone previously (despite several notable efforts). EXAFS data suggested that Fe3+ was located onto Ni sites in all cases and did not exist as a separate iron oxide phase.\n", "keywords": "direct combinatorial solid state synthesis\nEXAFS data\nFe\nFe3+\n\u201cgrind-free\u201d nanoprecursor route\nheat-treated\nheat-treated powders\nhigh-throughput reactor\niron oxide\nLa4Ni2FeO10\nlocate and approximate phase boundary\nnanoparticle co-precipitate precursors\nnew approach to the \u201cgrind-free\u201d nanoprecursor route to direct combinatorial solid state synthesis\nNi\nphase-pure heterometallic Ruddlesden Popper type La4Ni3\u2212xFexO10 materials\npowder XRD screen\nRAMSI\nrate of 7.5 samples an hour\nrobotic automation\nsynthesis and XRD characterisation\nsynthesise a range of nanoparticle co-precipitate precursors\n"}, {"id": "S0167273812003025", "text": "In conclusion, a new approach to the \u201cgrind-free\u201d nanoprecursor route to direct combinatorial solid state synthesis of several \u201cdifficult to make\u201d and hitherto unknown phase-pure heterometallic Ruddlesden Popper type La4Ni3\u2212xFexO10 materials has been described. The new approach used a high-throughput reactor and robotic automation (RAMSI) to rapidly synthesise a range of nanoparticle co-precipitate precursors in cloned libraries at a rate of 7.5 samples an hour. Each library could then be heat-treated at a different temperature and an initial powder XRD screen was used to locate and approximate phase boundary. A more focussed second synthesis and XRD characterisation of selected larger heat-treated powders was then performed to reconfirm the locations of the phase boundaries with the highest dopant level being achieved for La4Ni2FeO10 which is significantly greater Fe doping than has been achieved by anyone previously (despite several notable efforts). EXAFS data suggested that Fe3+ was located onto Ni sites in all cases and did not exist as a separate iron oxide phase.\n", "keywords": "direct combinatorial solid state synthesis\nEXAFS data\nFe\nFe3+\n\u201cgrind-free\u201d nanoprecursor route\nheat-treated\nheat-treated powders\nhigh-throughput reactor\niron oxide\nLa4Ni2FeO10\nlocate and approximate phase boundary\nnanoparticle co-precipitate precursors\nnew approach to the \u201cgrind-free\u201d nanoprecursor route to direct combinatorial solid state synthesis\nNi\nphase-pure heterometallic Ruddlesden Popper type La4Ni3\u2212xFexO10 materials\npowder XRD screen\nRAMSI\nrate of 7.5 samples an hour\nrobotic automation\nsynthesis and XRD characterisation\nsynthesise a range of nanoparticle co-precipitate precursors\n"}, {"id": "S0377221716303873", "text": "The iron ore may be extracted from blocks of 25\u00d725\u00d712meter3 located at three consecutive mining benches of 12meter height. For this case study, ten equally probable scenarios of iron content, phosphorous, silica, aluminum and LOI are used to quantify the joint uncertainty in the characteristics of the iron ore deposit considered and are the input to the SSTPS formulation proposed in the previous section. The simulated scenarios available were provided and generated using the stochastic simulated technique detailed in Boucher and Dimitrakopoulos (2012). The area considered is bounded by the limits of the given volume of production in the long-term first year production schedule provided. Fig. 4 shows 3 scenarios of iron ore content as well as the corresponding conventional and single estimated (average) representation of iron content (Fe2O3%) for the upper bench. In total, 734 blocks from 3525 to 21,150 tonnes, with Fe2O3 from 54.59% to 60.63%, P from 0.02% to 0.04%, SiO2 from 3.10% to 8.58%, Al2O3 from 0.53% to 1.88% and LOI from 8.75% to 11.75% are available.\n", "keywords": "734 blocks\nAl2O3\naluminum\nFe2O3\niron\niron content\niron ore\niron ore deposit\nLOI\nP\nphosphorous\nprobable scenarios\nquantify the joint uncertainty in the characteristics of the iron ore deposit\nscenarios of iron ore content\nsilica\nsimulated scenarios\nSiO2\nSSTPS formulation\nstochastic simulated technique\n"}, {"id": "S0377221716303873", "text": "The iron ore may be extracted from blocks of 25\u00d725\u00d712meter3 located at three consecutive mining benches of 12meter height. For this case study, ten equally probable scenarios of iron content, phosphorous, silica, aluminum and LOI are used to quantify the joint uncertainty in the characteristics of the iron ore deposit considered and are the input to the SSTPS formulation proposed in the previous section. The simulated scenarios available were provided and generated using the stochastic simulated technique detailed in Boucher and Dimitrakopoulos (2012). The area considered is bounded by the limits of the given volume of production in the long-term first year production schedule provided. Fig. 4 shows 3 scenarios of iron ore content as well as the corresponding conventional and single estimated (average) representation of iron content (Fe2O3%) for the upper bench. In total, 734 blocks from 3525 to 21,150 tonnes, with Fe2O3 from 54.59% to 60.63%, P from 0.02% to 0.04%, SiO2 from 3.10% to 8.58%, Al2O3 from 0.53% to 1.88% and LOI from 8.75% to 11.75% are available.\n", "keywords": "734 blocks\nAl2O3\naluminum\nFe2O3\niron\niron content\niron ore\niron ore deposit\nLOI\nP\nphosphorous\nprobable scenarios\nquantify the joint uncertainty in the characteristics of the iron ore deposit\nscenarios of iron ore content\nsilica\nsimulated scenarios\nSiO2\nSSTPS formulation\nstochastic simulated technique\n"}, {"id": "S0370269302013412", "text": "We consider cosmological consequences of a conformal-invariant formulation of Einstein's General Relativity where instead of the scale factor of the spatial metrics in the action functional a massless scalar (dilaton) field occurs which scales all masses including the Planck mass. Instead of the expansion of the universe we obtain the Hoyle\u2013Narlikar type of mass evolution, where the temperature history of the universe is replaced by the mass history. We show that this conformal-invariant cosmological model gives a satisfactory description of the new supernova Ia data for the effective magnitude\u2013redshift relation without a cosmological constant and make a prediction for the high-redshift behavior which deviates from that of standard cosmology for z>1.7.We consider cosmological consequences of a conformal-invariant formulation of Einstein's General Relativity where instead of the scale factor of the spatial metrics in the action functional a massless scalar (dilaton) field occurs which scales all masses including the Planck mass. Instead of the expansion of the universe we obtain the Hoyle\u2013Narlikar type of mass evolution, where the temperature history of the universe is replaced by the mass history. We show that this conformal-invariant cosmological model gives a satisfactory description of the new supernova Ia data for the effective magnitude\u2013redshift relation without a cosmological constant and make a prediction for the high-redshift behavior which deviates from that of standard cosmology for z>1.7.\n", "keywords": "conformal-invariant cosmological model\nconformal-invariant formulation of Einstein's General Relativity\nconsider cosmological consequences of a conformal-invariant formulation of Einstein's General Relativity\ncosmological constant\ndilaton\neffective magnitude\u2013redshift relation\nEinstein's General Relativity\nexpansion of the universe\nhigh-redshift behavior\nHoyle\u2013Narlikar type of mass evolution\nmasses\nmass history\nmassless scalar\nmassless scalar (dilaton) field\nPlanck mass\nscale factor\nstandard cosmology\nsupernova Ia data\ntemperature history of the universe\n"}, {"id": "S0968432814000250", "text": "Volume EM can be performed using transmission or scanning electron microscopes. Each approach has its own strengths and weaknesses, and the choice is dependant on the required lateral (x, y) and axial (z) resolution, and the size of the structure of interest. Historically, transmission electron microscopy (TEM) was the tool of choice for ultrastructural examination of biomedical specimens at sub-nanometer resolution. However, for many cell biology studies structural resolution is actually limited by the deposition of heavy metals onto membranes during sample preparation. In addition, voxel dimensions may only need to be half that of the smallest expected feature of interest (Briggman and Bock, 2012). Advances in scanning electron microscopy (SEM) technology are now driving a paradigm shift in electron imaging. SEMs with field emission electron sources and high efficiency electron detectors can achieve lateral resolutions in the order of 3nm, allowing visualisation of structures such as synaptic vesicles and membranes (De Winter et al., 2009; Knott et al., 2008; Vihinen et al., 2013; Villinger et al., 2012), though resolving individual leaflets of membrane bilayers remains a challenge (Vihinen et al., 2013). The use of low beam energies also limits the interaction volume, enhancing axial resolution (Hennig and Denk, 2007). In this review, volume imaging in both transmission and scanning EMs will be explored, moving from traditional manual techniques, through to the latest systems where aspects of both sample preparation and imaging have been automated.\n", "keywords": "cell biology studies\ndeposition of heavy metals\nelectron imaging\nenhancing axial resolution\nfield emission electron sources\nheavy metals\nhigh efficiency electron detectors\nimaging\ninteraction volume\nlatest systems\nlow beam energies\nmembrane bilayers\nmembranes\nresolving individual leaflets of membrane bilayers\nsample preparation\nscanning electron microscopes\nscanning electron microscopy\nscanning electron microscopy (SEM) technology\nSEM\nSEMs\nsynaptic vesicles\nTEM\ntraditional manual techniques\ntransmission\ntransmission and scanning EMs\ntransmission electron microscopy\nultrastructural examination of biomedical specimens\nvisualisation of structures\nVolume EM\nvolume imaging\n"}, {"id": "S0045782515003680", "text": "We consider the shape optimisation of two- and three-dimensional solids by combining multiresolution subdivision surfaces with immersed finite elements. As widely discussed in isogeometric analysis literature, the geometry representations used in today\u2019s computer aided design (CAD) and finite element analysis (FEA) software are inherently incompatible\u00a0 [1]. This is particularly limiting in shape optimisation during which a given CAD geometry model is to be iteratively updated based on the results of a finite element computation. The inherent shortcomings of present geometry and analysis representations have motivated the proliferation of various shape optimisation techniques. In the most prevalent approaches\u00a0a surrogate geometry model\u00a0 [2\u20138] or the analysis mesh\u00a0 [9,10] instead of the true CAD model is optimised, see also\u00a0 [11] and references therein. Generally, it is tedious or impossible to map the optimised surrogate geometry model or analysis mesh back to the original CAD model, which is essential for continuing with the design process and later for manufacturing purposes. Moreover, geometric design features are usually defined with respect to the CAD model and cannot be easily enforced on the surrogate model. Recently, the shape optimisation of shells, solids and other applications using isogeometric analysis has been explored; that is, through directly optimising the CAD geometry model\u00a0 [12\u201315].\n", "keywords": "analysis mesh\nCAD\nCAD geometry model\nCAD model\ncombining multiresolution subdivision surfaces\ncomputer aided design\ndirectly optimising\nFEA\nfinite element analysis\nfinite element computation\ngeometry and analysis representations\ngeometry representations\nimmersed finite elements\nisogeometric analysis\nisogeometric analysis literature\noptimised surrogate geometry model\nshape optimisation\nshape optimisation techniques\nshells\nsolids\nsurrogate geometry model\nsurrogate model\ntwo- and three-dimensional solids\n"}, {"id": "S0377025714000135", "text": "This conclusion is a consequence of the high jet speeds and small nozzle diameters in combination with the relatively high viscosity solvent and modest molecular weights of the polystyrene, which results in high Weissenberg numbers and moderate values of the extensibility, L studied here. As discussed in earlier papers [3,6], other jetting fluid combinations, such as those of de Gans et al. [4], lie in a different jetting regime where full extension does not occur and relaxation time controls the viscoelastic behaviour. Consequently inkjet fluid assessment methods need to provide a full characterisation including both linear and nonlinear viscoelastic properties. This complexity suggests assessments of inkjet fluids might have to include jetting from sets of DoD print head devices with different sensitivities to all the various VE parameters [37], rather than reliance on testing without jetting. This was not the expected outcome from the present work but does echo the very pragmatic viewpoint expressed as a \u201cmap of misery\u201d by Clasen et al. [38] and may provide a way forward for future R&D strategies towards ink testing.\n", "keywords": "DoD print head devices\ninkjet fluid assessment methods\ninkjet fluids\nink testing\njetting\njetting fluid combinations\n\u201cmap of misery\u201d\npolystyrene\nprovide a full characterisation including both linear and nonlinear viscoelastic properties\n"}, {"id": "S0370269304009049", "text": "One of the challenges in quantum chromodynamics (QCD) is the relativistic bound state problem. In the light-cone Hamiltonian approach [1] light-cone wave functions can be constructed in a boost invariant way. It is necessary to have reliable light-cone wave functions if one wants to calculate high energy scattering, especially exclusive reactions. Many parametrizations assume separability of the dependence on the longitudinal momentum fraction and transverse momentum which is very unlikely since the two momenta are coupled in the kinetic energy operator. Various approaches have been tried to compute such wave functions. One can use the usual equal time Hamiltonian [2] and transform the resulting wave functions into light-cone form with the help of kinematical on-shell equations. The light-cone Hamiltonian in a string picture is formulated in Ref.\u00a0[3]. More ambitious is the construction of an effective Hamiltonian including the gauge degrees of freedom explicitly and then solving the bound state problem. For mesons this approach [4,5] still needs many parameters to be fixed. Attempts have been made to solve the valence quark wave function for mesons in a simple Hamiltonian with a two-body potential\u00a0[6].\n", "keywords": "calculate high energy scattering\nconstruction of an effective Hamiltonian including the gauge degrees of freedom explicitly\nequal time Hamiltonian\nHamiltonian\nkinematical on-shell equations\nlight-cone Hamiltonian approach\nlight-cone wave functions\nmesons\nQCD\nquantum chromodynamics\nreactions\nrelativistic bound state problem\nreliable light-cone wave functions\nsimple Hamiltonian with a two-body potential\u00a0\nsolve the valence quark wave function for mesons\nsolving the bound state problem\ntransform the resulting wave functions into light-cone form\nvalence quark wave function\n"}, {"id": "S0370269304009049", "text": "One of the challenges in quantum chromodynamics (QCD) is the relativistic bound state problem. In the light-cone Hamiltonian approach [1] light-cone wave functions can be constructed in a boost invariant way. It is necessary to have reliable light-cone wave functions if one wants to calculate high energy scattering, especially exclusive reactions. Many parametrizations assume separability of the dependence on the longitudinal momentum fraction and transverse momentum which is very unlikely since the two momenta are coupled in the kinetic energy operator. Various approaches have been tried to compute such wave functions. One can use the usual equal time Hamiltonian [2] and transform the resulting wave functions into light-cone form with the help of kinematical on-shell equations. The light-cone Hamiltonian in a string picture is formulated in Ref.\u00a0[3]. More ambitious is the construction of an effective Hamiltonian including the gauge degrees of freedom explicitly and then solving the bound state problem. For mesons this approach [4,5] still needs many parameters to be fixed. Attempts have been made to solve the valence quark wave function for mesons in a simple Hamiltonian with a two-body potential\u00a0[6].\n", "keywords": "calculate high energy scattering\nconstruction of an effective Hamiltonian including the gauge degrees of freedom explicitly\nequal time Hamiltonian\nHamiltonian\nkinematical on-shell equations\nlight-cone Hamiltonian approach\nlight-cone wave functions\nmesons\nQCD\nquantum chromodynamics\nreactions\nrelativistic bound state problem\nreliable light-cone wave functions\nsimple Hamiltonian with a two-body potential\u00a0\nsolve the valence quark wave function for mesons\nsolving the bound state problem\ntransform the resulting wave functions into light-cone form\nvalence quark wave function\n"}, {"id": "S1364815216302122", "text": "When we formulate the downscaling problem as a multi-objective optimization problem, we face, however, the following problems. Minimizing the sum of different objectives is problematic, since they may have different units and ranges. Even with an appropriate scaling procedure there is a risk of treating the objectives unequally or getting trapped in a local minimum. Firstly, we can never know, what is the minimum value of each objective that can be achieved by the regression. Thus, designing an appropriate scaling procedure is difficult and one would need to decide on the relative importance of the different objectives in advance. Secondly, adding multiple, conflicting objectives very likely results in a fitness function with multiple local minima, which makes optimization more difficult. To avoid these problems, we have implemented fitness calculation according to the Strength Pareto Evolutionary Algorithm (SPEA) by Zitzler and Thiele (1999), instead of using a single (weighted) fitness or cost function. Approaches for multi-objective optimization like SPEA are widely used in evolutionary computation. In SPEA the fitness calculation during the fitting procedure is based on an intercomparison of the different models. Further, a finite set of so called Pareto optimal models (downscaling rules) is returned.\n", "keywords": "downscaling rules\nevolutionary computation\nfitness calculation\nmulti-objective optimization\noptimization\nPareto optimal models\nregression\nscaling procedure\nSPEA\nStrength Pareto Evolutionary Algorithm\nusing a single (weighted) fitness or cost function\n"}, {"id": "S2212667812000524", "text": "Digital libraries promise new societal benefits, especially for e-learning in digital or mobile times, starting with the elimination of the time and space constraints of traditional bricks-and-mortar libraries. The library and information professionals are required to acquire such knowledge and skills as the library is one of the highly IT influenced service profession. This paper gives an overview of current trends in digital library research consists of digital library characteristic, advantage, disadvantages and function. This paper also highlights on the impact of information technology on the traditional library.", "keywords": "acquire such knowledge and skills\nbricks-and-mortar libraries\nDigital libraries\ndigital library\ne-learning\nelimination of the time and space constraints\nhighlights on the impact of information technology\nlibrary\noverview of current trends in digital library research\n"}, {"id": "S2212667812000524", "text": "Digital libraries promise new societal benefits, especially for e-learning in digital or mobile times, starting with the elimination of the time and space constraints of traditional bricks-and-mortar libraries. The library and information professionals are required to acquire such knowledge and skills as the library is one of the highly IT influenced service profession. This paper gives an overview of current trends in digital library research consists of digital library characteristic, advantage, disadvantages and function. This paper also highlights on the impact of information technology on the traditional library.", "keywords": "acquire such knowledge and skills\nbricks-and-mortar libraries\nDigital libraries\ndigital library\ne-learning\nelimination of the time and space constraints\nhighlights on the impact of information technology\nlibrary\noverview of current trends in digital library research\n"}, {"id": "S2212667812000524", "text": "Digital libraries promise new societal benefits, especially for e-learning in digital or mobile times, starting with the elimination of the time and space constraints of traditional bricks-and-mortar libraries. The library and information professionals are required to acquire such knowledge and skills as the library is one of the highly IT influenced service profession. This paper gives an overview of current trends in digital library research consists of digital library characteristic, advantage, disadvantages and function. This paper also highlights on the impact of information technology on the traditional library.", "keywords": "acquire such knowledge and skills\nbricks-and-mortar libraries\nDigital libraries\ndigital library\ne-learning\nelimination of the time and space constraints\nhighlights on the impact of information technology\nlibrary\noverview of current trends in digital library research\n"}, {"id": "S2212667812000524", "text": "Digital libraries promise new societal benefits, especially for e-learning in digital or mobile times, starting with the elimination of the time and space constraints of traditional bricks-and-mortar libraries. The library and information professionals are required to acquire such knowledge and skills as the library is one of the highly IT influenced service profession. This paper gives an overview of current trends in digital library research consists of digital library characteristic, advantage, disadvantages and function. This paper also highlights on the impact of information technology on the traditional library.", "keywords": "acquire such knowledge and skills\nbricks-and-mortar libraries\nDigital libraries\ndigital library\ne-learning\nelimination of the time and space constraints\nhighlights on the impact of information technology\nlibrary\noverview of current trends in digital library research\n"}, {"id": "S2212667812000524", "text": "Digital libraries promise new societal benefits, especially for e-learning in digital or mobile times, starting with the elimination of the time and space constraints of traditional bricks-and-mortar libraries. The library and information professionals are required to acquire such knowledge and skills as the library is one of the highly IT influenced service profession. This paper gives an overview of current trends in digital library research consists of digital library characteristic, advantage, disadvantages and function. This paper also highlights on the impact of information technology on the traditional library.", "keywords": "acquire such knowledge and skills\nbricks-and-mortar libraries\nDigital libraries\ndigital library\ne-learning\nelimination of the time and space constraints\nhighlights on the impact of information technology\nlibrary\noverview of current trends in digital library research\n"}, {"id": "S092702561300267X", "text": "In previous publications the present authors proposed a method to incorporate the thermodynamics of ternary alloys and liquid diffusion-governed solidification kinetics into a multiphase volume average solidification model [23,24]. Back diffusion was disregarded. A way to access the thermodynamic data (e.g. Thermo-Calc [1]) through a tabulation and interpolation program ISAT (In Situ Adaptive Tabulation) was suggested. With the ISAT approach it is possible to perform an online call of the thermodynamic data and trace the formation of each individual solid phase (primary, peritectic, eutectic, etc.). As the number of calls of the thermodynamic data is equal to the product of the number of the discretized volume elements, the time steps and the calculation iterations per time step, the calculation becomes exhausting. Therefore, the current model is a modification of the previous model using a linearized phase diagram, and no online call of thermodynamic data is necessary. In addition, the model presented in this paper is extended to consider the back diffusion into the solid. With these modifications, the model can be used to perform casting process simulations with incorporated full diffusion-governed solidification kinetics for ternary alloys at a reasonable computation cost.\n", "keywords": "back diffusion\nBack diffusion\ncasting process simulations\ncurrent model\ndiscretized volume elements\nextended to consider the back diffusion into the solid\nfull diffusion-governed solidification kinetics\nincorporate the thermodynamics of ternary alloys and liquid diffusion-governed solidification kinetics into a multiphase volume average solidification model\nIn Situ Adaptive Tabulation\nISAT\nISAT approach\nliquid diffusion-governed solidification kinetics\nmodification of the previous model using a linearized phase diagram\nmodifications\nmultiphase volume average solidification model\nonline call of thermodynamic data\nperform casting process simulations with incorporated full diffusion-governed solidification kinetics\nsolid\ntabulation and interpolation program\nternary alloys\nthermodynamic data\n"}, {"id": "S0301010413004096", "text": "It is critical to the success of the NPD technique that the MOF complex adsorbs a significant amount of D2 to boost the observed signal. This technique therefore has disadvantages when studying the binding interaction within MOFs with low uptakes. Furthermore, static crystallographic studies cannot provide insights into the dynamics of the adsorbed gas molecules. Thus, it is very challenging to probe experimentally the H2 binding interactions within a porous host system which has very low gas uptake due to the lack of suitable characterisation techniques. We report herein the application of the in situ inelastic neutron scattering (INS) technique to permit direct observation of the dynamics of the binding interactions between adsorbed H2 molecules and an aluminium-based porous MOF, NOTT-300, exhibiting moderate porosity, narrow pore window and very low uptake of H2. This neutron spectroscopy study reveals that adsorbed H2 molecules do not interact with the organic ligand within the pore channels, and form very weak interactions with [Al(OH)2O4] moieties via a type of through-spacing interaction (Al-O\u22efH2). Interestingly, the very low H2 adsorption has been successfully characterised as weak binding interactions and, for the first time, we have found that the adsorbed H2 in the pore channel has a liquid type recoil motion at 5K (below its melting point) as a direct result of this weak interaction to the MOF host.\n", "keywords": "adsorbed gas molecules\nadsorbed H2\nadsorbed H2 molecules\nAl-O\u22efH2\n[Al(OH)2O4] moieties\naluminium-based porous MOF\nbinding interaction\ncharacterisation techniques\nD2\ngas\nH2\nH2 adsorption\nH2 binding interactions\nINS\nin situ inelastic neutron scattering\nliquid\nMOF\nMOF complex\nMOFs\nneutron spectroscopy\nNOTT-300\nNPD technique\norganic ligand\npermit direct observation of the dynamics of the binding interactions\nstatic crystallographic studies\nthrough-spacing interaction\nweak binding interactions\nweak interaction\n"}, {"id": "S0301010413004096", "text": "It is critical to the success of the NPD technique that the MOF complex adsorbs a significant amount of D2 to boost the observed signal. This technique therefore has disadvantages when studying the binding interaction within MOFs with low uptakes. Furthermore, static crystallographic studies cannot provide insights into the dynamics of the adsorbed gas molecules. Thus, it is very challenging to probe experimentally the H2 binding interactions within a porous host system which has very low gas uptake due to the lack of suitable characterisation techniques. We report herein the application of the in situ inelastic neutron scattering (INS) technique to permit direct observation of the dynamics of the binding interactions between adsorbed H2 molecules and an aluminium-based porous MOF, NOTT-300, exhibiting moderate porosity, narrow pore window and very low uptake of H2. This neutron spectroscopy study reveals that adsorbed H2 molecules do not interact with the organic ligand within the pore channels, and form very weak interactions with [Al(OH)2O4] moieties via a type of through-spacing interaction (Al-O\u22efH2). Interestingly, the very low H2 adsorption has been successfully characterised as weak binding interactions and, for the first time, we have found that the adsorbed H2 in the pore channel has a liquid type recoil motion at 5K (below its melting point) as a direct result of this weak interaction to the MOF host.\n", "keywords": "adsorbed gas molecules\nadsorbed H2\nadsorbed H2 molecules\nAl-O\u22efH2\n[Al(OH)2O4] moieties\naluminium-based porous MOF\nbinding interaction\ncharacterisation techniques\nD2\ngas\nH2\nH2 adsorption\nH2 binding interactions\nINS\nin situ inelastic neutron scattering\nliquid\nMOF\nMOF complex\nMOFs\nneutron spectroscopy\nNOTT-300\nNPD technique\norganic ligand\npermit direct observation of the dynamics of the binding interactions\nstatic crystallographic studies\nthrough-spacing interaction\nweak binding interactions\nweak interaction\n"}, {"id": "S0301010413004096", "text": "It is critical to the success of the NPD technique that the MOF complex adsorbs a significant amount of D2 to boost the observed signal. This technique therefore has disadvantages when studying the binding interaction within MOFs with low uptakes. Furthermore, static crystallographic studies cannot provide insights into the dynamics of the adsorbed gas molecules. Thus, it is very challenging to probe experimentally the H2 binding interactions within a porous host system which has very low gas uptake due to the lack of suitable characterisation techniques. We report herein the application of the in situ inelastic neutron scattering (INS) technique to permit direct observation of the dynamics of the binding interactions between adsorbed H2 molecules and an aluminium-based porous MOF, NOTT-300, exhibiting moderate porosity, narrow pore window and very low uptake of H2. This neutron spectroscopy study reveals that adsorbed H2 molecules do not interact with the organic ligand within the pore channels, and form very weak interactions with [Al(OH)2O4] moieties via a type of through-spacing interaction (Al-O\u22efH2). Interestingly, the very low H2 adsorption has been successfully characterised as weak binding interactions and, for the first time, we have found that the adsorbed H2 in the pore channel has a liquid type recoil motion at 5K (below its melting point) as a direct result of this weak interaction to the MOF host.\n", "keywords": "adsorbed gas molecules\nadsorbed H2\nadsorbed H2 molecules\nAl-O\u22efH2\n[Al(OH)2O4] moieties\naluminium-based porous MOF\nbinding interaction\ncharacterisation techniques\nD2\ngas\nH2\nH2 adsorption\nH2 binding interactions\nINS\nin situ inelastic neutron scattering\nliquid\nMOF\nMOF complex\nMOFs\nneutron spectroscopy\nNOTT-300\nNPD technique\norganic ligand\npermit direct observation of the dynamics of the binding interactions\nstatic crystallographic studies\nthrough-spacing interaction\nweak binding interactions\nweak interaction\n"}, {"id": "S2212667814000331", "text": "In order to solve the problem that the diesel engine PT fuel system is unable to field maintain, developed a portable signal acquisition and analysis system for diesel engine PT fuel system. Firstly, the PT pump work Principle was analyzed, and the PT pump failure mapping relation between reason and failure phenomenon was analyzed; Secondly, the diesel engine PT pump failure fuel pressure characteristics were analyzed; Lastly, using the portable signal acquisition and analysis system to diagnose the diesel engine PT fuel system, experiment results show that the system can correctly detect the diesel engine PT fuel system state.", "keywords": "analysis\nanalysis system\ndetect the diesel engine PT fuel system state\ndiagnose the diesel engine PT fuel system\ndiesel engine PT fuel\ndiesel engine PT fuel system\ndiesel engine PT pump\nfailure fuel pressure characteristics were analyzed\nportable signal acquisition and analysis system\nproblem that the diesel engine PT fuel system is unable to field maintain\nPT pump\nPT pump work Principle was analyzed\nsignal acquisition\n"}, {"id": "S2212667814000331", "text": "In order to solve the problem that the diesel engine PT fuel system is unable to field maintain, developed a portable signal acquisition and analysis system for diesel engine PT fuel system. Firstly, the PT pump work Principle was analyzed, and the PT pump failure mapping relation between reason and failure phenomenon was analyzed; Secondly, the diesel engine PT pump failure fuel pressure characteristics were analyzed; Lastly, using the portable signal acquisition and analysis system to diagnose the diesel engine PT fuel system, experiment results show that the system can correctly detect the diesel engine PT fuel system state.", "keywords": "analysis\nanalysis system\ndetect the diesel engine PT fuel system state\ndiagnose the diesel engine PT fuel system\ndiesel engine PT fuel\ndiesel engine PT fuel system\ndiesel engine PT pump\nfailure fuel pressure characteristics were analyzed\nportable signal acquisition and analysis system\nproblem that the diesel engine PT fuel system is unable to field maintain\nPT pump\nPT pump work Principle was analyzed\nsignal acquisition\n"}, {"id": "S1566253516300252", "text": "Methods for anomaly detection in a local context are the conceptual opposite to the afore-described centralized methods, which rely on globally shared models. In data mining, the notion of locality is often given as distance between data values (given a specific distance metric such as Euclidean distance). A data point is compared to the value of its nearest neighbors in terms of data distance [42]. However, the notion of locality can also be given in a geographical distance between the sources of the data. Many similar values (i.e., data with small distance among each other) result in a higher density, called clusters, while values that are less similar result in a lower density. Anomalies can fall outside of any cluster but, when frequently occurring, can form a cluster too. Determining if a datum is normal or anomalous compared to local neighborhood data is a challenge.\n", "keywords": "afore-described centralized methods\na geographical distance\nAnomalies\nanomaly detection\na specific distance metric such as Euclidean distance\ncan form a cluster\ncluster\nclusters\ncompared to the value of its nearest neighbors\ndata mining\nDetermining if a datum is normal or anomalous\ndistance between data values\nglobally shared models\nMany similar values\nnotion of locality\nthe notion of locality\nvalues that are less similar result in a lower density\n"}, {"id": "S1566253516300252", "text": "Methods for anomaly detection in a local context are the conceptual opposite to the afore-described centralized methods, which rely on globally shared models. In data mining, the notion of locality is often given as distance between data values (given a specific distance metric such as Euclidean distance). A data point is compared to the value of its nearest neighbors in terms of data distance [42]. However, the notion of locality can also be given in a geographical distance between the sources of the data. Many similar values (i.e., data with small distance among each other) result in a higher density, called clusters, while values that are less similar result in a lower density. Anomalies can fall outside of any cluster but, when frequently occurring, can form a cluster too. Determining if a datum is normal or anomalous compared to local neighborhood data is a challenge.\n", "keywords": "afore-described centralized methods\na geographical distance\nAnomalies\nanomaly detection\na specific distance metric such as Euclidean distance\ncan form a cluster\ncluster\nclusters\ncompared to the value of its nearest neighbors\ndata mining\nDetermining if a datum is normal or anomalous\ndistance between data values\nglobally shared models\nMany similar values\nnotion of locality\nthe notion of locality\nvalues that are less similar result in a lower density\n"}, {"id": "S0370269304006768", "text": "In our study we illustrate the properties of gauge invariant extensions of local functionals. We aim at clarifying, via specific examples, the relation between a functional which is local in a particular gauge (but not necessarily gauge invariant), and its gauge invariant extension (which is not necessarily local). We show that the non-localities found are not perturbatively local because they cannot be expressed in terms of an infinite derivative expansion. We believe that the implications of this observation have not been clearly emphasised in the literature, as attested by the absence of any debate about it in recent works. It is precisely these dangerous infrared modes that make it hard to define a gauge independent renormalisation for the gauge invariant extensions of local functionals. This observation supports the remark in [2] that the expectation value receives important contributions from both large and small distances. Our arguments on renormalisability are based on the notion of renormalisation in the modern sense [8] which relies on BRST cohomology theorems. The BRST terminology will therefore be frequently used here, even though it is not always necessary.\n", "keywords": "BRST cohomology theorems\nBRST terminology\nexpectation value\nfunctional which is local in a particular gauge\ngauge independent renormalisation\ngauge invariant extensions of local functionals\ninfrared modes\ninvariant extensions of local functionals\nits gauge invariant extension\nnfinite derivative expansion\nnon-localities found\nnotion of renormalisation\nrenormalisability\n"}, {"id": "S0301932214001499", "text": "In general, liquid film flows of practical relevance are turbulent and, hence, are associated with the presence of broadband interfacial waves on the film surface. A thorough understanding of the characteristic profiles, scales and dynamics of these interfacial waves is of essential importance in making accurate and reliable predictions of heat and mass transfer rates (Mathie and Markides, 2013a; Mathie et al., 2013). Previous efforts in downwards annular flow have focused on the spatio/temporal measurement of liquid film thickness, followed by in-depth statistical analyses of this film thickness (Webb and Hewitt, 1975; Belt et al., 2010; Alekseenko et al., 2012; Zhao et al., 2013). These efforts have contributed to a much improved understanding of the interfacial topology observed in downwards annular flows and also to the subsequent proposal of a series of correlations for the quantification of the mean film thickness, wave amplitudes and liquid entrainment rates into the gas phase (Ambrosini et al., 1991; Karapantsios and Karabelas, 1995; Azzopardi, 1997). On the other hand, less has been published on the velocity distribution and the flow structure within the liquid films, underneath the film surface. This can be related to the relative difficulty of these measurements caused by: (i) the extremely restricted measurement space, due to the small thickness of the liquid films (in the order of and often sub-mm), (ii) the highly disturbed and intermittent nature of the gas\u2013liquid interface, (iii) the entrainment of gas inside the liquid film and of liquid into the gas core, and (iv) the relatively high velocities of both the gas and liquid phases.\n", "keywords": "broadband interfacial waves\ndownwards annular flow\ndownwards annular flows\nentrainment of gas\nfilm\nfilm surface\nflow structure within the liquid films\ngas\ngas core\ngas\u2013liquid interface\ngas phase\nhigh velocities\ninterfacial topology\ninterfacial waves\nliquid\nliquid film\nliquid film flows\nliquid films\nliquid film thickness\nmean film thickness\nquantification of the mean film thickness, wave amplitudes and liquid entrainment rates\nspatio/temporal measurement\nstatistical analyses\nstatistical analyses of this film thickness\nthickness of the liquid films\nunderstanding of the characteristic profiles, scales and dynamics of these interfacial waves\nvelocity distribution\nwave\nwave amplitudes\n"}, {"id": "S0370269304008792", "text": "We investigate the density behavior of the symmetry energy with respect to isospin equilibration in the combined systems Ru(Zr)+Zr(Ru) at relativistic energies of\u00a00.4 and 1.528A\u00a0GeV. The study is performed within a relativistic framework and the contribution of the iso-vector, scalar \u03b4 field to the symmetry energy and the isospin dynamics is particularly explored. We find that the isospin mixing depends on the symmetry energy and a stiffer behavior leads to more transparency. The results are also nicely sensitive to the \u201cfine structure\u201d of the symmetry energy, i.e., to the covariant properties of the isovector meson fields.The isospin tracing appears much less dependent on the in medium neutron\u2013proton cross sections (\u03c3np) and this makes such observable very peculiar for the study of the isovector part of the nuclear equation of state.Within such a framework, comparisons with experiments support the introduction of the \u03b4 meson in the description of the iso-vector equation of state.", "keywords": "combined systems\ncomparisons with experiments\ncontribution of the iso-vector\nintroduction of the \u03b4 meson in the description of the iso-vector equation of state\ninvestigate the density behavior of the symmetry energy with respect to isospin equilibration\nisospin mixing\nisovector meson fields\nmedium neutron\u2013proton cross sections\nrelativistic energies\nrelativistic framework\nRu(Zr)+Zr(Ru)\nscalar \u03b4 field to the symmetry energy and the isospin dynamics is particularly explored\nstudy\nstudy of the isovector part of the nuclear equation of state\n\u03b4 meson\n\u03c3np\n"}, {"id": "S0370269304008792", "text": "We investigate the density behavior of the symmetry energy with respect to isospin equilibration in the combined systems Ru(Zr)+Zr(Ru) at relativistic energies of\u00a00.4 and 1.528A\u00a0GeV. The study is performed within a relativistic framework and the contribution of the iso-vector, scalar \u03b4 field to the symmetry energy and the isospin dynamics is particularly explored. We find that the isospin mixing depends on the symmetry energy and a stiffer behavior leads to more transparency. The results are also nicely sensitive to the \u201cfine structure\u201d of the symmetry energy, i.e., to the covariant properties of the isovector meson fields.The isospin tracing appears much less dependent on the in medium neutron\u2013proton cross sections (\u03c3np) and this makes such observable very peculiar for the study of the isovector part of the nuclear equation of state.Within such a framework, comparisons with experiments support the introduction of the \u03b4 meson in the description of the iso-vector equation of state.", "keywords": "combined systems\ncomparisons with experiments\ncontribution of the iso-vector\nintroduction of the \u03b4 meson in the description of the iso-vector equation of state\ninvestigate the density behavior of the symmetry energy with respect to isospin equilibration\nisospin mixing\nisovector meson fields\nmedium neutron\u2013proton cross sections\nrelativistic energies\nrelativistic framework\nRu(Zr)+Zr(Ru)\nscalar \u03b4 field to the symmetry energy and the isospin dynamics is particularly explored\nstudy\nstudy of the isovector part of the nuclear equation of state\n\u03b4 meson\n\u03c3np\n"}, {"id": "S0003491615001839", "text": "This work shows how our approach based on the combination of Statistical Mechanics and nonlinear PDEs theory provides us with a novel and powerful tool to tackle phase transitions. This method leads to solution of perhaps the most known test-case that exhibits a first order phase transition (semi-heuristically described) such as the van der Waals model. In particular we have obtained the first global mean field partition function (Eq. (9)), for a system of finite number of particles. The partition function is a solution to the Klein\u2013Gordon equation, reproduces the van der Waals isotherms away from the critical region and, in the thermodynamic limit N\u2192\u221e automatically encodes the Maxwell equal areas rule. The approach hereby presented is of remarkable simplicity, has been successfully applied to spin\u00a0 [17\u201319,14,16] and macroscopic thermodynamic systems\u00a0 [20,15] and can be further extended to include the larger class of models admitting partition functions of the form (4) to be used to extend to the critical region general equations of state of the form (7) including a class virial expansions.\n", "keywords": "class virial expansions\nfield partition function\nfirst order phase transition\ngeneral equations of state of the form\nKlein\u2013Gordon equation\nmacroscopic thermodynamic systems\nMaxwell equal areas rule\nnonlinear PDEs theory\npartition function\npartition functions of the form\nphase transitions\nspin\nStatistical Mechanics\nthermodynamic limit N\u2192\u221e\nvan der Waals isotherms\nvan der Waals model\n"}, {"id": "S0032386109007423", "text": "In general, the ion exchange capacity (IEC) is closely related to the proton conductivity of PEMs because the acid functionalities, such as sulfonic acid groups, contribute to the proton conduction in a membrane. Beyond a certain sulfonation degree, PEMs tend to absorb too much water or are even soluble in water, which negatively affect their mechanical resistance and water resistance [17,18]. Therefore, the improvement of proton conductivity using aromatic polymers with moderately adjusted IEC values has been under intense investigation [19\u201323]. To achieve high proton conductivity with moderate IEC values, the formation of ion channel structures, which enable effective proton conduction, has been studied. In the course of these studies, an ideal morphology has been pursued by microphase separation of segmented block copolymers in which hydrophilic sulfonated polymer segments form an interconnected three-dimensional network responsible for efficient proton transport, while a complementary network of hydrophobic non-sulfonated segments imparts a reinforcing effect, preventing excessive swelling in water and enhancing the mechanical properties. An image of the ideal morphology for PEMs is shown in Fig.\u00a02.\n", "keywords": "aromatic polymers\ncomplementary network of hydrophobic non-sulfonated segments\neffective proton conduction\nenhancing the mechanical properties\nformation of ion channel structures\nhigh proton conductivity\nhydrophilic sulfonated polymer segments\nhydrophobic non-sulfonated segments\nIEC\nimprovement of proton conductivity using aromatic polymers with moderately adjusted IEC values\ninterconnected three-dimensional network\nion channel structures\nion exchange capacity\nmembrane\nPEMs\npreventing excessive swelling in water\nproton\nproton conduction\nproton conductivity\nproton transport\nsegmented block copolymers\nsulfonic acid groups\nwater\n"}, {"id": "S0032386109007423", "text": "In general, the ion exchange capacity (IEC) is closely related to the proton conductivity of PEMs because the acid functionalities, such as sulfonic acid groups, contribute to the proton conduction in a membrane. Beyond a certain sulfonation degree, PEMs tend to absorb too much water or are even soluble in water, which negatively affect their mechanical resistance and water resistance [17,18]. Therefore, the improvement of proton conductivity using aromatic polymers with moderately adjusted IEC values has been under intense investigation [19\u201323]. To achieve high proton conductivity with moderate IEC values, the formation of ion channel structures, which enable effective proton conduction, has been studied. In the course of these studies, an ideal morphology has been pursued by microphase separation of segmented block copolymers in which hydrophilic sulfonated polymer segments form an interconnected three-dimensional network responsible for efficient proton transport, while a complementary network of hydrophobic non-sulfonated segments imparts a reinforcing effect, preventing excessive swelling in water and enhancing the mechanical properties. An image of the ideal morphology for PEMs is shown in Fig.\u00a02.\n", "keywords": "aromatic polymers\ncomplementary network of hydrophobic non-sulfonated segments\neffective proton conduction\nenhancing the mechanical properties\nformation of ion channel structures\nhigh proton conductivity\nhydrophilic sulfonated polymer segments\nhydrophobic non-sulfonated segments\nIEC\nimprovement of proton conductivity using aromatic polymers with moderately adjusted IEC values\ninterconnected three-dimensional network\nion channel structures\nion exchange capacity\nmembrane\nPEMs\npreventing excessive swelling in water\nproton\nproton conduction\nproton conductivity\nproton transport\nsegmented block copolymers\nsulfonic acid groups\nwater\n"}, {"id": "S0032386109007423", "text": "In general, the ion exchange capacity (IEC) is closely related to the proton conductivity of PEMs because the acid functionalities, such as sulfonic acid groups, contribute to the proton conduction in a membrane. Beyond a certain sulfonation degree, PEMs tend to absorb too much water or are even soluble in water, which negatively affect their mechanical resistance and water resistance [17,18]. Therefore, the improvement of proton conductivity using aromatic polymers with moderately adjusted IEC values has been under intense investigation [19\u201323]. To achieve high proton conductivity with moderate IEC values, the formation of ion channel structures, which enable effective proton conduction, has been studied. In the course of these studies, an ideal morphology has been pursued by microphase separation of segmented block copolymers in which hydrophilic sulfonated polymer segments form an interconnected three-dimensional network responsible for efficient proton transport, while a complementary network of hydrophobic non-sulfonated segments imparts a reinforcing effect, preventing excessive swelling in water and enhancing the mechanical properties. An image of the ideal morphology for PEMs is shown in Fig.\u00a02.\n", "keywords": "aromatic polymers\ncomplementary network of hydrophobic non-sulfonated segments\neffective proton conduction\nenhancing the mechanical properties\nformation of ion channel structures\nhigh proton conductivity\nhydrophilic sulfonated polymer segments\nhydrophobic non-sulfonated segments\nIEC\nimprovement of proton conductivity using aromatic polymers with moderately adjusted IEC values\ninterconnected three-dimensional network\nion channel structures\nion exchange capacity\nmembrane\nPEMs\npreventing excessive swelling in water\nproton\nproton conduction\nproton conductivity\nproton transport\nsegmented block copolymers\nsulfonic acid groups\nwater\n"}, {"id": "S0032386109007423", "text": "In general, the ion exchange capacity (IEC) is closely related to the proton conductivity of PEMs because the acid functionalities, such as sulfonic acid groups, contribute to the proton conduction in a membrane. Beyond a certain sulfonation degree, PEMs tend to absorb too much water or are even soluble in water, which negatively affect their mechanical resistance and water resistance [17,18]. Therefore, the improvement of proton conductivity using aromatic polymers with moderately adjusted IEC values has been under intense investigation [19\u201323]. To achieve high proton conductivity with moderate IEC values, the formation of ion channel structures, which enable effective proton conduction, has been studied. In the course of these studies, an ideal morphology has been pursued by microphase separation of segmented block copolymers in which hydrophilic sulfonated polymer segments form an interconnected three-dimensional network responsible for efficient proton transport, while a complementary network of hydrophobic non-sulfonated segments imparts a reinforcing effect, preventing excessive swelling in water and enhancing the mechanical properties. An image of the ideal morphology for PEMs is shown in Fig.\u00a02.\n", "keywords": "aromatic polymers\ncomplementary network of hydrophobic non-sulfonated segments\neffective proton conduction\nenhancing the mechanical properties\nformation of ion channel structures\nhigh proton conductivity\nhydrophilic sulfonated polymer segments\nhydrophobic non-sulfonated segments\nIEC\nimprovement of proton conductivity using aromatic polymers with moderately adjusted IEC values\ninterconnected three-dimensional network\nion channel structures\nion exchange capacity\nmembrane\nPEMs\npreventing excessive swelling in water\nproton\nproton conduction\nproton conductivity\nproton transport\nsegmented block copolymers\nsulfonic acid groups\nwater\n"}, {"id": "S0010938X15002954", "text": "There have been relatively few attempts to observe and in some cases extract the average current density from video images taken of growing 2D pits. Frankel presented a method to directly measure the average anodic current density from the growing pit boundary velocity in Al [33], an Al alloy [34] and Ni\u2013Fe [35] thin films. Subsequently, Ryan et al. [27,36] determined the anodic current density in pits propagating as 2D disks in stainless steel thin films by measuring the pit edge movement velocity. Ernst and Newman [11,12,37] studied stability of pit growth in detail and measured the kinetics of 2D pit propagation in depth and width and compared the results with kinetics in 1D pencil electrodes. They developed a semi-quantitative model for pit propagation which explained the lacy pit cover formation during the pit growth, although they did not measure current density within the pit. More recently, Tang and Davenport [38] tracked the pit boundary movement and computed the instantaneous but average current density in Fe-Co thin films. However, there have been no previous attempts to quantify the local current density during inhomogeneous growth of pits, although such local variation in current density has long been recognised [7].\n", "keywords": "1D pencil electrodes\n2D disks\n2D pit\n2D pit propagation\n2D pits\nAl\nAl alloy\nanodic current\ncurrent\nFe-Co thin films\nlacy pit cover\nlacy pit cover formation\nmeasured the kinetics of 2D pit propagation\nNi\u2013Fe [35] thin films\npit\npit edge\npit growth\npit propagation\npits\nsemi-quantitative model\nstainless steel thin films\nvideo images\n"}, {"id": "S0010938X15002954", "text": "There have been relatively few attempts to observe and in some cases extract the average current density from video images taken of growing 2D pits. Frankel presented a method to directly measure the average anodic current density from the growing pit boundary velocity in Al [33], an Al alloy [34] and Ni\u2013Fe [35] thin films. Subsequently, Ryan et al. [27,36] determined the anodic current density in pits propagating as 2D disks in stainless steel thin films by measuring the pit edge movement velocity. Ernst and Newman [11,12,37] studied stability of pit growth in detail and measured the kinetics of 2D pit propagation in depth and width and compared the results with kinetics in 1D pencil electrodes. They developed a semi-quantitative model for pit propagation which explained the lacy pit cover formation during the pit growth, although they did not measure current density within the pit. More recently, Tang and Davenport [38] tracked the pit boundary movement and computed the instantaneous but average current density in Fe-Co thin films. However, there have been no previous attempts to quantify the local current density during inhomogeneous growth of pits, although such local variation in current density has long been recognised [7].\n", "keywords": "1D pencil electrodes\n2D disks\n2D pit\n2D pit propagation\n2D pits\nAl\nAl alloy\nanodic current\ncurrent\nFe-Co thin films\nlacy pit cover\nlacy pit cover formation\nmeasured the kinetics of 2D pit propagation\nNi\u2013Fe [35] thin films\npit\npit edge\npit growth\npit propagation\npits\nsemi-quantitative model\nstainless steel thin films\nvideo images\n"}, {"id": "S0010938X15002954", "text": "There have been relatively few attempts to observe and in some cases extract the average current density from video images taken of growing 2D pits. Frankel presented a method to directly measure the average anodic current density from the growing pit boundary velocity in Al [33], an Al alloy [34] and Ni\u2013Fe [35] thin films. Subsequently, Ryan et al. [27,36] determined the anodic current density in pits propagating as 2D disks in stainless steel thin films by measuring the pit edge movement velocity. Ernst and Newman [11,12,37] studied stability of pit growth in detail and measured the kinetics of 2D pit propagation in depth and width and compared the results with kinetics in 1D pencil electrodes. They developed a semi-quantitative model for pit propagation which explained the lacy pit cover formation during the pit growth, although they did not measure current density within the pit. More recently, Tang and Davenport [38] tracked the pit boundary movement and computed the instantaneous but average current density in Fe-Co thin films. However, there have been no previous attempts to quantify the local current density during inhomogeneous growth of pits, although such local variation in current density has long been recognised [7].\n", "keywords": "1D pencil electrodes\n2D disks\n2D pit\n2D pit propagation\n2D pits\nAl\nAl alloy\nanodic current\ncurrent\nFe-Co thin films\nlacy pit cover\nlacy pit cover formation\nmeasured the kinetics of 2D pit propagation\nNi\u2013Fe [35] thin films\npit\npit edge\npit growth\npit propagation\npits\nsemi-quantitative model\nstainless steel thin films\nvideo images\n"}, {"id": "S0963869515000572", "text": "Shear horizontal (SH) ultrasound waves are guided waves (they have propagation properties affected by the geometry of the propagation medium), with symmetric and anti-symmetric modes; phase and group speeds are dependent on frequency, sample thickness, and the bulk shear wave speed [11,12]. The properties of the different modes can be very useful, such as in thickness measurement [13], but in this case they are a complication. SH0 has a thickness independent speed, equal to the shear wave speed, and is non-dispersive (the phase and group speed are equal to the shear wave speed for all frequencies). The oscillation direction of SH ultrasound is in the plane of the surface where the wave was generated, and perpendicular to the propagation direction, as shown in Fig. 1, with respect to a reference interface, which is typically a sample surface. Under certain conditions, such as over short propagation distances, SH waves can be treated as bulk waves.\n", "keywords": "bulk shear wave speed\nbulk waves\nguided waves\noscillation\nphase and group speed\npropagation\nSH0\nShear horizontal (SH) ultrasound waves\nshear wave speed\nSH ultrasound\nSH waves\nwave\n"}, {"id": "S0927025615006357", "text": "In this paper, crystal plasticity model, in combination with XFEM, has been applied to study cyclic deformation and fatigue crack growth in a nickel-based superalloy LSHR (Low Solvus High Refractory) at high temperature. The first objective of this research was to develop and evaluate a RVE-based finite element model with the incorporation of a realistic material microstructure. The second objective of this work was to determine the parameters of a crystal plasticity constitutive model to describe the cyclic deformation behaviour of the material by using a user-defined material subroutine (UMAT) interfaced with the finite element package ABAQUS. The model parameters were calibrated from extensive finite element analyses to fit the monotonic, stress relaxation and cyclic test data. The third objective was to predict crack growth by combining the XFEM technique and the calibrated crystal plasticity UMAT, for which accumulated plastic strain was used as the fracture criterion.\n", "keywords": "ABAQUS\ncalibrated crystal plasticity UMAT\ncrack growth\ncrystal plasticity constitutive model\ncrystal plasticity model\ncyclic deformation\ndetermine the parameters of a crystal plasticity constitutive model\ndevelop and evaluate a RVE-based finite element model\nfinite element analyses\nfracture criterion\nLow Solvus High Refractory\nLSHR\nmonotonic, stress relaxation and cyclic test data\nnickel-based superalloy LSHR\nplastic strain\npredict crack growth\nRVE-based finite element model\nstudy cyclic deformation and fatigue crack growth\nUMAT\nuser-defined material subroutine\nXFEM\nXFEM technique\n"}, {"id": "S0009261409006666", "text": "We have presented spectrally resolved femtosecond three-pulse photon echo measurements on Zn(II)\u2013OEP, Ni(II)\u2013OEP and Co(II)\u2013OEP. Increased degree of freedom in scans of time delays allows one to separate and extract specific type of spectroscopic information in complex molecules by studying spectral and temporal evolution of the photon echo signals. By varying the population times, population relaxation dynamics and inhomogeneous broadening is revealed in the photon echo spectra. Time-integrated photon echo signals show two different timescales. The electronic relaxation timescale is found to be sub 50fs whereas the timescale for intramolecular vibrational relaxation, occurring in Q00 band, was found to be over a picosecond for Co(II)\u2013OEP and Ni(II)\u2013OEP and within a picosecond for Zn(II)\u2013OEP.\n", "keywords": "Co(II)\u2013OEP\ncomplex molecules\nelectronic relaxation timescale\nIncreased degree of freedom\ninhomogeneous broadening\nintramolecular vibrational relaxation\nNi(II)\u2013OEP\nphoton echo signals\nphoton echo spectra\npopulation relaxation dynamics\npopulation times\nQ00 band\nseparate and extract specific type of spectroscopic information\nspectral and temporal evolution of the photon echo signals\nspectrally resolved femtosecond three-pulse photon echo measurements\nsub 50fs\nTime-integrated photon echo signals\ntimescale\ntimescales\nvarying the population times\nwithin a picosecond for Zn(II)\u2013OEP\nZn(II)\u2013OEP\n"}, {"id": "S0009261409006666", "text": "We have presented spectrally resolved femtosecond three-pulse photon echo measurements on Zn(II)\u2013OEP, Ni(II)\u2013OEP and Co(II)\u2013OEP. Increased degree of freedom in scans of time delays allows one to separate and extract specific type of spectroscopic information in complex molecules by studying spectral and temporal evolution of the photon echo signals. By varying the population times, population relaxation dynamics and inhomogeneous broadening is revealed in the photon echo spectra. Time-integrated photon echo signals show two different timescales. The electronic relaxation timescale is found to be sub 50fs whereas the timescale for intramolecular vibrational relaxation, occurring in Q00 band, was found to be over a picosecond for Co(II)\u2013OEP and Ni(II)\u2013OEP and within a picosecond for Zn(II)\u2013OEP.\n", "keywords": "Co(II)\u2013OEP\ncomplex molecules\nelectronic relaxation timescale\nIncreased degree of freedom\ninhomogeneous broadening\nintramolecular vibrational relaxation\nNi(II)\u2013OEP\nphoton echo signals\nphoton echo spectra\npopulation relaxation dynamics\npopulation times\nQ00 band\nseparate and extract specific type of spectroscopic information\nspectral and temporal evolution of the photon echo signals\nspectrally resolved femtosecond three-pulse photon echo measurements\nsub 50fs\nTime-integrated photon echo signals\ntimescale\ntimescales\nvarying the population times\nwithin a picosecond for Zn(II)\u2013OEP\nZn(II)\u2013OEP\n"}, {"id": "S0370269304009116", "text": "We prove the uniqueness of the supersymmetric Salam\u2013Sezgin (Minkowski)4\u00d7S2 ground state among all non-singular solutions with a four-dimensional Poincar\u00e9, de\u00a0Sitter or anti-de\u00a0Sitter symmetry. We construct the most general solutions with an axial symmetry in the two-dimensional internal space, and show that included amongst these is a family that is non-singular away from a conical defect at one pole of a distorted 2-sphere. These solutions admit the interpretation of 3-branes with negative tension.\n", "keywords": "3-branes with negative tension\naxial symmetry in the two-dimensional internal space,\nconstruct the most general solutions\nde\u00a0Sitter or anti-de\u00a0Sitter symmetry\ninterpretation of 3-branes with negative tension\nnon-singular away from a conical defect\nnon-singular solutions with a four-dimensional Poincar\u00e9\nprove the uniqueness\n"}, {"id": "S1071581916300854", "text": "We have developed a systematic, quantified understanding of a specific problem: the design of mobile-friendly unique identifiers. But our results also apply to the design of other text-based services. There has been a trend toward bespoke and adaptive keyboards (e.g., Dunlop and Levine, 2012; Karrenbauer and Oulasvirta, 2014; Leiva et al., 2015; Wiseman et al., 2013). More often than not, though, input devices are a fixed constraint in the design of a service. Most users are typing on the keyboard that came with their phone. Those keyboards have advantages, limitations and quirks. The mode-switching that most touchscreen keyboards require to reach numbers and capital letters is at the root of design improvements we propose in this paper. When designing services, it is vital to be aware of the fixed constraints of a system and to then focus on the aspects of a service's design that can be controlled. Making changes to input data in this way is a cheap, quick and easy way to improve user experience.\n", "keywords": "bespoke and adaptive keyboards\ndesigning services\ndesign of a service\ndesign of mobile-friendly unique identifiers\ndesign of other text-based services\ninput devices\nmode-switching\n"}, {"id": "S1071581916300854", "text": "We have developed a systematic, quantified understanding of a specific problem: the design of mobile-friendly unique identifiers. But our results also apply to the design of other text-based services. There has been a trend toward bespoke and adaptive keyboards (e.g., Dunlop and Levine, 2012; Karrenbauer and Oulasvirta, 2014; Leiva et al., 2015; Wiseman et al., 2013). More often than not, though, input devices are a fixed constraint in the design of a service. Most users are typing on the keyboard that came with their phone. Those keyboards have advantages, limitations and quirks. The mode-switching that most touchscreen keyboards require to reach numbers and capital letters is at the root of design improvements we propose in this paper. When designing services, it is vital to be aware of the fixed constraints of a system and to then focus on the aspects of a service's design that can be controlled. Making changes to input data in this way is a cheap, quick and easy way to improve user experience.\n", "keywords": "bespoke and adaptive keyboards\ndesigning services\ndesign of a service\ndesign of mobile-friendly unique identifiers\ndesign of other text-based services\ninput devices\nmode-switching\n"}, {"id": "S0167931713006904", "text": "Copper electro-chemical deposition (ECD) of through silicon via (TSV) is a key challenge of 3D integration. This paper presents a numerical modeling of TSV filling concerning the influence of the accelerator and the suppressor. The diffusion\u2013adsorption model was used in the simulation and effects of the additives were incorporated in the model. The boundary conditions were derived from a set of experimental Tafel curves with different concentrations of additives, which provided a quick and accurate way for copper ECD process prediction without complicated surface kinetic parameters fitting. The level set method (LSM) was employed to track the copper and electrolyte interface. The simulation results were in good agreement with the experiments. For a given feature size, the current density for superfilling could be predicted, which provided a guideline for ECD process optimization.\n", "keywords": "3D integration\naccelerator and the suppressor.\nadditives\ncopper ECD process prediction\nCopper electro-chemical deposition\nCopper electro-chemical deposition (ECD) of through silicon via (TSV)\ndiffusion\u2013adsorption model\nECD\nECD process optimization\nlevel set method\nLSM\nnumerical modeling of TSV filling\nset of experimental Tafel curves\nsimulation\nsimulation and effects of the additives\nsuperfilling\nthrough silicon via\ntrack the copper and electrolyte interface\nTSV\nTSV filling\n"}, {"id": "S0167931713006904", "text": "Copper electro-chemical deposition (ECD) of through silicon via (TSV) is a key challenge of 3D integration. This paper presents a numerical modeling of TSV filling concerning the influence of the accelerator and the suppressor. The diffusion\u2013adsorption model was used in the simulation and effects of the additives were incorporated in the model. The boundary conditions were derived from a set of experimental Tafel curves with different concentrations of additives, which provided a quick and accurate way for copper ECD process prediction without complicated surface kinetic parameters fitting. The level set method (LSM) was employed to track the copper and electrolyte interface. The simulation results were in good agreement with the experiments. For a given feature size, the current density for superfilling could be predicted, which provided a guideline for ECD process optimization.\n", "keywords": "3D integration\naccelerator and the suppressor.\nadditives\ncopper ECD process prediction\nCopper electro-chemical deposition\nCopper electro-chemical deposition (ECD) of through silicon via (TSV)\ndiffusion\u2013adsorption model\nECD\nECD process optimization\nlevel set method\nLSM\nnumerical modeling of TSV filling\nset of experimental Tafel curves\nsimulation\nsimulation and effects of the additives\nsuperfilling\nthrough silicon via\ntrack the copper and electrolyte interface\nTSV\nTSV filling\n"}, {"id": "S221267161200162X", "text": "A fuzzy-Hammerstein model predictive control method is proposed for a continuous stirred-tank reactor (CSTR). In this paper T-S fuzzy model is used to approximate the static nonlinear characteristics of Hammerstein model, and a linear autoregressive model is used to solve the results of optimal control. The designed nonlinear predictive controller using Hammerstein model make good use of the ability of universal approach nonlinear of T-S model, and divide the question of nonlinear predictive control into the nonlinear model recongnization and the question of linear predictive control. The application results of CSTR process show the proposed control method has good control performance compared to PID controller.", "keywords": "continuous stirred-tank reactor\ncontrol method\nCSTR\nCSTR process\nfuzzy-Hammerstein model\nfuzzy-Hammerstein model predictive control method\nHammerstein model\nlinear autoregressive mode\nlinear predictive control\nnonlinear model recongnization\nnonlinear predictive control\nnonlinear predictive controller\noptimal control\nPID controller\nstatic nonlinear characteristics of Hammerstein model\nT-S fuzzy model\nT-S model\nuniversal approach nonlinear\n"}, {"id": "S221267161200162X", "text": "A fuzzy-Hammerstein model predictive control method is proposed for a continuous stirred-tank reactor (CSTR). In this paper T-S fuzzy model is used to approximate the static nonlinear characteristics of Hammerstein model, and a linear autoregressive model is used to solve the results of optimal control. The designed nonlinear predictive controller using Hammerstein model make good use of the ability of universal approach nonlinear of T-S model, and divide the question of nonlinear predictive control into the nonlinear model recongnization and the question of linear predictive control. The application results of CSTR process show the proposed control method has good control performance compared to PID controller.", "keywords": "continuous stirred-tank reactor\ncontrol method\nCSTR\nCSTR process\nfuzzy-Hammerstein model\nfuzzy-Hammerstein model predictive control method\nHammerstein model\nlinear autoregressive mode\nlinear predictive control\nnonlinear model recongnization\nnonlinear predictive control\nnonlinear predictive controller\noptimal control\nPID controller\nstatic nonlinear characteristics of Hammerstein model\nT-S fuzzy model\nT-S model\nuniversal approach nonlinear\n"}, {"id": "S2212667814001166", "text": "Along with the expansion of computer-based climate simulations, efficient visualization and analysis of massive climate data are becoming more important than ever. In this paper, we try to explore the factors behide climate changes by combining window query and time-varying data mining techniques. With constant query time and acceptable storage cost, the algorithms presented support various queries on 3d time-varying datasets, such as average, min, and max value. A new time-varying data analysis algorithm is given, which is especially suitable for analyzing big data. All these algorithms have been implemented on and integrated into a visual analysis system, with tiled-LCD ultra-resolution display. Experimental results on several datasets from practical applications are presented.", "keywords": "3d time-varying datasets\nalgorithms\nanalyzing big data\ncombining window query and time-varying data mining techniques\ncomputer-based climate simulations\nexpansion of computer-based climate simulations\nexplore the factors behide climate changes\nimplemented on and integrated into a visual analysis system\nmassive climate data\nqueries\nseveral datasets from practical applications\ntiled-LCD ultra-resolution display\ntime-varying data analysis algorithm\nvisualization and analysis of massive climate data\n"}, {"id": "S2212667814001166", "text": "Along with the expansion of computer-based climate simulations, efficient visualization and analysis of massive climate data are becoming more important than ever. In this paper, we try to explore the factors behide climate changes by combining window query and time-varying data mining techniques. With constant query time and acceptable storage cost, the algorithms presented support various queries on 3d time-varying datasets, such as average, min, and max value. A new time-varying data analysis algorithm is given, which is especially suitable for analyzing big data. All these algorithms have been implemented on and integrated into a visual analysis system, with tiled-LCD ultra-resolution display. Experimental results on several datasets from practical applications are presented.", "keywords": "3d time-varying datasets\nalgorithms\nanalyzing big data\ncombining window query and time-varying data mining techniques\ncomputer-based climate simulations\nexpansion of computer-based climate simulations\nexplore the factors behide climate changes\nimplemented on and integrated into a visual analysis system\nmassive climate data\nqueries\nseveral datasets from practical applications\ntiled-LCD ultra-resolution display\ntime-varying data analysis algorithm\nvisualization and analysis of massive climate data\n"}, {"id": "S0957417416301786", "text": "These results demonstrate that SW-SVR predicts complicated micrometeorological data with the best prediction performance and the lowest computational complexity compared with standard algorithms. In particular, we found that dynamic aggregation of models built from very little extracted data by D-SDC is effective for compatibility of high prediction performance and low computational complexity. However, there are problems to be solved in SW-SVR. Firstly, the prediction performance of SW-SVR sometimes deteriorates despite an increase of training data. In particular, this problem occurred under the conditions that prediction horizons are 6 h as shown in Fig. 3. This is because data extracted by D-SDC involves unnecessary training data for highly accurate prediction. If D-SDC extracts the same data as the extracted data when training periods are shorter, the prediction performance of SW-SVR never deteriorates due to an increase of training data. Therefore, we must review both feature mapping and algorithms of D-SDC so as to avoid extracting unnecessary training data. Meanwhile, SW-SVR is based on a combination of several algorithms: kernel approximation, PLS regression, k-means, D-SDC, and linear SVR. Moreover, each algorithm has several parameters. Therefore, SW-SVR has more varied parameters, and it takes more time to tune the parameters. In this experiment, we used a grid search roughly so as to decide the parameters in a certain time. However, there is still room for improvement in the prediction performance by using other approaches such as a genetic algorithm instead of a grid search (Huang & Wang, 2006).\n", "keywords": "combination of several algorithms\nD-SDC\ndynamic aggregation of models\ngenetic algorithm\ngrid search\nkernel approximation\nk-means\nlinear SVR\nmicrometeorological data\nPLS regression\nstandard algorithms\nSW-SVR\ntraining data\ntune the parameters\n"}, {"id": "S0045782512003234", "text": "An attempt of a quite comprehensive answer to this question is made hereafter, within the following structure of the remaining paper: first, we introduce the mathematical systems biology of bone, starting from the work of Pivonka et al. [25,26], and extending it to mechanoregulatory feedback control (Section 2). Then, we introduce a continuum micromechanics representation adopted from Hellmich et al. [30], in order to scale elasticity and strains from the level of the extravascular bone matrix to that of cortical bone1In this paper, we restrict ourselves to cortical bone, due to its major importance in providing sufficient load-carrying capacity. However, extension of the coupled approach proposed here to trabecular bone is straightforward; it merely requires recalibration of underlying parameters.1 and vice versa (Section 3). The micromechanics formulation is fed with composition quantities derived from the systems biology approach, which, in turn, is provided with mechanical stimuli gained from the micromechanics model. We then apply the coupled approach to biochemical and mechanical conditions typical for postmenopausal osteoporosis (Section 4) and microgravity exposure (Section 5), and discuss key sensitivity features (Section 6). After emphasizing the potentials and limitations of the presented approach (Section 7), we conclude the paper in (Section 8).\n", "keywords": "biochemical and mechanical conditions\ncomposition quantities\ncontinuum micromechanics representation\ncortical bone\ncoupled approach\nmathematical systems biology of bone\nmechanical stimuli\nmechanoregulatory feedback control\nmicromechanics formulation\nmicromechanics model\npotentials and limitations of the presented approach\nproviding sufficient load-carrying capacity\nrecalibration of underlying parameters\nscale elasticity and strains from the level of the extravascular bone matrix to that of cortical bone\nsystems biology approach\ntrabecular bone\n"}, {"id": "S0370269302012492", "text": "In this section we wish to calculate the cross section for the absorption of massless scalars by the self-dual string in the world volume of the M-theory five-brane. We will adopt an entirely world volume approach similar to that of\u00a0[21\u201323]. We begin by writing the equation satisfied by the s-wave with energy \u03c9, \u03c6(r,t)=\u03c6(r)ei\u03c9t, of the linear fluctuations of the four overall transverse scalars about the self-dual string, (it is known that there are problems when one considers higher angular momentum modes [23], one must take care with the validity of the linearized approximation, this is discussed in [13]): (15)\u03c1\u22123dd\u03c1\u03c13dd\u03c1+1+R6\u03c96\u03c16\u03c6(\u03c1)=0, where \u03c1=r\u03c9, R=Q1/3\u2113p. Note, as pointed out by\u00a0[11] world volume solitons have a much sharper potential than the Coulomb type potential typical of brane solutions in supergravity; thus this scattering is different to that of the string in six-dimensional supergravity. Nevertheless, for small \u03c9R one may solve this problem by matching an approximate solution in the inner region to an approximate solution in the outer region; this follows closely the supergravity calculation\u00a0[24].\n", "keywords": "approximate solution\nbrane solutions\ncalculate the cross section for the absorption of massless scalars\nentirely world volume approach\nhigher angular momentum modes\nlinear fluctuations\nlinearized approximation\nmassless scalars\nM-theory five-brane\nself-dual string\nsmall \u03c9R\nsupergravity calculation\ns-wave\nworld volume solitons\n"}, {"id": "S0045782511003823", "text": "In the present work we use the mortar finite element method for the coupling of nonconforming discretized sub-domains in the framework of nonlinear elasticity. The mortar method has been shown to preserve optimal convergence rates (see Laursen (2002) [25] for details) and is variationally consistent. We show that the method can be applied to isogeometric analysis with little effort, once the framework of NURBS based shape functions has been implemented. Furthermore, a specific coordinate augmentation technique allows the design of an energy\u2013momentum scheme for the constrained mechanical system under consideration. The excellent performance of the redesigned mortar method as well as the energy\u2013momentum scheme is illustrated in representative numerical examples.In the present work we use the mortar finite element method for the coupling of nonconforming discretized sub-domains in the framework of nonlinear elasticity. The mortar method has been shown to preserve optimal convergence rates (see Laursen (2002) [25] for details) and is variationally consistent. We show that the method can be applied to isogeometric analysis with little effort, once the framework of NURBS based shape functions has been implemented. Furthermore, a specific coordinate augmentation technique allows the design of an energy\u2013momentum scheme for the constrained mechanical system under consideration. The excellent performance of the redesigned mortar method as well as the energy\u2013momentum scheme is illustrated in representative numerical examples.\n", "keywords": "constrained mechanical system\nconvergence rates\ncoordinate augmentation technique\ncoupling of nonconforming discretized sub-domains in the framework of nonlinear elasticity\nenergy\u2013momentum scheme\nframework of NURBS based shape functions\nisogeometric analysis\nmechanical system\nmortar finite element\nmortar finite element method\nmortar method\noptimal convergence rates\n"}, {"id": "S0045782511003823", "text": "In the present work we use the mortar finite element method for the coupling of nonconforming discretized sub-domains in the framework of nonlinear elasticity. The mortar method has been shown to preserve optimal convergence rates (see Laursen (2002) [25] for details) and is variationally consistent. We show that the method can be applied to isogeometric analysis with little effort, once the framework of NURBS based shape functions has been implemented. Furthermore, a specific coordinate augmentation technique allows the design of an energy\u2013momentum scheme for the constrained mechanical system under consideration. The excellent performance of the redesigned mortar method as well as the energy\u2013momentum scheme is illustrated in representative numerical examples.In the present work we use the mortar finite element method for the coupling of nonconforming discretized sub-domains in the framework of nonlinear elasticity. The mortar method has been shown to preserve optimal convergence rates (see Laursen (2002) [25] for details) and is variationally consistent. We show that the method can be applied to isogeometric analysis with little effort, once the framework of NURBS based shape functions has been implemented. Furthermore, a specific coordinate augmentation technique allows the design of an energy\u2013momentum scheme for the constrained mechanical system under consideration. The excellent performance of the redesigned mortar method as well as the energy\u2013momentum scheme is illustrated in representative numerical examples.\n", "keywords": "constrained mechanical system\nconvergence rates\ncoordinate augmentation technique\ncoupling of nonconforming discretized sub-domains in the framework of nonlinear elasticity\nenergy\u2013momentum scheme\nframework of NURBS based shape functions\nisogeometric analysis\nmechanical system\nmortar finite element\nmortar finite element method\nmortar method\noptimal convergence rates\n"}, {"id": "S0010938X15301189", "text": "Failure of structural components is a major concern in the nuclear power industry and represents not only a safety issue, but also a hazard to economic performance. Stress corrosion cracking (SCC), and especially intergranular stress corrosion cracking (IGSCC), have proved to be a significant potential cause of failures in the nuclear industry in materials such as Alloy 600 (74% Ni, 16% Cr and 8% Fe) and stainless steels, especially in Pressurised Water Reactors (PWR) [1\u20135]. Stress corrosion cracking in pressurized water reactors (PWSCC) occurs in Alloy 600 in safety critical components, such as steam generator tubes, heater sleeves, pressurized instrument penetrations and control rod drive mechanisms [2,6,7]. Understanding the mechanisms that control SCC in this alloy will allow for continued extensions of life in current plant as well as safer designs of future nuclear reactors.\n", "keywords": "74% Ni, 16% Cr and 8% Fe\nalloy\nAlloy 600\ncontrol rod drive mechanisms\nFailure of structural components\nheater sleeves\nIGSCC\nintergranular stress corrosion cracking\nnuclear reactors\nplant\nPressurised Water Reactors\npressurized instrument penetrations\npressurized water reactors\nPWR\nPWSCC\nsafer designs of future nuclear reactors\nsafety critical components\nSCC\nstainless steels\nsteam generator tubes\nStress corrosion cracking\nstructural components\n"}, {"id": "S0010938X15301189", "text": "Failure of structural components is a major concern in the nuclear power industry and represents not only a safety issue, but also a hazard to economic performance. Stress corrosion cracking (SCC), and especially intergranular stress corrosion cracking (IGSCC), have proved to be a significant potential cause of failures in the nuclear industry in materials such as Alloy 600 (74% Ni, 16% Cr and 8% Fe) and stainless steels, especially in Pressurised Water Reactors (PWR) [1\u20135]. Stress corrosion cracking in pressurized water reactors (PWSCC) occurs in Alloy 600 in safety critical components, such as steam generator tubes, heater sleeves, pressurized instrument penetrations and control rod drive mechanisms [2,6,7]. Understanding the mechanisms that control SCC in this alloy will allow for continued extensions of life in current plant as well as safer designs of future nuclear reactors.\n", "keywords": "74% Ni, 16% Cr and 8% Fe\nalloy\nAlloy 600\ncontrol rod drive mechanisms\nFailure of structural components\nheater sleeves\nIGSCC\nintergranular stress corrosion cracking\nnuclear reactors\nplant\nPressurised Water Reactors\npressurized instrument penetrations\npressurized water reactors\nPWR\nPWSCC\nsafer designs of future nuclear reactors\nsafety critical components\nSCC\nstainless steels\nsteam generator tubes\nStress corrosion cracking\nstructural components\n"}, {"id": "S0010938X15301189", "text": "Failure of structural components is a major concern in the nuclear power industry and represents not only a safety issue, but also a hazard to economic performance. Stress corrosion cracking (SCC), and especially intergranular stress corrosion cracking (IGSCC), have proved to be a significant potential cause of failures in the nuclear industry in materials such as Alloy 600 (74% Ni, 16% Cr and 8% Fe) and stainless steels, especially in Pressurised Water Reactors (PWR) [1\u20135]. Stress corrosion cracking in pressurized water reactors (PWSCC) occurs in Alloy 600 in safety critical components, such as steam generator tubes, heater sleeves, pressurized instrument penetrations and control rod drive mechanisms [2,6,7]. Understanding the mechanisms that control SCC in this alloy will allow for continued extensions of life in current plant as well as safer designs of future nuclear reactors.\n", "keywords": "74% Ni, 16% Cr and 8% Fe\nalloy\nAlloy 600\ncontrol rod drive mechanisms\nFailure of structural components\nheater sleeves\nIGSCC\nintergranular stress corrosion cracking\nnuclear reactors\nplant\nPressurised Water Reactors\npressurized instrument penetrations\npressurized water reactors\nPWR\nPWSCC\nsafer designs of future nuclear reactors\nsafety critical components\nSCC\nstainless steels\nsteam generator tubes\nStress corrosion cracking\nstructural components\n"}, {"id": "S0010938X15301189", "text": "Failure of structural components is a major concern in the nuclear power industry and represents not only a safety issue, but also a hazard to economic performance. Stress corrosion cracking (SCC), and especially intergranular stress corrosion cracking (IGSCC), have proved to be a significant potential cause of failures in the nuclear industry in materials such as Alloy 600 (74% Ni, 16% Cr and 8% Fe) and stainless steels, especially in Pressurised Water Reactors (PWR) [1\u20135]. Stress corrosion cracking in pressurized water reactors (PWSCC) occurs in Alloy 600 in safety critical components, such as steam generator tubes, heater sleeves, pressurized instrument penetrations and control rod drive mechanisms [2,6,7]. Understanding the mechanisms that control SCC in this alloy will allow for continued extensions of life in current plant as well as safer designs of future nuclear reactors.\n", "keywords": "74% Ni, 16% Cr and 8% Fe\nalloy\nAlloy 600\ncontrol rod drive mechanisms\nFailure of structural components\nheater sleeves\nIGSCC\nintergranular stress corrosion cracking\nnuclear reactors\nplant\nPressurised Water Reactors\npressurized instrument penetrations\npressurized water reactors\nPWR\nPWSCC\nsafer designs of future nuclear reactors\nsafety critical components\nSCC\nstainless steels\nsteam generator tubes\nStress corrosion cracking\nstructural components\n"}, {"id": "S0370269304009062", "text": "The microwave background is not the only universal photon field that has to be taken in consideration. Especially interesting is the isotropic infrared and optical background (IRB). The number density of IRB is smaller than that of MBR by more that two orders of magnitude. On the other hand, protons of lower energy can interact on the IRB, and the smaller number density has to be weighted with the higher flux of interacting protons. The present Universe is optically thin to 1019\u00a0eV and lower energy protons, but even at low redshifts the proton interaction rate quickly increases. This is different from the interactions on MBR, where the interacting protons quickly lose their energy even at z=0. The cosmological evolution of UHECR injection is thus of major importance for the contribution of such interactions to the flux of cosmogenic neutrinos.\n", "keywords": "cosmological evolution of UHECR injection\nflux of cosmogenic neutrinos\ninteracting protons\ninteractions on MBR\nIRB\nisotropic infrared and optical background\nlower energy protons\nMBR\nmicrowave background\npresent Universe\nproton\nproton interaction rate\nprotons\nquickly lose their energy even at z=0\ntaken in consideration\nthe smaller number density has to be weighted with the higher flux of interacting protons\nuniversal photon field\n"}, {"id": "S0301932213001985", "text": "In the current CLSVOF method, the normal vector is calculated directly by discretising the LS gradient using a finite difference scheme. By appropriately choosing one of three finite difference schemes (central, forward, or backward differencing), it has been demonstrated that thin liquid ligaments can be well resolved see Xiao (2012). Although a high order discretisation scheme (e.g. 5th order WENO) has been found necessary for LS evolution in pure LS methods to reduce mass error, low order LS discretisation schemes (2nd order is used here) can produce accurate results when the LS equation is solved and constrained as indicated above in a CLSVOF method (see Xiao, 2012), since the VOF method maintains 2nd order accuracy. This is a further reason to adopt the CLSVOF method, which has been used for all the following simulations of liquid jet primary breakup.\n", "keywords": "5th order WENO\ncentral, forward, or backward differencing\nCLSVOF method\nfinite difference scheme\nfinite difference schemes\nhigh order discretisation scheme\nliquid\nlow order LS discretisation schemes\nLS\nLS methods\nreduce mass error\nsimulations of liquid jet primary breakup\nthin liquid ligaments\nVOF method\n"}, {"id": "S0370269304007701", "text": "Solitons present the possibility of extended objects as stable states within Quantum Field Theory. Although these solutions are obtained from semi-classical arguments in weak coupling limit, their validity as quantal states is justified based on the associated topological conservation laws. A more curious occurrence is that of fermionic zero-energy modes trapped on such solutions. Their presence requires, according to well-known arguments\u00a0[1,2], an assignment of half-integer fermion number to the solitonic states. In the usual treatment, the back reaction of the fermion zero-modes on the soliton itself is ignored. However, the fractional values of the fermionic charge have interesting consequence for the fate of the soliton if the latter is not strictly stable. The reason for this is that if the configuration were to relax to trivial vacuum in isolation, there is no particle-like state available for carrying the fractional value of the fermionic charge. Dynamical stability of such objects was pointed out in\u00a0[3], in cosmological context in [4,5] and more recently in [6\u20138]. Fractional fermion number phenomenon also occurs in condensed matter systems and its wide ranging implications call for a systematic understanding of the phenomenon.\n", "keywords": "assignment of half-integer fermion number to the solitonic states\nassociated topological conservation laws\ncarrying the fractional value of the fermionic charge\ncondensed matter systems\nDynamical stability of such objects\nfermionic zero-energy modes trapped on such solutions\nfractional values of the fermionic charge\nquantal states\nsemi-classical arguments in weak coupling limit\nSolitons\nsystematic understanding of the phenomenon\nthe back reaction of the fermion zero-modes on the soliton\nthe fate of the soliton\nvacuum\n"}, {"id": "S0370269304007701", "text": "Solitons present the possibility of extended objects as stable states within Quantum Field Theory. Although these solutions are obtained from semi-classical arguments in weak coupling limit, their validity as quantal states is justified based on the associated topological conservation laws. A more curious occurrence is that of fermionic zero-energy modes trapped on such solutions. Their presence requires, according to well-known arguments\u00a0[1,2], an assignment of half-integer fermion number to the solitonic states. In the usual treatment, the back reaction of the fermion zero-modes on the soliton itself is ignored. However, the fractional values of the fermionic charge have interesting consequence for the fate of the soliton if the latter is not strictly stable. The reason for this is that if the configuration were to relax to trivial vacuum in isolation, there is no particle-like state available for carrying the fractional value of the fermionic charge. Dynamical stability of such objects was pointed out in\u00a0[3], in cosmological context in [4,5] and more recently in [6\u20138]. Fractional fermion number phenomenon also occurs in condensed matter systems and its wide ranging implications call for a systematic understanding of the phenomenon.\n", "keywords": "assignment of half-integer fermion number to the solitonic states\nassociated topological conservation laws\ncarrying the fractional value of the fermionic charge\ncondensed matter systems\nDynamical stability of such objects\nfermionic zero-energy modes trapped on such solutions\nfractional values of the fermionic charge\nquantal states\nsemi-classical arguments in weak coupling limit\nSolitons\nsystematic understanding of the phenomenon\nthe back reaction of the fermion zero-modes on the soliton\nthe fate of the soliton\nvacuum\n"}, {"id": "S0370269304007701", "text": "Solitons present the possibility of extended objects as stable states within Quantum Field Theory. Although these solutions are obtained from semi-classical arguments in weak coupling limit, their validity as quantal states is justified based on the associated topological conservation laws. A more curious occurrence is that of fermionic zero-energy modes trapped on such solutions. Their presence requires, according to well-known arguments\u00a0[1,2], an assignment of half-integer fermion number to the solitonic states. In the usual treatment, the back reaction of the fermion zero-modes on the soliton itself is ignored. However, the fractional values of the fermionic charge have interesting consequence for the fate of the soliton if the latter is not strictly stable. The reason for this is that if the configuration were to relax to trivial vacuum in isolation, there is no particle-like state available for carrying the fractional value of the fermionic charge. Dynamical stability of such objects was pointed out in\u00a0[3], in cosmological context in [4,5] and more recently in [6\u20138]. Fractional fermion number phenomenon also occurs in condensed matter systems and its wide ranging implications call for a systematic understanding of the phenomenon.\n", "keywords": "assignment of half-integer fermion number to the solitonic states\nassociated topological conservation laws\ncarrying the fractional value of the fermionic charge\ncondensed matter systems\nDynamical stability of such objects\nfermionic zero-energy modes trapped on such solutions\nfractional values of the fermionic charge\nquantal states\nsemi-classical arguments in weak coupling limit\nSolitons\nsystematic understanding of the phenomenon\nthe back reaction of the fermion zero-modes on the soliton\nthe fate of the soliton\nvacuum\n"}, {"id": "S0370269304007701", "text": "Solitons present the possibility of extended objects as stable states within Quantum Field Theory. Although these solutions are obtained from semi-classical arguments in weak coupling limit, their validity as quantal states is justified based on the associated topological conservation laws. A more curious occurrence is that of fermionic zero-energy modes trapped on such solutions. Their presence requires, according to well-known arguments\u00a0[1,2], an assignment of half-integer fermion number to the solitonic states. In the usual treatment, the back reaction of the fermion zero-modes on the soliton itself is ignored. However, the fractional values of the fermionic charge have interesting consequence for the fate of the soliton if the latter is not strictly stable. The reason for this is that if the configuration were to relax to trivial vacuum in isolation, there is no particle-like state available for carrying the fractional value of the fermionic charge. Dynamical stability of such objects was pointed out in\u00a0[3], in cosmological context in [4,5] and more recently in [6\u20138]. Fractional fermion number phenomenon also occurs in condensed matter systems and its wide ranging implications call for a systematic understanding of the phenomenon.\n", "keywords": "assignment of half-integer fermion number to the solitonic states\nassociated topological conservation laws\ncarrying the fractional value of the fermionic charge\ncondensed matter systems\nDynamical stability of such objects\nfermionic zero-energy modes trapped on such solutions\nfractional values of the fermionic charge\nquantal states\nsemi-classical arguments in weak coupling limit\nSolitons\nsystematic understanding of the phenomenon\nthe back reaction of the fermion zero-modes on the soliton\nthe fate of the soliton\nvacuum\n"}, {"id": "S0370269304007701", "text": "Solitons present the possibility of extended objects as stable states within Quantum Field Theory. Although these solutions are obtained from semi-classical arguments in weak coupling limit, their validity as quantal states is justified based on the associated topological conservation laws. A more curious occurrence is that of fermionic zero-energy modes trapped on such solutions. Their presence requires, according to well-known arguments\u00a0[1,2], an assignment of half-integer fermion number to the solitonic states. In the usual treatment, the back reaction of the fermion zero-modes on the soliton itself is ignored. However, the fractional values of the fermionic charge have interesting consequence for the fate of the soliton if the latter is not strictly stable. The reason for this is that if the configuration were to relax to trivial vacuum in isolation, there is no particle-like state available for carrying the fractional value of the fermionic charge. Dynamical stability of such objects was pointed out in\u00a0[3], in cosmological context in [4,5] and more recently in [6\u20138]. Fractional fermion number phenomenon also occurs in condensed matter systems and its wide ranging implications call for a systematic understanding of the phenomenon.\n", "keywords": "assignment of half-integer fermion number to the solitonic states\nassociated topological conservation laws\ncarrying the fractional value of the fermionic charge\ncondensed matter systems\nDynamical stability of such objects\nfermionic zero-energy modes trapped on such solutions\nfractional values of the fermionic charge\nquantal states\nsemi-classical arguments in weak coupling limit\nSolitons\nsystematic understanding of the phenomenon\nthe back reaction of the fermion zero-modes on the soliton\nthe fate of the soliton\nvacuum\n"}, {"id": "S0011227515000648", "text": "The product change between batches #1/#2 and the others is the most influential on the test results. The redesign and upgrade to 110-nm process technology reduces the pass rate at LNT by approximately half. This is mainly caused by the increased incidence of erase and program timeouts with some contribution from long erase and program times and bit errors. The difference in pass rates at 88K between batches #3/#4 and #5/#6, which use the same process technology with the same dimensions, can be explained by the fabrication in different assembly lines, where other processes or base materials may have been changed. This means different tolerances in base materials and production process, which are more pronounced the lower the temperature. Some of the differences of technology scale may reflect shifts in transistor parameters such as transconductance/gain, threshold voltage, and threshold slope [7].\n", "keywords": "110-nm process technology\n#5/#6\nassembly lines\nbatches #1/#2\nbatches #3/#4\nbit errors\nerase and program timeouts\nLNT\nlong erase\npass rates\nprocess technology\nproduct change\nprogram times\nthreshold slope\nthreshold voltage\ntolerances\ntransconductance/gain\ntransistor parameters\n"}, {"id": "S2214657115000179", "text": "Aeroengine turbine disks often consist of paramagnetic, that means non-ferromagnetic Nickel based alloys. Sometimes, parasitic small ferromagnetic particles can be included in these disks that may decrease the mechanical stability. For this reason, in case of a suspicion disks are to be analysed with respect to ferromagnetic inclusions. These inclusions generate a magnetic density which can be measured by a flux gate magnetometer using the magnetic remanence method [1]. The detection principle of ferromagnetic impurities in non-magnetic metallic materials is based on their remanence. Before such a measurement can be carried out, the aeroengine turbine disks are premagnetised in axial direction. As ferromagnetic materials show the well-known hysteresis behaviour, those materials can be magnetised by a strong magnetic field which drives the magnetic material into saturation. When removing the magnetic field, the remanence is left. This remaining flux density is used to detect them in non-magnetic materials.\n", "keywords": "Aeroengine turbine disks\ndecrease the mechanical stability\ndetection principle of ferromagnetic impurities\nferromagnetic inclusions\nflux gate magnetometer\nhysteresis behaviour\nmagnetic density\nmagnetic remanence method\nmeasurement\nnon-ferromagnetic Nickel based alloys\nparamagnetic\nparasitic small ferromagnetic particles\npremagnetised\nremaining flux density\nremoving the magnetic field\nstrong magnetic field\n"}, {"id": "S2214657115000179", "text": "Aeroengine turbine disks often consist of paramagnetic, that means non-ferromagnetic Nickel based alloys. Sometimes, parasitic small ferromagnetic particles can be included in these disks that may decrease the mechanical stability. For this reason, in case of a suspicion disks are to be analysed with respect to ferromagnetic inclusions. These inclusions generate a magnetic density which can be measured by a flux gate magnetometer using the magnetic remanence method [1]. The detection principle of ferromagnetic impurities in non-magnetic metallic materials is based on their remanence. Before such a measurement can be carried out, the aeroengine turbine disks are premagnetised in axial direction. As ferromagnetic materials show the well-known hysteresis behaviour, those materials can be magnetised by a strong magnetic field which drives the magnetic material into saturation. When removing the magnetic field, the remanence is left. This remaining flux density is used to detect them in non-magnetic materials.\n", "keywords": "Aeroengine turbine disks\ndecrease the mechanical stability\ndetection principle of ferromagnetic impurities\nferromagnetic inclusions\nflux gate magnetometer\nhysteresis behaviour\nmagnetic density\nmagnetic remanence method\nmeasurement\nnon-ferromagnetic Nickel based alloys\nparamagnetic\nparasitic small ferromagnetic particles\npremagnetised\nremaining flux density\nremoving the magnetic field\nstrong magnetic field\n"}, {"id": "S2214657115000179", "text": "Aeroengine turbine disks often consist of paramagnetic, that means non-ferromagnetic Nickel based alloys. Sometimes, parasitic small ferromagnetic particles can be included in these disks that may decrease the mechanical stability. For this reason, in case of a suspicion disks are to be analysed with respect to ferromagnetic inclusions. These inclusions generate a magnetic density which can be measured by a flux gate magnetometer using the magnetic remanence method [1]. The detection principle of ferromagnetic impurities in non-magnetic metallic materials is based on their remanence. Before such a measurement can be carried out, the aeroengine turbine disks are premagnetised in axial direction. As ferromagnetic materials show the well-known hysteresis behaviour, those materials can be magnetised by a strong magnetic field which drives the magnetic material into saturation. When removing the magnetic field, the remanence is left. This remaining flux density is used to detect them in non-magnetic materials.\n", "keywords": "Aeroengine turbine disks\ndecrease the mechanical stability\ndetection principle of ferromagnetic impurities\nferromagnetic inclusions\nflux gate magnetometer\nhysteresis behaviour\nmagnetic density\nmagnetic remanence method\nmeasurement\nnon-ferromagnetic Nickel based alloys\nparamagnetic\nparasitic small ferromagnetic particles\npremagnetised\nremaining flux density\nremoving the magnetic field\nstrong magnetic field\n"}, {"id": "S0021999115003423", "text": "The remainder of our discussion proceeds as follows. In Section 2 we briefly describe the problem of cell tracking and introduce our approach to cell tracking, which may be regarded as fitting a mathematical model to experimental image data sets. We present the geometric evolution law model we seek to fit, which is a simplification of recently developed models in the literature that show good agreement with experiments [8,10\u201312,4,13,9]. We finish Section 2 by reformulating our model into the phase field framework, which appears more suitable for the problem in hand, and we formulate the cell tracking problem as a PDE constrained optimisation problem. In Section 3 we propose an algorithm for the resolution of the PDE constrained optimisation problem and we discuss some practical aspects related to the implementation. In particular we note that the theoretical and computational framework may be applied directly to multi-cell image data sets and raw image data sets (of sufficient quality) without segmentation. In Section 4 we present some numerical examples for the case of 2d single and multi-cell image data sets. Finally in Section 5 we present some conclusions of our study and discuss future extensions and applications of the work.\n", "keywords": "algorithm\ncell tracking problem\nexperimental image data sets\ngeometric evolution law model\nimage data sets\nintroduce our approach to cell tracking\nmathematical model\nmulti-cell image data sets\nour model\nPDE constrained optimisation problem\nphase field framework\nproblem of cell tracking\nraw image data sets\nsegmentation\ntheoretical and computational framework\n"}, {"id": "S2212667814000380", "text": "The behavior of cellular beam is described using design methods according to BS: 5950, considering particularly the strength of tee sections and web post element. Such behavior is derived from parametric study involving finite element analysis using software ANSYS. The design method is based on plastic analysis of beam section at ultimate loads and elastic analysis at serviceability loads. The procedure of design of cellular beam is illustrated and an example based on design method is worked out and its verification is done for checking the suitability.", "keywords": "ANSYS\ncellular beam\ndesign method\ndesign methods\nelastic analysis\nfinite element analysis\nplastic analysis of beam section\nprocedure of design of cellular beam is illustrated\n"}, {"id": "S0022311514000919", "text": "The dashed curve represents the PuO2 molar fraction on the sample surface. It shows that, following the UO2\u2013PuO2 phase boundaries, rather well established in this compositional range (see Section 4.3 below), the newly formed liquid surface is initially enriched in plutonium dioxide. Subsequently, due to fast diffusion in the liquid phase, the initial sample composition (x(PuO2)=0.25) tends to be rapidly restored. It is however clear, from the simulation, that the fast cooling occurring after the end of the laser pulse leads to onset of solidification before the initial composition is fully recovered in the liquid. A surface solid crust forms then upon freezing before the total liquid mass has crystallised (see insets in Fig. 4). The double inflection during cooling in this case corresponds to the solidification onset on the sample surface (first inflection) and to the disappearance of the last liquid inside the material (second inflection). The highest recalescence temperature represents the solidification point of a composition very close to the initial one (approximately \u00b10.01 on x(PuO2) in the current example), except for small segregation effects. These latter have been studied also experimentally in the present research, by post-melting material characterisation.\n", "keywords": "cooling\nenriched in plutonium dioxide\nfreezing\nlaser pulse\nliquid\nliquid surface\nplutonium dioxide\npost-melting material characterisation\nPuO2\nsample surface\nsimulation\nsmall segregation effects\nsolidification\nsurface solid crust\nUO2\u2013PuO2 phase boundaries\n"}, {"id": "S0022311514000919", "text": "The dashed curve represents the PuO2 molar fraction on the sample surface. It shows that, following the UO2\u2013PuO2 phase boundaries, rather well established in this compositional range (see Section 4.3 below), the newly formed liquid surface is initially enriched in plutonium dioxide. Subsequently, due to fast diffusion in the liquid phase, the initial sample composition (x(PuO2)=0.25) tends to be rapidly restored. It is however clear, from the simulation, that the fast cooling occurring after the end of the laser pulse leads to onset of solidification before the initial composition is fully recovered in the liquid. A surface solid crust forms then upon freezing before the total liquid mass has crystallised (see insets in Fig. 4). The double inflection during cooling in this case corresponds to the solidification onset on the sample surface (first inflection) and to the disappearance of the last liquid inside the material (second inflection). The highest recalescence temperature represents the solidification point of a composition very close to the initial one (approximately \u00b10.01 on x(PuO2) in the current example), except for small segregation effects. These latter have been studied also experimentally in the present research, by post-melting material characterisation.\n", "keywords": "cooling\nenriched in plutonium dioxide\nfreezing\nlaser pulse\nliquid\nliquid surface\nplutonium dioxide\npost-melting material characterisation\nPuO2\nsample surface\nsimulation\nsmall segregation effects\nsolidification\nsurface solid crust\nUO2\u2013PuO2 phase boundaries\n"}, {"id": "S2212667812000032", "text": "Aspect-oriented Programming (AOP) can well solve the cross-cutting concerns. Because of the different features of aspect, AOP requires new techniques for testing. First, this paper proposes a model to test aspect-oriented software. In order to support the testing model of the first three steps, we propose the algorithm of selecting aspect relevant test cases. Then, we develop a new tool to implement the theoretical of automating select test case. Finally, a case of the Bank Account System is studied to illustrate our testing approach.", "keywords": "algorithm of selecting aspect relevant test cases\nAOP\nAspect-oriented Programming\nautomating select test case\nBank Account System\ndevelop a new tool to implement the theoretical of automating select test case\nmodel to test aspect-oriented software\nsolve the cross-cutting concerns\ntesting model\n"}, {"id": "S2212667812000032", "text": "Aspect-oriented Programming (AOP) can well solve the cross-cutting concerns. Because of the different features of aspect, AOP requires new techniques for testing. First, this paper proposes a model to test aspect-oriented software. In order to support the testing model of the first three steps, we propose the algorithm of selecting aspect relevant test cases. Then, we develop a new tool to implement the theoretical of automating select test case. Finally, a case of the Bank Account System is studied to illustrate our testing approach.", "keywords": "algorithm of selecting aspect relevant test cases\nAOP\nAspect-oriented Programming\nautomating select test case\nBank Account System\ndevelop a new tool to implement the theoretical of automating select test case\nmodel to test aspect-oriented software\nsolve the cross-cutting concerns\ntesting model\n"}, {"id": "S0370269304008858", "text": "The reason to investigate the BFKL and DGLAP equations in the case of supersymmetric theories is based on a common belief, that the high symmetry may significantly simplify the structure of these equations. Indeed, it was found in the leading logarithmic approximation (LLA)\u00a0[10], that the so-called quasi-partonic operators in N=1 SYM are unified in supermultiplets with anomalous dimensions obtained from universal anomalous dimensions \u03b3uni(j) by shifting its arguments by an integer number. Further, the anomalous dimension matrices for twist-2 operators are fixed by the superconformal invariance\u00a0[10]. Calculations in the maximally extended N=4 SYM, where the coupling constant is not renormalized, give even more remarkable results. Namely, it turns out, that here all twist-2 operators enter in the same multiplet, their anomalous dimension matrix is fixed completely by the super-conformal invariance and its universal anomalous dimension in LLA is proportional to \u03a8(j\u22121)\u2212\u03a8(1), which means, that the evolution equations for the matrix elements of quasi-partonic operators in the multicolor limit Nc\u2192\u221e are equivalent to the Schr\u00f6dinger equation for an integrable Heisenberg spin model\u00a0[11,12]. In QCD the integrability remains only in a small sector of the quasi-partonic operators\u00a0[13]. In the case of N=4 SYM the equations for other sets of operators are also integrable\u00a0[14\u201316]. Evolution equations for quasi-partonic operators are written in an explicitly super-conformal form in Ref.\u00a0[17].\n", "keywords": "anomalous dimension matrices for twist-2 operators\nanomalous dimension matrix\nanomalous dimensions obtained from universal anomalous dimensions \u03b3uni(j) by shifting its arguments by an integer number\nCalculations in the maximally extended N=4 SYM, where the coupling constant is not renormalized\nEvolution equations for quasi-partonic operators\nevolution equations for the matrix elements of quasi-partonic operators in the multicolor limit Nc\u2192\u221e\nfixed by the superconformal invariance\u00a0\nhigh symmetry may significantly simplify the structure of these equations\ninvestigate the BFKL and DGLAP equations in the case of supersymmetric theories\nleading logarithmic approximation\nLLA\nN=4 SYM\nsuper-conformal invariance\ntwist-2 operators\n"}, {"id": "S0370269304008858", "text": "The reason to investigate the BFKL and DGLAP equations in the case of supersymmetric theories is based on a common belief, that the high symmetry may significantly simplify the structure of these equations. Indeed, it was found in the leading logarithmic approximation (LLA)\u00a0[10], that the so-called quasi-partonic operators in N=1 SYM are unified in supermultiplets with anomalous dimensions obtained from universal anomalous dimensions \u03b3uni(j) by shifting its arguments by an integer number. Further, the anomalous dimension matrices for twist-2 operators are fixed by the superconformal invariance\u00a0[10]. Calculations in the maximally extended N=4 SYM, where the coupling constant is not renormalized, give even more remarkable results. Namely, it turns out, that here all twist-2 operators enter in the same multiplet, their anomalous dimension matrix is fixed completely by the super-conformal invariance and its universal anomalous dimension in LLA is proportional to \u03a8(j\u22121)\u2212\u03a8(1), which means, that the evolution equations for the matrix elements of quasi-partonic operators in the multicolor limit Nc\u2192\u221e are equivalent to the Schr\u00f6dinger equation for an integrable Heisenberg spin model\u00a0[11,12]. In QCD the integrability remains only in a small sector of the quasi-partonic operators\u00a0[13]. In the case of N=4 SYM the equations for other sets of operators are also integrable\u00a0[14\u201316]. Evolution equations for quasi-partonic operators are written in an explicitly super-conformal form in Ref.\u00a0[17].\n", "keywords": "anomalous dimension matrices for twist-2 operators\nanomalous dimension matrix\nanomalous dimensions obtained from universal anomalous dimensions \u03b3uni(j) by shifting its arguments by an integer number\nCalculations in the maximally extended N=4 SYM, where the coupling constant is not renormalized\nEvolution equations for quasi-partonic operators\nevolution equations for the matrix elements of quasi-partonic operators in the multicolor limit Nc\u2192\u221e\nfixed by the superconformal invariance\u00a0\nhigh symmetry may significantly simplify the structure of these equations\ninvestigate the BFKL and DGLAP equations in the case of supersymmetric theories\nleading logarithmic approximation\nLLA\nN=4 SYM\nsuper-conformal invariance\ntwist-2 operators\n"}, {"id": "S0370269304008858", "text": "The reason to investigate the BFKL and DGLAP equations in the case of supersymmetric theories is based on a common belief, that the high symmetry may significantly simplify the structure of these equations. Indeed, it was found in the leading logarithmic approximation (LLA)\u00a0[10], that the so-called quasi-partonic operators in N=1 SYM are unified in supermultiplets with anomalous dimensions obtained from universal anomalous dimensions \u03b3uni(j) by shifting its arguments by an integer number. Further, the anomalous dimension matrices for twist-2 operators are fixed by the superconformal invariance\u00a0[10]. Calculations in the maximally extended N=4 SYM, where the coupling constant is not renormalized, give even more remarkable results. Namely, it turns out, that here all twist-2 operators enter in the same multiplet, their anomalous dimension matrix is fixed completely by the super-conformal invariance and its universal anomalous dimension in LLA is proportional to \u03a8(j\u22121)\u2212\u03a8(1), which means, that the evolution equations for the matrix elements of quasi-partonic operators in the multicolor limit Nc\u2192\u221e are equivalent to the Schr\u00f6dinger equation for an integrable Heisenberg spin model\u00a0[11,12]. In QCD the integrability remains only in a small sector of the quasi-partonic operators\u00a0[13]. In the case of N=4 SYM the equations for other sets of operators are also integrable\u00a0[14\u201316]. Evolution equations for quasi-partonic operators are written in an explicitly super-conformal form in Ref.\u00a0[17].\n", "keywords": "anomalous dimension matrices for twist-2 operators\nanomalous dimension matrix\nanomalous dimensions obtained from universal anomalous dimensions \u03b3uni(j) by shifting its arguments by an integer number\nCalculations in the maximally extended N=4 SYM, where the coupling constant is not renormalized\nEvolution equations for quasi-partonic operators\nevolution equations for the matrix elements of quasi-partonic operators in the multicolor limit Nc\u2192\u221e\nfixed by the superconformal invariance\u00a0\nhigh symmetry may significantly simplify the structure of these equations\ninvestigate the BFKL and DGLAP equations in the case of supersymmetric theories\nleading logarithmic approximation\nLLA\nN=4 SYM\nsuper-conformal invariance\ntwist-2 operators\n"}, {"id": "S0370269304008858", "text": "The reason to investigate the BFKL and DGLAP equations in the case of supersymmetric theories is based on a common belief, that the high symmetry may significantly simplify the structure of these equations. Indeed, it was found in the leading logarithmic approximation (LLA)\u00a0[10], that the so-called quasi-partonic operators in N=1 SYM are unified in supermultiplets with anomalous dimensions obtained from universal anomalous dimensions \u03b3uni(j) by shifting its arguments by an integer number. Further, the anomalous dimension matrices for twist-2 operators are fixed by the superconformal invariance\u00a0[10]. Calculations in the maximally extended N=4 SYM, where the coupling constant is not renormalized, give even more remarkable results. Namely, it turns out, that here all twist-2 operators enter in the same multiplet, their anomalous dimension matrix is fixed completely by the super-conformal invariance and its universal anomalous dimension in LLA is proportional to \u03a8(j\u22121)\u2212\u03a8(1), which means, that the evolution equations for the matrix elements of quasi-partonic operators in the multicolor limit Nc\u2192\u221e are equivalent to the Schr\u00f6dinger equation for an integrable Heisenberg spin model\u00a0[11,12]. In QCD the integrability remains only in a small sector of the quasi-partonic operators\u00a0[13]. In the case of N=4 SYM the equations for other sets of operators are also integrable\u00a0[14\u201316]. Evolution equations for quasi-partonic operators are written in an explicitly super-conformal form in Ref.\u00a0[17].\n", "keywords": "anomalous dimension matrices for twist-2 operators\nanomalous dimension matrix\nanomalous dimensions obtained from universal anomalous dimensions \u03b3uni(j) by shifting its arguments by an integer number\nCalculations in the maximally extended N=4 SYM, where the coupling constant is not renormalized\nEvolution equations for quasi-partonic operators\nevolution equations for the matrix elements of quasi-partonic operators in the multicolor limit Nc\u2192\u221e\nfixed by the superconformal invariance\u00a0\nhigh symmetry may significantly simplify the structure of these equations\ninvestigate the BFKL and DGLAP equations in the case of supersymmetric theories\nleading logarithmic approximation\nLLA\nN=4 SYM\nsuper-conformal invariance\ntwist-2 operators\n"}, {"id": "S0370269304008858", "text": "The reason to investigate the BFKL and DGLAP equations in the case of supersymmetric theories is based on a common belief, that the high symmetry may significantly simplify the structure of these equations. Indeed, it was found in the leading logarithmic approximation (LLA)\u00a0[10], that the so-called quasi-partonic operators in N=1 SYM are unified in supermultiplets with anomalous dimensions obtained from universal anomalous dimensions \u03b3uni(j) by shifting its arguments by an integer number. Further, the anomalous dimension matrices for twist-2 operators are fixed by the superconformal invariance\u00a0[10]. Calculations in the maximally extended N=4 SYM, where the coupling constant is not renormalized, give even more remarkable results. Namely, it turns out, that here all twist-2 operators enter in the same multiplet, their anomalous dimension matrix is fixed completely by the super-conformal invariance and its universal anomalous dimension in LLA is proportional to \u03a8(j\u22121)\u2212\u03a8(1), which means, that the evolution equations for the matrix elements of quasi-partonic operators in the multicolor limit Nc\u2192\u221e are equivalent to the Schr\u00f6dinger equation for an integrable Heisenberg spin model\u00a0[11,12]. In QCD the integrability remains only in a small sector of the quasi-partonic operators\u00a0[13]. In the case of N=4 SYM the equations for other sets of operators are also integrable\u00a0[14\u201316]. Evolution equations for quasi-partonic operators are written in an explicitly super-conformal form in Ref.\u00a0[17].\n", "keywords": "anomalous dimension matrices for twist-2 operators\nanomalous dimension matrix\nanomalous dimensions obtained from universal anomalous dimensions \u03b3uni(j) by shifting its arguments by an integer number\nCalculations in the maximally extended N=4 SYM, where the coupling constant is not renormalized\nEvolution equations for quasi-partonic operators\nevolution equations for the matrix elements of quasi-partonic operators in the multicolor limit Nc\u2192\u221e\nfixed by the superconformal invariance\u00a0\nhigh symmetry may significantly simplify the structure of these equations\ninvestigate the BFKL and DGLAP equations in the case of supersymmetric theories\nleading logarithmic approximation\nLLA\nN=4 SYM\nsuper-conformal invariance\ntwist-2 operators\n"}, {"id": "S0370269304008858", "text": "The reason to investigate the BFKL and DGLAP equations in the case of supersymmetric theories is based on a common belief, that the high symmetry may significantly simplify the structure of these equations. Indeed, it was found in the leading logarithmic approximation (LLA)\u00a0[10], that the so-called quasi-partonic operators in N=1 SYM are unified in supermultiplets with anomalous dimensions obtained from universal anomalous dimensions \u03b3uni(j) by shifting its arguments by an integer number. Further, the anomalous dimension matrices for twist-2 operators are fixed by the superconformal invariance\u00a0[10]. Calculations in the maximally extended N=4 SYM, where the coupling constant is not renormalized, give even more remarkable results. Namely, it turns out, that here all twist-2 operators enter in the same multiplet, their anomalous dimension matrix is fixed completely by the super-conformal invariance and its universal anomalous dimension in LLA is proportional to \u03a8(j\u22121)\u2212\u03a8(1), which means, that the evolution equations for the matrix elements of quasi-partonic operators in the multicolor limit Nc\u2192\u221e are equivalent to the Schr\u00f6dinger equation for an integrable Heisenberg spin model\u00a0[11,12]. In QCD the integrability remains only in a small sector of the quasi-partonic operators\u00a0[13]. In the case of N=4 SYM the equations for other sets of operators are also integrable\u00a0[14\u201316]. Evolution equations for quasi-partonic operators are written in an explicitly super-conformal form in Ref.\u00a0[17].\n", "keywords": "anomalous dimension matrices for twist-2 operators\nanomalous dimension matrix\nanomalous dimensions obtained from universal anomalous dimensions \u03b3uni(j) by shifting its arguments by an integer number\nCalculations in the maximally extended N=4 SYM, where the coupling constant is not renormalized\nEvolution equations for quasi-partonic operators\nevolution equations for the matrix elements of quasi-partonic operators in the multicolor limit Nc\u2192\u221e\nfixed by the superconformal invariance\u00a0\nhigh symmetry may significantly simplify the structure of these equations\ninvestigate the BFKL and DGLAP equations in the case of supersymmetric theories\nleading logarithmic approximation\nLLA\nN=4 SYM\nsuper-conformal invariance\ntwist-2 operators\n"}, {"id": "S0370269304008858", "text": "The reason to investigate the BFKL and DGLAP equations in the case of supersymmetric theories is based on a common belief, that the high symmetry may significantly simplify the structure of these equations. Indeed, it was found in the leading logarithmic approximation (LLA)\u00a0[10], that the so-called quasi-partonic operators in N=1 SYM are unified in supermultiplets with anomalous dimensions obtained from universal anomalous dimensions \u03b3uni(j) by shifting its arguments by an integer number. Further, the anomalous dimension matrices for twist-2 operators are fixed by the superconformal invariance\u00a0[10]. Calculations in the maximally extended N=4 SYM, where the coupling constant is not renormalized, give even more remarkable results. Namely, it turns out, that here all twist-2 operators enter in the same multiplet, their anomalous dimension matrix is fixed completely by the super-conformal invariance and its universal anomalous dimension in LLA is proportional to \u03a8(j\u22121)\u2212\u03a8(1), which means, that the evolution equations for the matrix elements of quasi-partonic operators in the multicolor limit Nc\u2192\u221e are equivalent to the Schr\u00f6dinger equation for an integrable Heisenberg spin model\u00a0[11,12]. In QCD the integrability remains only in a small sector of the quasi-partonic operators\u00a0[13]. In the case of N=4 SYM the equations for other sets of operators are also integrable\u00a0[14\u201316]. Evolution equations for quasi-partonic operators are written in an explicitly super-conformal form in Ref.\u00a0[17].\n", "keywords": "anomalous dimension matrices for twist-2 operators\nanomalous dimension matrix\nanomalous dimensions obtained from universal anomalous dimensions \u03b3uni(j) by shifting its arguments by an integer number\nCalculations in the maximally extended N=4 SYM, where the coupling constant is not renormalized\nEvolution equations for quasi-partonic operators\nevolution equations for the matrix elements of quasi-partonic operators in the multicolor limit Nc\u2192\u221e\nfixed by the superconformal invariance\u00a0\nhigh symmetry may significantly simplify the structure of these equations\ninvestigate the BFKL and DGLAP equations in the case of supersymmetric theories\nleading logarithmic approximation\nLLA\nN=4 SYM\nsuper-conformal invariance\ntwist-2 operators\n"}, {"id": "S0370269304008858", "text": "The reason to investigate the BFKL and DGLAP equations in the case of supersymmetric theories is based on a common belief, that the high symmetry may significantly simplify the structure of these equations. Indeed, it was found in the leading logarithmic approximation (LLA)\u00a0[10], that the so-called quasi-partonic operators in N=1 SYM are unified in supermultiplets with anomalous dimensions obtained from universal anomalous dimensions \u03b3uni(j) by shifting its arguments by an integer number. Further, the anomalous dimension matrices for twist-2 operators are fixed by the superconformal invariance\u00a0[10]. Calculations in the maximally extended N=4 SYM, where the coupling constant is not renormalized, give even more remarkable results. Namely, it turns out, that here all twist-2 operators enter in the same multiplet, their anomalous dimension matrix is fixed completely by the super-conformal invariance and its universal anomalous dimension in LLA is proportional to \u03a8(j\u22121)\u2212\u03a8(1), which means, that the evolution equations for the matrix elements of quasi-partonic operators in the multicolor limit Nc\u2192\u221e are equivalent to the Schr\u00f6dinger equation for an integrable Heisenberg spin model\u00a0[11,12]. In QCD the integrability remains only in a small sector of the quasi-partonic operators\u00a0[13]. In the case of N=4 SYM the equations for other sets of operators are also integrable\u00a0[14\u201316]. Evolution equations for quasi-partonic operators are written in an explicitly super-conformal form in Ref.\u00a0[17].\n", "keywords": "anomalous dimension matrices for twist-2 operators\nanomalous dimension matrix\nanomalous dimensions obtained from universal anomalous dimensions \u03b3uni(j) by shifting its arguments by an integer number\nCalculations in the maximally extended N=4 SYM, where the coupling constant is not renormalized\nEvolution equations for quasi-partonic operators\nevolution equations for the matrix elements of quasi-partonic operators in the multicolor limit Nc\u2192\u221e\nfixed by the superconformal invariance\u00a0\nhigh symmetry may significantly simplify the structure of these equations\ninvestigate the BFKL and DGLAP equations in the case of supersymmetric theories\nleading logarithmic approximation\nLLA\nN=4 SYM\nsuper-conformal invariance\ntwist-2 operators\n"}, {"id": "S0393044012000198", "text": "RemarkThe purely radiative spacetimes used as reference solutions in our analysis are not perturbations of the Minkowski spacetime. A way of seeing this is to consider the Newman\u2013Penrose constants of the spacetime. The Newman\u2013Penrose constants are a set of absolutely conserved quantities defined as integrals of certain components of the Weyl tensor and the Maxwell fields over cuts of null infinity\u2014see\u00a0[19\u201321] for the Einstein\u2013Maxwell case. In\u00a0[22] it has been shown that the value of the Newman\u2013Penrose constants for a vacuum radiative spacetime coincides with the value of the rescaled Weyl spinor at i+\u2014this result can be extended to the electrovacuum case using the methods of this article. For the radiative spacetimes arising from the construction of\u00a0[17] it can be seen that the value of the Weyl spinor at i+ is essentially the mass quadrupole of the seed static spacetime. It follows, that the Newman\u2013Penrose constants of the radiative spacetime can take arbitrary values. On the other hand, for the Minkowski spacetime, the Newman\u2013Penrose constants are exactly zero, and those of perturbations thereof will be small. Thus, in this precise sense, our radiative spacetimes are, generically, not perturbations of the Minkowski spacetime, unless all the Newman\u2013Penrose constants vanish.\n", "keywords": "Maxwell fields\nMinkowski spacetime\nNewman\u2013Penrose constants\nradiative spacetime\nradiative spacetimes\nreference solutions\nvacuum radiative spacetime\nWeyl tensor\n"}, {"id": "S0022311515300295", "text": "Zirconium alloys are commonly used as the fuel cladding for water cooled nuclear fission reactors, mainly due to their low neutron cross-section, good corrosion resistance during normal operating conditions and sufficient mechanical strength [1]. Despite high corrosion resistance at normal operating temperatures (around 300\u00a0\u00b0C) [2], Zr alloys oxidise very rapidly when exposed to temperatures a few hundred degrees higher. This is an exothermic reaction, which can further accelerate oxidation and, at temperatures beyond 1000\u00a0\u00b0C, potentially lead to disintegration of the fuel rods, as highlighted during the Fukushima Daiichi nuclear accident. For this reason new research activities have been initiated worldwide to develop accident tolerant fuels (ATF). Additionally, ATFs could also provide further enhancements in corrosion performance during normal operating conditions enabling the development of fuel assemblies for very high burn-up.\n", "keywords": "accident tolerant fuels\nATF\ndevelop accident tolerant fuels\ndevelopment of fuel assemblies\ndisintegration of the fuel rods\nenhancements in corrosion performance\nexothermic reaction\nfuel assemblies\nfuel cladding\nfuel rods\noxidation\noxidise\nwater cooled nuclear fission reactors\nZirconium alloys\nZr alloys\n"}, {"id": "S0022311515300295", "text": "Zirconium alloys are commonly used as the fuel cladding for water cooled nuclear fission reactors, mainly due to their low neutron cross-section, good corrosion resistance during normal operating conditions and sufficient mechanical strength [1]. Despite high corrosion resistance at normal operating temperatures (around 300\u00a0\u00b0C) [2], Zr alloys oxidise very rapidly when exposed to temperatures a few hundred degrees higher. This is an exothermic reaction, which can further accelerate oxidation and, at temperatures beyond 1000\u00a0\u00b0C, potentially lead to disintegration of the fuel rods, as highlighted during the Fukushima Daiichi nuclear accident. For this reason new research activities have been initiated worldwide to develop accident tolerant fuels (ATF). Additionally, ATFs could also provide further enhancements in corrosion performance during normal operating conditions enabling the development of fuel assemblies for very high burn-up.\n", "keywords": "accident tolerant fuels\nATF\ndevelop accident tolerant fuels\ndevelopment of fuel assemblies\ndisintegration of the fuel rods\nenhancements in corrosion performance\nexothermic reaction\nfuel assemblies\nfuel cladding\nfuel rods\noxidation\noxidise\nwater cooled nuclear fission reactors\nZirconium alloys\nZr alloys\n"}, {"id": "S0022311514007119", "text": "In the calculations for the formation energy, the box size is set to 30a0\u00d730a0\u00d730a0, where a0 is the bcc Fe lattice parameter. For all calculations periodic boundary conditions and constant volume are used. The Monte Carlo algorithm used to determine the lowest energy configuration of the cluster [28] is organised as follows. First, the energetics of voids without helium are investigated. A vacancy is introduced into the simulation cell and the system is minimised using a conjugate gradient algorithm, yielding a single vacancy formation energy Evac of 1.72eV. Next, the atom with the highest potential energy is removed from the system and again the system is minimised. This scheme is iteratively continued to create voids up to the number of target vacancies and the formation energy of each is calculated. Next, helium atoms are introduced to the vacancies. The total system energy is measured and recorded. At this point, a Metropolis MC scheme [29] is used to find the low energy configurations. Every helium in the system is randomly displaced from its site up to a maximum of rmax (4.5\u00c5, the cut off distance for He\u2013He interactions) in each of the x, y and z directions and then minimised using the conjugate gradient algorithm. Each bubble is continued for a minimum of 10,000 steps. After that, the searches will be terminated if the system energy does not drop within a further 10 steps. A schematic of this iterative process is shown in Fig. 1.\n", "keywords": "box\nconjugate gradient algorithm\ndetermine the lowest energy configuration\nEvac\nFe lattice\nformation energy\nHe\nHe\u2013He interactions\nhelium\nhelium atoms\niterative process\nlow energy configurations\nMetropolis MC scheme\nminimised\nMonte Carlo algorithm\nsimulation cell\nsingle vacancy formation energy\ntotal system energy\nvoids without helium\n"}, {"id": "S0022311514007119", "text": "In the calculations for the formation energy, the box size is set to 30a0\u00d730a0\u00d730a0, where a0 is the bcc Fe lattice parameter. For all calculations periodic boundary conditions and constant volume are used. The Monte Carlo algorithm used to determine the lowest energy configuration of the cluster [28] is organised as follows. First, the energetics of voids without helium are investigated. A vacancy is introduced into the simulation cell and the system is minimised using a conjugate gradient algorithm, yielding a single vacancy formation energy Evac of 1.72eV. Next, the atom with the highest potential energy is removed from the system and again the system is minimised. This scheme is iteratively continued to create voids up to the number of target vacancies and the formation energy of each is calculated. Next, helium atoms are introduced to the vacancies. The total system energy is measured and recorded. At this point, a Metropolis MC scheme [29] is used to find the low energy configurations. Every helium in the system is randomly displaced from its site up to a maximum of rmax (4.5\u00c5, the cut off distance for He\u2013He interactions) in each of the x, y and z directions and then minimised using the conjugate gradient algorithm. Each bubble is continued for a minimum of 10,000 steps. After that, the searches will be terminated if the system energy does not drop within a further 10 steps. A schematic of this iterative process is shown in Fig. 1.\n", "keywords": "box\nconjugate gradient algorithm\ndetermine the lowest energy configuration\nEvac\nFe lattice\nformation energy\nHe\nHe\u2013He interactions\nhelium\nhelium atoms\niterative process\nlow energy configurations\nMetropolis MC scheme\nminimised\nMonte Carlo algorithm\nsimulation cell\nsingle vacancy formation energy\ntotal system energy\nvoids without helium\n"}, {"id": "S0165168416300603", "text": "Despite the fact that SRC-HE reduces the number of FEs, audio measurements extraction based on SRC would still be not suitable for real-time applications [39]. The previous SRC-HE module is then replaced by the generalised cross correlation phase transform (GCC-PHAT) introduced in Section 2.1, as this does not involve cumbersome point function estimations. The drawback is that the basic GCC algorithm can only detect one source at a time and it is known to be sensitive to room reverberations [5], however it is still effective under moderate reverberant environments (T60\u22480.3s) [40]. For these reasons, at first experiments where only a speaker is active at any given time are carried out, as it often happens in a polite conversation between two or more people. Speech segments using a voice activity detector (VAD) [41] are further extracted and processed using a GCC-PHAT step, for the signal to be more robust to reverberations. Thus, the measure vector obtained za (see Section 2.1) can now be rewritten as za={\u03c4m(t)}, where each component \u03c4m is the TDOA collected at the m-th microphone pair at each time step t. Since TDOAs are not linear in the speaker position, they must be input into an extended Kalman filter (EKF), as in [10] to get an audio position estimation.\n", "keywords": "audio position estimation\nEKF\nGCC algorithm\nGCC-PHAT\nGCC-PHAT step\ngeneralised cross correlation phase transform\nKalman filter\nm-th microphone pair\nspeaker\nSpeech segments\nSRC-HE\nVAD\nvoice activity detector\n"}, {"id": "S0168365913009036", "text": "Two methods of formulating anionic nanocomplexes were evaluated. In both, nanocomplexes were prepared in water at a range of molar charge ratios of L to D while the peptide P to D molar charge ratio was maintained constant at 3:1. Method 1 (L:D:P): DNA was first added to an anionic liposome (LA, LAP1 or LAP2) and incubated for 15min at room temperature and then the peptide was added with rapid mixing and incubated at room temperature for a further 20min; Method 2 (P:D:L): the peptide was added to the DNA and incubated for 15min at room temperature and then liposome was added with rapid mixing and incubated at room temperature for a further 20min. Irrespective of the method of order of mixing, all molar charge ratios in this study refer to L:P:D. Cationic formulations LPD and LCPRGPD were prepared in the order L:P:D as described previously; first, the peptide was added to the liposome DOTMA/DOPE or LCPRG followed by addition of the DNA with rapid mixing and incubated for 30min at room temperature to allow for complex formation [30]. The nanocomplexes prepared were termed LPD (liposome DOTMA/DOPE), LADP and PDLA (liposome LA), PDLAP1 (liposome LAP1), PDLAP2 (liposome LAP2), PDLAPRG (liposome LAPRG) and LCPRGPD (liposome LCPRG).\n", "keywords": "added\nan anionic liposome\nDNA\nDOPE\nDOTMA\nformulating anionic nanocomplexes\nincubated\nLADP\nLA, LAP1 or LAP2\nLCPRG\nLCPRGPD\nliposome\nliposome DOTMA/DOPE\nliposome LA\nliposome LAP1\nliposome LAP2\nliposome LAPRG\nliposome LCPRG\nLPD\nnanocomplexes\nPDLA\nPDLAP1\nPDLAP2\nPDLAPRG\npeptide\nprepared\nrapid mixing and incubated\n"}, {"id": "S1574119215000796", "text": "The proposed multihop routing protocol, PHASeR, applies the technique of blind forwarding in a MWSN, which increases the reliability of data delivery through its inherent use of multiple routes. This approach requires a gradient metric to be continuously maintained, which is problematic in a dynamic topology. The literature commonly uses either flooding or location awareness, however flooding creates large amounts of overhead and location determination schemes can often be inaccurate, power hungry and create the issue of the dead end problem. PHASeR uses a novel method of gradient maintenance in a mobile network, which requires the proactive sharing of only local topology information. This is facilitated by a global TDMA (time division multiple access) MAC (medium access control) layer and further reduces the amount of overhead, which in turn will decrease packet latency. PHASeR is also set apart by its use of encapsulation, which allows data from multiple nodes to be transmitted in the same packet in order to handle high volumes of traffic. It utilises node cooperation to create a robust multipath routing solution. As such, the contribution of this paper is a cross-layer routing protocol for MWSNs that can handle the constant flow of data from sensors in highly mobile situations.\n", "keywords": "cross-layer routing protocol\ncross-layer routing protocol for MWSNs\nencapsulation\ngradient maintenance\ngradient metric\nMAC\nmedium access control\nmobile network\nmultihop routing protocol\nMWSN\nMWSNs\nnode\nnode cooperation\nnodes\npacket latency\nPHASeR\nsensors\nTDMA\ntime division multiple access\n"}, {"id": "S0370269304009165", "text": "There are many possible applications for this mechanism. In this Letter, we have concentrated on its contribution to leptogenesis and baryogenesis. Our calculation is applicable in the phase when the fields are rolling. This rolling phase will start when the Hubble constant drops to a value comparable to the mass of the scalar fields. It is at this time in the cosmological evolution that CP violation is most efficient. After the fields have relaxed to their vacuum values, our CP violation mechanism turns off. We plan to discuss more details, in particular applications to concrete baryogenesis models, in a future publication [20]. Note that string cosmology and brane world scenarios may provide natural settings for the origin of the scalar fields required for our mechanism (e.g. see Ref.\u00a0[30] for a recent paper on how scalar fields from brane world scenarios can play a new role in spontaneous baryogenesis).\n", "keywords": "applications for this mechanism\napplications to concrete baryogenesis models\nbrane\nbrane world scenarios\nconcrete baryogenesis models\ncosmological evolution\nCP violation\nCP violation mechanism\nfields have relaxed to their vacuum values\nhow scalar fields from brane world scenarios can play a new role in spontaneous baryogenesis\nHubble constant drops to a value comparable to the mass of the scalar fields\nin the phase when the fields are rolling\nits contribution to leptogenesis and baryogenesis\nour mechanism\nrolling phase\nscalar fields\nspontaneous baryogenesis\nstring cosmology\n"}, {"id": "S0370269304009165", "text": "There are many possible applications for this mechanism. In this Letter, we have concentrated on its contribution to leptogenesis and baryogenesis. Our calculation is applicable in the phase when the fields are rolling. This rolling phase will start when the Hubble constant drops to a value comparable to the mass of the scalar fields. It is at this time in the cosmological evolution that CP violation is most efficient. After the fields have relaxed to their vacuum values, our CP violation mechanism turns off. We plan to discuss more details, in particular applications to concrete baryogenesis models, in a future publication [20]. Note that string cosmology and brane world scenarios may provide natural settings for the origin of the scalar fields required for our mechanism (e.g. see Ref.\u00a0[30] for a recent paper on how scalar fields from brane world scenarios can play a new role in spontaneous baryogenesis).\n", "keywords": "applications for this mechanism\napplications to concrete baryogenesis models\nbrane\nbrane world scenarios\nconcrete baryogenesis models\ncosmological evolution\nCP violation\nCP violation mechanism\nfields have relaxed to their vacuum values\nhow scalar fields from brane world scenarios can play a new role in spontaneous baryogenesis\nHubble constant drops to a value comparable to the mass of the scalar fields\nin the phase when the fields are rolling\nits contribution to leptogenesis and baryogenesis\nour mechanism\nrolling phase\nscalar fields\nspontaneous baryogenesis\nstring cosmology\n"}, {"id": "S003238610900086X", "text": "We deal with the intensity scattered by a random mixture of deuterated/hydrogenated PE chains. The algorithm used by us to evaluate the Kratky plots by sets of parallel polymer stems is very simplified. We checked it to be adequate in the reciprocal coordinate range under investigation [0<q(=4\u03c0sin\u03b8/\u03bb)\u22640.25\u00c5\u22121] comparing the results with more precise calculations. The scattering centres are identified with pseudo-atoms repeating after a constant distance of 1.27\u00c5 along straight lines coinciding with the stem axes, 100 scattering centres being placed on each stem; the scattering by atoms belonging to chain folds is neglected. The parallel stem axes are disposed according to a hexagonal setting \u2013 a rough approximation to the monoclinic, pseudo-hexagonal structure of PE \u2013 and the scattering centres have the same axial coordinates in all the stems. Defining an integer i going from 1 to the total number ns\u00b7100 of scattering centres, we have (q<1) [9](1A)q2\u00b7I(q)=C\u00b7(bH\u2212bD)2\u2211i=1ns\u00b7100\u2211j=1ns\u00b71004\u03c0qsin(q\u00b7dij)dij;dij2=\u0394ij2+(zj\u2212zi)2;q=2\u03c0sin\u03b8\u03bbwhere bH, bD respectively are the scattering lengths of hydrogen and deuterium, dij is the distance between C atoms, 2\u03b8 is the diffraction angle and \u03bb the wavelength. The i-th C atom coordinate along the stem axis is zi and \u0394ij is the distance between the stem axes where the atoms i and j belong. For all the stems we have the same set of zi coordinates. The sum in Eq. (1A) is extended to all the stems of the crystalline domain, see Figs. 2 and 10 for examples.\n", "keywords": "atoms\ncalculations\nC atoms\nchain folds\ncrystalline\ndeuterated\ndeuterium\nhydrogen\nhydrogenated\nKratky plots\nparallel polymer stems\nparallel stem axes\nPE\nPE chains\npseudo-atoms\nscattering\nscattering centres\nstem\nstem axes\nstems\n"}, {"id": "S003238610900086X", "text": "We deal with the intensity scattered by a random mixture of deuterated/hydrogenated PE chains. The algorithm used by us to evaluate the Kratky plots by sets of parallel polymer stems is very simplified. We checked it to be adequate in the reciprocal coordinate range under investigation [0<q(=4\u03c0sin\u03b8/\u03bb)\u22640.25\u00c5\u22121] comparing the results with more precise calculations. The scattering centres are identified with pseudo-atoms repeating after a constant distance of 1.27\u00c5 along straight lines coinciding with the stem axes, 100 scattering centres being placed on each stem; the scattering by atoms belonging to chain folds is neglected. The parallel stem axes are disposed according to a hexagonal setting \u2013 a rough approximation to the monoclinic, pseudo-hexagonal structure of PE \u2013 and the scattering centres have the same axial coordinates in all the stems. Defining an integer i going from 1 to the total number ns\u00b7100 of scattering centres, we have (q<1) [9](1A)q2\u00b7I(q)=C\u00b7(bH\u2212bD)2\u2211i=1ns\u00b7100\u2211j=1ns\u00b71004\u03c0qsin(q\u00b7dij)dij;dij2=\u0394ij2+(zj\u2212zi)2;q=2\u03c0sin\u03b8\u03bbwhere bH, bD respectively are the scattering lengths of hydrogen and deuterium, dij is the distance between C atoms, 2\u03b8 is the diffraction angle and \u03bb the wavelength. The i-th C atom coordinate along the stem axis is zi and \u0394ij is the distance between the stem axes where the atoms i and j belong. For all the stems we have the same set of zi coordinates. The sum in Eq. (1A) is extended to all the stems of the crystalline domain, see Figs. 2 and 10 for examples.\n", "keywords": "atoms\ncalculations\nC atoms\nchain folds\ncrystalline\ndeuterated\ndeuterium\nhydrogen\nhydrogenated\nKratky plots\nparallel polymer stems\nparallel stem axes\nPE\nPE chains\npseudo-atoms\nscattering\nscattering centres\nstem\nstem axes\nstems\n"}, {"id": "S0263822312000657", "text": "Functionally Graded Materials (FGMs), described in detail by Suresh and Mortensen [1], are a type of heterogeneous composite materials exhibiting gradual variation in volume fraction of their constituents from one surface of the material to the other, resulting in properties which vary continuously across the material. The idea of a Functionally Graded Material is not a new one, there are in fact many natural materials which exhibit this property. Study of bone, shell, balsawood and bamboo shows that they are all graded with their greatest strength on the outside, in areas where the greatest protection is required. However it was not until the 1980s in Japan [2] that the idea of a Functionally Graded Material was actively researched in order to gain advances in heat resistant materials for use in aerospace and nuclear fission reactors.\n", "keywords": "aerospace\nbalsawood\nbamboo\nbone\nFGMs\nFunctionally Graded Material\nFunctionally Graded Materials\nheat resistant materials\nheterogeneous composite materials\nnuclear fission reactors\nshell\nStudy of bone, shell, balsawood and bamboo\n"}, {"id": "S221267161200217X", "text": "The existing GO methodology algorithm is theoretical, and hard to solve with computer. In this paper, we research a new method to get the reliability of system based on GO methodology. According to some properties of the operators in GO chart, GO chart can be transformed into series structure, then the minimal path sets are induced based on Enumeration method from first operator to last one. It is very convenient for computer to calculate the system reliability with the new method based on minimal path sets. The case study indicates the method is suitable for practical engineering, which can be used to possess the quantitative analysis of complex GO methodology models.", "keywords": "calculate the system reliability\ncomplex GO methodology models\nEnumeration method\nGO chart\nGO methodology\nGO methodology algorithm\nminimal path sets\nminimal path sets are induced\nnew method to get the reliability of system based on GO methodology\npractical engineering\nquantitative analysis\nseries structure\n"}, {"id": "S2212667814001464", "text": "Recently, a network virtualization technology has attracted considerable attention as one of new generation network technologies. In this paper, in order to permit the rapid changing for a topology of a virtual network, we propose a new virtual network construction method based on the shortest path betweenness. In our proposed method, at first, a service provider receives a user's request for the reconfiguration of the constructed virtual network. In this case, the service provider reconfigures the topology of the constructed virtual network rapidly based on shortest path betweenness. We evaluate the performance of our proposed method with simulation, and we show the effectiveness of our proposed method.", "keywords": "constructed virtual network\nnetwork virtualization technology\nnew generation network technologies\nproposed method\nrapid changing for a topology of a virtual network\nreconfiguration of the constructed virtual network\nreconfigures the topology of the constructed virtual network\nservice provider\nshortest path betweenness\nsimulation\nvirtual network\nvirtual network construction method\n"}, {"id": "S0301932215002037", "text": "There is also a lack of agreement as to what constitutes churn flow. It is fairly certainly a gas continuous flow. There is growing agreement that there are huge waves present and some of the liquid is carried as drops. Sekoguchi and Mori (1997) and Sawai et\u00a0al. (2004) using measurements from their multiple probes (92 over an axial length of 2.325\u00a0m) obtained time/axial position/void fraction information. From this they were able to identify huge wave from amongst disturbance waves and slugs. They classified individual structures as huge waves from their size together with the fact that their velocities depended significantly on the corresponding axial length. This was in contrast to disturbance waves where the velocity of individual waves only increased slightly with the axial extent of these waves. They also found that the frequency of huge waves first increased and then decrease with increasing gas superficial velocity. Similarly, their velocities were found to deviate from the line for slug flow velocities and pass through a maximum and then a minimum.\n", "keywords": "churn\nchurn flow\ndisturbance waves\nfrequency of huge waves\ngas\ngas continuous flow\ngas superficial velocity\nhuge wave\nhuge waves\nidentify huge wave\nliquid\nprobes\nslug\nslug flow velocities\nslugs\ntime/axial position/void fraction information\nvelocity of individual waves\nwaves\n"}, {"id": "S037026930301801X", "text": "The measurements presented here provide evidence for the existence of di-cluster structures in 10\u201312,14Be. Certainly, if the breakup process samples the overlap between the wavefunctions of the ground state and the excited states, the first-chance cluster breakup cross-sections, shown in Fig.\u00a04(a), indicate that the xHe+A\u2212xHe cluster structure does not decrease over the mass range A=10, 12 and 14. Given also that the decay energy threshold increases with mass number, the present data may even indicate a slight increase in clustering. The breakup cross-sections also appear to demonstrate that these nuclei possess a stronger structural overlap with an \u03b1\u2013Xn\u2013\u03b1 configuration, although the reaction mechanics by which this final state is reached may be complex. That is to say that the dominant structural mode of the neutron rich isotopes may be identified with two alpha-particles plus valence neutrons. These comprehensive measurements of the neutron-removal and cluster breakup for the first time provide experimental data whereby the structure of the most neutron-rich Be isotopes can be modeled via their reactions.\n", "keywords": "alpha-particles\nbreakup cross-sections\nbreakup process\ncluster breakup\ncomprehensive measurements of the neutron-removal and cluster breakup\ndecay energy\ndi-cluster structures\ndominant structural mode\nfirst-chance cluster breakup cross-sections\nneutron-removal\nneutron-rich Be isotopes\nneutron rich isotopes\nnuclei\nreaction mechanics\nvalence neutrons\nwavefunctions of the ground state and the excited states\nxHe+A\u2212xHe cluster structure\n"}, {"id": "S037026930301801X", "text": "The measurements presented here provide evidence for the existence of di-cluster structures in 10\u201312,14Be. Certainly, if the breakup process samples the overlap between the wavefunctions of the ground state and the excited states, the first-chance cluster breakup cross-sections, shown in Fig.\u00a04(a), indicate that the xHe+A\u2212xHe cluster structure does not decrease over the mass range A=10, 12 and 14. Given also that the decay energy threshold increases with mass number, the present data may even indicate a slight increase in clustering. The breakup cross-sections also appear to demonstrate that these nuclei possess a stronger structural overlap with an \u03b1\u2013Xn\u2013\u03b1 configuration, although the reaction mechanics by which this final state is reached may be complex. That is to say that the dominant structural mode of the neutron rich isotopes may be identified with two alpha-particles plus valence neutrons. These comprehensive measurements of the neutron-removal and cluster breakup for the first time provide experimental data whereby the structure of the most neutron-rich Be isotopes can be modeled via their reactions.\n", "keywords": "alpha-particles\nbreakup cross-sections\nbreakup process\ncluster breakup\ncomprehensive measurements of the neutron-removal and cluster breakup\ndecay energy\ndi-cluster structures\ndominant structural mode\nfirst-chance cluster breakup cross-sections\nneutron-removal\nneutron-rich Be isotopes\nneutron rich isotopes\nnuclei\nreaction mechanics\nvalence neutrons\nwavefunctions of the ground state and the excited states\nxHe+A\u2212xHe cluster structure\n"}, {"id": "S2212667814000975", "text": "It has been acknowledged that megalopolises are playing a leading role in the processes of both economic development and culture change. Thereupon, the new emphases on sustainability of transportation system in megalopolis are creating new demands for adequate approach to measure its performance and diagnosis potential drawbacks. By examining the descriptions of sustainable transport system as well as its evaluating approach, a framework with the general applicability and easily accessible data resource for evaluating sustainability of transport system in megalopolis is developed based on nature of regional structure and the feature transport demand in megalopolis. The proposed framework is applied in the analysis and comparison of Jing-Jin-Ji and Yangtze River Delta..", "keywords": "approach to measure its performance and diagnosis potential drawbacks\nculture change\neconomic development\nframework\nproposed framework is applied\nsustainability of transportation system\ntransport system\n"}, {"id": "S2212667814000975", "text": "It has been acknowledged that megalopolises are playing a leading role in the processes of both economic development and culture change. Thereupon, the new emphases on sustainability of transportation system in megalopolis are creating new demands for adequate approach to measure its performance and diagnosis potential drawbacks. By examining the descriptions of sustainable transport system as well as its evaluating approach, a framework with the general applicability and easily accessible data resource for evaluating sustainability of transport system in megalopolis is developed based on nature of regional structure and the feature transport demand in megalopolis. The proposed framework is applied in the analysis and comparison of Jing-Jin-Ji and Yangtze River Delta..", "keywords": "approach to measure its performance and diagnosis potential drawbacks\nculture change\neconomic development\nframework\nproposed framework is applied\nsustainability of transportation system\ntransport system\n"}, {"id": "S2212667814000975", "text": "It has been acknowledged that megalopolises are playing a leading role in the processes of both economic development and culture change. Thereupon, the new emphases on sustainability of transportation system in megalopolis are creating new demands for adequate approach to measure its performance and diagnosis potential drawbacks. By examining the descriptions of sustainable transport system as well as its evaluating approach, a framework with the general applicability and easily accessible data resource for evaluating sustainability of transport system in megalopolis is developed based on nature of regional structure and the feature transport demand in megalopolis. The proposed framework is applied in the analysis and comparison of Jing-Jin-Ji and Yangtze River Delta..", "keywords": "approach to measure its performance and diagnosis potential drawbacks\nculture change\neconomic development\nframework\nproposed framework is applied\nsustainability of transportation system\ntransport system\n"}, {"id": "S2212667814000975", "text": "It has been acknowledged that megalopolises are playing a leading role in the processes of both economic development and culture change. Thereupon, the new emphases on sustainability of transportation system in megalopolis are creating new demands for adequate approach to measure its performance and diagnosis potential drawbacks. By examining the descriptions of sustainable transport system as well as its evaluating approach, a framework with the general applicability and easily accessible data resource for evaluating sustainability of transport system in megalopolis is developed based on nature of regional structure and the feature transport demand in megalopolis. The proposed framework is applied in the analysis and comparison of Jing-Jin-Ji and Yangtze River Delta..", "keywords": "approach to measure its performance and diagnosis potential drawbacks\nculture change\neconomic development\nframework\nproposed framework is applied\nsustainability of transportation system\ntransport system\n"}, {"id": "S1877750313000240", "text": "A few studies within the physiological domain are of special relevance to this work. These include a performance analysis of a blood-flow LB solver using a range of sparse and non-sparse geometries [21] and a performance prediction model for lattice-Boltzmann solvers [22,23]. This performance prediction model can be applied largely to our HemeLB application, although HemeLB uses a different decomposition technique and performs real-time rendering and visualisation tasks during the LB simulations. Mazzeo and Coveney [1] studied the scalability of an earlier version of HemeLB. However, the current performance characteristics of HemeLB are substantially enhanced due to numerous subsequent advances in the code, amongst others: an improved hierarchical, compressed file format; the use of ParMETIS to ensure good load-balance; the coalesced communication patterns to reduce the overhead of rendering; use of compile-time polymorphism to avoid virtual function calls in inner loops.\n", "keywords": "blood-flow LB solver\ncoalesced communication patterns\ncompile-time polymorphism\nHemeLB\nHemeLB application\nlattice-Boltzmann solvers\nLB simulations\nParMETIS\nperformance prediction model\nrendering\n"}, {"id": "S1877750313000240", "text": "A few studies within the physiological domain are of special relevance to this work. These include a performance analysis of a blood-flow LB solver using a range of sparse and non-sparse geometries [21] and a performance prediction model for lattice-Boltzmann solvers [22,23]. This performance prediction model can be applied largely to our HemeLB application, although HemeLB uses a different decomposition technique and performs real-time rendering and visualisation tasks during the LB simulations. Mazzeo and Coveney [1] studied the scalability of an earlier version of HemeLB. However, the current performance characteristics of HemeLB are substantially enhanced due to numerous subsequent advances in the code, amongst others: an improved hierarchical, compressed file format; the use of ParMETIS to ensure good load-balance; the coalesced communication patterns to reduce the overhead of rendering; use of compile-time polymorphism to avoid virtual function calls in inner loops.\n", "keywords": "blood-flow LB solver\ncoalesced communication patterns\ncompile-time polymorphism\nHemeLB\nHemeLB application\nlattice-Boltzmann solvers\nLB simulations\nParMETIS\nperformance prediction model\nrendering\n"}, {"id": "S2212667814000550", "text": "The low-carbon economic development has become the trend and orientation of regional economic development. As the residents of Heilongjiang province, their consumption is the most direct way to achieve the low-carbon lifestyle. Based on the research and discussion of the connotation of low-carbon consumption and its culture, behaviour, preferences and habits, it is concluded that the low-carbon consumption requires us to abide by the life-style of knowledge and culture. Therefore, it is obvious that the development of low-carbon economy is a complex and systematic project, involved the economic development mode, technological innovation mode, consumption values and changes of lifestyle.", "keywords": "carbon\ndevelopment of low-carbon economy\neconomic development\nlow-carbon consumption\nlow-carbon economic development\nlow-carbon economy\nregional economic development\ntechnological innovation\n"}, {"id": "S0370269304006082", "text": "The aim of this note is nothing more than to bring both approaches on equal footing and to relax the assumptions under which the results of [11,13] have been derived using the first approach. More concretely, we generalize the one-loop partition functions, as derived in [11,13] for levels being odd, to the case of even levels. Moreover, on the level of partition functions we implement additional dressings of the world-sheet parity symmetry and identify them with the dressings introduced in [12] in the crosscap state approach. As expected, all the physical information can be read off entirely from the various amplitudes. We will end up with a collection of very explicit and general one-loop partition functions and tadpole cancellation conditions covering simple current extensions of all 168 Gepner models with additional dressings of the parity symmetry. In fact providing a compact collection of the main relevant formulas for constructing supersymmetric Gepner model orientifolds was one of the motivations for writing this Letter. We hope that these expressions turn out to be useful for a systematic search for Standard-like models respectively for providing a statistical ensemble in the spirit of [29].\n", "keywords": "additional dressings of the world-sheet parity symmetry\nbring both approaches on equal footing\nconstructing supersymmetric Gepner model orientifolds\ncrosscap state approach\ncurrent extensions of all 168 Gepner models\ndressings\ngeneralize the one-loop partition functions,\none-loop partition functions\nparity symmetry\nproviding a statistical ensemble in the spirit of [29]\nrelax the assumptions\nsystematic search for Standard-like models\ntadpole cancellation conditions\n"}, {"id": "S0168365912006207", "text": "Unlike conventional materials used in nerve tissue engineering, PAs can be directly injected in vivo into models and spontaneously self-assemble into nanofibers in aqueous solutions. Furthermore, PAs can function as biomimetic materials exemplified by collagen-mimetic PAs [92]. Conventional materials often rely on electrospinning as a manufacturing method to achieve fiber-like structures suitable for use in nerve regeneration. The self-assembly nature of PAs allows them to circumvent costly manufacturing methods. However, in contrast to conventional manufacturing methods like electrospinning where quality and batch-to-batch variability can be tightly controlled, merely relying on self-assembly as a method of large-scale commercial production is still an experimental concept. Perhaps the next step would be to carefully compare and contrast the robustness of self-assemled PAs to electrospun nanofibers. Given that the constituent elements in PAs and external factors like pH can affect its structural assembly, parameters must be finely tuned and optimized in order for PA nanofibers to be used as a full-fledged commercialized medical product [93].\n", "keywords": "achieve fiber-like structures\na full-fledged commercialized medical product\na manufacturing method\nbiomimetic materials\ncollagen-mimetic PAs\ncompare and contrast\nconventional manufacturing methods\ncostly manufacturing methods\nelectrospinning\ninjected in vivo into models and spontaneously self-assemble into nanofibers in aqueous solutions\nlarge-scale commercial production\nPA nanofibers\nPAs\nrobustness of self-assemled PAs to electrospun nanofibers\nself-assembly\ntuned and optimized\n"}, {"id": "S0375960113010839", "text": "The goal of the glued trees (GT) algorithm for quantum search is the following: beginning from the left-most vertex of a given GT graph, traverse the graph and reach the right-most vertex, referred to as the target vertex. Childs et al. [1] use this algorithm to show quantum walk search to be fundamentally more effective than classical random walk search by presenting a class of graphs (the GT graphs) that force classical random walks to make exponentially many queries to an oracle encoding the structure of the graph, but that are traversable by quantum walks with a polynomial number of queries to such an oracle. In order to study the robustness of the algorithm to the detrimental effects of decoherence, we shall determine how effectively it achieves its goal when subjected to an increasing degree of phase damping noise. For this reason, we will focus on the probability that the walker is on the target vertex at the end of the walk. We thus consider GT graphs such as the one illustrated in Fig. 1(b), i.e. consisting of n layers before the gluing stage, and thus labelled as G\u2032n.\n", "keywords": "classical random walks\nclassical random walk search\nclass of graphs\nglued trees\ngluing stage\ngraph\nGT\nGT graph\nGT graphs\nincreasing degree of phase damping noise\nquantum search\nquantum walk search\n"}, {"id": "S0370269304007439", "text": "In contrast to the H particle, the situation for the \u0398+ baryon is very promising. Thus, in this Letter we explore the formation of the \u0398+-baryon within a new approach called parton-based Gribov\u2013Regge theory. It is realized in the Monte Carlo program NEXUS 3.97 [22,23]. In this model high energy hadronic and nuclear collisions are treated within a self-consistent quantum mechanical multiple scattering formalism. Elementary interactions, happening in parallel, correspond to underlying microscopic (predominantly soft) parton cascades and are described effectively as phenomenological soft pomeron exchanges. A pomeron can be seen as layers of a (soft) parton ladder, which is attached to projectile and target nucleons via leg partons. At high energies one accounts also for the contribution of perturbative (high pt) partons described by a so-called \u201csemihard pomeron\u201d\u2014a piece of the QCD parton ladder sandwiched between two soft pomerons which are connected to the projectile and to the target in the usual way. The spectator partons of both projectile and target nucleons, left after pomeron emissions, form nucleon remnants. The legs of the pomerons form color singlets, such as q\u2013q\u0304, q\u2013qq or q\u0304\u2013q\u0304q\u0304. The probability of q\u2013qq and q\u0304\u2013q\u0304q\u0304 is controlled by the parameter\u00a0Pqq and is fixed by the experimental yields on (multi-)strange baryons [23].\n", "keywords": "color singlets\nElementary interactions\nfixed by the experimental yields on (multi-)strange baryons\nhigh energy hadronic and nuclear collisions\nleg partons\nmicroscopic (predominantly soft) parton cascades\nMonte Carlo program\n(multi-)strange baryons\nNEXUS 3.97\nnucleon remnants\nparton-based Gribov\u2013Regge theory\nperturbative (high pt) partons\nphenomenological soft pomeron exchanges\npiece of the QCD parton ladder sandwiched between two soft pomerons which are connected to the projectile and to the target in the usual way\npomeron\npomeron emissions\npomerons\nprojectile and target nucleons\nq\u2013q\u0304\nq\u2013qq\nq\u0304\u2013q\u0304q\u0304\nself-consistent quantum mechanical multiple scattering formalism\n\u201csemihard pomeron\u201d\n(soft) parton ladder\nspectator partons\nthe formation of the \u0398+-baryon\n\u0398+ baryon\n"}, {"id": "S0032386109006612", "text": "In contrast with polymers, which are typically synthesized in the liquid phase, SWNTs are produced through a variety of synthesis techniques that typically involve the reaction of a gaseous carbon feedstock to form the nanotubes on catalyst particles. MWNTs were first observed in arc discharge fullerene reactors [1,26]; this technique was later adapted to produce SWNTs [3]. Similarly, the fullerene production method of laser ablation [27] was adapted to produce SWNTs (\u223c1.4nm diameter) in larger quantities on metal catalyst particles [28\u201330]. A number of chemical vapor deposition (CVD) processes have been developed to grow SWNTs and MWNTs, all involving the reaction of a gaseous carbon compound as feedstock. These processes include fluidized bed [31], \u201ccarpet\u201d growth of carbon nanotubes (CNTs) from catalyst particles embedded in a substrate [32\u201335] as shown in Fig.\u00a03, and \u201ccatalytic gas flow CVD\u201d [36,37]. One of the most effective, cheap, and scalable CVD techniques is the HiPco (high-pressure CO) process, which does not use pre-formed catalyst particles unlike most other CVD techniques [38].\n", "keywords": "carbon nanotubes\n\u201ccarpet\u201d growth of carbon nanotubes (CNTs) from catalyst particles embedded in a substrate\n\u201ccatalytic gas flow CVD\u201d\nchemical vapor deposition\nCNTs\nCVD\nCVD techniques\ndoes not use pre-formed catalyst particles\nfluidized bed\nHiPco (high-pressure CO) process\nmetal catalyst particles\nMWNTs\nobserved in arc discharge fullerene reactors\npolymers\nproduced through a variety of synthesis techniques\nSWNTs\nsynthesized in the liquid phase\nthe fullerene production method of laser ablation\nthe most effective, cheap, and scalable CVD techniques\nthe reaction of a gaseous carbon compound as feedstock\nthe reaction of a gaseous carbon feedstock to form the nanotubes on catalyst particles\nthis technique\n"}, {"id": "S0032386109006612", "text": "In contrast with polymers, which are typically synthesized in the liquid phase, SWNTs are produced through a variety of synthesis techniques that typically involve the reaction of a gaseous carbon feedstock to form the nanotubes on catalyst particles. MWNTs were first observed in arc discharge fullerene reactors [1,26]; this technique was later adapted to produce SWNTs [3]. Similarly, the fullerene production method of laser ablation [27] was adapted to produce SWNTs (\u223c1.4nm diameter) in larger quantities on metal catalyst particles [28\u201330]. A number of chemical vapor deposition (CVD) processes have been developed to grow SWNTs and MWNTs, all involving the reaction of a gaseous carbon compound as feedstock. These processes include fluidized bed [31], \u201ccarpet\u201d growth of carbon nanotubes (CNTs) from catalyst particles embedded in a substrate [32\u201335] as shown in Fig.\u00a03, and \u201ccatalytic gas flow CVD\u201d [36,37]. One of the most effective, cheap, and scalable CVD techniques is the HiPco (high-pressure CO) process, which does not use pre-formed catalyst particles unlike most other CVD techniques [38].\n", "keywords": "carbon nanotubes\n\u201ccarpet\u201d growth of carbon nanotubes (CNTs) from catalyst particles embedded in a substrate\n\u201ccatalytic gas flow CVD\u201d\nchemical vapor deposition\nCNTs\nCVD\nCVD techniques\ndoes not use pre-formed catalyst particles\nfluidized bed\nHiPco (high-pressure CO) process\nmetal catalyst particles\nMWNTs\nobserved in arc discharge fullerene reactors\npolymers\nproduced through a variety of synthesis techniques\nSWNTs\nsynthesized in the liquid phase\nthe fullerene production method of laser ablation\nthe most effective, cheap, and scalable CVD techniques\nthe reaction of a gaseous carbon compound as feedstock\nthe reaction of a gaseous carbon feedstock to form the nanotubes on catalyst particles\nthis technique\n"}, {"id": "S0377025714001931", "text": "Tack is an important property of a PSA as it quantifies its ability to form instantly a bond when brought into contact with a surface. The final adhesion and cohesive strength of the bond are influenced by numerous factors including the surface energies of the adhesive and substrate, dwell time, contact pressure, mechanical properties of the adhesive, as well as environmental conditions such as temperature and humidity [8]. Therefore, tack is important in many applications where an instant bond is required, however it is equally important when a \u2018clean\u2019 separation of the initially bonded surfaces is desirable. Many different methods for measuring the tack have been devised with the four main ones being the rolling ball, loop tack, quick stick and probe tack tests [9]. Each has its own advantages and disadvantages and the specific testing method should be selected based on the particular application.\n", "keywords": "adhesive\nbonded surfaces\n\u2018clean\u2019 separation\nloop tack\nprobe tack\nPSA\nquick stick\nrolling ball\nsubstrate\ntack\nTack\ntests\n"}, {"id": "S2212671612000431", "text": "In this paper, an implementation of a LBP (local binary pattern) based fast face recognition system on symbian platform is presented. First, face in picture taken from camera is detected using AdaBoost algorithm. Second, the pre-processing of the face is done, including eye location, geometric normalization, illumination normalization. During the face preprocessing, a rapid eye location method named ER (Eyeball Search) is proposed and implemented. Last, the improved LBP is adopted for recognition. Although the computational capability of the symbian platform is limited, the experimental results show good performance for recognition rate and time. in pressIn this paper, an implementation of a LBP (local binary pattern) based fast face recognition system on symbian platform is presented. First, face in picture taken from camera is detected using AdaBoost algorithm. Second, the pre-processing of the face is done, including eye location, geometric normalization, illumination normalization. During the face preprocessing, a rapid eye location method named ER (Eyeball Search) is proposed and implemented. Last, the improved LBP is adopted for recognition. Although the computational capability of the symbian platform is limited, the experimental results show good performance for recognition rate and time. in press\n", "keywords": "AdaBoost algorithm\nER\nEyeball Search\neye location\neye location, geometric normalization, illumination normalization\neye location method\nface preprocessing\nface recognition\nface recognition system\ngeometric normalization\nillumination normalization\nLBP\nlocal binary pattern\npreprocessing\npre-processing of the face\nrecognition\nsymbian platform\n"}, {"id": "S0098300413002720", "text": "Artificial Neural Networks (ANN) have been widely used in science and engineering problems. They attempt to model the ability of biological nervous systems to recognize patterns and objects. ANN basic architecture consists of networks of primitive functions capable of receiving multiple weighted inputs that are evaluated in terms of their success at discriminating the classes in \u03a4a. Different types of primitive functions and network configurations result in varying models (Hastie et al., 2009; Rojas, 1996). During training network connection weights are adjusted if the separation of inputs and predefined classes incurs an error. Convergence proceeds until the reduction in error between iterations reaches a decay threshold (Kotsiantis, 2007; Rojas, 1996). We use feed-forward networks with a single hidden layer of nodes, a so called Multi-Layer Perceptron (MLP) (Venables and Ripley, 2002), and select one of two possible parameters: size, the number nodes in the hidden layer.\n", "keywords": "adjusted if the separation of inputs and predefined classes incurs an error\nANN\nANN basic architecture\nArtificial Neural Networks\nConvergence\nDifferent types of primitive functions and network configurations\nfeed-forward networks with a single hidden layer of nodes\nMLP\nmodel the ability of biological nervous systems to recognize patterns and objects\nMulti-Layer Perceptron\nnetworks of primitive functions\nreceiving multiple weighted inputs\nscience and engineering problems\nselect one of two possible parameters\nthe reduction in error between iterations reaches a decay threshold\ntraining network connection\nweights\n"}, {"id": "S136481521630305X", "text": "Using measured data from two arable sites in the UK we have shown that lags can have significant impact on model evaluation and can affect both the level of correlation between measured and simulated data and the magnitude of the sums of the residuals. Also, we used the division of MSE to three constituent statistics (SB, SDSD and LCS) to show how the level of correlation can affect the sum of residuals. By dividing the algorithm-predicted series of lag values into monthly sets and examining the frequency distribution of the lags, certain patterns in these temporally patchy series have been identified. A challenging task in relation to time lags between observed and simulated daily data, is to determine their cause. This task becomes more difficult for model outputs such as soil N2O emissions that are driven by various interacting variables. Even more so, because the measured N2O datasets and the measured datasets of their drivers (e.g. soil moisture, soil N content) cover small time periods, they are not continuous and can vary widely in size. In this study we implemented the algorithm using measured and simulated data for soil moisture (first and second example) and soil mineral N (second example), and compared its results with the respective results for N2O. In our first example, we showed that the estimated lags in N2O prediction are related to the lags in soil moisture prediction in a way that changes gradually through time. In our second example, the lags in N2O prediction were explained by the lags in soil moisture and soil mineral N prediction, with which they had a positive relationship.\n", "keywords": "constituent statistics\nLCS\nmeasured and simulated data for soil moisture\nmodel evaluation\nMSE\nN2O\nN2O datasets\nN2O emissions\nN2O prediction\nSB\nSDSD\nsoil mineral N\nsoil mineral N prediction\nsoil moisture\nsoil moisture prediction\n"}, {"id": "S0021999113005652", "text": "This study proposes a new framework of a numerical modelling of the gas exchange between air and water across their interface, and subsequent chemical reaction in water based on an extended two-compartment model. The major purpose of this study is to provide a fundamental concept for modelling physicochemical processes of the gas exchange, followed by the chemical reaction in water. Demonstrating fundamental data and knowledge on the important environmental transport phenomena, especially the effects of the Schmidt number and the chemical reaction rate on the gas exchange mechanisms across the interface have also been attempted. The gas exchange processes are separated into two physicochemical substeps, the first is the gas\u2013liquid equilibrium between the two phases, and the second is the chemical reaction in the water phase. A first-order, irreversible chemical reaction of the gaseous material after its uptake into the water phase is assumed here to simplify interactions of the chemical reactions and turbulent transport phenomena in water. While a traditional two-compartment model assumes uniform concentration of a material in each compartment, the present two-compartment model uses a computational fluid dynamics (CFD) technique in the water compartment to evaluate temporal development of three-dimensional profiles of the velocity and concentration fields. A direct numerical simulation (DNS) approach is used to evaluate profiles of fluid velocities and concentrations in water, and several important turbulence statistics have been evaluated without using turbulent closures, and subgrid-scale models. We assume that a fluid flow in the water phase is a well-developed turbulent water layer of a low Reynolds number, and the Schmidt number is varied from 1 to 8 to observe the effects of the molecular diffusion of the gas in sub-interface water on the gas exchange rate at the interface. Six degrees of the nondimensional chemical reaction rate are used to find the effect of the chemical reaction rate on the gas exchange mechanisms. Extrapolations of the gas exchange rates and the related transport phenomena toward larger Schmidt number and the faster chemical reaction rate will also be examined to predict the gas exchange processes of the actual gases of Sc\u223cO(102) based on results from the present numerical experiments.\n", "keywords": "air\nCFD\nchemical reaction\nchemical reaction in the water phase\nchemical reaction in water\nchemical reaction rate\nchemical reactions\ncomputational fluid dynamics\nconcentrations in water\ndirect numerical simulation\nDNS\neffect of the chemical reaction rate on the gas exchange mechanisms\neffects of the Schmidt number\nevaluate profiles of fluid velocities and concentrations in water\nevaluate temporal development of three-dimensional profiles of the velocity and concentration fields\nfluid flow in the water phase\nfluid velocities\nframework\nfundamental concept\ngas\ngaseous material\ngases\ngas exchange\ngas exchange mechanisms\ngas exchange mechanisms across the interface\ngas exchange processes\ngas exchange rate at the interface\ngas exchange rates\ngas\u2013liquid equilibrium\nirreversible chemical reaction\nliquid\nmodelling physicochemical processes\nmolecular diffusion\nnew framework of a numerical modelling\nnondimensional chemical reaction rate\nnumerical experiments\nobserve the effects of the molecular diffusion\npredict the gas exchange processes\nSc\u223cO(102)\nsubgrid-scale models\ntransport phenomena\nturbulent transport phenomena in water\ntwo-compartment model\nwater\nwater phase\n"}, {"id": "S0021999113005652", "text": "This study proposes a new framework of a numerical modelling of the gas exchange between air and water across their interface, and subsequent chemical reaction in water based on an extended two-compartment model. The major purpose of this study is to provide a fundamental concept for modelling physicochemical processes of the gas exchange, followed by the chemical reaction in water. Demonstrating fundamental data and knowledge on the important environmental transport phenomena, especially the effects of the Schmidt number and the chemical reaction rate on the gas exchange mechanisms across the interface have also been attempted. The gas exchange processes are separated into two physicochemical substeps, the first is the gas\u2013liquid equilibrium between the two phases, and the second is the chemical reaction in the water phase. A first-order, irreversible chemical reaction of the gaseous material after its uptake into the water phase is assumed here to simplify interactions of the chemical reactions and turbulent transport phenomena in water. While a traditional two-compartment model assumes uniform concentration of a material in each compartment, the present two-compartment model uses a computational fluid dynamics (CFD) technique in the water compartment to evaluate temporal development of three-dimensional profiles of the velocity and concentration fields. A direct numerical simulation (DNS) approach is used to evaluate profiles of fluid velocities and concentrations in water, and several important turbulence statistics have been evaluated without using turbulent closures, and subgrid-scale models. We assume that a fluid flow in the water phase is a well-developed turbulent water layer of a low Reynolds number, and the Schmidt number is varied from 1 to 8 to observe the effects of the molecular diffusion of the gas in sub-interface water on the gas exchange rate at the interface. Six degrees of the nondimensional chemical reaction rate are used to find the effect of the chemical reaction rate on the gas exchange mechanisms. Extrapolations of the gas exchange rates and the related transport phenomena toward larger Schmidt number and the faster chemical reaction rate will also be examined to predict the gas exchange processes of the actual gases of Sc\u223cO(102) based on results from the present numerical experiments.\n", "keywords": "air\nCFD\nchemical reaction\nchemical reaction in the water phase\nchemical reaction in water\nchemical reaction rate\nchemical reactions\ncomputational fluid dynamics\nconcentrations in water\ndirect numerical simulation\nDNS\neffect of the chemical reaction rate on the gas exchange mechanisms\neffects of the Schmidt number\nevaluate profiles of fluid velocities and concentrations in water\nevaluate temporal development of three-dimensional profiles of the velocity and concentration fields\nfluid flow in the water phase\nfluid velocities\nframework\nfundamental concept\ngas\ngaseous material\ngases\ngas exchange\ngas exchange mechanisms\ngas exchange mechanisms across the interface\ngas exchange processes\ngas exchange rate at the interface\ngas exchange rates\ngas\u2013liquid equilibrium\nirreversible chemical reaction\nliquid\nmodelling physicochemical processes\nmolecular diffusion\nnew framework of a numerical modelling\nnondimensional chemical reaction rate\nnumerical experiments\nobserve the effects of the molecular diffusion\npredict the gas exchange processes\nSc\u223cO(102)\nsubgrid-scale models\ntransport phenomena\nturbulent transport phenomena in water\ntwo-compartment model\nwater\nwater phase\n"}, {"id": "S0370269304007798", "text": "States outside the constituent quark model have been hypothesized to exist almost since the introduction of color\u00a0[1\u20134]. Hybrid mesons, qq\u0304 states with an admixture of gluons, and glueballs, states with no quark content, rely on the self interaction property of gluons due to their color charge. Looking for glueballs would be the most obvious way to find evidence for states with constituent gluons; however, the search is hindered by the fact that these states may significantly mix with regular qq\u0304-mesons in the region where the lightest are predicted to occur. As such, they may not be observable as pure states and disentangling the observed spectra may be a very difficult task. Instead, hybrid mesons (qq\u0304gn) may be a better place to search for evidence of resonances outside the constituent quark model, especially since the lightest of theses states are predicted to have exotic quantum numbers of spin, parity, and charge conjugation, JPC, that is, combinations that are unattainable by regular qq\u0304-mesons.\n", "keywords": "color charge\nconstituent quark model\ndisentangling the observed spectra\nfind evidence for states with constituent gluons\nglueballs\ngluons\nhybrid mesons\nHybrid mesons\nJPC\nLooking for glueballs\nqq\u0304gn\nqq\u0304-mesons\nquark\nresonances outside the constituent quark model\n"}, {"id": "S107158191630074X", "text": "An obvious metric to measure the monitoring performance between the different conditions would be to compare how many clicks the users made in average for each condition. Furthermore of interest are the buffer values of the respective buffers at the time of the user's interaction with the simulation (e.g., the input buffer of a certain machine at the time of refilling it). A relatively high average buffer value can e.g. signify that the users do not trust that the respective mode of process monitoring conveys the need for interaction in time, leading the users to switching their attention to the process simulation in regular intervals, and performing interactions just in case. A low average buffer can, on the other hand, signify that the users rely on the respective conditions\u2019 ability to signal interaction needs. On the other hand, if e.g. an input buffer had already been completely depleted at the time of intervention, this may signify that the respective condition has failed to inform the users in time. In many cases, participants used double clicks for their interactions, while a single click would have been sufficient, a fact that was perhaps not communicated clearly enough to the participants. Therefore, if several clicks were performed directly one after another, only the first click was taken into account.\n", "keywords": "buffer\nbuffers\ncompare how many clicks the users made in average for each condition\ninput buffer\nsimulation\n"}, {"id": "S0888613X16301062", "text": "The first step of PB, the enumeration of the conditional sample space through abductive logic programming, could be compared to \u201clogical inference\u201d in ProbLog [9]. While both languages aim to generate a propositional formula and compile it into a decision diagram, \u201clogical inference\u201d in PB is based on abductive logic programming, while ProbLog grounds the relevant parts of the probabilistic program. Moreover, in PB compilation of the boolean formulas is performed using (RO)BDDs, while ProbLog can use a wider range of decision diagrams, e.g. sentential decision diagrams (SDD), deterministic, decomposable negation normal form (d-DNNF). These differences reflect the different aims of the two PPLs: ProbLog focuses on models where \u201clogical inference\u201d needs to be efficient, and the resulting representation, the decision diagrams, need to be compact, while PB focuses on models where \u201clogical inference\u201d is typically easy, however it must be applied repeatedly, according to the nature and the number of the observations. However, in future work, PB could benefit from the use of more compact decision diagrams.\n", "keywords": "d-DNNF\ndecision diagrams\ndeterministic, decomposable negation normal form\nenumeration of the conditional sample space\n\u201clogical inference\u201d\nlogical inference\n\u201clogical inference\u201d in PB\nPB\nPB compilation\nPPLs\nProbLog\n(RO)BDDs\nSDD\nsentential decision diagrams\nuse of more compact decision diagrams\n"}, {"id": "S0888613X16301062", "text": "The first step of PB, the enumeration of the conditional sample space through abductive logic programming, could be compared to \u201clogical inference\u201d in ProbLog [9]. While both languages aim to generate a propositional formula and compile it into a decision diagram, \u201clogical inference\u201d in PB is based on abductive logic programming, while ProbLog grounds the relevant parts of the probabilistic program. Moreover, in PB compilation of the boolean formulas is performed using (RO)BDDs, while ProbLog can use a wider range of decision diagrams, e.g. sentential decision diagrams (SDD), deterministic, decomposable negation normal form (d-DNNF). These differences reflect the different aims of the two PPLs: ProbLog focuses on models where \u201clogical inference\u201d needs to be efficient, and the resulting representation, the decision diagrams, need to be compact, while PB focuses on models where \u201clogical inference\u201d is typically easy, however it must be applied repeatedly, according to the nature and the number of the observations. However, in future work, PB could benefit from the use of more compact decision diagrams.\n", "keywords": "d-DNNF\ndecision diagrams\ndeterministic, decomposable negation normal form\nenumeration of the conditional sample space\n\u201clogical inference\u201d\nlogical inference\n\u201clogical inference\u201d in PB\nPB\nPB compilation\nPPLs\nProbLog\n(RO)BDDs\nSDD\nsentential decision diagrams\nuse of more compact decision diagrams\n"}, {"id": "S0301010415002256", "text": "For decades, vibronic coupling models [1\u20134] have served as bridges connecting nuclear dynamics studies with the static studies of electronic structure calculations [5]. The vibronic coupling model is a simple polynomial expansion of diabatic potential energy surfaces and couplings. The expansion coefficients are chosen so that the eigenvalues of the potential operator map on to the adiabatic potential surfaces. This diabatisation by ansatz circumvents many of the problems of describing non-adiabatic systems. It is also the inspiration for a diabatisation scheme that is used in modern, direct-dynamic methods that include non-adiabatic effects [6]. For a model Hamiltonian to correctly approximate the eigenvectors of the true Hamiltonian it has to span the totally symmetric irreducible representation (IrRep) of the point groups the molecule belongs to, at the appropriate symmetric geometries [7]. In recent times, many articles have demonstrated the advantages of using symmetry when constructing analytic model potentials [8\u201312], most often in the context of permutation-inversion groups [13].\n", "keywords": "analytic model potentials\ndiabatic potential energy surfaces and couplings\ndiabatisation\ndiabatisation scheme\ndirect-dynamic methods\nelectronic structure calculations\nexpansion coefficients\nirreducible representation\nIrRep\nmodel Hamiltonian\nnon-adiabatic systems\nnuclear dynamics studies\npermutation-inversion groups\npolynomial expansion\nsymmetric geometries\ntrue Hamiltonian\nvibronic coupling model\nvibronic coupling models\n"}, {"id": "S0370269304009979", "text": "On the other hand, the other local fields except the gravitational field are not always localized on the brane even in the warped geometry. Indeed, in the Randall\u2013Sundrum model in five dimensions\u00a0[2], the following facts are well known: spin\u00a00 field is localized on a brane with positive tension which also localizes the graviton while the spin 1/2 and 3/2 fields are localized not on a brane with positive tension but on a brane with negative tension\u00a0[6]. Spin\u00a01 field is not localized neither on a brane with positive tension nor on a brane with negative tension\u00a0[7]. In six space\u2013time dimensions, the spin\u00a01 gauge field is also localized on the brane\u00a0[8]. Thus, in order to fulfill the localization of Standard Model particles on a brane with positive tension, it seems that some additional interactions except the gravitational interaction must be also introduced in the bulk. There is a lot of papers devoted to the different localization mechanisms of the bulk fields in various brane world models.\n", "keywords": "brane world models\nfive dimensions\nintroduced in the bulk\nlocalization\nlocalization mechanisms\nlocalized on the brane\nlocalized on the brane\u00a0\nlocalizes the graviton\nnegative tension\npositive tension\nRandall\u2013Sundrum model\nsix space\u2013time dimensions\nspin\u00a00 field is localized on a brane\nStandard Model\nwarped geometry\n"}, {"id": "S0370269304009979", "text": "On the other hand, the other local fields except the gravitational field are not always localized on the brane even in the warped geometry. Indeed, in the Randall\u2013Sundrum model in five dimensions\u00a0[2], the following facts are well known: spin\u00a00 field is localized on a brane with positive tension which also localizes the graviton while the spin 1/2 and 3/2 fields are localized not on a brane with positive tension but on a brane with negative tension\u00a0[6]. Spin\u00a01 field is not localized neither on a brane with positive tension nor on a brane with negative tension\u00a0[7]. In six space\u2013time dimensions, the spin\u00a01 gauge field is also localized on the brane\u00a0[8]. Thus, in order to fulfill the localization of Standard Model particles on a brane with positive tension, it seems that some additional interactions except the gravitational interaction must be also introduced in the bulk. There is a lot of papers devoted to the different localization mechanisms of the bulk fields in various brane world models.\n", "keywords": "brane world models\nfive dimensions\nintroduced in the bulk\nlocalization\nlocalization mechanisms\nlocalized on the brane\nlocalized on the brane\u00a0\nlocalizes the graviton\nnegative tension\npositive tension\nRandall\u2013Sundrum model\nsix space\u2013time dimensions\nspin\u00a00 field is localized on a brane\nStandard Model\nwarped geometry\n"}, {"id": "S0038092X15003059", "text": "In addition, the prediction of solar cell\u2019s temperature is very important for the electrical characterisation of CPV modules. Rodrigo et al. (2014) reviewed various methods for the calculation of the cell temperature in High Concentrator PV (HCPV) modules. The methods were categorised based on: (1) heat sink temperature, (2) electrical parameters and (3) atmospheric parameters. The first two categories are based on direct measurements of CPV modules in indoor or outdoor experimental setups and presented the highest degree of accuracy (Root Mean Square Error (RMSE) 1.7\u20132.5K). Most of the methods reviewed by Rodrigo et al. (2014) calculate the cell temperature at open-circuit conditions. Methods that predict the cell temperature at maximum power point (MPP) operation offer a more realistic approach since they include the electrical energy generation of the solar cells (i.e. real operating conditions); Yandt et al. (2012) described a method predicting the cell temperature at MPP based on electrical parameters and Fern\u00e1ndez et al. (2014b) based on heat sink temperature with absolute RMSE 0.55\u20131.44K. Fern\u00e1ndez et al. (2014a) also proposed an artificial neural network model to estimate the cell temperature based on atmospheric parameters and an open-circuit voltage model based on electrical parameters (Fernandez et al., 2013a) offering good accuracy (RMSE 3.2K and 2.5K respectively (Rodrigo et al., 2014)). The main disadvantage of the aforementioned methods is that an experimental setup is required to obtain the parameters used for the cell temperature calculation.\n", "keywords": "artificial neural network model\ncalculate the cell temperature\ncalculation of the cell temperature\ncell\ncell temperature calculation\nCPV\nelectrical energy generation\nestimate the cell temperature\nHCPV\nHigh Concentrator PV\nmaximum power point\nMPP\nopen-circuit conditions\nopen-circuit voltage model\npredicting the cell temperature at MPP\nprediction of solar cell\u2019s temperature\nsolar cell\nsolar cells\n"}, {"id": "S0375960115004120", "text": "Another remarkable feature of the quantum field treatment can be revealed from the investigation of the vacuum state. For a classical field, vacuum is realized by simply setting the potential to zero resulting in an unaltered, free evolution of the particle's plane wave (|\u03c8I\u3009=|\u03c8III\u3009=|k0\u3009). In the quantized treatment, vacuum is represented by an initial Fock state |n0=0\u3009 which still interacts with the particle and yields as final state |\u03a8III\u3009 behind the field region(19)|\u03a8I\u3009=|k0\u3009\u2297|0\u3009\u21d2|\u03a8III\u3009=\u2211n=0\u221et0n|k\u2212n\u3009\u2297|n\u3009 with a photon exchange probability(20)P0,n=|t0n|2=1n!e\u2212\u039b2\u039b2n The particle thus transfers energy to the vacuum field leading to a Poissonian distributed final photon number. Let's consider, for example, a superconducting resonant circuit as source of the field. The magnetic field along the axis of a properly shaped coil is well approximated by the rectangular form. A particle with a magnetic dipole moment passing through the coil then interacts with the circuit and excites it with a measurable loss of kinetic energy even if the circuit is initially uncharged and there is classically no field it can couple to. The phenomenon that vacuum in quantum field theory does not mean to \u201cno influence\u201d as known from Casimir forces or Lamb shift is clearly visible here as well.\n", "keywords": "Casimir forces or Lamb shift\nmagnetic field\nparticle\nparticle's plane wave\nproperly shaped coil\nquantized treatment\nquantum field treatment\nsuperconducting resonant circuit\nvacuum\nvacuum state\n"}, {"id": "S0168365913001521", "text": "The mesoporous silica particles were prepared by the surfactant self-assembly method described previously [18,24]. Briefly, a homogeneous solution of the soluble silica precursor, tetraethylorthosilicate (TEOS; Sigma-Aldrich Corp., St. Louis, MO), and hydrochloric acid was mixed in ethanol and water. A surfactant, cetyltrimethylammonium bromide (CTAB; Sigma-Aldrich Corp., St. Louis, MO), with an initial concentration much less than the critical micelle concentration was added to lower the surface tension of the liquid mixture and act as the mesoporous structure-directing template. Aerosol solutions of soluble silica plus surfactant were then generated with nitrogen as a carrier atomizing gas using a commercially available atomizer (Model 9392A, TSI, Inc., St. Paul, MN). The aerosol droplets were solidified in a tube furnace at 400\u00b0C until dry. Once dried, a durapore membrane filter, kept at 80\u00b0C, was used to collect the particles. As a final step, the surfactant was removed at 400\u00b0C for 5h via calcination. The surface of the mesoporous silica core in these studies was chemically modified with 10wt.% or 15wt.% by aminopropyltriethoxysilane (APTES; Sigma-Aldrich Corp., St. Louis, MO) conducted identically as previously described [17] to create a positive surface charge to increase loading efficiency of negatively charged cargo. Further, Liu and colleagues report the colloidal stability of these protocells with lipid bilayers, excess amount of liposomes (50\u03bcg liposomes per 0.5mg silica were used [18]).\n", "keywords": "aerosol droplets\nAerosol solutions\naminopropyltriethoxysilane\nAPTES\natomizer\natomizing gas\ncalcination\ncetyltrimethylammonium bromide\nCTAB\ndurapore membrane filter\nethanol\nhomogeneous solution\nhydrochloric acid\nliposomes\nmesoporous silica\nmesoporous silica core\nmesoporous structure-directing template\nnitrogen\nprotocells\nsoluble silica\nsoluble silica precursor\nsurfactant\nsurfactant self-assembly\nTEOS\ntetraethylorthosilicate\ntube furnace\nwater.\n"}, {"id": "S0168365913001521", "text": "The mesoporous silica particles were prepared by the surfactant self-assembly method described previously [18,24]. Briefly, a homogeneous solution of the soluble silica precursor, tetraethylorthosilicate (TEOS; Sigma-Aldrich Corp., St. Louis, MO), and hydrochloric acid was mixed in ethanol and water. A surfactant, cetyltrimethylammonium bromide (CTAB; Sigma-Aldrich Corp., St. Louis, MO), with an initial concentration much less than the critical micelle concentration was added to lower the surface tension of the liquid mixture and act as the mesoporous structure-directing template. Aerosol solutions of soluble silica plus surfactant were then generated with nitrogen as a carrier atomizing gas using a commercially available atomizer (Model 9392A, TSI, Inc., St. Paul, MN). The aerosol droplets were solidified in a tube furnace at 400\u00b0C until dry. Once dried, a durapore membrane filter, kept at 80\u00b0C, was used to collect the particles. As a final step, the surfactant was removed at 400\u00b0C for 5h via calcination. The surface of the mesoporous silica core in these studies was chemically modified with 10wt.% or 15wt.% by aminopropyltriethoxysilane (APTES; Sigma-Aldrich Corp., St. Louis, MO) conducted identically as previously described [17] to create a positive surface charge to increase loading efficiency of negatively charged cargo. Further, Liu and colleagues report the colloidal stability of these protocells with lipid bilayers, excess amount of liposomes (50\u03bcg liposomes per 0.5mg silica were used [18]).\n", "keywords": "aerosol droplets\nAerosol solutions\naminopropyltriethoxysilane\nAPTES\natomizer\natomizing gas\ncalcination\ncetyltrimethylammonium bromide\nCTAB\ndurapore membrane filter\nethanol\nhomogeneous solution\nhydrochloric acid\nliposomes\nmesoporous silica\nmesoporous silica core\nmesoporous structure-directing template\nnitrogen\nprotocells\nsoluble silica\nsoluble silica precursor\nsurfactant\nsurfactant self-assembly\nTEOS\ntetraethylorthosilicate\ntube furnace\nwater.\n"}, {"id": "S0168365913001521", "text": "The mesoporous silica particles were prepared by the surfactant self-assembly method described previously [18,24]. Briefly, a homogeneous solution of the soluble silica precursor, tetraethylorthosilicate (TEOS; Sigma-Aldrich Corp., St. Louis, MO), and hydrochloric acid was mixed in ethanol and water. A surfactant, cetyltrimethylammonium bromide (CTAB; Sigma-Aldrich Corp., St. Louis, MO), with an initial concentration much less than the critical micelle concentration was added to lower the surface tension of the liquid mixture and act as the mesoporous structure-directing template. Aerosol solutions of soluble silica plus surfactant were then generated with nitrogen as a carrier atomizing gas using a commercially available atomizer (Model 9392A, TSI, Inc., St. Paul, MN). The aerosol droplets were solidified in a tube furnace at 400\u00b0C until dry. Once dried, a durapore membrane filter, kept at 80\u00b0C, was used to collect the particles. As a final step, the surfactant was removed at 400\u00b0C for 5h via calcination. The surface of the mesoporous silica core in these studies was chemically modified with 10wt.% or 15wt.% by aminopropyltriethoxysilane (APTES; Sigma-Aldrich Corp., St. Louis, MO) conducted identically as previously described [17] to create a positive surface charge to increase loading efficiency of negatively charged cargo. Further, Liu and colleagues report the colloidal stability of these protocells with lipid bilayers, excess amount of liposomes (50\u03bcg liposomes per 0.5mg silica were used [18]).\n", "keywords": "aerosol droplets\nAerosol solutions\naminopropyltriethoxysilane\nAPTES\natomizer\natomizing gas\ncalcination\ncetyltrimethylammonium bromide\nCTAB\ndurapore membrane filter\nethanol\nhomogeneous solution\nhydrochloric acid\nliposomes\nmesoporous silica\nmesoporous silica core\nmesoporous structure-directing template\nnitrogen\nprotocells\nsoluble silica\nsoluble silica precursor\nsurfactant\nsurfactant self-assembly\nTEOS\ntetraethylorthosilicate\ntube furnace\nwater.\n"}, {"id": "S0168365913001521", "text": "The mesoporous silica particles were prepared by the surfactant self-assembly method described previously [18,24]. Briefly, a homogeneous solution of the soluble silica precursor, tetraethylorthosilicate (TEOS; Sigma-Aldrich Corp., St. Louis, MO), and hydrochloric acid was mixed in ethanol and water. A surfactant, cetyltrimethylammonium bromide (CTAB; Sigma-Aldrich Corp., St. Louis, MO), with an initial concentration much less than the critical micelle concentration was added to lower the surface tension of the liquid mixture and act as the mesoporous structure-directing template. Aerosol solutions of soluble silica plus surfactant were then generated with nitrogen as a carrier atomizing gas using a commercially available atomizer (Model 9392A, TSI, Inc., St. Paul, MN). The aerosol droplets were solidified in a tube furnace at 400\u00b0C until dry. Once dried, a durapore membrane filter, kept at 80\u00b0C, was used to collect the particles. As a final step, the surfactant was removed at 400\u00b0C for 5h via calcination. The surface of the mesoporous silica core in these studies was chemically modified with 10wt.% or 15wt.% by aminopropyltriethoxysilane (APTES; Sigma-Aldrich Corp., St. Louis, MO) conducted identically as previously described [17] to create a positive surface charge to increase loading efficiency of negatively charged cargo. Further, Liu and colleagues report the colloidal stability of these protocells with lipid bilayers, excess amount of liposomes (50\u03bcg liposomes per 0.5mg silica were used [18]).\n", "keywords": "aerosol droplets\nAerosol solutions\naminopropyltriethoxysilane\nAPTES\natomizer\natomizing gas\ncalcination\ncetyltrimethylammonium bromide\nCTAB\ndurapore membrane filter\nethanol\nhomogeneous solution\nhydrochloric acid\nliposomes\nmesoporous silica\nmesoporous silica core\nmesoporous structure-directing template\nnitrogen\nprotocells\nsoluble silica\nsoluble silica precursor\nsurfactant\nsurfactant self-assembly\nTEOS\ntetraethylorthosilicate\ntube furnace\nwater.\n"}, {"id": "S0021999115008207", "text": "Multi-phase flows are common, in fact quite general, in environmental and industrial processes. Broadly these may be modelled as continuous problems where phases are mixed (e.g. oil\u2013water homogenisation [36], sediment transport [18]) or interface problems where phases are distinct and interact at the interface (e.g. gas-assisted injection moulding [21], liquid jet breakup [40]). In some cases flows start as interface problems but as mixing occurs at the interface they become effectively continuous, at least locally. Air entrainment, perhaps due to wave breaking, is an obvious example. We consider here two-phase interface problems where the interface remains distinct and the density difference is high, e.g. air and water, and where one phase may be considered incompressible. The interface is transient and may become highly distorted and interconnected. Such problems have been tackled with mesh-based methods using periodic (or adaptive) re-meshing or additional phase tracking functions [40]. However, these approaches can be time-consuming to implement and prone to errors in surface representation [50] or mass conservation [34].\n", "keywords": "air\nAir entrainment\ndistorted and interconnected\nenvironmental and industrial processes\ngas-assisted injection moulding\ninterface problems\nliquid jet breakup\nmesh-based methods\nmixing\nMulti-phase flows\noil\u2013water homogenisation\nphases are mixed\nphase tracking functions\nre-meshing\nsediment transport\ntwo-phase interface problems\nwater\nwave breaking\n"}, {"id": "S0021999115008207", "text": "Multi-phase flows are common, in fact quite general, in environmental and industrial processes. Broadly these may be modelled as continuous problems where phases are mixed (e.g. oil\u2013water homogenisation [36], sediment transport [18]) or interface problems where phases are distinct and interact at the interface (e.g. gas-assisted injection moulding [21], liquid jet breakup [40]). In some cases flows start as interface problems but as mixing occurs at the interface they become effectively continuous, at least locally. Air entrainment, perhaps due to wave breaking, is an obvious example. We consider here two-phase interface problems where the interface remains distinct and the density difference is high, e.g. air and water, and where one phase may be considered incompressible. The interface is transient and may become highly distorted and interconnected. Such problems have been tackled with mesh-based methods using periodic (or adaptive) re-meshing or additional phase tracking functions [40]. However, these approaches can be time-consuming to implement and prone to errors in surface representation [50] or mass conservation [34].\n", "keywords": "air\nAir entrainment\ndistorted and interconnected\nenvironmental and industrial processes\ngas-assisted injection moulding\ninterface problems\nliquid jet breakup\nmesh-based methods\nmixing\nMulti-phase flows\noil\u2013water homogenisation\nphases are mixed\nphase tracking functions\nre-meshing\nsediment transport\ntwo-phase interface problems\nwater\nwave breaking\n"}, {"id": "S0370269304007567", "text": "Each hit position inside the drift chambers was calculated from the drift time digitized by a flash analog-to-digital converter. The calculation was carried out based on a relation between the hit position and the drift time (x\u2013t relation). The x\u2013t relation was precisely calculated by a drift chamber simulation package, GARFIELD [20], and a gas property simulation package, MAGBOLTZ [21]. Although the chambers were constructed carefully with a tolerance of 100\u00a0\u03bcm, there was a small position deviation of wires and field-shaping patterns, which could locally modify the electric field. In order to take account of the limited accuracy in the chamber manufacturing, a correction was commonly applied to the calculated x\u2013t relation throughout the experiments. The correction was obtained to minimize the \u03c72 in the fitting of straight tracks of clean muon events observed on the ground without magnetic field. The correction was as small as expected from the accuracy in the chamber manufacturing. During the observations, the x\u2013t relation was affected by the variation in the pressure and temperature of the chamber gas. In order to take account of these time-dependent variations, the x\u2013t relation was calibrated for each data-taking run. Especially in calibrating the x\u2013t relation of ODCs, an absolute reference positions were provided by SciFi, which are not affected by the variation in the pressure nor temperature.\n", "keywords": "chamber\nchamber gas\nchamber manufacturing\nchambers\ndrift chambers\ndrift chamber simulation package\nflash analog-to-digital converter\nGARFIELD\ngas property simulation package\nMAGBOLTZ\nmagnetic field\nODCs\nrelation between the hit position and the drift time\nx\u2013t relation\n"}, {"id": "S1574119211001544", "text": "As future work on the protocol, we would promote two items. Firstly, the two mobility models that we have considered in this work propose possible way to capture social context in the way nodes move in the physical space, yet still potentially allowing nodes to explore the geographical regions considered in its entirety. Further insights to the performance potential could be given through the assessment of the protocol with other mobilities that can extend the physical region of movement as well as impose potential restrictions on the nodes mobility, for example by forcing similar nodes to move within specifically defined areas. Secondly, the different forwarding modes introduced in Section\u00a0 3.3 express different levels of cooperation across the network. The push-community mode, for example, is a form of interest-community selfishness and assumes reciprocation in the nodes\u2019 behaviour. The vulnerability (resp. resilience) of the protocol to different instances of node misbehaviours is a research item worth exploring.\n", "keywords": "extend the physical region of movement\nforcing similar nodes to move within specifically defined areas\nforwarding modes\ngeographical regions\nimpose potential restrictions\nmobility models\nnetwork\nnode\nnodes\nnodes mobility\nperformance potential\nphysical space\nprotocol\npush-community mode\n"}, {"id": "S0377025715000051", "text": "A convenient and widely reported technique for detection of the GP involves measurements of the complex shear modulus, G\u2217, over a range of frequencies, \u03c9, in oscillatory shear. At the GP the elastic and viscous components of the complex modulus, G\u2032 and G\u2033, respectively scale in oscillatory frequency, \u03c9, as G\u2032(\u03c9)\u223cG\u2033(\u03c9)\u223c\u03c9\u03b1 where \u03b1 is termed the stress relaxation exponent [15]. Thus, the GP may be identified as the instant where the G\u2032 and G\u2033 scale in frequency according to identical power laws [15], behaviour corresponding to attainment of a frequency independent phase angle, \u03b4(=atan(G\u2033/G\u2032)). GP measurements may involve \u2018frequency sweeps\u2019 with repeated consecutive application of a set of small amplitude oscillatory shear, SAOS, waveforms [15,16], or by Fourier Transform Mechanical Spectroscopy, FTMS, in which G\u2217(\u03c9) is found by simultaneous application of several harmonic frequencies in a composite waveform and its subsequent Fourier analysis [17,18]. Frequency sweeps are limited to relatively slow gelation processes due to sample mutation and interpolation errors [9,19,20]. FTMS may overcome these limitations, but is unsuitable for markedly strain sensitive materials, such as fibrin gels, due to the strain amplitude of the composite waveform exceeding the linear viscoelastic range (LVR) [9].\n", "keywords": "application of several harmonic frequencies\ncomposite waveform\ndetection of the GP\nfibrin gels\nFourier analysis\nFourier Transform Mechanical Spectroscopy\nfrequency sweeps\nFTMS\ngelation processes\nGP measurements\nmarkedly strain sensitive materials\nmeasurements of the complex shear modulus\nSAOS\nsmall amplitude oscillatory shear\nwaveforms\n"}, {"id": "S0305054816300867", "text": "The scheduling process we adopt matches a multiple stage stochastic programming approach. Standard two-stage stochastic programs with linear or convex functions are often solved using the L-shaped method or Bender's decomposition [44,6,7]. However, our recourse decision (scheduled cancellations) is still anticipative to further uncertainty, namely the second shift surgery durations, unavailability and cancellations. As such, the decision problem can be viewed as a three-stage recourse model [5,6]. Solving the scheduling problem is further complicated because the recourse function is integer. Laporte and Louveaux [26] propose modified L-shaped decomposition with adjusted optimal cuts for two stage stochastic program with integer recourse. Angulo et al. [1] alternately generate optimal cuts of the linear sub-problem and the integer sub-problem, which improves the practical convergence (see also [15,8]). We follow a sample average approximation approach (SAA) which uses this framework. Moreover, we prove and exploit a specific relationship between the first-stage realization and the optimal number of scheduled cancellations to speed up the computation of integer cuts. We use Jensen's inequality [17] to upper bound\u00a0the minus\u00a0second (and third) stage cost, a technique that was proposed by Batun et al. [3].\n", "keywords": "Bender's decomposition\ndecision problem\nfirst-stage realization\ngenerate optimal cuts of the linear sub-problem and the integer sub-problem\ninteger cuts\ninteger recourse\ninteger sub-problem\nJensen's inequality\nlinear or convex functions\nlinear sub-problem\nL-shaped method\nmodified L-shaped decomposition\nmultiple stage stochastic programming approach\nrecourse decision\nrecourse function\nSAA\nsample average approximation approach\nscheduled cancellations\nscheduling problem\nscheduling process\nthree-stage recourse model\ntwo stage stochastic program\ntwo-stage stochastic programs\nupper bound\u00a0the minus\u00a0second (and third) stage cost\n"}, {"id": "S0305054816300867", "text": "The scheduling process we adopt matches a multiple stage stochastic programming approach. Standard two-stage stochastic programs with linear or convex functions are often solved using the L-shaped method or Bender's decomposition [44,6,7]. However, our recourse decision (scheduled cancellations) is still anticipative to further uncertainty, namely the second shift surgery durations, unavailability and cancellations. As such, the decision problem can be viewed as a three-stage recourse model [5,6]. Solving the scheduling problem is further complicated because the recourse function is integer. Laporte and Louveaux [26] propose modified L-shaped decomposition with adjusted optimal cuts for two stage stochastic program with integer recourse. Angulo et al. [1] alternately generate optimal cuts of the linear sub-problem and the integer sub-problem, which improves the practical convergence (see also [15,8]). We follow a sample average approximation approach (SAA) which uses this framework. Moreover, we prove and exploit a specific relationship between the first-stage realization and the optimal number of scheduled cancellations to speed up the computation of integer cuts. We use Jensen's inequality [17] to upper bound\u00a0the minus\u00a0second (and third) stage cost, a technique that was proposed by Batun et al. [3].\n", "keywords": "Bender's decomposition\ndecision problem\nfirst-stage realization\ngenerate optimal cuts of the linear sub-problem and the integer sub-problem\ninteger cuts\ninteger recourse\ninteger sub-problem\nJensen's inequality\nlinear or convex functions\nlinear sub-problem\nL-shaped method\nmodified L-shaped decomposition\nmultiple stage stochastic programming approach\nrecourse decision\nrecourse function\nSAA\nsample average approximation approach\nscheduled cancellations\nscheduling problem\nscheduling process\nthree-stage recourse model\ntwo stage stochastic program\ntwo-stage stochastic programs\nupper bound\u00a0the minus\u00a0second (and third) stage cost\n"}, {"id": "S0045782515001231", "text": "Isogeometric analysis. The central idea of isogeometric analysis is to use the same ansatz functions for the discretization of the partial differential equation at hand, as are used for the representation of the problem geometry. Usually, the problem geometry \u03a9 is represented in computer aided design (CAD) by means of NURBS or T-splines. This concept, originally invented in\u00a0 [1] for finite element methods (IGAFEM) has proved very fruitful in applications\u00a0 [1,2]; see also the monograph\u00a0 [3]. Since CAD directly provides a parametrization of the boundary \u2202\u03a9, this makes the boundary element method (BEM) the most attractive numerical scheme, if applicable (i.e.,\u00a0provided that the fundamental solution of the differential operator is explicitly known). Isogeometric BEM (IGABEM) has first been considered for 2D BEM in\u00a0 [4] and for 3D BEM in\u00a0 [5]. Unlike standard BEM with piecewise polynomials which is well-studied in the literature, cf.\u00a0the monographs\u00a0 [6,7] and the references therein, the numerical analysis of IGABEM is essentially open. We only refer to\u00a0 [2,8\u201310] for numerical experiments and to\u00a0 [11] for some quadrature analysis. In particular, a\u00a0posteriori error estimation has been well-studied for standard BEM, e.g.,\u00a0 [12\u201318] as well as the recent overview article\u00a0 [19], but has not been treated for IGABEM so far. The purpose of the present work is to shed some first light on a\u00a0posteriori error analysis for IGABEM which provides some mathematical foundation of a corresponding adaptive algorithm.\n", "keywords": "2D BEM\n3D BEM\nadaptive algorithm\nansatz functions\na\u00a0posteriori error analysis\na\u00a0posteriori error estimation\nBEM\nboundary element method\nCAD\ncomputer aided design\nfinite element methods\nIGABEM\nIGAFEM\nisogeometric analysis\nIsogeometric analysis\nIsogeometric BEM\nmathematical foundation of a corresponding adaptive algorithm\nnumerical analysis of IGABEM\nnumerical scheme\nNURBS\nproblem geometry\nquadrature analysis\nshed some first light on a\u00a0posteriori error analysis for IGABEM\nstandard BEM\nT-splines\n"}, {"id": "S0045782515001231", "text": "Isogeometric analysis. The central idea of isogeometric analysis is to use the same ansatz functions for the discretization of the partial differential equation at hand, as are used for the representation of the problem geometry. Usually, the problem geometry \u03a9 is represented in computer aided design (CAD) by means of NURBS or T-splines. This concept, originally invented in\u00a0 [1] for finite element methods (IGAFEM) has proved very fruitful in applications\u00a0 [1,2]; see also the monograph\u00a0 [3]. Since CAD directly provides a parametrization of the boundary \u2202\u03a9, this makes the boundary element method (BEM) the most attractive numerical scheme, if applicable (i.e.,\u00a0provided that the fundamental solution of the differential operator is explicitly known). Isogeometric BEM (IGABEM) has first been considered for 2D BEM in\u00a0 [4] and for 3D BEM in\u00a0 [5]. Unlike standard BEM with piecewise polynomials which is well-studied in the literature, cf.\u00a0the monographs\u00a0 [6,7] and the references therein, the numerical analysis of IGABEM is essentially open. We only refer to\u00a0 [2,8\u201310] for numerical experiments and to\u00a0 [11] for some quadrature analysis. In particular, a\u00a0posteriori error estimation has been well-studied for standard BEM, e.g.,\u00a0 [12\u201318] as well as the recent overview article\u00a0 [19], but has not been treated for IGABEM so far. The purpose of the present work is to shed some first light on a\u00a0posteriori error analysis for IGABEM which provides some mathematical foundation of a corresponding adaptive algorithm.\n", "keywords": "2D BEM\n3D BEM\nadaptive algorithm\nansatz functions\na\u00a0posteriori error analysis\na\u00a0posteriori error estimation\nBEM\nboundary element method\nCAD\ncomputer aided design\nfinite element methods\nIGABEM\nIGAFEM\nisogeometric analysis\nIsogeometric analysis\nIsogeometric BEM\nmathematical foundation of a corresponding adaptive algorithm\nnumerical analysis of IGABEM\nnumerical scheme\nNURBS\nproblem geometry\nquadrature analysis\nshed some first light on a\u00a0posteriori error analysis for IGABEM\nstandard BEM\nT-splines\n"}, {"id": "S2212667812000780", "text": "With the development of sport normal students in china, Some ideas to teaching and learning that view learning as a simple process of knowledge have become outdated and ineffective, therefore, In order to improving the quality of teaching and learning on sport normal students in china, this author discussed some factors on promoting the level of teaching and learning for sport normal students, such as implementation principle, curriculum design, education policy, and so on. The meaning of results and their implication of future research are discussed.", "keywords": "curriculum design\neducation policy\nfactors on promoting the level of teaching and learning\nfuture research\nimplementation principle\nimproving the quality of teaching and learning on sport normal students\nknowledge\nlearning\nteaching\n"}, {"id": "S2212667812000780", "text": "With the development of sport normal students in china, Some ideas to teaching and learning that view learning as a simple process of knowledge have become outdated and ineffective, therefore, In order to improving the quality of teaching and learning on sport normal students in china, this author discussed some factors on promoting the level of teaching and learning for sport normal students, such as implementation principle, curriculum design, education policy, and so on. The meaning of results and their implication of future research are discussed.", "keywords": "curriculum design\neducation policy\nfactors on promoting the level of teaching and learning\nfuture research\nimplementation principle\nimproving the quality of teaching and learning on sport normal students\nknowledge\nlearning\nteaching\n"}, {"id": "S2212667812000780", "text": "With the development of sport normal students in china, Some ideas to teaching and learning that view learning as a simple process of knowledge have become outdated and ineffective, therefore, In order to improving the quality of teaching and learning on sport normal students in china, this author discussed some factors on promoting the level of teaching and learning for sport normal students, such as implementation principle, curriculum design, education policy, and so on. The meaning of results and their implication of future research are discussed.", "keywords": "curriculum design\neducation policy\nfactors on promoting the level of teaching and learning\nfuture research\nimplementation principle\nimproving the quality of teaching and learning on sport normal students\nknowledge\nlearning\nteaching\n"}, {"id": "S2212667812000780", "text": "With the development of sport normal students in china, Some ideas to teaching and learning that view learning as a simple process of knowledge have become outdated and ineffective, therefore, In order to improving the quality of teaching and learning on sport normal students in china, this author discussed some factors on promoting the level of teaching and learning for sport normal students, such as implementation principle, curriculum design, education policy, and so on. The meaning of results and their implication of future research are discussed.", "keywords": "curriculum design\neducation policy\nfactors on promoting the level of teaching and learning\nfuture research\nimplementation principle\nimproving the quality of teaching and learning on sport normal students\nknowledge\nlearning\nteaching\n"}, {"id": "S2212667812000780", "text": "With the development of sport normal students in china, Some ideas to teaching and learning that view learning as a simple process of knowledge have become outdated and ineffective, therefore, In order to improving the quality of teaching and learning on sport normal students in china, this author discussed some factors on promoting the level of teaching and learning for sport normal students, such as implementation principle, curriculum design, education policy, and so on. The meaning of results and their implication of future research are discussed.", "keywords": "curriculum design\neducation policy\nfactors on promoting the level of teaching and learning\nfuture research\nimplementation principle\nimproving the quality of teaching and learning on sport normal students\nknowledge\nlearning\nteaching\n"}, {"id": "S2212667812000780", "text": "With the development of sport normal students in china, Some ideas to teaching and learning that view learning as a simple process of knowledge have become outdated and ineffective, therefore, In order to improving the quality of teaching and learning on sport normal students in china, this author discussed some factors on promoting the level of teaching and learning for sport normal students, such as implementation principle, curriculum design, education policy, and so on. The meaning of results and their implication of future research are discussed.", "keywords": "curriculum design\neducation policy\nfactors on promoting the level of teaching and learning\nfuture research\nimplementation principle\nimproving the quality of teaching and learning on sport normal students\nknowledge\nlearning\nteaching\n"}, {"id": "S2212667812000780", "text": "With the development of sport normal students in china, Some ideas to teaching and learning that view learning as a simple process of knowledge have become outdated and ineffective, therefore, In order to improving the quality of teaching and learning on sport normal students in china, this author discussed some factors on promoting the level of teaching and learning for sport normal students, such as implementation principle, curriculum design, education policy, and so on. The meaning of results and their implication of future research are discussed.", "keywords": "curriculum design\neducation policy\nfactors on promoting the level of teaching and learning\nfuture research\nimplementation principle\nimproving the quality of teaching and learning on sport normal students\nknowledge\nlearning\nteaching\n"}, {"id": "S2212667812000810", "text": "With employment of utilizing the investigation, expert interviews and comparison, this article investigates the curricula construction, curricula design and curricula content for sports free normal students. On the basis of the investigation, this article analyzed the theoretical framework of curricular construction and proposed some suggestions. We hope that it can provide some evidences for curricula design for sports free normal students.", "keywords": "curricula design\nexpert interviews\ninvestigates the curricula construction, curricula design and curricula content\ninvestigation\ntheoretical framework\n"}, {"id": "S0098300413002124", "text": "In this section, we use the terrain data processing as an example to describe the geodetic data transformation method. Since Google Maps/Earth server only gives the terrain data in graphical display, we have to get terrain digital data from other sources. The fine-resolution (3\u2033 or finer) terrain data bases such as SRTM (Shuttle Radar Topographical Mission) or USGS's DEM (Digital Elevation Model) data are necessary. Moreover, since 3DWF is used to model the fine-scale (meters up to 100m) atmospheric flow, it needs fine resolution terrain data. In this project, we use the terrain elevation data set from SRTM (Farr et al. 2007) with 3-arcsecond (~90m resolution at the equator) resolution. The data covers the land area, nearly global from 56S to 60N latitudes. We use the processed version 4 SRTM data set as described in Gamache (2005) in which some of the missing data holes were filled. The original data is organized in WGS84 (World Geodetic System 84) geodetic coordinate system. When the data are applied to the 3DWF model, they are transformed to the local East, North and Up (ENU) coordinate (see Fig. 3). Since the 3DWF is a fine scale wind model and its entire model domain is not intended to be larger than 20\u00d720km, this Cartesian coordinate system is a good choice with very little distortion due to the curvature of the Earth's surface. The transformation from the WGS84 data to the ENU coordinate is performed as follows (Fukushima, 2006; Featherstone and Claessens, 2008).\n", "keywords": "3DWF\nCartesian coordinate system\nDEM\ndescribe the geodetic data transformation method\nDigital Elevation Model\nEast, North and Up\nENU\nfine-resolution (3\u2033 or finer) terrain data bases\nfine-scale (meters up to 100m) atmospheric flow\nShuttle Radar Topographical Mission\nSRTM\nterrain data processing\nterrain digital data\nWGS84\nWorld Geodetic System 84\n"}, {"id": "S0301010414003115", "text": "The optimised structure at the B3LYP/aug-cc-pVTZ level was then used to perform calculations of the lowest electronic singlet excited states with the coupled cluster linear response (LR) coupled cluster hierarchy CCS, CC2, CCSD and CC3, along with perturbative corrected methods CIS(D) and CCSDR(3). The correlated response methods were performed with an all-electron atomic natural orbital (ANO) basis set contracted to 6s5p4d3f1g on manganese, [47] together with the cc-pVTZ basis set on the oxygen atoms. The all-electron correlated calculations invoked a 13 orbital frozen core (O 1s, Mn 1s2s2p3s3p). Trial calculations correlating these orbitals only had a minor effect on excitation energies. For comparison the EOM-CCSD method with the cc-pVTZ basis on all atoms was tested to compare with LR-CCSD. These formally give exactly the same excitation energies, although the transition moments are more accurate for LR-CCSD. Abelian symmetry (D2) was used in all correlated excited state calculations.\n", "keywords": "6s5p4d3f1g\nAbelian symmetry\nall-electron correlated calculations\nANO\natomic natural orbital\nB3LYP/aug-cc-pVTZ\nCC2\nCC3\ncc-pVTZ\nCCS\nCCSD\nCCSDR(3)\nCIS(D)\ncorrelated response methods\ncoupled cluster hierarchy\nD2\nEOM-CCSD method\nexcitation energies\nexcited state calculations\nlinear response\nlowest electronic singlet\nLR\nLR-CCSD\nmanganese\nMn\nO\noptimised structure\noxygen\nperturbative corrected methods\ntransition moments\n"}, {"id": "S0021999114007396", "text": "In this work, we have developed a simple numerical scheme based on the Galerkin finite element method for a multi-term time fractional diffusion equation which involves multiple Caputo fractional derivatives in time. A complete error analysis of the space semidiscrete Galerkin scheme is provided. The theory covers the practically very important case of nonsmooth initial data and right hand side. The analysis relies essentially on some new regularity results of the multi-term time fractional diffusion equation. Further, we have developed a fully discrete scheme based on a finite difference discretization of the Caputo fractional derivatives. The stability and error estimate of the fully discrete scheme were established, provided that the solution is smooth. The extensive numerical experiments in one- and two-dimension fully confirmed our convergence analysis: the empirical convergence rates agree well with the theoretical predictions for both smooth and nonsmooth data.\n", "keywords": "empirical convergence rates\nfinite difference discretization of the Caputo fractional derivatives\nfully discrete scheme\nGalerkin finite element method\nGalerkin scheme\nmultiple Caputo fractional derivatives in time\nmulti-term time fractional diffusion equation\nnew regularity results\nnumerical experiments\nnumerical scheme\none- and two-dimension\n"}, {"id": "S0021999114007396", "text": "In this work, we have developed a simple numerical scheme based on the Galerkin finite element method for a multi-term time fractional diffusion equation which involves multiple Caputo fractional derivatives in time. A complete error analysis of the space semidiscrete Galerkin scheme is provided. The theory covers the practically very important case of nonsmooth initial data and right hand side. The analysis relies essentially on some new regularity results of the multi-term time fractional diffusion equation. Further, we have developed a fully discrete scheme based on a finite difference discretization of the Caputo fractional derivatives. The stability and error estimate of the fully discrete scheme were established, provided that the solution is smooth. The extensive numerical experiments in one- and two-dimension fully confirmed our convergence analysis: the empirical convergence rates agree well with the theoretical predictions for both smooth and nonsmooth data.\n", "keywords": "empirical convergence rates\nfinite difference discretization of the Caputo fractional derivatives\nfully discrete scheme\nGalerkin finite element method\nGalerkin scheme\nmultiple Caputo fractional derivatives in time\nmulti-term time fractional diffusion equation\nnew regularity results\nnumerical experiments\nnumerical scheme\none- and two-dimension\n"}, {"id": "S0021999114007396", "text": "In this work, we have developed a simple numerical scheme based on the Galerkin finite element method for a multi-term time fractional diffusion equation which involves multiple Caputo fractional derivatives in time. A complete error analysis of the space semidiscrete Galerkin scheme is provided. The theory covers the practically very important case of nonsmooth initial data and right hand side. The analysis relies essentially on some new regularity results of the multi-term time fractional diffusion equation. Further, we have developed a fully discrete scheme based on a finite difference discretization of the Caputo fractional derivatives. The stability and error estimate of the fully discrete scheme were established, provided that the solution is smooth. The extensive numerical experiments in one- and two-dimension fully confirmed our convergence analysis: the empirical convergence rates agree well with the theoretical predictions for both smooth and nonsmooth data.\n", "keywords": "empirical convergence rates\nfinite difference discretization of the Caputo fractional derivatives\nfully discrete scheme\nGalerkin finite element method\nGalerkin scheme\nmultiple Caputo fractional derivatives in time\nmulti-term time fractional diffusion equation\nnew regularity results\nnumerical experiments\nnumerical scheme\none- and two-dimension\n"}, {"id": "S221266781400149X", "text": "In this paper, adaptive beamforming techniques for smart antennas based upon Least Mean Squares (LMS), Sample Matrix Inversion (SMI), Recursive Least Squares (RLS) and Conjugate Gradient Method (CGM) are discussed and analyzed. The beamforming performance is studied by varying the element spacing and the number of antenna array elements for each algorithm. These four algorithms are compared for their rate of convergence, beamforming and null steering performance (beamwidth, null depths and maximum side lobe level).", "keywords": "adaptive beamforming techniques\nalgorithm\nalgorithms\nbeamforming\nbeamforming performance\nbeamwidth\nCGM\nConjugate Gradient Method\ndiscussed and analyzed\nLeast Mean Squares\nLMS\nmaximum side lobe level\nnull depths\nnull steering performance\nrate of convergence\nRecursive Least Squares\nRLS\nSample Matrix Inversion\nsmart antennas\nSMI\nvarying the element spacing and the number of antenna array elements\n"}, {"id": "S0021999113005718", "text": "Numerical simulation of the gas flow through such non-trivial internal geometries is, however, extremely challenging. This is because conventional continuum fluid dynamics, which assumes that locally a gas is close to a state of thermodynamic equilibrium, becomes invalid or inaccurate as the smallest characteristic scale of the geometry (e.g. the channel height) approaches the mean distance between molecular collisions, \u03bb [1]. An accurate and flexible modelling alternative for these cases is the direct simulation Monte Carlo method (DSMC) [2]. However, DSMC can be prohibitively expensive for internal-flow applications, which typically have a geometry of high-aspect ratio (i.e. are extremely long, relative to their cross-section). The high-aspect ratio creates a formidable multiscale problem: processes need to be resolved occurring over the smallest characteristic scale of the geometry (e.g. a channel\u02bcs height), as well as over the largest characteristic scale of the geometry (e.g. the length of a long channel network), simultaneously.\n", "keywords": "accurate and flexible modelling alternative\nchannel height\nconventional continuum fluid dynamics\ndirect simulation Monte Carlo method\nDSMC\ngas flow\nhigh-aspect ratio\ninternal-flow applications\nlong channel network\nmolecular collisions\nmultiscale problem\nNumerical simulation\nthermodynamic equilibrium\n"}, {"id": "S0021999113005718", "text": "Numerical simulation of the gas flow through such non-trivial internal geometries is, however, extremely challenging. This is because conventional continuum fluid dynamics, which assumes that locally a gas is close to a state of thermodynamic equilibrium, becomes invalid or inaccurate as the smallest characteristic scale of the geometry (e.g. the channel height) approaches the mean distance between molecular collisions, \u03bb [1]. An accurate and flexible modelling alternative for these cases is the direct simulation Monte Carlo method (DSMC) [2]. However, DSMC can be prohibitively expensive for internal-flow applications, which typically have a geometry of high-aspect ratio (i.e. are extremely long, relative to their cross-section). The high-aspect ratio creates a formidable multiscale problem: processes need to be resolved occurring over the smallest characteristic scale of the geometry (e.g. a channel\u02bcs height), as well as over the largest characteristic scale of the geometry (e.g. the length of a long channel network), simultaneously.\n", "keywords": "accurate and flexible modelling alternative\nchannel height\nconventional continuum fluid dynamics\ndirect simulation Monte Carlo method\nDSMC\ngas flow\nhigh-aspect ratio\ninternal-flow applications\nlong channel network\nmolecular collisions\nmultiscale problem\nNumerical simulation\nthermodynamic equilibrium\n"}, {"id": "S0168365913003295", "text": "A limitation of the pharmacyte approach is the one-time nature of the intervention: ACT T-cells can only be loaded once with a cargo of adjuvant drug prior to transfer, and the duration of stimulation is inherently limited by expansion of the cell population in vivo, since cell-bound particles are diluted with each cell division. We hypothesized that a strategy to target supporting drugs to T-cells with nanoparticle drug carriers directly in vivo would enable transferred lymphocytes to be repeatedly stimulated with supporting adjuvant drugs, and thereby provide continuous supporting signals over the prolonged durations that might be necessary for elimination of large tumor burdens. Such \u201cre-arming\u201d of T-cells with supporting drugs could be achieved by repeated administration of targeted particles, allowing adoptively-transferred T-cells to be restimulated multiple times directly in vivo, while the use of internalizing targeting ligands would minimize the likelihood of immune responses against the nanoparticle carrier. To our knowledge, only two prior studies have attempted to target nanoparticles to T-cells in vivo [17,18]. In both of these studies, particles were targeted to T-cells via peptide-MHC ligands that bind to specific T-cell receptors. However, peptide-MHC-functionalized nanoparticles have recently been shown to deliver an anergizing/tolerizing signal to T-cells [18,19] \u2014 which is ideal for treating graft rejection or autoimmunity, but runs counter to the goals of cancer immunotherapy.\n", "keywords": "ACT T-cells\nadjuvant drug\nadministration of targeted particles\nadoptively-transferred T-cells\ncancer immunotherapy\ncell\ncell-bound particles\ncell division\nelimination of large tumor burdens\nexpansion of the cell population in vivo\ninternalizing targeting ligands\nnanoparticle carrier\nnanoparticle drug carriers\nnanoparticles\npeptide-MHC-functionalized nanoparticles\npeptide-MHC ligands\npharmacyte approach\n\u201cre-arming\u201d of T-cells with supporting drugs\nstimulation\nsupporting adjuvant drugs\nsupporting drugs\ntargeted particles\nT-cell receptors\nT-cells\ntransferred lymphocytes\ntreating graft rejection or autoimmunity\n"}, {"id": "S0301679X14000449", "text": "Fig. 11 shows the wear-mode map of RH ceramics, in which the early-stage friction coefficients and the surface roughness of the pure surface were chosen. The value of the fracture toughness of RH ceramics was calculated based on the reference data in other literature [22]. The Sc of RH ceramics was smaller than Sc,critical under all tested conditions during the initial stage of friction. Thus, the initial wear mode of RH ceramics was powder formation or plowing. In addition, powder formation and plowing can be distinguished using a dimensionless parameter (Sc\u204e) and a critical parameter (Sc,critical\u204e).(3)Sc\u204e=HvRmaxKIc(4)Sc,critical\u204e=51+10\u03bcwhere Hv is the Vickers hardness of RH ceramics [Pa]. The initial wear mode of RH ceramics was determined as powder formation under all tested conditions, as demonstrated in Fig. 12(a). Furthermore, the wear-mode map at 2\u00d7104 cycles was constructed, as shown in Fig. 12(b). In the map, all plots moved near the transition curve to plowing. In particular, the plots for RH ceramics sliding against stainless steel or Al2O3 balls were nearer than SiC or Si3N4 balls. Therefore, RH ceramics sliding against SiC and Si3N4 balls showed relatively higher wear than the other counterpart materials. Nevertheless, these results from the wear-mode maps indicated that the wear mode of RH ceramics was powder formation accompanied with microcracks under all tested conditions in this study, resulting in low wear (<5\u00d710\u22129mm2/N). Indeed, the observation of the worn surfaces revealed that the catastrophic wear of RH ceramics accompanied by large brittle fracture was prevented overall, as shown in Fig. 13.\n", "keywords": "Al2O3 balls\ncritical parameter\ndimensionless parameter\nplowing\npowder formation\nSc\u204e\nSc,critical\u204e\nSc\u204e=HvRmaxKIc(4)Sc,critical\u204e=51+10\u03bc\nSiC and Si3N4 balls\nSiC or Si3N4 balls\nstainless steel\nVickers hardness\nwear-mode map\nwear-mode maps\n"}, {"id": "S002002551630384X", "text": "An early attempt to combine sets and networks in a single visualization relied on first drawing an Euler diagram then placing a graph inside it [30], however the sets were often visualized with convoluted, difficult to follow curves. In addition, only limited kinds of set data could be shown as the system was limited to well-formed Euler diagrams. Compound graphs can be used to represent restricted kinds of grouped network data [8]. Graph clusters are visualized with transparent hulls by Santamaria and Theron [39]. However, the technique removes edges from the graph and it is not sufficiently sophisticated for arbitrary overlapping sets. Itoh et\u00a0al. [24] proposed to overlay pie-like glyphs over the nodes in a graph to encode multiple categories. Each set is hence represented using disconnected regions that are linked by having the same colour. This causes difficulties with tasks that involve finding relations between sets such as T1, T3 and T4 in Section\u00a05.3. A related class of techniques visualize grouping information over graphs using convex hulls, such as Vizster [22]. However, they do not support visualizing set overlaps.\n", "keywords": "combine sets and networks in a single visualization\nCompound graphs\nconvex hulls\nEuler diagram\nEuler diagrams\nfinding relations between sets\nglyphs\nGraph clusters\nnetwork data\nsingle visualization\nT1, T3 and T4\ntransparent hulls\nvisualize grouping information over graphs\nVizster\n"}, {"id": "S221267161400105X", "text": "In this paper a comparison between two popular feature extraction methods is presented. Scale-invariant feature transform (or SIFT) is the first method. The Speeded up robust features (or SURF) is presented as second. These two methods are tested on set of depth maps. Ten defined gestures of left hand are in these depth maps. The Microsoft Kinect camera is used for capturing the images [1]. The Support vector machine (or SVM) is used as classification method. The results are accuracy of SVM prediction on selected images.In this paper a comparison between two popular feature extraction methods is presented. Scale-invariant feature transform (or SIFT) is the first method. The Speeded up robust features (or SURF) is presented as second. These two methods are tested on set of depth maps. Ten defined gestures of left hand are in these depth maps. The Microsoft Kinect camera is used for capturing the images [1]. The Support vector machine (or SVM) is used as classification method. The results are accuracy of SVM prediction on selected images.\n", "keywords": "classification\nclassification method\ncomparison between two popular feature extraction methods\ndepth maps\nfeature extraction\nMicrosoft Kinect camera\nScale-invariant feature transform\nSIFT\nSpeeded up robust features\nSupport vector machine\nSURF\nSVM\n"}, {"id": "S221267161400105X", "text": "In this paper a comparison between two popular feature extraction methods is presented. Scale-invariant feature transform (or SIFT) is the first method. The Speeded up robust features (or SURF) is presented as second. These two methods are tested on set of depth maps. Ten defined gestures of left hand are in these depth maps. The Microsoft Kinect camera is used for capturing the images [1]. The Support vector machine (or SVM) is used as classification method. The results are accuracy of SVM prediction on selected images.In this paper a comparison between two popular feature extraction methods is presented. Scale-invariant feature transform (or SIFT) is the first method. The Speeded up robust features (or SURF) is presented as second. These two methods are tested on set of depth maps. Ten defined gestures of left hand are in these depth maps. The Microsoft Kinect camera is used for capturing the images [1]. The Support vector machine (or SVM) is used as classification method. The results are accuracy of SVM prediction on selected images.\n", "keywords": "classification\nclassification method\ncomparison between two popular feature extraction methods\ndepth maps\nfeature extraction\nMicrosoft Kinect camera\nScale-invariant feature transform\nSIFT\nSpeeded up robust features\nSupport vector machine\nSURF\nSVM\n"}, {"id": "S221267161400105X", "text": "In this paper a comparison between two popular feature extraction methods is presented. Scale-invariant feature transform (or SIFT) is the first method. The Speeded up robust features (or SURF) is presented as second. These two methods are tested on set of depth maps. Ten defined gestures of left hand are in these depth maps. The Microsoft Kinect camera is used for capturing the images [1]. The Support vector machine (or SVM) is used as classification method. The results are accuracy of SVM prediction on selected images.In this paper a comparison between two popular feature extraction methods is presented. Scale-invariant feature transform (or SIFT) is the first method. The Speeded up robust features (or SURF) is presented as second. These two methods are tested on set of depth maps. Ten defined gestures of left hand are in these depth maps. The Microsoft Kinect camera is used for capturing the images [1]. The Support vector machine (or SVM) is used as classification method. The results are accuracy of SVM prediction on selected images.\n", "keywords": "classification\nclassification method\ncomparison between two popular feature extraction methods\ndepth maps\nfeature extraction\nMicrosoft Kinect camera\nScale-invariant feature transform\nSIFT\nSpeeded up robust features\nSupport vector machine\nSURF\nSVM\n"}, {"id": "S221267161400105X", "text": "In this paper a comparison between two popular feature extraction methods is presented. Scale-invariant feature transform (or SIFT) is the first method. The Speeded up robust features (or SURF) is presented as second. These two methods are tested on set of depth maps. Ten defined gestures of left hand are in these depth maps. The Microsoft Kinect camera is used for capturing the images [1]. The Support vector machine (or SVM) is used as classification method. The results are accuracy of SVM prediction on selected images.In this paper a comparison between two popular feature extraction methods is presented. Scale-invariant feature transform (or SIFT) is the first method. The Speeded up robust features (or SURF) is presented as second. These two methods are tested on set of depth maps. Ten defined gestures of left hand are in these depth maps. The Microsoft Kinect camera is used for capturing the images [1]. The Support vector machine (or SVM) is used as classification method. The results are accuracy of SVM prediction on selected images.\n", "keywords": "classification\nclassification method\ncomparison between two popular feature extraction methods\ndepth maps\nfeature extraction\nMicrosoft Kinect camera\nScale-invariant feature transform\nSIFT\nSpeeded up robust features\nSupport vector machine\nSURF\nSVM\n"}, {"id": "S0963869514001078", "text": "It is known that as the temperature of the sample rises, the Lorentz mechanism remains dominant until Tc of steel is reached (770\u00b0C for a low carbon steel), when the magnetostrictive mechanism becomes more efficient [15]. Previously this has been thought due to a thin ferromagnetic oxide layer on the sample surface, the surface being cooler than the bulk of the material [16,17]. This layer concentrates the magnetic field, increasing generation efficiency. Recent studies also show that rearrangement of the magnetic moments from ordered domains to a disordered state at a magnetic phase transition lowers the magnetostrictive constant. This ferromagnetic to paramagnetic transition is accompanied by large changes in the efficiency of electromagnetic ultrasound generation leading to the use of EMATs as a method of studying phase transitions in magnetic alloys [18].\n", "keywords": "electromagnetic ultrasound generation\nEMATs\nferromagnetic oxide\nferromagnetic to paramagnetic transition\ngeneration efficiency\nLorentz mechanism\nlow carbon steel\nmagnetic alloys\nmagnetic field\nmagnetic phase transition\nmagnetostrictive mechanism\nsteel\nstudying phase transitions\n"}, {"id": "S0375960113006725", "text": "Observations show that in the same area with dimensions of a few tenths of a parsec could be many sources, some of which only emits OH lines, and some \u2013 only lines H2O. The only known in physics the emission mechanism that can give tremendous power within a narrow range of the spectrum, is coherent (i.e. the same phase and direction) light lasers, which are called optical lasers, and radio-masers. Cosmic maser radio sources emitting in the lines of the molecules have an extremely high brightness temperature radiation Tb. In the molecules of methanol masers (CH3OH) Tb value can reach 109\u00a0K, with masers hydroxyl molecules (OH) 6\u00d71012\u00a0K. The typical size of the maser clusters is about 1014\u20131015\u00a0m and the neutron star radius is of the order of 10 km. Thus, the radiation dilution coefficient is equaled approximately (2.5\u00d710\u221223)\u2013(2.5\u00d710\u221221) and, therefore, \u03bcB2B2/4(h\u03bd)2\u223c(2.4\u00d710\u22125)\u2013(2.4\u00d710\u22127) for the hydrogen line 21 cm and of the order 10\u22125\u201310\u22127 for the OH 18 cm line or the same order as Eq. (1).\n", "keywords": "CH3OH\nCosmic maser radio sources\nemission mechanism\nH2O\nhydrogen line\nlight lasers\nmaser clusters\nmasers hydroxyl molecules\nmethanol masers\nneutron star\nObservations\nOH\nOH lines\noptical lasers\nradiation dilution coefficient\nradio-masers\n"}, {"id": "S0021961414003255", "text": "Moreover, one observes segregation effects by the XRD analysis, which probably took place at high temperature, and were partially quenched to room temperature. The phase analysis showed up to three distinct phases, which should have hence a distinct measurable phase transition temperature, if they crystallise from the liquid on the surface. In the thermograms these effects are not observable as different solidification arrest or clear inflections. The proportion of new appearing phases is small and therefore the latent heat released by this new phase will be also small. The reflected light signal technique only showed one phase change during cooling. As well, the location of this segregation cannot be determined exactly in the molten pool or later in the re-solidified material. At the surface, where the temperature is measured, the material analysis by Raman spectroscopy has not shown signs of segregation, so that also the uncertainties in composition for the phase transition are taken from the uncertainties from the XRD analysis for the most abundant phase at each composition in re-solidified material.\n", "keywords": "clear inflections\ncrystallise\nlatent heat\nliquid\nmaterial analysis by Raman spectroscopy\nmolten pool\nphase change\nphase transition\nquenched\nRaman spectroscopy\nreflected light signal technique\nre-solidified material\nsegregation\nsegregation effects\nsolidification arrest\nsurface\nthermograms\nXRD analysis\n"}, {"id": "S0021961414003255", "text": "Moreover, one observes segregation effects by the XRD analysis, which probably took place at high temperature, and were partially quenched to room temperature. The phase analysis showed up to three distinct phases, which should have hence a distinct measurable phase transition temperature, if they crystallise from the liquid on the surface. In the thermograms these effects are not observable as different solidification arrest or clear inflections. The proportion of new appearing phases is small and therefore the latent heat released by this new phase will be also small. The reflected light signal technique only showed one phase change during cooling. As well, the location of this segregation cannot be determined exactly in the molten pool or later in the re-solidified material. At the surface, where the temperature is measured, the material analysis by Raman spectroscopy has not shown signs of segregation, so that also the uncertainties in composition for the phase transition are taken from the uncertainties from the XRD analysis for the most abundant phase at each composition in re-solidified material.\n", "keywords": "clear inflections\ncrystallise\nlatent heat\nliquid\nmaterial analysis by Raman spectroscopy\nmolten pool\nphase change\nphase transition\nquenched\nRaman spectroscopy\nreflected light signal technique\nre-solidified material\nsegregation\nsegregation effects\nsolidification arrest\nsurface\nthermograms\nXRD analysis\n"}, {"id": "S0032386110004039", "text": "An application of ROMP derived random copolymers is the covalent incorporation of optical sensor moieties into a polymer matrix. ROM polymers have been tested as matrix materials for the oxygen sensing phosphorescent complex platinum tetrakis(pentafluorophenyl)porphyrin. A correlation between the nature of the ROM polymer\u2019s side chain and the optical response of the sensor molecules has been established [34]. Several works are dedicated to the synthesis of ROMP-able optical sensor molecules such as phenantroimidazoles [35,36], europium complexes [37] or xanthene dyes [38], their random copolymerization and the evaluation of their sensing profiles in the copolymers. Another application comprises random copolymers with covalently bound eosin and/or ethyl dimethylamino benzoate units which were tested as macroinitiators for the photopolymerization of acrylates aiming at an initiator/coinitiator system which combines good polymerization activity with improved migration stability [39].\n", "keywords": "acrylates\ncopolymerization\ncopolymers\ncovalent incorporation of optical sensor moieties into a polymer matrix\ncovalently bound eosin\nethyl dimethylamino benzoate units\neuropium complexes\nevaluation of their sensing profiles in the copolymers\ninitiator/coinitiator system\nmacroinitiators\nmatrix materials\noptical sensor moieties\noxygen\nphenantroimidazoles\nphosphorescent complex platinum tetrakis(pentafluorophenyl)porphyrin\npolymerization\npolymer matrix\nrandom copolymers with covalently bound eosin and/or ethyl dimethylamino benzoate units\nROMP-able optical sensor molecules\nROMP derived random copolymers\nROM polymers\nROM polymer\u2019s side chain\nsensor molecules\nsynthesis of ROMP-able optical sensor molecules\nxanthene dyes\n"}, {"id": "S2212671612002351", "text": "In this paper, a novel position estimation method of prism was proposed for single-lens stereovision system. The prism with multi faces was considered as a single optical system composed of some refractive planes. A transformation matrix which can express the relationship between an object point and its image by the refraction of prism was derived based on geometrical optics, and a mathematical model was introduced which can denote the position of prism with arbitrary faces only by 7 parameters. This model can extend the application of single-lens stereovision system using prism to a more widely area. Experimentation results are presented to prove the effectiveness and robustness of our proposed model.", "keywords": "effectiveness\nexpress the relationship\nextend the application of single-lens stereovision system\ngeometrical optics\nimage\nmathematical model\nobject point\nposition estimation method\nprism\nrefraction of prism\nrefractive planes\nrobustness\nsingle-lens stereovision system\nsingle optical system\ntransformation matrix\n"}, {"id": "S1386505616301769", "text": "The purported advantages of EMR implementation in urban slums are widely promoted. Increasingly capable health information systems could facilitate communication, help coordinate care, and improve the continuity of care in disadvantaged communities like Kibera. However, available systems may not have the ability to simplify care or improve efficiency where funding and human resources are scarce, infrastructure is unreliable and health data demands are opportunistic, not strategic. This study described perceptions of local EMR stakeholders in two urban slum clinics. They shared many observations that may be important for other EMR initiatives to heed, and worried most about the sustainability of EMR initiatives in like communities. The future for EMR use in urban slums is promising. Innovative new technologies, such as mobile applications and point-of-care laboratory tests, could extend the reach of EMRs where infrastructure is wanting. New cloud-based EMR ecosystems, where data is collected and stored centrally could leverage cell phone networks to promote more health information sharing, coordination of care and ultimately better outcomes for vulnerable populations.Summary pointsWhat was already known on the topic?\u2022Rapid urbanization is associated with growth in the number and size of urban slums and an associated rise in the burden of disease, further worsening an already fragmented and inefficient health care system.\n", "keywords": "cloud-based EMR ecosystems\nEMR\nEMR implementation\nEMRs\nhealth information systems\nmobile applications\n"}, {"id": "S0370269304009359", "text": "Table 1 lists 8 pairs of B decays. In fact, there are more decay pairs, since many of the particles in the final states can be observed as either pseudoscalar\u00a0(P) or vector\u00a0(V) mesons. Note that certain decays are written in terms of VV final states, while others are have PP\u00a0states. There are three reasons for this. First, some decays involve a final-state\u00a0\u03c00. However, experimentally it will be necessary to find the decay vertices of the final particles. This is virtually impossible for a\u00a0\u03c00, and so we always use a\u00a0\u03c10. Second, some pairs of decays are related by SU(3) in the SM only if an (ss\u00af) quark pair is used. However, there are no P's which are pure (ss\u00af). The mesons \u03b7 and \u03b7\u2032 have an (ss\u00af) component, but they also have significant (uu\u00af) and (dd\u00af) pieces. As a result the b\u00af\u2192s\u00af and b\u00af\u2192d\u00af decays are not really related by SU(3) in the SM if the final state involves an \u03b7 or\u00a0\u03b7\u2032. We therefore consider instead the vector meson \u03d5 which is essentially a pure (ss\u00af) quark state. Finally, we require that both B0 and B\u00af0 be able to decay to the final state. This cannot happen if the final state contains a single K0 (or K\u00af0) meson. However, it can occur if this final-state particle is an excited neutral kaon. In this case one decay involves K*0, while the other has K\u00af*0. Assuming that the vector meson is detected via its decay to \u03c8Ks\u03c00 (as in the measurement of sin2\u03b2 via Bd0(t)\u2192J/\u03c8K*), then both B0 and B\u00af0 can decay to the same final state.\n", "keywords": "B0\nB\u00af0\nB decays\ndecay\ndecay pairs\ndecays\ndecay to the final state\ndecay to \u03c8Ks\u03c00\nexcited neutral kaon\nfinal particles\nfinal-state particle\nfind the decay vertices\nK\u00af*0\nK*0\nmeasurement of sin2\u03b2\nmeson\nmesons\npairs of decays\nquark\nquark pair\nquark state\nSM\nvector meson\n\u03b7\n\u03b7\u2032\n\u03d5\n\u03c8Ks\u03c00\n"}, {"id": "S0370269304009359", "text": "Table 1 lists 8 pairs of B decays. In fact, there are more decay pairs, since many of the particles in the final states can be observed as either pseudoscalar\u00a0(P) or vector\u00a0(V) mesons. Note that certain decays are written in terms of VV final states, while others are have PP\u00a0states. There are three reasons for this. First, some decays involve a final-state\u00a0\u03c00. However, experimentally it will be necessary to find the decay vertices of the final particles. This is virtually impossible for a\u00a0\u03c00, and so we always use a\u00a0\u03c10. Second, some pairs of decays are related by SU(3) in the SM only if an (ss\u00af) quark pair is used. However, there are no P's which are pure (ss\u00af). The mesons \u03b7 and \u03b7\u2032 have an (ss\u00af) component, but they also have significant (uu\u00af) and (dd\u00af) pieces. As a result the b\u00af\u2192s\u00af and b\u00af\u2192d\u00af decays are not really related by SU(3) in the SM if the final state involves an \u03b7 or\u00a0\u03b7\u2032. We therefore consider instead the vector meson \u03d5 which is essentially a pure (ss\u00af) quark state. Finally, we require that both B0 and B\u00af0 be able to decay to the final state. This cannot happen if the final state contains a single K0 (or K\u00af0) meson. However, it can occur if this final-state particle is an excited neutral kaon. In this case one decay involves K*0, while the other has K\u00af*0. Assuming that the vector meson is detected via its decay to \u03c8Ks\u03c00 (as in the measurement of sin2\u03b2 via Bd0(t)\u2192J/\u03c8K*), then both B0 and B\u00af0 can decay to the same final state.\n", "keywords": "B0\nB\u00af0\nB decays\ndecay\ndecay pairs\ndecays\ndecay to the final state\ndecay to \u03c8Ks\u03c00\nexcited neutral kaon\nfinal particles\nfinal-state particle\nfind the decay vertices\nK\u00af*0\nK*0\nmeasurement of sin2\u03b2\nmeson\nmesons\npairs of decays\nquark\nquark pair\nquark state\nSM\nvector meson\n\u03b7\n\u03b7\u2032\n\u03d5\n\u03c8Ks\u03c00\n"}, {"id": "S0021999115008372", "text": "The need to represent scale interactions in weather and climate prediction models has, for many decades, motivated research into the use of adaptive meshes [3,34,38]. R-adaptivity \u2013 mesh redistribution \u2013 involves deforming a mesh in order to vary local resolution and was first considered for atmospheric modelling more than twenty years ago by Dietachmayer and Droegemeier [14]. It is an attractive form of adaptivity since it does not involve altering the mesh connectivity, does not create load balancing problems because points are never created or destroyed, does not require mapping of solutions between meshes [26], does not lead to sudden changes in resolution and can be retro-fitted into existing models. Variational methods exist which attempt to control resolution in different directions for r-adaptive meshes (e.g. [23,25]). Alternatively, the solution of the Monge\u2013Amp\u00e8re equation to generate an optimally transported (OT) mesh based on a scalar valued monitor function is a useful form of r-adaptive mesh generation because it generates a mesh equidistributed with respect to a monitor function and does not lead to mesh tangling [7]. We will see that the optimal transport problem on the sphere leads to a slightly different equation of Monge\u2013Amp\u00e8re type, which has not before been solved numerically on the surface of a sphere, which would be necessary for weather and climate prediction using r-adaptivity.\n", "keywords": "adaptive meshes\nadaptivity\naltering the mesh connectivity\natmospheric modelling\ncontrol resolution in different directions for r-adaptive meshes\ndeforming a mesh\nmapping of solutions between meshes\nmesh\nmeshes\nmesh redistribution\nmesh tangling\nMonge\u2013Amp\u00e8re\nMonge\u2013Amp\u00e8re equation\nmonitor function\noptimally transported (OT) mesh\noptimal transport problem on the sphere\nr-adaptive meshes\nr-adaptive mesh generation\nr-adaptivity\nR-adaptivity\nrepresent scale interactions\nscalar valued monitor function\nsphere\nVariational methods\nweather and climate prediction\nweather and climate prediction models\n"}, {"id": "S0021999115008372", "text": "The need to represent scale interactions in weather and climate prediction models has, for many decades, motivated research into the use of adaptive meshes [3,34,38]. R-adaptivity \u2013 mesh redistribution \u2013 involves deforming a mesh in order to vary local resolution and was first considered for atmospheric modelling more than twenty years ago by Dietachmayer and Droegemeier [14]. It is an attractive form of adaptivity since it does not involve altering the mesh connectivity, does not create load balancing problems because points are never created or destroyed, does not require mapping of solutions between meshes [26], does not lead to sudden changes in resolution and can be retro-fitted into existing models. Variational methods exist which attempt to control resolution in different directions for r-adaptive meshes (e.g. [23,25]). Alternatively, the solution of the Monge\u2013Amp\u00e8re equation to generate an optimally transported (OT) mesh based on a scalar valued monitor function is a useful form of r-adaptive mesh generation because it generates a mesh equidistributed with respect to a monitor function and does not lead to mesh tangling [7]. We will see that the optimal transport problem on the sphere leads to a slightly different equation of Monge\u2013Amp\u00e8re type, which has not before been solved numerically on the surface of a sphere, which would be necessary for weather and climate prediction using r-adaptivity.\n", "keywords": "adaptive meshes\nadaptivity\naltering the mesh connectivity\natmospheric modelling\ncontrol resolution in different directions for r-adaptive meshes\ndeforming a mesh\nmapping of solutions between meshes\nmesh\nmeshes\nmesh redistribution\nmesh tangling\nMonge\u2013Amp\u00e8re\nMonge\u2013Amp\u00e8re equation\nmonitor function\noptimally transported (OT) mesh\noptimal transport problem on the sphere\nr-adaptive meshes\nr-adaptive mesh generation\nr-adaptivity\nR-adaptivity\nrepresent scale interactions\nscalar valued monitor function\nsphere\nVariational methods\nweather and climate prediction\nweather and climate prediction models\n"}, {"id": "S0079642514000784", "text": "Gas sorption, storage and separation in carbon materials are mainly based on physisorption on the surfaces and particularly depend on the electrostatic and dispersion (i.e., vdW) interactions. The former can be tuned by introducing charge variations in the material, and the latter by chemical substitution. The strength of the interaction is determined by the surface characteristics of the adsorbent and the properties of targeted adsorbate molecule, including but not limited to the size and shape of the adsorbate molecule along with its polarizability, magnetic susceptibility, permanent dipole moment, and quadrupole moment. Li et al. summarise the adsorption-related physical parameters of many gas or vapour adsorbates, and herein Table 1 we show a few of those of interest, H2, N2, CO, CO2, CH4, NH3, SO2 and H2S [90]. For instance, an adsorbent with a high specific surface area is a good candidate for adsorption of a molecule with high polarizability but no polarity. Adsorbents with highly polarised surfaces are good for adsorbate molecules with a high dipole moment. The adsorbents with high electric field gradient surfaces are found to be ideal for the high quadrupole moment adsorbate molecules [91]. Normally, the binding or adsorption strength with a carbon nanostructure is relatively low for H2 and N2; intermediate for CO, CH4 and CO2; and relatively high for H2S, NH3 and H2O. Thus, surface modifications, such as doping, functionalization and improving the pore structure and specific surface area of nanocarbons, are important to enhance gas adsorption. For this purpose, graphene offers a great scope for tailor-made carbonaceous adsorbents.\n", "keywords": "adsorbate molecule\nadsorbate molecules\nadsorbent\nadsorbents\nAdsorbents\ncarbonaceous adsorbents\ncarbon materials\nCH4\nchemical substitution\nCO\nCO2\ndoping\nelectrostatic and dispersion (i.e., vdW) interactions\nenhance gas adsorption\nfunctionalization\ngas or vapour adsorbates\nGas sorption, storage and separation\ngraphene\nH2\nH2O\nH2S\nhigh quadrupole moment adsorbate molecules\nimproving the pore structure and specific surface area of nanocarbons\nintroducing charge variations in the material\nmolecule\nN2\nnanocarbons\nNH3\nphysisorption\nSO2\nsurface modifications\n"}, {"id": "S0079642514000784", "text": "Gas sorption, storage and separation in carbon materials are mainly based on physisorption on the surfaces and particularly depend on the electrostatic and dispersion (i.e., vdW) interactions. The former can be tuned by introducing charge variations in the material, and the latter by chemical substitution. The strength of the interaction is determined by the surface characteristics of the adsorbent and the properties of targeted adsorbate molecule, including but not limited to the size and shape of the adsorbate molecule along with its polarizability, magnetic susceptibility, permanent dipole moment, and quadrupole moment. Li et al. summarise the adsorption-related physical parameters of many gas or vapour adsorbates, and herein Table 1 we show a few of those of interest, H2, N2, CO, CO2, CH4, NH3, SO2 and H2S [90]. For instance, an adsorbent with a high specific surface area is a good candidate for adsorption of a molecule with high polarizability but no polarity. Adsorbents with highly polarised surfaces are good for adsorbate molecules with a high dipole moment. The adsorbents with high electric field gradient surfaces are found to be ideal for the high quadrupole moment adsorbate molecules [91]. Normally, the binding or adsorption strength with a carbon nanostructure is relatively low for H2 and N2; intermediate for CO, CH4 and CO2; and relatively high for H2S, NH3 and H2O. Thus, surface modifications, such as doping, functionalization and improving the pore structure and specific surface area of nanocarbons, are important to enhance gas adsorption. For this purpose, graphene offers a great scope for tailor-made carbonaceous adsorbents.\n", "keywords": "adsorbate molecule\nadsorbate molecules\nadsorbent\nadsorbents\nAdsorbents\ncarbonaceous adsorbents\ncarbon materials\nCH4\nchemical substitution\nCO\nCO2\ndoping\nelectrostatic and dispersion (i.e., vdW) interactions\nenhance gas adsorption\nfunctionalization\ngas or vapour adsorbates\nGas sorption, storage and separation\ngraphene\nH2\nH2O\nH2S\nhigh quadrupole moment adsorbate molecules\nimproving the pore structure and specific surface area of nanocarbons\nintroducing charge variations in the material\nmolecule\nN2\nnanocarbons\nNH3\nphysisorption\nSO2\nsurface modifications\n"}, {"id": "S0079642514000784", "text": "Gas sorption, storage and separation in carbon materials are mainly based on physisorption on the surfaces and particularly depend on the electrostatic and dispersion (i.e., vdW) interactions. The former can be tuned by introducing charge variations in the material, and the latter by chemical substitution. The strength of the interaction is determined by the surface characteristics of the adsorbent and the properties of targeted adsorbate molecule, including but not limited to the size and shape of the adsorbate molecule along with its polarizability, magnetic susceptibility, permanent dipole moment, and quadrupole moment. Li et al. summarise the adsorption-related physical parameters of many gas or vapour adsorbates, and herein Table 1 we show a few of those of interest, H2, N2, CO, CO2, CH4, NH3, SO2 and H2S [90]. For instance, an adsorbent with a high specific surface area is a good candidate for adsorption of a molecule with high polarizability but no polarity. Adsorbents with highly polarised surfaces are good for adsorbate molecules with a high dipole moment. The adsorbents with high electric field gradient surfaces are found to be ideal for the high quadrupole moment adsorbate molecules [91]. Normally, the binding or adsorption strength with a carbon nanostructure is relatively low for H2 and N2; intermediate for CO, CH4 and CO2; and relatively high for H2S, NH3 and H2O. Thus, surface modifications, such as doping, functionalization and improving the pore structure and specific surface area of nanocarbons, are important to enhance gas adsorption. For this purpose, graphene offers a great scope for tailor-made carbonaceous adsorbents.\n", "keywords": "adsorbate molecule\nadsorbate molecules\nadsorbent\nadsorbents\nAdsorbents\ncarbonaceous adsorbents\ncarbon materials\nCH4\nchemical substitution\nCO\nCO2\ndoping\nelectrostatic and dispersion (i.e., vdW) interactions\nenhance gas adsorption\nfunctionalization\ngas or vapour adsorbates\nGas sorption, storage and separation\ngraphene\nH2\nH2O\nH2S\nhigh quadrupole moment adsorbate molecules\nimproving the pore structure and specific surface area of nanocarbons\nintroducing charge variations in the material\nmolecule\nN2\nnanocarbons\nNH3\nphysisorption\nSO2\nsurface modifications\n"}, {"id": "S0079642514000784", "text": "Gas sorption, storage and separation in carbon materials are mainly based on physisorption on the surfaces and particularly depend on the electrostatic and dispersion (i.e., vdW) interactions. The former can be tuned by introducing charge variations in the material, and the latter by chemical substitution. The strength of the interaction is determined by the surface characteristics of the adsorbent and the properties of targeted adsorbate molecule, including but not limited to the size and shape of the adsorbate molecule along with its polarizability, magnetic susceptibility, permanent dipole moment, and quadrupole moment. Li et al. summarise the adsorption-related physical parameters of many gas or vapour adsorbates, and herein Table 1 we show a few of those of interest, H2, N2, CO, CO2, CH4, NH3, SO2 and H2S [90]. For instance, an adsorbent with a high specific surface area is a good candidate for adsorption of a molecule with high polarizability but no polarity. Adsorbents with highly polarised surfaces are good for adsorbate molecules with a high dipole moment. The adsorbents with high electric field gradient surfaces are found to be ideal for the high quadrupole moment adsorbate molecules [91]. Normally, the binding or adsorption strength with a carbon nanostructure is relatively low for H2 and N2; intermediate for CO, CH4 and CO2; and relatively high for H2S, NH3 and H2O. Thus, surface modifications, such as doping, functionalization and improving the pore structure and specific surface area of nanocarbons, are important to enhance gas adsorption. For this purpose, graphene offers a great scope for tailor-made carbonaceous adsorbents.\n", "keywords": "adsorbate molecule\nadsorbate molecules\nadsorbent\nadsorbents\nAdsorbents\ncarbonaceous adsorbents\ncarbon materials\nCH4\nchemical substitution\nCO\nCO2\ndoping\nelectrostatic and dispersion (i.e., vdW) interactions\nenhance gas adsorption\nfunctionalization\ngas or vapour adsorbates\nGas sorption, storage and separation\ngraphene\nH2\nH2O\nH2S\nhigh quadrupole moment adsorbate molecules\nimproving the pore structure and specific surface area of nanocarbons\nintroducing charge variations in the material\nmolecule\nN2\nnanocarbons\nNH3\nphysisorption\nSO2\nsurface modifications\n"}, {"id": "S0079642514000784", "text": "Gas sorption, storage and separation in carbon materials are mainly based on physisorption on the surfaces and particularly depend on the electrostatic and dispersion (i.e., vdW) interactions. The former can be tuned by introducing charge variations in the material, and the latter by chemical substitution. The strength of the interaction is determined by the surface characteristics of the adsorbent and the properties of targeted adsorbate molecule, including but not limited to the size and shape of the adsorbate molecule along with its polarizability, magnetic susceptibility, permanent dipole moment, and quadrupole moment. Li et al. summarise the adsorption-related physical parameters of many gas or vapour adsorbates, and herein Table 1 we show a few of those of interest, H2, N2, CO, CO2, CH4, NH3, SO2 and H2S [90]. For instance, an adsorbent with a high specific surface area is a good candidate for adsorption of a molecule with high polarizability but no polarity. Adsorbents with highly polarised surfaces are good for adsorbate molecules with a high dipole moment. The adsorbents with high electric field gradient surfaces are found to be ideal for the high quadrupole moment adsorbate molecules [91]. Normally, the binding or adsorption strength with a carbon nanostructure is relatively low for H2 and N2; intermediate for CO, CH4 and CO2; and relatively high for H2S, NH3 and H2O. Thus, surface modifications, such as doping, functionalization and improving the pore structure and specific surface area of nanocarbons, are important to enhance gas adsorption. For this purpose, graphene offers a great scope for tailor-made carbonaceous adsorbents.\n", "keywords": "adsorbate molecule\nadsorbate molecules\nadsorbent\nadsorbents\nAdsorbents\ncarbonaceous adsorbents\ncarbon materials\nCH4\nchemical substitution\nCO\nCO2\ndoping\nelectrostatic and dispersion (i.e., vdW) interactions\nenhance gas adsorption\nfunctionalization\ngas or vapour adsorbates\nGas sorption, storage and separation\ngraphene\nH2\nH2O\nH2S\nhigh quadrupole moment adsorbate molecules\nimproving the pore structure and specific surface area of nanocarbons\nintroducing charge variations in the material\nmolecule\nN2\nnanocarbons\nNH3\nphysisorption\nSO2\nsurface modifications\n"}, {"id": "S0079642514000784", "text": "Gas sorption, storage and separation in carbon materials are mainly based on physisorption on the surfaces and particularly depend on the electrostatic and dispersion (i.e., vdW) interactions. The former can be tuned by introducing charge variations in the material, and the latter by chemical substitution. The strength of the interaction is determined by the surface characteristics of the adsorbent and the properties of targeted adsorbate molecule, including but not limited to the size and shape of the adsorbate molecule along with its polarizability, magnetic susceptibility, permanent dipole moment, and quadrupole moment. Li et al. summarise the adsorption-related physical parameters of many gas or vapour adsorbates, and herein Table 1 we show a few of those of interest, H2, N2, CO, CO2, CH4, NH3, SO2 and H2S [90]. For instance, an adsorbent with a high specific surface area is a good candidate for adsorption of a molecule with high polarizability but no polarity. Adsorbents with highly polarised surfaces are good for adsorbate molecules with a high dipole moment. The adsorbents with high electric field gradient surfaces are found to be ideal for the high quadrupole moment adsorbate molecules [91]. Normally, the binding or adsorption strength with a carbon nanostructure is relatively low for H2 and N2; intermediate for CO, CH4 and CO2; and relatively high for H2S, NH3 and H2O. Thus, surface modifications, such as doping, functionalization and improving the pore structure and specific surface area of nanocarbons, are important to enhance gas adsorption. For this purpose, graphene offers a great scope for tailor-made carbonaceous adsorbents.\n", "keywords": "adsorbate molecule\nadsorbate molecules\nadsorbent\nadsorbents\nAdsorbents\ncarbonaceous adsorbents\ncarbon materials\nCH4\nchemical substitution\nCO\nCO2\ndoping\nelectrostatic and dispersion (i.e., vdW) interactions\nenhance gas adsorption\nfunctionalization\ngas or vapour adsorbates\nGas sorption, storage and separation\ngraphene\nH2\nH2O\nH2S\nhigh quadrupole moment adsorbate molecules\nimproving the pore structure and specific surface area of nanocarbons\nintroducing charge variations in the material\nmolecule\nN2\nnanocarbons\nNH3\nphysisorption\nSO2\nsurface modifications\n"}, {"id": "S0370269304008494", "text": "One major goal of current nuclear physics is the observation of at least partial restoration of chiral symmetry. Since the chiral order parameter \u3008q\u0304q\u3009 is expected to decrease by about 30% already at normal nuclear matter density [1\u20134], any in-medium change due to the dropping quark condensate should in principle be observable in photonuclear reactions. The conjecture that such a partial restoration of chiral symmetry causes a softening and narrowing of the \u03c3 meson as the chiral partner of the pion in the nuclear medium [5,6] has led to the idea of measuring the \u03c00\u03c00 invariant mass distribution near the 2\u03c0 threshold in photon induced reactions on nuclei [7]. In contrast to its questionable nature as a proper quasiparticle in vacuum, the \u03c3 meson might develop a much narrower peak at finite baryon density due to phase-space suppression for the \u03c3\u2192\u03c0\u03c0 decay, hence making it possible to explore its properties when embedded in a nuclear many-body system [8\u201311]. Measuring a threshold enhancement of the \u03c00\u03c00 invariant mass spectrum might serve as a signal for the partial restoration of chiral symmetry inside nuclei and, therefore, give information about one of the most fundamental features of QCD.\n", "keywords": "chiral order parameter\nchiral partner\nMeasuring a threshold enhancement of the \u03c00\u03c00 invariant mass spectrum\nmeasuring the \u03c00\u03c00 invariant mass distribution\nnuclear many-body system\nnuclear matter\nnuclear medium\nnuclear physics\nnuclei\nobservation of at least partial restoration of chiral symmetry\npartial restoration of chiral symmetry\nphoton\nphoton induced reactions\nphotonuclear reactions\npion\nQCD\nq\u0304q\nquark condensate\nquasiparticle\n\u03c3 meson\n\u03c3\u2192\u03c0\u03c0 decay\n"}, {"id": "S0370269304008494", "text": "One major goal of current nuclear physics is the observation of at least partial restoration of chiral symmetry. Since the chiral order parameter \u3008q\u0304q\u3009 is expected to decrease by about 30% already at normal nuclear matter density [1\u20134], any in-medium change due to the dropping quark condensate should in principle be observable in photonuclear reactions. The conjecture that such a partial restoration of chiral symmetry causes a softening and narrowing of the \u03c3 meson as the chiral partner of the pion in the nuclear medium [5,6] has led to the idea of measuring the \u03c00\u03c00 invariant mass distribution near the 2\u03c0 threshold in photon induced reactions on nuclei [7]. In contrast to its questionable nature as a proper quasiparticle in vacuum, the \u03c3 meson might develop a much narrower peak at finite baryon density due to phase-space suppression for the \u03c3\u2192\u03c0\u03c0 decay, hence making it possible to explore its properties when embedded in a nuclear many-body system [8\u201311]. Measuring a threshold enhancement of the \u03c00\u03c00 invariant mass spectrum might serve as a signal for the partial restoration of chiral symmetry inside nuclei and, therefore, give information about one of the most fundamental features of QCD.\n", "keywords": "chiral order parameter\nchiral partner\nMeasuring a threshold enhancement of the \u03c00\u03c00 invariant mass spectrum\nmeasuring the \u03c00\u03c00 invariant mass distribution\nnuclear many-body system\nnuclear matter\nnuclear medium\nnuclear physics\nnuclei\nobservation of at least partial restoration of chiral symmetry\npartial restoration of chiral symmetry\nphoton\nphoton induced reactions\nphotonuclear reactions\npion\nQCD\nq\u0304q\nquark condensate\nquasiparticle\n\u03c3 meson\n\u03c3\u2192\u03c0\u03c0 decay\n"}, {"id": "S0370269304009013", "text": "Several methods based on dynamical assumptions were suggested for determination of the P-parity of the \u0398+ [13]. According to a general theorem [14], in order to determine the parity of one particle in a binary reaction one has to know polarizations at least of two fermions participating in this reaction. Model independent methods for determination of the P-parity of the \u0398+ were suggested recently in Refs. [15,16] for pp-collision, and in Ref. [17] for photoproduction of the \u0398+. The method of Refs. [15,16], based on the assumption that the spin of the \u0398+ equals 12, suggests to measure the spin\u2013spin correlation parameter in the reaction p\u2192p\u2192\u2192\u03a3+\u0398+ near the threshold. We generalize here this method for an arbitrary spin of the \u0398+ and both isospins T=0 and T=1 of the NN channel of the NN\u2192Y\u0398+ reaction. Furthermore, we consider a polarization transfer from a nucleon to the hyperon Y in this reaction. Our consideration is model independent, since it is based only on conservation of the P-parity, total angular momentum and isospin in the reaction and the generalized Pauli principle for nucleons.\n", "keywords": "binary reaction\nconservation of the P-parity\ndetermination of the P-parity\ndetermine the parity\nfermions\ngeneral theorem\nisospin\nmeasure the spin\u2013spin correlation parameter\nModel independent methods\nnucleon\nnucleons\nparticle\nphotoproduction\npolarization transfer\npp-collision\nreaction\nthe hyperon Y\nthis reaction\ntotal angular momentum\n"}, {"id": "S0370269304009013", "text": "Several methods based on dynamical assumptions were suggested for determination of the P-parity of the \u0398+ [13]. According to a general theorem [14], in order to determine the parity of one particle in a binary reaction one has to know polarizations at least of two fermions participating in this reaction. Model independent methods for determination of the P-parity of the \u0398+ were suggested recently in Refs. [15,16] for pp-collision, and in Ref. [17] for photoproduction of the \u0398+. The method of Refs. [15,16], based on the assumption that the spin of the \u0398+ equals 12, suggests to measure the spin\u2013spin correlation parameter in the reaction p\u2192p\u2192\u2192\u03a3+\u0398+ near the threshold. We generalize here this method for an arbitrary spin of the \u0398+ and both isospins T=0 and T=1 of the NN channel of the NN\u2192Y\u0398+ reaction. Furthermore, we consider a polarization transfer from a nucleon to the hyperon Y in this reaction. Our consideration is model independent, since it is based only on conservation of the P-parity, total angular momentum and isospin in the reaction and the generalized Pauli principle for nucleons.\n", "keywords": "binary reaction\nconservation of the P-parity\ndetermination of the P-parity\ndetermine the parity\nfermions\ngeneral theorem\nisospin\nmeasure the spin\u2013spin correlation parameter\nModel independent methods\nnucleon\nnucleons\nparticle\nphotoproduction\npolarization transfer\npp-collision\nreaction\nthe hyperon Y\nthis reaction\ntotal angular momentum\n"}, {"id": "S0045782514001492", "text": "Energy conservation is critical to ensure stability of a numerical method, especially for contact and collision problems\u00a0 [28,43]. A number of conserving schemes have been developed to ensure energy conservation. These schemes make use of the penalty regulation of normal contact constraint and inherit the conservation property from continuum problems. These conservation schemes can conveniently be combined with the finite element method to simulate frictionless\u00a0 [44] and frictional\u00a0 [43] contact and collision. Hesch and Betsch\u00a0 [45] formulated the node-to-segment contact method and solved large deformation contact problems with the conserving scheme. More recently, an energy and momentum-conserving temporal discretization scheme\u00a0 [46] was developed for adhesive contact problems without considering friction and dissipation. Even though the conserving scheme improves numerical stability, it also inherits from the penalty method the difficulty of having to determine penalty parameters. In order to remove penalty sensitivity, Chawla and Laursen\u00a0 [47] proposed an energy and momentum conserving algorithm, which makes use of Lagrange multipliers instead of penalty parameters.\n", "keywords": "adhesive contact problems\nconservation schemes\nconserving scheme\ncontact and collision problems\ncontinuum problems\ndissipation\nenergy and momentum conserving algorithm\nenergy and momentum-conserving temporal discretization scheme\nenergy conservation\nEnergy conservation\nfinite element method\nfriction\nlarge deformation contact problems\nnode-to-segment contact method\npenalty regulation\nsimulate frictionless\u00a0 [44] and frictional\u00a0 [43] contact and collision\n"}, {"id": "S0045782514001492", "text": "Energy conservation is critical to ensure stability of a numerical method, especially for contact and collision problems\u00a0 [28,43]. A number of conserving schemes have been developed to ensure energy conservation. These schemes make use of the penalty regulation of normal contact constraint and inherit the conservation property from continuum problems. These conservation schemes can conveniently be combined with the finite element method to simulate frictionless\u00a0 [44] and frictional\u00a0 [43] contact and collision. Hesch and Betsch\u00a0 [45] formulated the node-to-segment contact method and solved large deformation contact problems with the conserving scheme. More recently, an energy and momentum-conserving temporal discretization scheme\u00a0 [46] was developed for adhesive contact problems without considering friction and dissipation. Even though the conserving scheme improves numerical stability, it also inherits from the penalty method the difficulty of having to determine penalty parameters. In order to remove penalty sensitivity, Chawla and Laursen\u00a0 [47] proposed an energy and momentum conserving algorithm, which makes use of Lagrange multipliers instead of penalty parameters.\n", "keywords": "adhesive contact problems\nconservation schemes\nconserving scheme\ncontact and collision problems\ncontinuum problems\ndissipation\nenergy and momentum conserving algorithm\nenergy and momentum-conserving temporal discretization scheme\nenergy conservation\nEnergy conservation\nfinite element method\nfriction\nlarge deformation contact problems\nnode-to-segment contact method\npenalty regulation\nsimulate frictionless\u00a0 [44] and frictional\u00a0 [43] contact and collision\n"}, {"id": "S0254058415300766", "text": "Half metallic ferromagnets (HMF) have attracted enormous interest due to their applications in spintronic devices [1]. Dilute magnetic semiconductors (DMSs) are considered to be the best materials to show half metallicity. These materials have two components, one being a semiconducting material with diamagnetic properties while the other is a magnetic dopant such as transition metal having un-paired d electrons [2]. The major advantage of these materials is utilization of electron's spin as information carrier since advanced functionalities in spintronic devices can be viable by the use of spin degree of freedom along with the charge of electrons [3]. The major issue regarding the applicability of these materials is to enhance the Curie temperature above room temperature. That's why the research interest shifted towards large band gap materials. A lot of work has been reported on DMSs with different II\u2013VI and III\u2013V semiconductors as host material such as, ZnS, CdS, GaN, ZnO, ZnSe, ZnTe, TiO2, SnO2 [4\u201312].\n", "keywords": "CdS\nDilute magnetic semiconductors\nDMSs\nelectrons\nenhance the Curie temperature above room temperature\nGaN\nHalf metallic ferromagnets\nHMF\nhost material\nII\u2013VI and III\u2013V semiconductors\nlarge band gap materials\nmagnetic dopant\nmaterials to show half metallicity\nsemiconducting material\nSnO2\nspintronic devices\nTiO2\ntransition metal\nun-paired d electrons\nutilization of electron's spin as information carrier\nZnO\nZnS\nZnSe\nZnTe\n"}, {"id": "S0254058415300766", "text": "Half metallic ferromagnets (HMF) have attracted enormous interest due to their applications in spintronic devices [1]. Dilute magnetic semiconductors (DMSs) are considered to be the best materials to show half metallicity. These materials have two components, one being a semiconducting material with diamagnetic properties while the other is a magnetic dopant such as transition metal having un-paired d electrons [2]. The major advantage of these materials is utilization of electron's spin as information carrier since advanced functionalities in spintronic devices can be viable by the use of spin degree of freedom along with the charge of electrons [3]. The major issue regarding the applicability of these materials is to enhance the Curie temperature above room temperature. That's why the research interest shifted towards large band gap materials. A lot of work has been reported on DMSs with different II\u2013VI and III\u2013V semiconductors as host material such as, ZnS, CdS, GaN, ZnO, ZnSe, ZnTe, TiO2, SnO2 [4\u201312].\n", "keywords": "CdS\nDilute magnetic semiconductors\nDMSs\nelectrons\nenhance the Curie temperature above room temperature\nGaN\nHalf metallic ferromagnets\nHMF\nhost material\nII\u2013VI and III\u2013V semiconductors\nlarge band gap materials\nmagnetic dopant\nmaterials to show half metallicity\nsemiconducting material\nSnO2\nspintronic devices\nTiO2\ntransition metal\nun-paired d electrons\nutilization of electron's spin as information carrier\nZnO\nZnS\nZnSe\nZnTe\n"}, {"id": "S0167931713002487", "text": "Ge (100) wafers (n- and p-type) were cleaned in ultra high vacuum (<10\u22126mbar) at 500\u00b0C and 600\u00b0C for 10min to evaporate any native oxide and so achieve an oxide free surface. Subsequently, wafers were exposed to an Al flux for a range of times to deposit ultrathin Al layers. The samples were then oxidized at ambient temperatures in the MBE load lock to produce Al2O3 layers. The samples were transferred within 1min to an Oxford Instruments OpAL reactor and thin films of HfO2 were deposited on the Al2O3 using atomic layer deposition (ALD). The HfO2 depositions used a [(CpMe)2HfOMeMe] precursor coupled with an O2 plasma as the oxidizing species. Between 30 and 130 ALD cycles were used to grow HfO2 thicknesses from 1.6 to 7nm at 250\u00b0C. For electrical measurements, circular gold contacts of area 1.96\u00d710\u22123cm2 were deposited onto the films to form MOS gate electrodes and Al was deposited on the back of the Ge wafers to provide an ohmic contact. After preliminary measurements, the samples were annealed in forming gas (FGA) at 350\u00b0C for 30min. The oxide leakage current was measured using a Keithley 230B voltage source and Keithley 617B electrometer. The HP 4192A low frequency (LF) impedance analyzer at small signal frequencies between 100Hz to 1MHz was used to perform high frequency capacitance\u2013voltage (HF CV) measurements.\n", "keywords": "Al\nAl2O3\nAl2O3 layers\nALD\nAl flux\nannealed in forming gas\natomic layer deposition\ncircular gold contacts\ncleaned in ultra high vacuum\n[(CpMe)2HfOMeMe] precursor\nelectrical measurements\nevaporate any native oxide\nFGA\nfilms\nforming gas\nGe (100) wafers\nGe wafers\ngrow HfO2 thicknesses\nHfO2\nHfO2 depositions\nhigh frequency capacitance\u2013voltage (HF CV) measurements\nHP 4192A low frequency (LF) impedance analyzer\nKeithley 230B voltage source\nKeithley 617B electrometer\nMBE load lock\nmeasured\nMOS gate electrodes\nn- and p-type\nnative oxide\nO2 plasma\nohmic contact\nOxford Instruments OpAL reactor\noxide free surface\noxide leakage current\noxidized\noxidizing species\nproduce Al2O3 layers\nsamples\nthin films of HfO2\nto perform high frequency capacitance\u2013voltage (HF CV) measurements\nultra high vacuum\nultrathin Al layers\nwafers\n"}, {"id": "S003238610801080X", "text": "Up to now, morphological studies of the multi-component polymeric materials have been carried out by various microscopic and scattering methods. Optical microscopes, transmission electron microscopes (TEMs), scanning electron microscopes (SEMs) and atomic force microscopes (AFMs) are commercially available and widely used. The biggest advantage of microscopy is that they provide intuitive real-space representations of the various morphologies. However, when it comes to \u201cmeasurements\u201d, especially in a quantitative way, microscopy sometimes lacks a statistical accuracy due to the small field of view. In contrast, the scattering methods provide much a superior statistical accuracy than that of microscopy simply because the observation volume is larger than that of the microscopes. One must remember, however, that the scattering methods normally require \u201c(hypothesized) models\u201d for data analysis in advance: They do not provide an intuitive insight into the morphologies as does microscopy. After all, for the complete characterization of a specific morphology, one may need to first know the morphologies from the microscopy and subsequently to evaluate the structural parameters by scattering on the basis of the morphology; the two methods are complementary.\n", "keywords": "AFMs\natomic force microscopes\ndata analysis\n\u201c(hypothesized) models\u201d\nmicroscopes\nmicroscopic and scattering methods\nmicroscopy\nmorphological studies\nmulti-component polymeric materials\nOptical microscopes\nscanning electron microscopes\nscattering methods\nSEMs\nTEMs\ntransmission electron microscopes\n"}, {"id": "S1570870516301822", "text": "MWSN routing protocols generally take influence from both WSN and mobile ad hoc network (MANET) routing protocols, which all share common limitations, such as bandwidth, power and cost. WSNs often share the same aim as MWSNs, in that they wish to route data from many sensors to a single sink. However, WSNs are normally considered to be static and so the associated routing protocols are often unable to cope in a mobile scenario [10]. Alternatively, MANET protocols are designed to be able to cope with the mobility of nodes, however they aim to allow end-to-end communication to occur between any two nodes [2]. This extra functionality is often not required by MWSNs and so the additional overhead is unnecessary. Combined with the high packet delivery ratios and low delays that are demanded by emerging applications, the ideal routing solution for a MWSN is one that can handle the mobility of nodes and allows data to be forwarded from the sensors to the sink in a reliable and timely manner. This set of requirements make the problem of routing in a MWSN a unique challenge, which will require new specifically designed solutions. For this reason there have been many routing protocols designed for MWSNs. As such, this section will give an overview of the current literature, which highlights the different techniques and commonly used protocols in MWSN routing.\n", "keywords": "MANET\nMANET protocols\nmobile ad hoc network\nMWSN\nMWSN routing\nMWSN routing protocols\nMWSNs\nroute data\nrouting in a MWSN\nrouting protocols\nsensors\nWSN\nWSNs\n"}, {"id": "S1746809416300933", "text": "ObjectiveElectrically evoked auditory steady-state responses (EASSRs) are neural potentials measured in the electroencephalogram (EEG) in response to periodic pulse trains presented, for example, through a cochlear implant (CI). EASSRs could potentially be used for objective CI fitting. However, EEG signals are contaminated with electrical CI artifacts. In this paper, we characterized the CI artifacts for monopolar mode stimulation and evaluated at which pulse rate, linear interpolation over the signal part contaminated with CI artifact is successful.MethodsCI artifacts were characterized by means of their amplitude growth functions and duration.ResultsCI artifact durations were between 0.7 and 1.7ms, at contralateral recording electrodes. At ipsilateral recording electrodes, CI artifact durations are range from 0.7 to larger than 2ms.ConclusionAt contralateral recording electrodes, the artifact was shorter than the interpulse interval across subjects for 500pps, which was not always the case for 900pps.SignificanceCI artifact-free EASSRs are crucial for reliable CI fitting and neuroscience research. The CI artifact has been characterized and linear interpolation allows to remove it at contralateral recording electrodes for stimulation at 500pps.\n", "keywords": "characterized the CI artifacts for monopolar mode stimulation\nCI\nCI artifact\nCI artifacts\nCI fitting\ncochlear implant\ncontralateral recording electrodes\nEASSRs\nEEG\nEEG signals\nElectrically evoked auditory steady-state responses\nelectroencephalogram\nipsilateral recording electrodes\nlinear interpolation\nneural potentials\nneuroscience research\n"}, {"id": "S0032386107010518", "text": "Copper-catalyzed Huisgen cycloadditions have been recently extensively studied by polymer chemists for the synthesis of functional polymers (either end-functional or side-functional). The post-functionalization of synthetic polymers is an important feature of macromolecular engineering as many polymerization mechanisms are rather sensitive to the presence of bulky or functional groups. For example, a wide variety of telechelic polymers (i.e. polymers with defined chain-ends) can be efficiently prepared using a combination of atom transfer radical polymerization (ATRP) and CuAAC. This strategy was independently reported in early 2005 by van Hest and Opsteen [31], Lutz et\u00a0al. [32], and Matyjaszewski et\u00a0al. [33]. Such step was important since ATRP is a very popular polymerization method in modern materials science [34,35]. Indeed, ATRP is a facile technique, which allows the preparation of well-defined polymers with narrow molecular weight distribution, predictable chain length, controlled microstructure, defined chain-ends and controlled architecture [36\u201341]. However, the range of possibilities of ATRP can be further broadened by CuAAC. For instance, the \u03c9-bromine chain-ends of polymers prepared by ATRP can be transformed into azides by nucleophilic substitution and subsequently reacted with functional alkynes (Scheme 3) [32]. Due to the very high chemoselectivity of CuAAC, this method is highly modular and may be used to synthesize a wide range of \u03c9-functional polymers. Moreover, the formed triazole rings are not \u201cpassive\u201d spacers but interesting functions exhibiting H-bonds capability, aromaticity and rigidity.\n", "keywords": "alkynes\natom transfer radical polymerization\nATRP\nazides\nCopper-catalyzed Huisgen cycloadditions\nCuAAC\nfunctional polymers\nH-bonds\nmacromolecular engineering\nnucleophilic substitution\npolymerization\npolymers\npolymers with defined chain-ends\npreparation of well-defined polymers\nsynthesis of functional polymers\nsynthetic polymers\ntelechelic polymers\ntriazole rings\n\u03c9-bromine chain-ends of polymers\n\u03c9-functional polymers\n"}, {"id": "S0032386109005485", "text": "Inverse miniemulsion polymerization is a water-in-oil (W/O) heterogeneous polymerization process that forms kinetically stable macroemulsions at, below, or around the critical micellar concentration (CMC). This process contains aqueous droplets (including water-soluble monomers) stably dispersed, with the aid of oil-soluble surfactants, in a continuous organic medium. Stable inverse miniemulsions are formed under high shear by either a homogenizer or a high speed mechanical stirrer. Oil-soluble nonionic surfactants with hydrophilic-lipophilic balance (HLB) value around 4 are used to implement colloidal stability of the resulting inverse emulsion. Upon addition of radical initiators, polymerization occurs within the aqueous droplets producing colloidal particles (Fig.\u00a02) [83]. Several reports have demonstrated the preparation of stable particles of hydrophilic and water-soluble polymers [86\u201389], polyaniline nanoparticles [90], and organic\u2013inorganic hybrid particles [91\u201393]. This method also allows for the preparation of crosslinked microgels in the presence of difunctional crosslinkers [27,94\u2013100]. In addition, CRP techniques including ATRP [78,79,82,101,102] and RAFT [103] in inverse miniemulsion have been explored to prepare well-defined nanoparticles and nanogels.\n", "keywords": "aqueous droplets\nATRP\nCMC\ncolloidal particles\ncritical micellar concentration\ncrosslinked microgels\nCRP techniques\ndifunctional crosslinkers\nheterogeneous polymerization process\nhigh speed mechanical stirrer\nhomogenizer\nimplement colloidal stability of the resulting inverse emulsion\ninverse emulsion\ninverse miniemulsion\nInverse miniemulsion polymerization\nkinetically stable macroemulsions\nnanogels\nnanoparticles\nOil-soluble nonionic surfactants\noil-soluble surfactants\norganic\u2013inorganic hybrid particles\norganic medium\npolyaniline nanoparticles\npolymerization\npreparation of crosslinked microgels in the presence of difunctional crosslinkers\nradical initiators\nRAFT\nStable inverse miniemulsions\nstable particles of hydrophilic and water-soluble polymers\nwater-in-oil\nwater-soluble monomers\nW/O\n"}, {"id": "S0032386109005485", "text": "Inverse miniemulsion polymerization is a water-in-oil (W/O) heterogeneous polymerization process that forms kinetically stable macroemulsions at, below, or around the critical micellar concentration (CMC). This process contains aqueous droplets (including water-soluble monomers) stably dispersed, with the aid of oil-soluble surfactants, in a continuous organic medium. Stable inverse miniemulsions are formed under high shear by either a homogenizer or a high speed mechanical stirrer. Oil-soluble nonionic surfactants with hydrophilic-lipophilic balance (HLB) value around 4 are used to implement colloidal stability of the resulting inverse emulsion. Upon addition of radical initiators, polymerization occurs within the aqueous droplets producing colloidal particles (Fig.\u00a02) [83]. Several reports have demonstrated the preparation of stable particles of hydrophilic and water-soluble polymers [86\u201389], polyaniline nanoparticles [90], and organic\u2013inorganic hybrid particles [91\u201393]. This method also allows for the preparation of crosslinked microgels in the presence of difunctional crosslinkers [27,94\u2013100]. In addition, CRP techniques including ATRP [78,79,82,101,102] and RAFT [103] in inverse miniemulsion have been explored to prepare well-defined nanoparticles and nanogels.\n", "keywords": "aqueous droplets\nATRP\nCMC\ncolloidal particles\ncritical micellar concentration\ncrosslinked microgels\nCRP techniques\ndifunctional crosslinkers\nheterogeneous polymerization process\nhigh speed mechanical stirrer\nhomogenizer\nimplement colloidal stability of the resulting inverse emulsion\ninverse emulsion\ninverse miniemulsion\nInverse miniemulsion polymerization\nkinetically stable macroemulsions\nnanogels\nnanoparticles\nOil-soluble nonionic surfactants\noil-soluble surfactants\norganic\u2013inorganic hybrid particles\norganic medium\npolyaniline nanoparticles\npolymerization\npreparation of crosslinked microgels in the presence of difunctional crosslinkers\nradical initiators\nRAFT\nStable inverse miniemulsions\nstable particles of hydrophilic and water-soluble polymers\nwater-in-oil\nwater-soluble monomers\nW/O\n"}, {"id": "S221267161200220X", "text": "For providing the government with effective monitoring of the trends of the economic variables in the future and good reference for developing a reasonable policy, in this paper, we establish a time series model on China's Foreign Direct Investment (FDI) by using wavelet analysis and intervention analysis and time series analysis and predict the trend of FDI in the next several years. This model eliminates the interference of noise for predicting by using wavelet analysis, and describes the autocorrelation and time-varying volatility of the financial time series by using ARIMA- GARCH-M model. The simulation results show that this model explains the dynamic structure of China's FDI trends well.", "keywords": "ARIMA- GARCH-M model\nautocorrelation\ndeveloping a reasonable policy\neffective monitoring\nestablish a time series model\nFDI\nfinancial time series\nForeign Direct Investment\ninterference of noise\nintervention analysis\npredict the trend of FDI\ntime series analysis\ntime series model\ntime-varying volatility\ntrends of the economic variables\nwavelet analysis\n"}, {"id": "S2212667812000949", "text": "By referring to many relevant data and essays, this paper aims at discussing and analyzing the importance of hip-push applied in walking race,based on the point of the view on sports biomechanics .With redard to the existing problems,the authors have made an objective analysis on the sports-biomechanics factors that can influence the race,hoping to provide a theoretical basis for the deep development and training of walking race.", "keywords": "data and essays\ndeep development and training of walking race\ndiscussing and analyzing the importance of hip-push applied in walking race\nhip-push\nobjective analysis on the sports-biomechanics factors\nsports biomechanics\ntheoretical basis\n"}, {"id": "S2212667812000949", "text": "By referring to many relevant data and essays, this paper aims at discussing and analyzing the importance of hip-push applied in walking race,based on the point of the view on sports biomechanics .With redard to the existing problems,the authors have made an objective analysis on the sports-biomechanics factors that can influence the race,hoping to provide a theoretical basis for the deep development and training of walking race.", "keywords": "data and essays\ndeep development and training of walking race\ndiscussing and analyzing the importance of hip-push applied in walking race\nhip-push\nobjective analysis on the sports-biomechanics factors\nsports biomechanics\ntheoretical basis\n"}, {"id": "S2212667812000949", "text": "By referring to many relevant data and essays, this paper aims at discussing and analyzing the importance of hip-push applied in walking race,based on the point of the view on sports biomechanics .With redard to the existing problems,the authors have made an objective analysis on the sports-biomechanics factors that can influence the race,hoping to provide a theoretical basis for the deep development and training of walking race.", "keywords": "data and essays\ndeep development and training of walking race\ndiscussing and analyzing the importance of hip-push applied in walking race\nhip-push\nobjective analysis on the sports-biomechanics factors\nsports biomechanics\ntheoretical basis\n"}, {"id": "S2212667812000949", "text": "By referring to many relevant data and essays, this paper aims at discussing and analyzing the importance of hip-push applied in walking race,based on the point of the view on sports biomechanics .With redard to the existing problems,the authors have made an objective analysis on the sports-biomechanics factors that can influence the race,hoping to provide a theoretical basis for the deep development and training of walking race.", "keywords": "data and essays\ndeep development and training of walking race\ndiscussing and analyzing the importance of hip-push applied in walking race\nhip-push\nobjective analysis on the sports-biomechanics factors\nsports biomechanics\ntheoretical basis\n"}, {"id": "S0370269304008731", "text": "A scenario is proposed for bi-large lepton mixing in the framework of nearly threefold degenerate Majorana neutrinos. In our proposal, we impose Z3 symmetry in the neutrino sector at a high energy scale to account for the threefold degenerate neutrinos and the maximal mixing between \u03bd\u03bc and \u03bd\u03c4. In order to obtain the atmospheric neutrino mass splitting while keeping the maximal mixing between \u03bd\u03bc and \u03bd\u03c4, we introduce a small perturbation to the neutrino mass matrix without breaking Z3 symmetry. On the other hand, the solar neutrino mixing arises due to the non-diagonal charged lepton mass matrix, and the desirable large mixing and mass splitting for the solar neutrino oscillation can be obtained by radiative corrections.", "keywords": "atmospheric neutrino mass splitting while keeping the maximal mixing between \u03bd\u03bc and \u03bd\u03c4\nbi-large lepton mixing\ndegenerate Majorana neutrinos\ndegenerate neutrinos\nintroduce a small perturbation to the neutrino mass matrix without breaking Z3 symmetry\nlarge mixing and mass splitting for the solar neutrino oscillation\nneutrino\nneutrino mass matrix\nneutrino sector\nnon-diagonal charged lepton mass matrix\nradiative corrections\nsolar neutrino\nsolar neutrino mixing\n"}, {"id": "S0370269304006720", "text": "Classical, two-dimensional sigma models on compact symmetric spaces G/H are integrable by virtue of conserved quantities which can arise as integrals of local or non-local functions of the underlying fields (the accounts in [1\u20135] contain references to the extensive literature). Since these models are asymptotically free and strongly coupled in the infrared, their quantum properties are not straightforward to determine. Nevertheless, following L\u00fcscher [6], Abdalla, Forger and Gomes showed [7] that, in a G/H sigma model with H simple,11Here, and throughout this Letter, we shall use \u2018simple\u2019 to mean that the corresponding Lie algebra has no non-trivial ideals. Hence U(1) is simple in our terminology, in addition to the usual non-Abelian simple groups of the Cartan\u2013Killing classification [13]. the first conserved non-local charge survives quantization (after an appropriate renormalization [6\u20138]), which suffices to ensure quantum integrability of the theory. By contrast, calculations using the 1/N expansion reveal anomalies that spoil the conservation of the quantum non-local charges in the CPN\u22121=SU(N)/SU(N\u22121)\u00d7U(1) models for N>2, and in the wider class of theories based on the complex Grassmannians SU(N)/SU(n)\u00d7SU(N\u2212n)\u00d7U(1) for N>n>1 [9].\n", "keywords": "calculations using the 1/N expansion\ncomplex Grassmannians SU(N)/SU(n)\u00d7SU(N\u2212n)\u00d7U(1) for N>n>1\nconservation of the quantum non-local charges\ncorresponding Lie algebra has no non-trivial ideals\nCPN\u22121=SU(N)/SU(N\u22121)\u00d7U(1) models for N>2\nG/H sigma model with H simple\nintegrals of local or non-local functions\nquantization\nquantum integrability\nrenormalization\nsimple\ntwo-dimensional sigma models on compact symmetric spaces G/H\n"}, {"id": "S0370269304006720", "text": "Classical, two-dimensional sigma models on compact symmetric spaces G/H are integrable by virtue of conserved quantities which can arise as integrals of local or non-local functions of the underlying fields (the accounts in [1\u20135] contain references to the extensive literature). Since these models are asymptotically free and strongly coupled in the infrared, their quantum properties are not straightforward to determine. Nevertheless, following L\u00fcscher [6], Abdalla, Forger and Gomes showed [7] that, in a G/H sigma model with H simple,11Here, and throughout this Letter, we shall use \u2018simple\u2019 to mean that the corresponding Lie algebra has no non-trivial ideals. Hence U(1) is simple in our terminology, in addition to the usual non-Abelian simple groups of the Cartan\u2013Killing classification [13]. the first conserved non-local charge survives quantization (after an appropriate renormalization [6\u20138]), which suffices to ensure quantum integrability of the theory. By contrast, calculations using the 1/N expansion reveal anomalies that spoil the conservation of the quantum non-local charges in the CPN\u22121=SU(N)/SU(N\u22121)\u00d7U(1) models for N>2, and in the wider class of theories based on the complex Grassmannians SU(N)/SU(n)\u00d7SU(N\u2212n)\u00d7U(1) for N>n>1 [9].\n", "keywords": "calculations using the 1/N expansion\ncomplex Grassmannians SU(N)/SU(n)\u00d7SU(N\u2212n)\u00d7U(1) for N>n>1\nconservation of the quantum non-local charges\ncorresponding Lie algebra has no non-trivial ideals\nCPN\u22121=SU(N)/SU(N\u22121)\u00d7U(1) models for N>2\nG/H sigma model with H simple\nintegrals of local or non-local functions\nquantization\nquantum integrability\nrenormalization\nsimple\ntwo-dimensional sigma models on compact symmetric spaces G/H\n"}, {"id": "S0749603615302184", "text": "However, the measured reflectivity is less than the predicted value (\u223c96%), which is likely to relate to, amongst other factors, the roughness of the GaN/AlN interfaces particularly for the first layer in the DBR stack and the non-uniformity of the DBR layer thicknesses. Using STEM measurements of the thickness of each layer (on the a-plane) through the thickness of the stack, we calculate a new model (green curve) in which the overall reflectivity is reduced to 85%. This implies that variations in layer thickness through the stack are the main source of the reduced reflectivity in comparison to the model. In fact, a closer look at the cross-sectional STEM data and a careful extraction of layer thickness have revealed that whilst the layer thicknesses are fairly consistent through the DBR stack in the wing regions, there is a monotonic variation in the measured layer thicknesses in the window regions. (The GaN layer width smoothly increases, while the AlN layer thickness decreases through the DBR stack.). This observation could potentially be of practical importance, for samples grown on templates with a uniform defect density, as one could achieve much better reflectivities simply by altering the growth time to counteract the change in growth rate. This possibility is the subject of ongoing investigations. In addition, the presence of cracks and trenches in the top surface may also reduce the measured reflectivity further.\n", "keywords": "AlN layer\naltering the growth time\ncalculate a new model\ncross-sectional STEM data\nDBR layer\nDBR stack\nDBR stack in the wing regions\nextraction of layer thickness\nGaN/AlN interfaces\nGaN layer\nSTEM measurements\nthe window regions\n"}, {"id": "S0039602899010869", "text": "The final contribution to the force is the van der Waals interaction. It includes the following contributions: (i) between the macroscopic Si tip of conical shape with the sphere of radius R at the end [27] and semi-infinite substrate; (ii) the dispersion forces between the atoms in the sample treated atomistically; and (iii) the interaction between the macroscopic part of the tip and the sample atoms. The first contribution is calculated analytically [27]. In fact, the macroscopic contribution to the van der Waals force is the same in each of the three systems described below, as it depends only on the tip\u2013surface separation, macroscopic sphere radius, cone-angle and Hamaker constant of the system [27]. All these quantities are identical in each system we look at, so that the van der Waals force acts as a background attractive force independent of the microscopic properties of the system [8]. The Hamaker constant needed for the calculation of the macroscopic van der Waals force is estimated to be 0.5eV [32].\n", "keywords": "atoms\ncalculation\ndispersion forces\nmacroscopic part of the tip\nmacroscopic Si tip\nmacroscopic van der Waals force\nsample atoms\nsemi-infinite substrate\ntip\u2013surface separation\nvan der Waals force\nvan der Waals interaction\n"}, {"id": "S0039602899010869", "text": "The final contribution to the force is the van der Waals interaction. It includes the following contributions: (i) between the macroscopic Si tip of conical shape with the sphere of radius R at the end [27] and semi-infinite substrate; (ii) the dispersion forces between the atoms in the sample treated atomistically; and (iii) the interaction between the macroscopic part of the tip and the sample atoms. The first contribution is calculated analytically [27]. In fact, the macroscopic contribution to the van der Waals force is the same in each of the three systems described below, as it depends only on the tip\u2013surface separation, macroscopic sphere radius, cone-angle and Hamaker constant of the system [27]. All these quantities are identical in each system we look at, so that the van der Waals force acts as a background attractive force independent of the microscopic properties of the system [8]. The Hamaker constant needed for the calculation of the macroscopic van der Waals force is estimated to be 0.5eV [32].\n", "keywords": "atoms\ncalculation\ndispersion forces\nmacroscopic part of the tip\nmacroscopic Si tip\nmacroscopic van der Waals force\nsample atoms\nsemi-infinite substrate\ntip\u2013surface separation\nvan der Waals force\nvan der Waals interaction\n"}, {"id": "S0039602899010869", "text": "The final contribution to the force is the van der Waals interaction. It includes the following contributions: (i) between the macroscopic Si tip of conical shape with the sphere of radius R at the end [27] and semi-infinite substrate; (ii) the dispersion forces between the atoms in the sample treated atomistically; and (iii) the interaction between the macroscopic part of the tip and the sample atoms. The first contribution is calculated analytically [27]. In fact, the macroscopic contribution to the van der Waals force is the same in each of the three systems described below, as it depends only on the tip\u2013surface separation, macroscopic sphere radius, cone-angle and Hamaker constant of the system [27]. All these quantities are identical in each system we look at, so that the van der Waals force acts as a background attractive force independent of the microscopic properties of the system [8]. The Hamaker constant needed for the calculation of the macroscopic van der Waals force is estimated to be 0.5eV [32].\n", "keywords": "atoms\ncalculation\ndispersion forces\nmacroscopic part of the tip\nmacroscopic Si tip\nmacroscopic van der Waals force\nsample atoms\nsemi-infinite substrate\ntip\u2013surface separation\nvan der Waals force\nvan der Waals interaction\n"}, {"id": "S221450951400014X", "text": "This figure demonstrates that changes in the measure of bitumen content create sizable differences in the stiffness modulus of asphaltic samples that include waste glass cullet. As the percentage of glass increases, the measure of the stiffness modulus of modified asphalt increases too. But with pass of optimum measure of glass the stiffness modulus of asphaltic samples decrease. This trend in total of percentages of bitumen content is existing. Due to that waste glass cullet has no suction; the trend does not extend to measuring the stiffness modulus of asphaltic samples including waste glass cullet with different percentage of bitumen content. Glass particles do not absorb any bituminous material, so it is necessary to decrease the bitumen content with the addition of glass cullet. According to Fig. 2 and the results of the Marshall tests, the optimum bitumen measures decrease significantly in samples that include higher percentages of waste glass cullet. As the percentage of optimum bitumen content is 1% more in samples without waste glass cullet in comparison with saphaltic samples that include 20% waste glass cullet. The stiffness modulus of asphaltic samples that include waste glass cullet increased due to additional interlocking between the aggregate and the angularity of particles of glass cullet content. The increase in the intrusive friction angle because of the glass particles\u2019 increased angularity is the main reason for the addition of the stiffness modulus of asphaltic samples that include waste glass cullet. But as the percentage of glass content reaches greater than 15%, the particles\u2019 abundance cause slip these particles on together. The stiffness modulus of samples decreases as the percentage of glass cullet increases. The variations in the stiffness modulus of asphaltic samples that include different percentages of waste glass cullet at different temperature are shown in Fig. 3.\n", "keywords": "asphaltic samples\nbitumen\nbituminous material\nchanges in the measure of bitumen content\ndifferences in the stiffness modulus of asphaltic samples\nglass\nglass cullet\nglass particles\u2019\nGlass particles\ninterlocking\nMarshall tests\nmodified asphalt\nparticles\nsaphaltic samples\nstiffness modulus\nwaste glass cullet\n"}, {"id": "S0010938X1500195X", "text": "Poor oxidation behavior is the major barrier to the increased use of Ti-based alloys in high-temperature structural applications. The demand to increase the service temperature of these alloys beyond 550\u00b0C (the typical temperature limit) requires careful study to understand the role that composition has on the oxidation behavior of Ti-based alloys [1\u20133]. The attempt to overcome this limitation in Ti-based alloys has led to the production of alloys with substantially improved oxidation resistance such as \u03b2-21S and also development of coatings and pre-oxidation techniques [1,4\u20136]. While it is tempting to extrapolate the oxidation behavior (e.g. oxidation rate law, depth of oxygen ingress and scale thickness) observed for a limited number of compositions under a certain oxidation condition to a broader compositional range, there are numerous examples in the literature where deviations from the expected relations are observed [7,8].\n", "keywords": "alloys\nalloys with substantially improved oxidation resistance\ncoatings\ndevelopment of coatings and pre-oxidation techniques\noxidation\noxygen\npre-oxidation techniques\nTi-based alloys\nunderstand the role that composition has on the oxidation behavior of Ti-based alloys\n\u03b2-21S\n"}, {"id": "S0010938X1500195X", "text": "Poor oxidation behavior is the major barrier to the increased use of Ti-based alloys in high-temperature structural applications. The demand to increase the service temperature of these alloys beyond 550\u00b0C (the typical temperature limit) requires careful study to understand the role that composition has on the oxidation behavior of Ti-based alloys [1\u20133]. The attempt to overcome this limitation in Ti-based alloys has led to the production of alloys with substantially improved oxidation resistance such as \u03b2-21S and also development of coatings and pre-oxidation techniques [1,4\u20136]. While it is tempting to extrapolate the oxidation behavior (e.g. oxidation rate law, depth of oxygen ingress and scale thickness) observed for a limited number of compositions under a certain oxidation condition to a broader compositional range, there are numerous examples in the literature where deviations from the expected relations are observed [7,8].\n", "keywords": "alloys\nalloys with substantially improved oxidation resistance\ncoatings\ndevelopment of coatings and pre-oxidation techniques\noxidation\noxygen\npre-oxidation techniques\nTi-based alloys\nunderstand the role that composition has on the oxidation behavior of Ti-based alloys\n\u03b2-21S\n"}, {"id": "S0010938X1500195X", "text": "Poor oxidation behavior is the major barrier to the increased use of Ti-based alloys in high-temperature structural applications. The demand to increase the service temperature of these alloys beyond 550\u00b0C (the typical temperature limit) requires careful study to understand the role that composition has on the oxidation behavior of Ti-based alloys [1\u20133]. The attempt to overcome this limitation in Ti-based alloys has led to the production of alloys with substantially improved oxidation resistance such as \u03b2-21S and also development of coatings and pre-oxidation techniques [1,4\u20136]. While it is tempting to extrapolate the oxidation behavior (e.g. oxidation rate law, depth of oxygen ingress and scale thickness) observed for a limited number of compositions under a certain oxidation condition to a broader compositional range, there are numerous examples in the literature where deviations from the expected relations are observed [7,8].\n", "keywords": "alloys\nalloys with substantially improved oxidation resistance\ncoatings\ndevelopment of coatings and pre-oxidation techniques\noxidation\noxygen\npre-oxidation techniques\nTi-based alloys\nunderstand the role that composition has on the oxidation behavior of Ti-based alloys\n\u03b2-21S\n"}, {"id": "S0370269304009177", "text": "In the brane system appearing in string/D-brane theory, the stableness is the most important requirement. We find some stable brane configurations in the SUSY bulk-boundary theory. We systematically solve the singular field equation using a general mathematical result about the free-wave solution in S1/Z2-space. The two scalars, the extra-component of the bulk-vector (A5) and the bulk-scalar (\u03a6), constitute the solutions. Their different roles are clarified. The importance of the \u201cparallel\u201d configuration is disclosed. The boundary condition (of A5) and the boundary matter fields are two important elements for making the localized configuration. Among all solutions, the solution (c1=\u22121, c2=\u22121) is expected to be the thin-wall limit of a kink solution. We present a bulk Higgs model corresponding to the non-singular solution. The model is expected to give a non-singular and stable brane solution in the SUSY bulk-boundary theory.\n", "keywords": "A5\nboundary condition\nbrane system\nbulk Higgs model\nbulk-scalar\nc1=\u22121, c2=\u22121\ndifferent roles are clarified\nextra-component of the bulk-vector\ngeneral mathematical result about the free-wave solution\nkink solution\nnon-singular and stable brane solution\nnon-singular solution\nsolution\nsolve the singular field equation\nstable brane configurations\nstableness\nstring/D-brane theory\nSUSY bulk-boundary theory\nthin-wall limit\n\u03a6\n"}, {"id": "S2212667814001488", "text": "This paper presents general results on the Java source code snippet detection problem. We propose the tool which uses graph and subgraph isomorphism detection. A number of solutions for all of these tasks have been proposed in the literature. However, although that all these solutions are really fast, they compare just the constant static trees. Our solution offers to enter an input sample dynamically with the Scripthon language while preserving an acceptable speed. We used several optimizations to achieve very low number of comparisons during the matching algorithm.\n", "keywords": "compare just the constant static trees\ncomparisons\nenter an input sample dynamically\ngraph and subgraph isomorphism detection\nJava source code snippet detection\nmatching algorithm\npreserving an acceptable speed\nScripthon language\nseveral optimizations\n"}, {"id": "S2212667814001488", "text": "This paper presents general results on the Java source code snippet detection problem. We propose the tool which uses graph and subgraph isomorphism detection. A number of solutions for all of these tasks have been proposed in the literature. However, although that all these solutions are really fast, they compare just the constant static trees. Our solution offers to enter an input sample dynamically with the Scripthon language while preserving an acceptable speed. We used several optimizations to achieve very low number of comparisons during the matching algorithm.\n", "keywords": "compare just the constant static trees\ncomparisons\nenter an input sample dynamically\ngraph and subgraph isomorphism detection\nJava source code snippet detection\nmatching algorithm\npreserving an acceptable speed\nScripthon language\nseveral optimizations\n"}, {"id": "S0021999113003422", "text": "Similar numerical oscillations to those described above also emerge in the ISPM when utilising classical IBM kernels due to their lack of regularity (with discontinuous second derivatives). Furthermore, it is important to remark that the immersed structure stresses are captured in the Lagrangian description and hence, in order to compute them accurately, it is important to ensure that these spurious oscillations are not introduced via the kernel interpolation functions. In this paper, the authors have specifically designed a new family of kernel functions which do not introduce these spurious oscillations. The kernel functions are obtained by taking into account discrete reproducibility conditions as originally introduced by Peskin [14] (in our case, tailor-made for Cartesian staggered grids) and regularity requirements to prevent the appearance of spurious oscillations when computing derivatives. A Maple computer program has been developed to obtain explicit expressions for the new kernels.\n", "keywords": "a new family of kernel functions which do not introduce these spurious oscillations\nCartesian staggered grids\ncomputing derivatives\nIBM kernels\nISPM\nkernel functions\nkernel interpolation functions\nkernels\nLagrangian description\nMaple computer program\nnumerical oscillations\nobtain explicit expressions for the new kernels\noscillations\n"}, {"id": "S0021999113003422", "text": "Similar numerical oscillations to those described above also emerge in the ISPM when utilising classical IBM kernels due to their lack of regularity (with discontinuous second derivatives). Furthermore, it is important to remark that the immersed structure stresses are captured in the Lagrangian description and hence, in order to compute them accurately, it is important to ensure that these spurious oscillations are not introduced via the kernel interpolation functions. In this paper, the authors have specifically designed a new family of kernel functions which do not introduce these spurious oscillations. The kernel functions are obtained by taking into account discrete reproducibility conditions as originally introduced by Peskin [14] (in our case, tailor-made for Cartesian staggered grids) and regularity requirements to prevent the appearance of spurious oscillations when computing derivatives. A Maple computer program has been developed to obtain explicit expressions for the new kernels.\n", "keywords": "a new family of kernel functions which do not introduce these spurious oscillations\nCartesian staggered grids\ncomputing derivatives\nIBM kernels\nISPM\nkernel functions\nkernel interpolation functions\nkernels\nLagrangian description\nMaple computer program\nnumerical oscillations\nobtain explicit expressions for the new kernels\noscillations\n"}, {"id": "S0377221716301357", "text": "As mentioned earlier, this paper represents ongoing efforts to efficiently address the stochastic MPSP. Future work may consider investigating whether the algorithm would be as successful or not in solving variants of the MPSP that include more operational constraints, such as variable cut-off grade, grade blending, and stockpiling, as it is in solving the \u201cclassical\u201d variant considered in this paper. Indeed, it is a general-purpose algorithm and should be applicable to any of these variants. Other research avenues include considering other strategies for updating the penalties within PH and other methods for solving the sub-problems. Finally, another important research direction is the development of other efficient solution approaches. Since it has been observed empirically that the problem formulation often achieves small integrality gaps, one approach could be to solve the linear relaxation of the problem using an efficient algorithm and then to use an LP-rounding procedure to get an integer solution.\n", "keywords": "general-purpose algorithm\ngrade blending\nLP-rounding procedure\nMPSP\noperational constraints\nPH\nsolve the linear relaxation\nstochastic MPSP\nstockpiling\nvariable cut-off grade\n"}, {"id": "S0045782514000607", "text": "As mentioned previously, the weakly penalized system can be thought of as a generalized formulation which can result in the PL, penalty or statically condensed PL formulations depending on the choice of the projection operator. The equivalence of these methods under the weakly penalized regime, allows us to combine and take advantage of the good characteristics of each method. For instance, the weakly penalized formulation combines the simplified structure of the penalty method with the convergence characteristics of the PL formulation. However, due to the stiffness of the linear system at high values of the bulk modulus, the penalized formulations (classic penalty/weakly penalized) exhibit deteriorated nonlinear convergence. This stands in stark contrast to the PL method which (for inf\u2013sup stable schemes) exhibits fast convergence even for high bulk modulus. However, we observe that, when the choice of \u03c0h provides equivalence with the discrete PL method, poor nonlinear convergence is observed though, in principle, the convergence should be similar. Examining the update formulae for both weakly penalized and PL approaches (see Appendix C), we observe that deteriorated convergence stems from: (1) initial residual amplification, and (2) the amplification of the residual.\n", "keywords": "amplification of the residual\nclassic penalty/weakly penalized\nconvergence\ndeteriorated convergence\ngeneralized formulation\ninf\u2013sup stable schemes\ninitial residual amplification\nnonlinear convergence\npenalized formulations\npenalty method\nPL\nPL formulation\nPL, penalty or statically condensed PL formulations\npoor nonlinear convergence\nweakly penalized\nweakly penalized and PL approaches\nweakly penalized formulation\n"}, {"id": "S1361841516300342", "text": "Probabilistic and stochastic approaches can facilitate the search for local and global optima. Evolutionary algorithms, such as genetic population (Jomier et\u00a0al., 2006; Rivest-Henault et\u00a0al., 2012; Ruijters et\u00a0al., 2009), are considered as a strategy that is \u201cless likely to get stuck in a local optimum\u201d (Ruijters\u00a0et\u00a0al., 2009). A cost function consisting of the \u201csum of the Gaussian-blurred intensity values in the [DSA] at the projected model points\u201d (Jomier\u00a0et\u00a0al., 2006) is optimized using a genetic algorithm optimizer. Other authors \u201cuse the Condensation form of sequential Monte Carlo sampling to estimate a cost function gradient\u201d (Florin\u00a0et\u00a0al., 2005) for finding the global minimum. Besides, the Kalman filter is successfully adopted (Curwen et\u00a0al., 1994; Feldmar et\u00a0al., 1997; Toledo et\u00a0al., 1998).\n", "keywords": "Condensation form of sequential Monte Carlo sampling\ncost function\nestimate a cost function gradien\nEvolutionary algorithms\ngenetic algorithm optimizer\ngenetic population\nKalman filter\nProbabilistic and stochastic approaches\nsearch for local and global optima\n"}, {"id": "S2212671612000741", "text": "A sentence alignment model based on combined clues and Kernel Extensional Matrix Matching (KEMM) method is proposed. In this model, a similarity matrix for sentence aligning is formed by the similarities of bilingual sentences calculated by the combined clues, such as lexicon, morphology, length and special symbols, etc.; then this similarity matrix is used to construct a select matrix for sentence aligning; finally, obtains the sentence alignments by KEMM. Experimental results illustrated that our model outperforms over the Gale's system on handling any types of sentence alignments, with 30% total sentence alignment error rate decreasing.A sentence alignment model based on combined clues and Kernel Extensional Matrix Matching (KEMM) method is proposed. In this model, a similarity matrix for sentence aligning is formed by the similarities of bilingual sentences calculated by the combined clues, such as lexicon, morphology, length and special symbols, etc.; then this similarity matrix is used to construct a select matrix for sentence aligning; finally, obtains the sentence alignments by KEMM. Experimental results illustrated that our model outperforms over the Gale's system on handling any types of sentence alignments, with 30% total sentence alignment error rate decreasing.\n", "keywords": "construct a select matrix\nGale's system\nKEMM\nKernel Extensional Matrix Matching\nsentence aligning\nsentence alignment\nsimilarity matrix\n"}, {"id": "S0370269304009189", "text": "The spins and parities of \u0398+ and \u039e\u2212\u2212 are not yet known experimentally. In this new wave of pentaquark research, most theoretical papers take the spin equal to\u00a01/2. The parity is more controversial. In chiral soliton or Skyrme models the parity is positive\u00a0[4]. In constituent quark models it is usually positive. In the present approach, the parity of the pentaquark is given by P=(\u2212)\u2113+1, where \u2113 is the angular momentum associated with the relative coordinates of the q4 subsystem. We analyze the case where the subsystem of four light quarks is in a state of orbital symmetry [31]O and carries an angular momentum \u2113=1. Although the kinetic energy of such a state is higher than that of the totally symmetric [4]O state, the [31]O symmetry is the most favourable both for the flavour\u2013spin interaction\u00a0[12] and the colour\u2013spin interaction\u00a0[13]. In the first case the statement is confirmed by the comparison between the realistic calculations for positive parity [12] and negative parity\u00a0[14], based on the same quark model\u00a0[15]. In Ref.\u00a0[12] the antiquark was heavy, c or b, and accordingly the interaction between light quarks and the heavy antiquark was neglected, consistent with the heavy quark limit. In Ref.\u00a0[16] an attractive spin\u2013spin interaction between s\u0304 and the light quarks was incorporated and shown that a stable or narrow positive parity uudds\u0304 pentaquark can be accommodated within such a model. This interaction has a form that corresponds to \u03b7 meson exchange [17] and its role is to lower the energy of the whole system.\n", "keywords": "\u2113\nangular momentum associated with the relative coordinates of the q4 subsystem\nantiquark\nchiral soliton\ncolour\u2013spin interaction\ncomparison between the realistic calculations for positive parity [12] and negative parity\u00a0[14], based on the same quark model\u00a0[15]\nconstituent quark models\nflavour\u2013spin interaction\nheavy antiquark\nheavy quark\ninteraction between light quarks and the heavy antiquark was neglected, consistent with the heavy quark limit\nkinetic energy\nlight quarks\nlower the energy of the whole system\nnew wave of pentaquark research\nO symmetry\nP=(\u2212)\u2113+1\nparity of the pentaquark\npentaquark\nquark\nquark model\ns\u0304\nSkyrme models\nspins and parities of \u0398+ and \u039e\u2212\u2212\nspin\u2013spin interaction\nsubsystem of four light quarks is in a state of orbital symmetry\ntake the spin equal to\u00a01/2\ntotally symmetric [4]O state\nuudds\u0304 pentaquark\n\u03b7 meson\n\u03b7 meson exchange\n\u0398+\n\u039e\u2212\u2212\n"}, {"id": "S2212667814001476", "text": "Security issues of data hosted in a Cloud Computing provider remain hidden seen excessive marketing that led to a totally unrealistic view of cloud computing security. Although Cloud Computing has not yet reached the level of maturity expected by its customers, and that the problems of confidentiality, integrity, reliability and consistency (CIRC) are still open, the researchers in this field have already considered a future cloud strategy which aims: a better QoS, reliability and high availability, it is the Multi-Clouds, Cloud of Clouds or Interclouds.This paper will present the security limitations in the single Cloud and the usefulness of adopting rather Multi-Clouds strategy to reduce security risks, through the use of DepSky which is a virtual storage system that ensures better availability and high confidentiality of data.", "keywords": "better availability and high confidentiality\nCIRC\nCloud Computing\nCloud Computing provider\ncloud computing security\nCloud of Clouds\nconfidentiality, integrity, reliability and consistency\ndata\nDepSky\nexcessive marketing\nfuture cloud strategy\nInterclouds\nMulti-Clouds\nMulti-Clouds strategy\npresent the security limitations in the single Cloud\nsecurity limitations\nsecurity risks\nsingle Cloud\nusefulness of adopting rather Multi-Clouds strategy\nvirtual storage system\n"}, {"id": "S2212667814001476", "text": "Security issues of data hosted in a Cloud Computing provider remain hidden seen excessive marketing that led to a totally unrealistic view of cloud computing security. Although Cloud Computing has not yet reached the level of maturity expected by its customers, and that the problems of confidentiality, integrity, reliability and consistency (CIRC) are still open, the researchers in this field have already considered a future cloud strategy which aims: a better QoS, reliability and high availability, it is the Multi-Clouds, Cloud of Clouds or Interclouds.This paper will present the security limitations in the single Cloud and the usefulness of adopting rather Multi-Clouds strategy to reduce security risks, through the use of DepSky which is a virtual storage system that ensures better availability and high confidentiality of data.", "keywords": "better availability and high confidentiality\nCIRC\nCloud Computing\nCloud Computing provider\ncloud computing security\nCloud of Clouds\nconfidentiality, integrity, reliability and consistency\ndata\nDepSky\nexcessive marketing\nfuture cloud strategy\nInterclouds\nMulti-Clouds\nMulti-Clouds strategy\npresent the security limitations in the single Cloud\nsecurity limitations\nsecurity risks\nsingle Cloud\nusefulness of adopting rather Multi-Clouds strategy\nvirtual storage system\n"}, {"id": "S2212667814001476", "text": "Security issues of data hosted in a Cloud Computing provider remain hidden seen excessive marketing that led to a totally unrealistic view of cloud computing security. Although Cloud Computing has not yet reached the level of maturity expected by its customers, and that the problems of confidentiality, integrity, reliability and consistency (CIRC) are still open, the researchers in this field have already considered a future cloud strategy which aims: a better QoS, reliability and high availability, it is the Multi-Clouds, Cloud of Clouds or Interclouds.This paper will present the security limitations in the single Cloud and the usefulness of adopting rather Multi-Clouds strategy to reduce security risks, through the use of DepSky which is a virtual storage system that ensures better availability and high confidentiality of data.", "keywords": "better availability and high confidentiality\nCIRC\nCloud Computing\nCloud Computing provider\ncloud computing security\nCloud of Clouds\nconfidentiality, integrity, reliability and consistency\ndata\nDepSky\nexcessive marketing\nfuture cloud strategy\nInterclouds\nMulti-Clouds\nMulti-Clouds strategy\npresent the security limitations in the single Cloud\nsecurity limitations\nsecurity risks\nsingle Cloud\nusefulness of adopting rather Multi-Clouds strategy\nvirtual storage system\n"}, {"id": "S2212671612002181", "text": "The number of hidden nodes is a critical factor for the generalization of ELM. Generally, it is heavy for time consumption to obtain the optimal number of hidden nodes with trial-and-error. A novel algorithm is proposed to optimize the hidden node number to guarantee good generalization, which employs the PSO in the optimization process with structural risk minimization principle. The simulation results indicate our algorithm for the optimal number of hidden nodes is reasonable and feasible with 6 datasets on benchmark problems by the accuracy comparisons.", "keywords": "6 datasets\naccuracy comparisons\nalgorithm\nELM\ngeneralization\ngeneralization of ELM\nhidden node\nhidden nodes\nnovel algorithm\nnovel algorithm is proposed\nobtain the optimal number of hidden nodes\noptimization process\noptimize the hidden node number\nPSO\nstructural risk minimization principle\ntime consumption\ntrial-and-error\n"}, {"id": "S2212671612002181", "text": "The number of hidden nodes is a critical factor for the generalization of ELM. Generally, it is heavy for time consumption to obtain the optimal number of hidden nodes with trial-and-error. A novel algorithm is proposed to optimize the hidden node number to guarantee good generalization, which employs the PSO in the optimization process with structural risk minimization principle. The simulation results indicate our algorithm for the optimal number of hidden nodes is reasonable and feasible with 6 datasets on benchmark problems by the accuracy comparisons.", "keywords": "6 datasets\naccuracy comparisons\nalgorithm\nELM\ngeneralization\ngeneralization of ELM\nhidden node\nhidden nodes\nnovel algorithm\nnovel algorithm is proposed\nobtain the optimal number of hidden nodes\noptimization process\noptimize the hidden node number\nPSO\nstructural risk minimization principle\ntime consumption\ntrial-and-error\n"}, {"id": "S2212671612002181", "text": "The number of hidden nodes is a critical factor for the generalization of ELM. Generally, it is heavy for time consumption to obtain the optimal number of hidden nodes with trial-and-error. A novel algorithm is proposed to optimize the hidden node number to guarantee good generalization, which employs the PSO in the optimization process with structural risk minimization principle. The simulation results indicate our algorithm for the optimal number of hidden nodes is reasonable and feasible with 6 datasets on benchmark problems by the accuracy comparisons.", "keywords": "6 datasets\naccuracy comparisons\nalgorithm\nELM\ngeneralization\ngeneralization of ELM\nhidden node\nhidden nodes\nnovel algorithm\nnovel algorithm is proposed\nobtain the optimal number of hidden nodes\noptimization process\noptimize the hidden node number\nPSO\nstructural risk minimization principle\ntime consumption\ntrial-and-error\n"}, {"id": "S2212671612002181", "text": "The number of hidden nodes is a critical factor for the generalization of ELM. Generally, it is heavy for time consumption to obtain the optimal number of hidden nodes with trial-and-error. A novel algorithm is proposed to optimize the hidden node number to guarantee good generalization, which employs the PSO in the optimization process with structural risk minimization principle. The simulation results indicate our algorithm for the optimal number of hidden nodes is reasonable and feasible with 6 datasets on benchmark problems by the accuracy comparisons.", "keywords": "6 datasets\naccuracy comparisons\nalgorithm\nELM\ngeneralization\ngeneralization of ELM\nhidden node\nhidden nodes\nnovel algorithm\nnovel algorithm is proposed\nobtain the optimal number of hidden nodes\noptimization process\noptimize the hidden node number\nPSO\nstructural risk minimization principle\ntime consumption\ntrial-and-error\n"}, {"id": "S2212671612002181", "text": "The number of hidden nodes is a critical factor for the generalization of ELM. Generally, it is heavy for time consumption to obtain the optimal number of hidden nodes with trial-and-error. A novel algorithm is proposed to optimize the hidden node number to guarantee good generalization, which employs the PSO in the optimization process with structural risk minimization principle. The simulation results indicate our algorithm for the optimal number of hidden nodes is reasonable and feasible with 6 datasets on benchmark problems by the accuracy comparisons.", "keywords": "6 datasets\naccuracy comparisons\nalgorithm\nELM\ngeneralization\ngeneralization of ELM\nhidden node\nhidden nodes\nnovel algorithm\nnovel algorithm is proposed\nobtain the optimal number of hidden nodes\noptimization process\noptimize the hidden node number\nPSO\nstructural risk minimization principle\ntime consumption\ntrial-and-error\n"}, {"id": "S2212671612002181", "text": "The number of hidden nodes is a critical factor for the generalization of ELM. Generally, it is heavy for time consumption to obtain the optimal number of hidden nodes with trial-and-error. A novel algorithm is proposed to optimize the hidden node number to guarantee good generalization, which employs the PSO in the optimization process with structural risk minimization principle. The simulation results indicate our algorithm for the optimal number of hidden nodes is reasonable and feasible with 6 datasets on benchmark problems by the accuracy comparisons.", "keywords": "6 datasets\naccuracy comparisons\nalgorithm\nELM\ngeneralization\ngeneralization of ELM\nhidden node\nhidden nodes\nnovel algorithm\nnovel algorithm is proposed\nobtain the optimal number of hidden nodes\noptimization process\noptimize the hidden node number\nPSO\nstructural risk minimization principle\ntime consumption\ntrial-and-error\n"}, {"id": "S2212671612002181", "text": "The number of hidden nodes is a critical factor for the generalization of ELM. Generally, it is heavy for time consumption to obtain the optimal number of hidden nodes with trial-and-error. A novel algorithm is proposed to optimize the hidden node number to guarantee good generalization, which employs the PSO in the optimization process with structural risk minimization principle. The simulation results indicate our algorithm for the optimal number of hidden nodes is reasonable and feasible with 6 datasets on benchmark problems by the accuracy comparisons.", "keywords": "6 datasets\naccuracy comparisons\nalgorithm\nELM\ngeneralization\ngeneralization of ELM\nhidden node\nhidden nodes\nnovel algorithm\nnovel algorithm is proposed\nobtain the optimal number of hidden nodes\noptimization process\noptimize the hidden node number\nPSO\nstructural risk minimization principle\ntime consumption\ntrial-and-error\n"}, {"id": "S2212671612002181", "text": "The number of hidden nodes is a critical factor for the generalization of ELM. Generally, it is heavy for time consumption to obtain the optimal number of hidden nodes with trial-and-error. A novel algorithm is proposed to optimize the hidden node number to guarantee good generalization, which employs the PSO in the optimization process with structural risk minimization principle. The simulation results indicate our algorithm for the optimal number of hidden nodes is reasonable and feasible with 6 datasets on benchmark problems by the accuracy comparisons.", "keywords": "6 datasets\naccuracy comparisons\nalgorithm\nELM\ngeneralization\ngeneralization of ELM\nhidden node\nhidden nodes\nnovel algorithm\nnovel algorithm is proposed\nobtain the optimal number of hidden nodes\noptimization process\noptimize the hidden node number\nPSO\nstructural risk minimization principle\ntime consumption\ntrial-and-error\n"}, {"id": "S2212671612002181", "text": "The number of hidden nodes is a critical factor for the generalization of ELM. Generally, it is heavy for time consumption to obtain the optimal number of hidden nodes with trial-and-error. A novel algorithm is proposed to optimize the hidden node number to guarantee good generalization, which employs the PSO in the optimization process with structural risk minimization principle. The simulation results indicate our algorithm for the optimal number of hidden nodes is reasonable and feasible with 6 datasets on benchmark problems by the accuracy comparisons.", "keywords": "6 datasets\naccuracy comparisons\nalgorithm\nELM\ngeneralization\ngeneralization of ELM\nhidden node\nhidden nodes\nnovel algorithm\nnovel algorithm is proposed\nobtain the optimal number of hidden nodes\noptimization process\noptimize the hidden node number\nPSO\nstructural risk minimization principle\ntime consumption\ntrial-and-error\n"}, {"id": "S2212671612002181", "text": "The number of hidden nodes is a critical factor for the generalization of ELM. Generally, it is heavy for time consumption to obtain the optimal number of hidden nodes with trial-and-error. A novel algorithm is proposed to optimize the hidden node number to guarantee good generalization, which employs the PSO in the optimization process with structural risk minimization principle. The simulation results indicate our algorithm for the optimal number of hidden nodes is reasonable and feasible with 6 datasets on benchmark problems by the accuracy comparisons.", "keywords": "6 datasets\naccuracy comparisons\nalgorithm\nELM\ngeneralization\ngeneralization of ELM\nhidden node\nhidden nodes\nnovel algorithm\nnovel algorithm is proposed\nobtain the optimal number of hidden nodes\noptimization process\noptimize the hidden node number\nPSO\nstructural risk minimization principle\ntime consumption\ntrial-and-error\n"}, {"id": "S2212671612002181", "text": "The number of hidden nodes is a critical factor for the generalization of ELM. Generally, it is heavy for time consumption to obtain the optimal number of hidden nodes with trial-and-error. A novel algorithm is proposed to optimize the hidden node number to guarantee good generalization, which employs the PSO in the optimization process with structural risk minimization principle. The simulation results indicate our algorithm for the optimal number of hidden nodes is reasonable and feasible with 6 datasets on benchmark problems by the accuracy comparisons.", "keywords": "6 datasets\naccuracy comparisons\nalgorithm\nELM\ngeneralization\ngeneralization of ELM\nhidden node\nhidden nodes\nnovel algorithm\nnovel algorithm is proposed\nobtain the optimal number of hidden nodes\noptimization process\noptimize the hidden node number\nPSO\nstructural risk minimization principle\ntime consumption\ntrial-and-error\n"}, {"id": "S0301010409001219", "text": "It has been known [9,14,18,22] that the fragmentation processes in polyatomic molecules induced by an intense ultrafast laser field can sometimes exhibit sensitive dependence on the instantaneous phase characteristics of the laser field. Depending on the change in sign the chirped laser pulses, fragmentation could be either enhanced or suppressed [14,18,22]. Controlling the outcome of such laser induced molecular fragmentation with chirped femtosecond laser pulses has brought forth a number of experimental and theoretical effects in the recent years. However, efforts are continuing for a specific fragment channel enhancement, which is difficult since it also is a function of the molecular system under study [20,22\u201324]. Here we report the observation of a coherently enhanced fragmentation pathway of n-propyl benzene, which seems to have such specific fragmentation channel available. We found that for n-propyl benzene, the relative yield of C3H3+ is extremely sensitive to the phase of the laser pulse as compared to any of the other possible channels. In fact, there is almost an order of magnitude enhancement in the yield of C3H3+ when negatively chirped pulses are used, while there is no effect with the positive chirp. Moreover, the relative yield of all the other heavier fragment ions resulting from interaction of the strong field with the molecule is not sensitive to the sign of the chirp, within the noise level.\n", "keywords": "C3H3+\nchannels\nchirp\nchirped femtosecond laser pulses\nchirped laser pulses\nfragmentation\nfragmentation channel\nfragmentation processes\nfragment channel enhancement\nheavier fragment ions\nintense ultrafast laser field\nlaser field\nlaser induced molecular fragmentation\nlaser pulse\nmolecular system\nmolecule\nnegatively chirped pulses\nn-propyl benzene\nobservation of a coherently enhanced fragmentation pathway\npolyatomic molecules\npositive chirp\n"}, {"id": "S0045782512002599", "text": "Finite Element Tearing and Interconnecting (FETI) methods are a powerful approach to designing solvers for large-scale problems in computational mechanics. The numerical simulation problem is subdivided into a number of independent sub-problems, which are then coupled in appropriate ways. NURBS- (Non-Uniform Rational B-spline) based isogeometric analysis (IGA) applied to complex geometries requires to represent the computational domain as a collection of several NURBS geometries. Since there is a natural decomposition of the computational domain into several subdomains, NURBS-based IGA is particularly well suited for using FETI methods.This paper proposes the new IsogEometric Tearing and Interconnecting (IETI) method, which combines the advanced solver design of FETI with the exact geometry representation of IGA. We describe the IETI framework for two classes of simple model problems (Poisson and linearized elasticity) and discuss the coupling of the subdomains along interfaces (both for matching interfaces and for interfaces with T-joints, i.e. hanging nodes). Special attention is paid to the construction of a suitable preconditioner for the iterative linear solver used for the interface problem. We report several computational experiments to demonstrate the performance of the proposed IETI method.\n", "keywords": "advanced solver design\ncomplex geometries\ncomputational domain as a collection of several NURBS geometries\nconstruction of a suitable preconditioner for the iterative linear solver\ncoupling of the subdomains along interfaces\ndemonstrate the performance of the proposed IETI method\nexact geometry representation\nFETI\nFETI methods\nFinite Element Tearing and Interconnecting\nhanging nodes\nIETI\nIETI framework\nIGA\ninterfaces with T-joints\nisogeometric analysis\nIsogEometric Tearing and Interconnecting\nlinearized elasticity\nNon-Uniform Rational B-spline\nnumber of independent sub-problems\nnumerical simulation problem\nNURBS\nNURBS-based IGA\nPoisson\nseveral subdomains\nsimple model problems\nsolvers for large-scale problems in computational mechanics\n"}, {"id": "S0370269301015222", "text": "Recent astronomical observations of high redshift type Ia supernovae performed by two groups [1\u20133] as well as the power spectrum of the cosmic microwave background radiation obtained by the BOOMERANG [4] and MAXIMA-1 [5] experiments seem to indicate that at present the Universe is in a state of accelerated expansion. If one analyzes these data within the Friedmann\u2013Robertson\u2013Walker (FRW) standard model of cosmology their most natural interpretation is that the Universe is spatially flat and that the (baryonic plus dark) matter density \u03c1 is about one third of the critical density \u03c1crit. Most interestingly, the dominant contribution to the energy density is provided by the cosmological constant \u039b. The vacuum energy density (1.1)\u03c1\u039b\u2261\u039b/(8\u03c0G) is about twice as large as \u03c1, i.e., about two thirds of the critical density. With \u03a9M\u2261\u03c1/\u03c1crit, \u03a9\u039b\u2261\u03c1\u039b/\u03c1crit and \u03a9tot\u2261\u03a9M+\u03a9\u039b: (1.2)\u03a9M\u22481/3,\u03a9\u039b\u22482/3,\u03a9tot\u22481. This implies that the deceleration parameter q is approximately \u22121/2. While originally the cosmological constant problem [6] was related to the question why \u039b is so unnaturally small, the discovery of the important role played by \u03c1\u039b has shifted the emphasis toward the \u201ccoincidence problem\u201d, the question why \u03c1 and \u03c1\u039b happen to be of the same order of magnitude precisely at this very moment [7].\n", "keywords": "astronomical observations\nBOOMERANG\ncoincidence problem\ncosmological constant\ncosmological constant problem\ncosmology\ncritical density\ndeceleration parameter\nFriedmann\u2013Robertson\u2013Walker (FRW) standard model\nhigh redshift type Ia supernovae\nmatter density\nMAXIMA-1\npower spectrum of the cosmic microwave background radiation\nq\nquestion why \u039b is so unnaturally small\nvacuum energy density\nwhy \u03c1 and \u03c1\u039b happen to be of the same order of magnitude precisely at this very moment\n\u039b\n\u03c1\n\u03c1crit\n\u03c1\u039b\n\u03c1\u039b\u2261\u039b/(8\u03c0G)\n"}, {"id": "S0370269301015222", "text": "Recent astronomical observations of high redshift type Ia supernovae performed by two groups [1\u20133] as well as the power spectrum of the cosmic microwave background radiation obtained by the BOOMERANG [4] and MAXIMA-1 [5] experiments seem to indicate that at present the Universe is in a state of accelerated expansion. If one analyzes these data within the Friedmann\u2013Robertson\u2013Walker (FRW) standard model of cosmology their most natural interpretation is that the Universe is spatially flat and that the (baryonic plus dark) matter density \u03c1 is about one third of the critical density \u03c1crit. Most interestingly, the dominant contribution to the energy density is provided by the cosmological constant \u039b. The vacuum energy density (1.1)\u03c1\u039b\u2261\u039b/(8\u03c0G) is about twice as large as \u03c1, i.e., about two thirds of the critical density. With \u03a9M\u2261\u03c1/\u03c1crit, \u03a9\u039b\u2261\u03c1\u039b/\u03c1crit and \u03a9tot\u2261\u03a9M+\u03a9\u039b: (1.2)\u03a9M\u22481/3,\u03a9\u039b\u22482/3,\u03a9tot\u22481. This implies that the deceleration parameter q is approximately \u22121/2. While originally the cosmological constant problem [6] was related to the question why \u039b is so unnaturally small, the discovery of the important role played by \u03c1\u039b has shifted the emphasis toward the \u201ccoincidence problem\u201d, the question why \u03c1 and \u03c1\u039b happen to be of the same order of magnitude precisely at this very moment [7].\n", "keywords": "astronomical observations\nBOOMERANG\ncoincidence problem\ncosmological constant\ncosmological constant problem\ncosmology\ncritical density\ndeceleration parameter\nFriedmann\u2013Robertson\u2013Walker (FRW) standard model\nhigh redshift type Ia supernovae\nmatter density\nMAXIMA-1\npower spectrum of the cosmic microwave background radiation\nq\nquestion why \u039b is so unnaturally small\nvacuum energy density\nwhy \u03c1 and \u03c1\u039b happen to be of the same order of magnitude precisely at this very moment\n\u039b\n\u03c1\n\u03c1crit\n\u03c1\u039b\n\u03c1\u039b\u2261\u039b/(8\u03c0G)\n"}, {"id": "S0370269301015222", "text": "Recent astronomical observations of high redshift type Ia supernovae performed by two groups [1\u20133] as well as the power spectrum of the cosmic microwave background radiation obtained by the BOOMERANG [4] and MAXIMA-1 [5] experiments seem to indicate that at present the Universe is in a state of accelerated expansion. If one analyzes these data within the Friedmann\u2013Robertson\u2013Walker (FRW) standard model of cosmology their most natural interpretation is that the Universe is spatially flat and that the (baryonic plus dark) matter density \u03c1 is about one third of the critical density \u03c1crit. Most interestingly, the dominant contribution to the energy density is provided by the cosmological constant \u039b. The vacuum energy density (1.1)\u03c1\u039b\u2261\u039b/(8\u03c0G) is about twice as large as \u03c1, i.e., about two thirds of the critical density. With \u03a9M\u2261\u03c1/\u03c1crit, \u03a9\u039b\u2261\u03c1\u039b/\u03c1crit and \u03a9tot\u2261\u03a9M+\u03a9\u039b: (1.2)\u03a9M\u22481/3,\u03a9\u039b\u22482/3,\u03a9tot\u22481. This implies that the deceleration parameter q is approximately \u22121/2. While originally the cosmological constant problem [6] was related to the question why \u039b is so unnaturally small, the discovery of the important role played by \u03c1\u039b has shifted the emphasis toward the \u201ccoincidence problem\u201d, the question why \u03c1 and \u03c1\u039b happen to be of the same order of magnitude precisely at this very moment [7].\n", "keywords": "astronomical observations\nBOOMERANG\ncoincidence problem\ncosmological constant\ncosmological constant problem\ncosmology\ncritical density\ndeceleration parameter\nFriedmann\u2013Robertson\u2013Walker (FRW) standard model\nhigh redshift type Ia supernovae\nmatter density\nMAXIMA-1\npower spectrum of the cosmic microwave background radiation\nq\nquestion why \u039b is so unnaturally small\nvacuum energy density\nwhy \u03c1 and \u03c1\u039b happen to be of the same order of magnitude precisely at this very moment\n\u039b\n\u03c1\n\u03c1crit\n\u03c1\u039b\n\u03c1\u039b\u2261\u039b/(8\u03c0G)\n"}, {"id": "S0370269301015222", "text": "Recent astronomical observations of high redshift type Ia supernovae performed by two groups [1\u20133] as well as the power spectrum of the cosmic microwave background radiation obtained by the BOOMERANG [4] and MAXIMA-1 [5] experiments seem to indicate that at present the Universe is in a state of accelerated expansion. If one analyzes these data within the Friedmann\u2013Robertson\u2013Walker (FRW) standard model of cosmology their most natural interpretation is that the Universe is spatially flat and that the (baryonic plus dark) matter density \u03c1 is about one third of the critical density \u03c1crit. Most interestingly, the dominant contribution to the energy density is provided by the cosmological constant \u039b. The vacuum energy density (1.1)\u03c1\u039b\u2261\u039b/(8\u03c0G) is about twice as large as \u03c1, i.e., about two thirds of the critical density. With \u03a9M\u2261\u03c1/\u03c1crit, \u03a9\u039b\u2261\u03c1\u039b/\u03c1crit and \u03a9tot\u2261\u03a9M+\u03a9\u039b: (1.2)\u03a9M\u22481/3,\u03a9\u039b\u22482/3,\u03a9tot\u22481. This implies that the deceleration parameter q is approximately \u22121/2. While originally the cosmological constant problem [6] was related to the question why \u039b is so unnaturally small, the discovery of the important role played by \u03c1\u039b has shifted the emphasis toward the \u201ccoincidence problem\u201d, the question why \u03c1 and \u03c1\u039b happen to be of the same order of magnitude precisely at this very moment [7].\n", "keywords": "astronomical observations\nBOOMERANG\ncoincidence problem\ncosmological constant\ncosmological constant problem\ncosmology\ncritical density\ndeceleration parameter\nFriedmann\u2013Robertson\u2013Walker (FRW) standard model\nhigh redshift type Ia supernovae\nmatter density\nMAXIMA-1\npower spectrum of the cosmic microwave background radiation\nq\nquestion why \u039b is so unnaturally small\nvacuum energy density\nwhy \u03c1 and \u03c1\u039b happen to be of the same order of magnitude precisely at this very moment\n\u039b\n\u03c1\n\u03c1crit\n\u03c1\u039b\n\u03c1\u039b\u2261\u039b/(8\u03c0G)\n"}, {"id": "S0963869514000954", "text": "Global optimisation algorithms are used in this study to solve the optimisation problem as they are known to be efficient in incorporating statistical information and dealing with complicated objective functions that have multiple local minima/maxima. The genetic algorithm (GA) is such a global optimisation technique that mimics biological evolution processes and is used in this particular study. The algorithm starts with a random selection of a population from the decision variable domain (X). The genetic algorithm repeatedly modifies this population. At each step, the algorithm selects a group of individual values from the population (parent) which are evolved through crossover or mutation to produce members of the next generation. This process is repeated for several generations until an optimum solution is reached. See [19] for a fuller description of the GA.\n", "keywords": "crossover\ndecision variable domain\nGA\ngenetic algorithm\nGlobal optimisation algorithms\nglobal optimisation technique\nincorporating statistical information\nlocal minima/maxima\nmimics biological evolution processes\nmutation\nobjective functions\nparent\npopulation\nrandom selection of a population\nselects a group of individual values\nsolve the optimisation problem\nX\n"}, {"id": "S0370269304007749", "text": "The charmonium production has long been considered as a good process for investigating both perturbative and nonperturbative properties of quantum chromodynamics (QCD), because of the relatively large difference between the scale at which the charm\u2013quark pair is produced at the parton level and the scale at which it evolves into a quarkonium. In particular, comparing to hadron colliders, e+e\u2212 colliders, provide a cleaner environment to study the charmonium productions and decays. However, some puzzles arise from the recent measurements on the prompt J/\u03c8 productions at BaBar and Belle\u00a0[1\u20133]. For the inclusive J/\u03c8 productions, the cross section is much larger than the predictions of nonrelativistic quantum chromodynamics (NRQCD)\u00a0[4]; there is also an over-abundance of the four-charm\u2013quark processes including the exclusive J/\u03c8 and charmonium productions; there is no apparent signal in the hard J/\u03c8 spectrum which has been predicted by the J/\u03c8gg production mode as well as the color-octet mechanism in NRQCD. To provide plausible solutions and explanations for these conflicts, theorists have studied the possibilities of the contribution from two-virtual-photon mediate processes\u00a0[5], large higher-order QCD corrections\u00a0[6,7], collinear suppression at the end-point region of the J/\u03c8 momentum\u00a0[7,8], contribution from the J/\u03c8-glueball associated production\u00a0[9] and contribution from a very light scalar boson\u00a0[10].\n", "keywords": "BaBar\nBelle\u00a0\ncharmonium production\ncollinear suppression at the end-point region of the J/\u03c8 momentum\ncolor-octet mechanism in NRQCD\ncontribution from a very light scalar boson\ncontribution from the J/\u03c8-glueball associated production\u00a0\ne+e\u2212 colliders\nexclusive J/\u03c8 and charmonium productions\nfour-charm\u2013quark processes\ninclusive J/\u03c8 productions\ninvestigating both perturbative and nonperturbative properties of quantum chromodynamics (QCD\nJ/\u03c8gg production mode\nlarge higher-order QCD corrections\nnonrelativistic quantum chromodynamics\nNRQCD\npossibilities of the contribution from two-virtual-photon mediate processes\nQCD\nquantum chromodynamics\nrecent measurements on the prompt J/\u03c8\nstudy the charmonium productions and decays\n"}, {"id": "S1010603009002676", "text": "In this paper, we present our experimental observations on how solvents can vary the TPA and TPF properties of fluorescent rhodamine (Rh) dyes Rh6G, RhB and Rh101. Rhodamines are well-known xanthenes dyes, which have been extensively used for many widespread applications in single-molecule detection [24], DNA-sequence determination [25], fluorescence labelling [26], etc. due to their strong fluorescence over the visible spectral region. Molecular geometries of rhodamine dyes are well-known [27,28] and indicate that all the structures are non-centrosymmetric. In general, for centrosymmetric molecules, TPA is forbidden when tuned to the transitions at one-half of the excitation frequencies. However, for non-centrosymmetric molecules due to symmetry relaxations, the single-photon absorption (SPA) peaks and TPA peaks may coincide. So we set our primary aim to find the effect of solvent polarity on the correlation of SPA and TPA peaks for all the dyes.\n", "keywords": "centrosymmetric molecules\nDNA-sequence determination\ndyes\nexperimental observations on how solvents can vary the TPA and TPF properties of fluorescent rhodamine (Rh) dyes\nfluorescence\nfluorescence labelling\nfluorescent rhodamine\nfluorescent rhodamine (Rh) dyes\nnon-centrosymmetric molecules\nRh\nRh101\nRh6G\nRhB\nrhodamine dyes\nRhodamines\nsingle-molecule detection\nsingle-photon absorption\nsingle-photon absorption (SPA) peaks\nSPA\nsymmetry relaxations\nTPA\nTPA peaks\nTPF\nxanthenes dyes\n"}, {"id": "S0022311514005480", "text": "The second stress state is a tri-axial tensile stress designed to represent the zone ahead of an advancing crack tip. Micro-scale lateral cracks have been observed in the oxide layer, and appear to form very close to or at the metal\u2013oxide interface (Fig. 1). Finite element analysis by Parise et al. indicated that these cracks form as a result of localised tensile stresses above peaks in the metal\u2013oxide interface roughness [31]. These cracks are considered separate to any nano-scale cracks that might result from the tetragonal to monoclinic phase transformation. An assumption is made here that whether the micro-scale lateral cracks form via fracture of the oxide or by de-bonding at the interface a triaxial tensile stress state will still be present. In manufactured partially stabilised zirconia cracks would be expected to destabilise the tetragonal phase. This is simulated by applying tensile stress in direction 1, 2 and 3. As this the maximum stress at the crack tip is not known, the applied tensile stresses cover a range from 0.1GPa up to a maximum stress value of 2.2GPa as it is approximately equal to three times the fracture strength of bulk fracture strength for manufactured stabilized zirconia [34]. For the biaxial compressive and triaxial tensile stress states it is the trends in behaviour rather than the absolute values that are considered of greatest importance for this work.\n", "keywords": "advancing crack tip\napplied tensile stresses\napplying tensile stress\nbiaxial compressive and triaxial tensile stress states\ncracks\nde-bonding\nFinite element analysis\nfracture\nlocalised tensile stresses\nmanufactured partially stabilised zirconia\nmanufactured stabilized zirconia\nmaximum stress\nmetal\u2013oxide interface\noxide\noxide layer\nsecond stress state\nsimulated by applying tensile stress in direction 1, 2 and 3\ntetragonal phase\ntetragonal to monoclinic phase transformation\ntri-axial tensile stress\ntriaxial tensile stress\n"}, {"id": "S0927025614006181", "text": "The Discrete Element Method applied to spheres is well established as a reasonably realistic tool, in a wide range of engineering disciplines, for modelling packing and flow of granular materials; Asmar et al. [8] describes the fundamentals of this method as applied by code developed in-house at Nottingham; since these are widely documented the details are not reproduced here, simply a summary. It applies an explicit time stepping approach to numerically integrate the translational and rotational motion of each particle from the resulting forces and moments acting on them at each timestep. The inter-particle and particle wall contacts are modelled using the linear spring\u2013dashpot\u2013slider analogy. Contact forces are modelled in the normal and tangential directions with respect to the line connecting the particles centres. Particle elastic stiffness is set so sphere \u201coverlap\u201d is not significant and moderate contact damping is applied. Particle cohesion can also be modelled but is assumed to be negligible in the current study. The translational and rotational motion of each particle is modelled using a half step leap-frog Verlet numerical integration scheme to update particle positions and velocities. Near-neighbour lists are used to increase the computational efficiency of determining particle contacts and a zoning method is used each time the list is composed; that is the system is divided into cubic regions, each particle centre is within one zone, and potential contacting particles are within the same or next-door neighbour zones. Full details are given in Asmar et al. [8].\n", "keywords": "contact damping\nContact forces\ncontacting particles\nDiscrete Element Method\nexplicit time stepping approach\nforces\ngranular materials\nhalf step leap-frog Verlet numerical integration scheme\ninter-particle and particle wall contacts\nlinear spring\u2013dashpot\u2013slider analogy\nmodelling packing and flow of granular materials\nmoments\nNear-neighbour lists\nnumerically integrate\nparticle\nparticle centre\nParticle cohesion\nparticle contacts\nParticle elastic\nParticle elastic stiffness\nparticle positions\nparticles centres\nsphere\nsphere \u201coverlap\u201d\nspheres\ntranslational and rotational motion\nvelocities\nzoning method\n"}, {"id": "S0927025614006181", "text": "The Discrete Element Method applied to spheres is well established as a reasonably realistic tool, in a wide range of engineering disciplines, for modelling packing and flow of granular materials; Asmar et al. [8] describes the fundamentals of this method as applied by code developed in-house at Nottingham; since these are widely documented the details are not reproduced here, simply a summary. It applies an explicit time stepping approach to numerically integrate the translational and rotational motion of each particle from the resulting forces and moments acting on them at each timestep. The inter-particle and particle wall contacts are modelled using the linear spring\u2013dashpot\u2013slider analogy. Contact forces are modelled in the normal and tangential directions with respect to the line connecting the particles centres. Particle elastic stiffness is set so sphere \u201coverlap\u201d is not significant and moderate contact damping is applied. Particle cohesion can also be modelled but is assumed to be negligible in the current study. The translational and rotational motion of each particle is modelled using a half step leap-frog Verlet numerical integration scheme to update particle positions and velocities. Near-neighbour lists are used to increase the computational efficiency of determining particle contacts and a zoning method is used each time the list is composed; that is the system is divided into cubic regions, each particle centre is within one zone, and potential contacting particles are within the same or next-door neighbour zones. Full details are given in Asmar et al. [8].\n", "keywords": "contact damping\nContact forces\ncontacting particles\nDiscrete Element Method\nexplicit time stepping approach\nforces\ngranular materials\nhalf step leap-frog Verlet numerical integration scheme\ninter-particle and particle wall contacts\nlinear spring\u2013dashpot\u2013slider analogy\nmodelling packing and flow of granular materials\nmoments\nNear-neighbour lists\nnumerically integrate\nparticle\nparticle centre\nParticle cohesion\nparticle contacts\nParticle elastic\nParticle elastic stiffness\nparticle positions\nparticles centres\nsphere\nsphere \u201coverlap\u201d\nspheres\ntranslational and rotational motion\nvelocities\nzoning method\n"}, {"id": "S0927025614006181", "text": "The Discrete Element Method applied to spheres is well established as a reasonably realistic tool, in a wide range of engineering disciplines, for modelling packing and flow of granular materials; Asmar et al. [8] describes the fundamentals of this method as applied by code developed in-house at Nottingham; since these are widely documented the details are not reproduced here, simply a summary. It applies an explicit time stepping approach to numerically integrate the translational and rotational motion of each particle from the resulting forces and moments acting on them at each timestep. The inter-particle and particle wall contacts are modelled using the linear spring\u2013dashpot\u2013slider analogy. Contact forces are modelled in the normal and tangential directions with respect to the line connecting the particles centres. Particle elastic stiffness is set so sphere \u201coverlap\u201d is not significant and moderate contact damping is applied. Particle cohesion can also be modelled but is assumed to be negligible in the current study. The translational and rotational motion of each particle is modelled using a half step leap-frog Verlet numerical integration scheme to update particle positions and velocities. Near-neighbour lists are used to increase the computational efficiency of determining particle contacts and a zoning method is used each time the list is composed; that is the system is divided into cubic regions, each particle centre is within one zone, and potential contacting particles are within the same or next-door neighbour zones. Full details are given in Asmar et al. [8].\n", "keywords": "contact damping\nContact forces\ncontacting particles\nDiscrete Element Method\nexplicit time stepping approach\nforces\ngranular materials\nhalf step leap-frog Verlet numerical integration scheme\ninter-particle and particle wall contacts\nlinear spring\u2013dashpot\u2013slider analogy\nmodelling packing and flow of granular materials\nmoments\nNear-neighbour lists\nnumerically integrate\nparticle\nparticle centre\nParticle cohesion\nparticle contacts\nParticle elastic\nParticle elastic stiffness\nparticle positions\nparticles centres\nsphere\nsphere \u201coverlap\u201d\nspheres\ntranslational and rotational motion\nvelocities\nzoning method\n"}, {"id": "S0927025614006181", "text": "The Discrete Element Method applied to spheres is well established as a reasonably realistic tool, in a wide range of engineering disciplines, for modelling packing and flow of granular materials; Asmar et al. [8] describes the fundamentals of this method as applied by code developed in-house at Nottingham; since these are widely documented the details are not reproduced here, simply a summary. It applies an explicit time stepping approach to numerically integrate the translational and rotational motion of each particle from the resulting forces and moments acting on them at each timestep. The inter-particle and particle wall contacts are modelled using the linear spring\u2013dashpot\u2013slider analogy. Contact forces are modelled in the normal and tangential directions with respect to the line connecting the particles centres. Particle elastic stiffness is set so sphere \u201coverlap\u201d is not significant and moderate contact damping is applied. Particle cohesion can also be modelled but is assumed to be negligible in the current study. The translational and rotational motion of each particle is modelled using a half step leap-frog Verlet numerical integration scheme to update particle positions and velocities. Near-neighbour lists are used to increase the computational efficiency of determining particle contacts and a zoning method is used each time the list is composed; that is the system is divided into cubic regions, each particle centre is within one zone, and potential contacting particles are within the same or next-door neighbour zones. Full details are given in Asmar et al. [8].\n", "keywords": "contact damping\nContact forces\ncontacting particles\nDiscrete Element Method\nexplicit time stepping approach\nforces\ngranular materials\nhalf step leap-frog Verlet numerical integration scheme\ninter-particle and particle wall contacts\nlinear spring\u2013dashpot\u2013slider analogy\nmodelling packing and flow of granular materials\nmoments\nNear-neighbour lists\nnumerically integrate\nparticle\nparticle centre\nParticle cohesion\nparticle contacts\nParticle elastic\nParticle elastic stiffness\nparticle positions\nparticles centres\nsphere\nsphere \u201coverlap\u201d\nspheres\ntranslational and rotational motion\nvelocities\nzoning method\n"}, {"id": "S0032386108009397", "text": "The viscoelastic behavior of elastomers containing small amounts of unattached chains has been investigated to characterize the dynamics of the polymer chains trapped in fixed networks [66\u201368]. Polymer chains trapped in fixed networks constitute a simpler system for the study of the polymer chain dynamics than the corresponding uncrosslinked polymer melts. This is because the complicated effect of the motion of the surrounding chains on the dynamics of the probe chain \u2013 called \u201cconstraint release\u201d [69] \u2013 is absent in the fixed network systems. Most of the earlier studies employed randomly crosslinked elastomers as host networks. In this case, precise control of the mesh size of the host networks is not possible, and the mesh size has a broad distribution. The end-linking systems give the host networks a more uniform mesh size, and they can control the mesh size by the size of the precursor chains. We investigated the dynamic viscoelasticity of end-linked PDMS elastomers containing unattached linear PDMS as functions of the size of the unattached chains (Mg) and the network mesh (Mx) (Fig.\u00a09a) [70]. We employed two types of host networks with Mx>Me and Mx<Me where Me (\u224810,000 for PDMS) is the molecular mass between adjacent entanglements in the molten state. The Mx>Me and Mx<Me networks (designated as NL and NS, respectively) were designed by end-linking the long (Mn=84,000) and short precursor chains (Mn=4,550), respectively. The mesh of the NL networks is dominated by trapped entanglements, while that of the NS network is governed by chemical cross-links.\n", "keywords": "characterize the dynamics of the polymer chains\nchemical cross-links.\nconstraint release\ncontrol of the mesh size\ncrosslinked elastomers\nelastomers\nmesh\nMg\nMx\nMx<Me\nMx>Me\nnetwork mesh\nNL\nNS\nPDMS\nPDMS elastomers\npolymer chain\npolymer chains\nPolymer chains\npolymer melts\nprecursor chains\nprobe chain\nunattached chains\nviscoelastic behavior\n"}, {"id": "S0032386108009397", "text": "The viscoelastic behavior of elastomers containing small amounts of unattached chains has been investigated to characterize the dynamics of the polymer chains trapped in fixed networks [66\u201368]. Polymer chains trapped in fixed networks constitute a simpler system for the study of the polymer chain dynamics than the corresponding uncrosslinked polymer melts. This is because the complicated effect of the motion of the surrounding chains on the dynamics of the probe chain \u2013 called \u201cconstraint release\u201d [69] \u2013 is absent in the fixed network systems. Most of the earlier studies employed randomly crosslinked elastomers as host networks. In this case, precise control of the mesh size of the host networks is not possible, and the mesh size has a broad distribution. The end-linking systems give the host networks a more uniform mesh size, and they can control the mesh size by the size of the precursor chains. We investigated the dynamic viscoelasticity of end-linked PDMS elastomers containing unattached linear PDMS as functions of the size of the unattached chains (Mg) and the network mesh (Mx) (Fig.\u00a09a) [70]. We employed two types of host networks with Mx>Me and Mx<Me where Me (\u224810,000 for PDMS) is the molecular mass between adjacent entanglements in the molten state. The Mx>Me and Mx<Me networks (designated as NL and NS, respectively) were designed by end-linking the long (Mn=84,000) and short precursor chains (Mn=4,550), respectively. The mesh of the NL networks is dominated by trapped entanglements, while that of the NS network is governed by chemical cross-links.\n", "keywords": "characterize the dynamics of the polymer chains\nchemical cross-links.\nconstraint release\ncontrol of the mesh size\ncrosslinked elastomers\nelastomers\nmesh\nMg\nMx\nMx<Me\nMx>Me\nnetwork mesh\nNL\nNS\nPDMS\nPDMS elastomers\npolymer chain\npolymer chains\nPolymer chains\npolymer melts\nprecursor chains\nprobe chain\nunattached chains\nviscoelastic behavior\n"}, {"id": "S0032386108009397", "text": "The viscoelastic behavior of elastomers containing small amounts of unattached chains has been investigated to characterize the dynamics of the polymer chains trapped in fixed networks [66\u201368]. Polymer chains trapped in fixed networks constitute a simpler system for the study of the polymer chain dynamics than the corresponding uncrosslinked polymer melts. This is because the complicated effect of the motion of the surrounding chains on the dynamics of the probe chain \u2013 called \u201cconstraint release\u201d [69] \u2013 is absent in the fixed network systems. Most of the earlier studies employed randomly crosslinked elastomers as host networks. In this case, precise control of the mesh size of the host networks is not possible, and the mesh size has a broad distribution. The end-linking systems give the host networks a more uniform mesh size, and they can control the mesh size by the size of the precursor chains. We investigated the dynamic viscoelasticity of end-linked PDMS elastomers containing unattached linear PDMS as functions of the size of the unattached chains (Mg) and the network mesh (Mx) (Fig.\u00a09a) [70]. We employed two types of host networks with Mx>Me and Mx<Me where Me (\u224810,000 for PDMS) is the molecular mass between adjacent entanglements in the molten state. The Mx>Me and Mx<Me networks (designated as NL and NS, respectively) were designed by end-linking the long (Mn=84,000) and short precursor chains (Mn=4,550), respectively. The mesh of the NL networks is dominated by trapped entanglements, while that of the NS network is governed by chemical cross-links.\n", "keywords": "characterize the dynamics of the polymer chains\nchemical cross-links.\nconstraint release\ncontrol of the mesh size\ncrosslinked elastomers\nelastomers\nmesh\nMg\nMx\nMx<Me\nMx>Me\nnetwork mesh\nNL\nNS\nPDMS\nPDMS elastomers\npolymer chain\npolymer chains\nPolymer chains\npolymer melts\nprecursor chains\nprobe chain\nunattached chains\nviscoelastic behavior\n"}, {"id": "S0032386108009397", "text": "The viscoelastic behavior of elastomers containing small amounts of unattached chains has been investigated to characterize the dynamics of the polymer chains trapped in fixed networks [66\u201368]. Polymer chains trapped in fixed networks constitute a simpler system for the study of the polymer chain dynamics than the corresponding uncrosslinked polymer melts. This is because the complicated effect of the motion of the surrounding chains on the dynamics of the probe chain \u2013 called \u201cconstraint release\u201d [69] \u2013 is absent in the fixed network systems. Most of the earlier studies employed randomly crosslinked elastomers as host networks. In this case, precise control of the mesh size of the host networks is not possible, and the mesh size has a broad distribution. The end-linking systems give the host networks a more uniform mesh size, and they can control the mesh size by the size of the precursor chains. We investigated the dynamic viscoelasticity of end-linked PDMS elastomers containing unattached linear PDMS as functions of the size of the unattached chains (Mg) and the network mesh (Mx) (Fig.\u00a09a) [70]. We employed two types of host networks with Mx>Me and Mx<Me where Me (\u224810,000 for PDMS) is the molecular mass between adjacent entanglements in the molten state. The Mx>Me and Mx<Me networks (designated as NL and NS, respectively) were designed by end-linking the long (Mn=84,000) and short precursor chains (Mn=4,550), respectively. The mesh of the NL networks is dominated by trapped entanglements, while that of the NS network is governed by chemical cross-links.\n", "keywords": "characterize the dynamics of the polymer chains\nchemical cross-links.\nconstraint release\ncontrol of the mesh size\ncrosslinked elastomers\nelastomers\nmesh\nMg\nMx\nMx<Me\nMx>Me\nnetwork mesh\nNL\nNS\nPDMS\nPDMS elastomers\npolymer chain\npolymer chains\nPolymer chains\npolymer melts\nprecursor chains\nprobe chain\nunattached chains\nviscoelastic behavior\n"}, {"id": "S2212671612000637", "text": "The Hamiltonian approach and the variational approach are utilized to treat the relativistic harmonic oscillator for the amplitude-frequency relationship. The nice reliability is shown by the result comparison with that from open literature. The simplicity and efficiency of the methods are also disclosed for different range of the initial amplitude during looking for the amplitude-frequency relationship for the nonlinear relativistic harmonic oscillator.", "keywords": "amplitude-frequency relationship\nHamiltonian approach\ninitial amplitude\nnonlinear relativistic harmonic oscillator\nopen literature\nrelativistic harmonic oscillator\nresult comparison\ntreat the relativistic harmonic oscillator\nvariational approach\n"}, {"id": "S1364815216303061", "text": "In representing wetland-river interactions involving GIWs, many models assume that the wetland can discharge into a river but cannot receive overbank flows from it. In such models, the volume of water (or water level elevation) in a wetland and its corresponding threshold value (predominantly controlled by outlet elevation) are the prime determinants of wetland outflow (Feng et\u00a0al., 2012; Hammer and Kadlec, 1986; Johnson et\u00a0al., 2010; Kadlec and Wallace, 2009; Powell et\u00a0al., 2008; Voldseth et\u00a0al., 2007; Wen et\u00a0al., 2013; Zhang and Mitsch, 2005). However, in regions characterised by widespread riparian wetlands that are hydraulically connected with adjacent rivers, wetland-river interaction is likely to be bidirectional. Such interactions should be quantified according to hydraulic principles involving relative river and wetland water level elevations as well as the properties of the connection between the two (Kouwen, 2013; Liu et\u00a0al., 2008; Min et\u00a0al., 2010; Nyarko, 2007; Restrepo et\u00a0al., 1998). In the WATFLOOD model, for instance, riparian wetland-river interaction is modelled using the principle of Dupuit-Forchheimer lateral/radial groundwater flow (Kouwen, 2013). Since exchange between riparian wetlands and rivers can occur over the surface and/or through the subsurface, Restrepo et\u00a0al. (1998) incorporated an equivalent transmissivity expression, obtained for wetland vegetation and the subsurface soil, into the Darcy flow equation of the MODFLOW model.\n", "keywords": "Darcy flow equation\nequivalent transmissivity expression\ninteractions should be quantified according to hydraulic principles\nMODFLOW\noutlet elevation\nprinciple of Dupuit-Forchheimer lateral/radial groundwater flow\nproperties of the connection between the two\nrelative river and wetland water level elevations\nrepresenting wetland-river interactions involving GIWs\nriparian wetland-river interaction is modelled\nsubsurface soil\nWATFLOOD\nwetland vegetation\n"}, {"id": "S0009261415002730", "text": "Both methods of structure solution reveal a bent conformation of the central terthiophene units of the DOTT molecule as is clearly visible in all three cases in Figure 5. However, there is a fundamental difference in the conformation of the octyl side chains. Whilst for the single crystal phase at T=100K linearly extended chains are observed (Figure 5B), a defined rotation of the octyl chains relative to the terthiophene unit is found for the three thin film phases (Figure 5A). The rotation angle of about \u00b170\u00b0 results from a twist of the first CC single bond at the link between the terthiophene unit and the octyl chain (see arrows Figure 5A). Two features of this rotated conformation are interesting. First, a molecule with rotated side chains represents the equilibrium state of an isolated single DOTT molecule as obtained by combined MD and VASP calculations [33]. Second, the rotated conformation of the octyl chains allows a dense packing of the octyl side chains for both molecules. Interestingly, the single crystal structure at room temperature shows the twisted as well as the linear conformation of the octyl side chains within one molecule (Figure 5C).\n", "keywords": "bent conformation\ncentral terthiophene units\ncombined MD and VASP calculations\ncrystal structure\ndefined rotation of the octyl chains\ndense packing\nDOTT molecule\nlinearly extended chains\noctyl chain\noctyl chains\noctyl side chains\nrotated side chains\nstructure solution\nterthiophene unit\nthin film\ntwist of the first CC single bond\n"}, {"id": "S0009261415002730", "text": "Both methods of structure solution reveal a bent conformation of the central terthiophene units of the DOTT molecule as is clearly visible in all three cases in Figure 5. However, there is a fundamental difference in the conformation of the octyl side chains. Whilst for the single crystal phase at T=100K linearly extended chains are observed (Figure 5B), a defined rotation of the octyl chains relative to the terthiophene unit is found for the three thin film phases (Figure 5A). The rotation angle of about \u00b170\u00b0 results from a twist of the first CC single bond at the link between the terthiophene unit and the octyl chain (see arrows Figure 5A). Two features of this rotated conformation are interesting. First, a molecule with rotated side chains represents the equilibrium state of an isolated single DOTT molecule as obtained by combined MD and VASP calculations [33]. Second, the rotated conformation of the octyl chains allows a dense packing of the octyl side chains for both molecules. Interestingly, the single crystal structure at room temperature shows the twisted as well as the linear conformation of the octyl side chains within one molecule (Figure 5C).\n", "keywords": "bent conformation\ncentral terthiophene units\ncombined MD and VASP calculations\ncrystal structure\ndefined rotation of the octyl chains\ndense packing\nDOTT molecule\nlinearly extended chains\noctyl chain\noctyl chains\noctyl side chains\nrotated side chains\nstructure solution\nterthiophene unit\nthin film\ntwist of the first CC single bond\n"}, {"id": "S2212667812000792", "text": "A process-driven model is presented to build an instinctive and efficient higher educational administrative management system to overcome problems most universities facing. With this model, processes are identified explicitly and the routine of educational administration is broken into small tasks. Each task has designated role of executors. A process describes the activities and relationships among them. A prototype of higher educational administrative system is built with Bonita open solution. The demo shows that the process-driven higher educational administrative system helps end users understand processes they are involved and focus on what to do.", "keywords": "administrative management system\nadministrative system\nBonita open solution\neducational administration\novercome problems\nprocess-driven model\nprocesses are identified\nunderstand processes they are involved\n"}, {"id": "S0377221716302259", "text": "Regarding the implications of the results of this paper, we note two points. From a practical point of view, we have endowed the weighted additive model with a distance function structure, which takes negative values for points located outside the technology and non-negative values for points into the production possibility set. In this respect, the weighted additive distance function methodologically supports the branch of the literature that resorts to the weighted additive model or some related approach to measure productivity over time (see, for example, Mahlberg & Sahoo, 2011 or Chang et al., 2012). From a theoretical point of view, we have provided a new distance function with some interesting properties in contrast to the usual ones, mainly (1) when technical inefficiency has to be estimated, the weighted additive distance function coincides with the weighted additive model, which means that technical inefficiency is measured following the Pareto-Koopmans notion of efficiency; and (2) when productivity has to be determined and decomposed over time the weighted additive distance function emerges as an attractive tool to be used for cross-period evaluation of returns to scale changes, since this distance function is always feasible, even under Variable Returns to Scale.\n", "keywords": "a new distance function\ndetermined and decomposed over time\ndistance function\nimplications of the results\nmeans that technical inefficiency is measured following the Pareto-Koopmans notion of efficiency\nmeasure productivity over time\nsome related approach\ntakes negative values for points located outside the technology and non-negative values for points into the production possibility set\nthe weighted additive distance function\nthe weighted additive model with a distance function structure\nweighted additive distance function\nweighted additive model\n"}, {"id": "S0167931714004456", "text": "PDMS (Polydimethylsiloxane) has become by far the most popular material in the academic microfluidics community because it is inexpensive, easy to fabricate by replication of molds made using rapid prototyping or other techniques, flexible, optically transparent, biocompatible and its fabrication does not require high capital investment and cleanroom conditions. Various techniques have been adapted to fabricate microfluidic structures in PDMS, including wet and dry etching [20\u201322], photolithographic patterning of a photosensitive PDMS [23], and laser ablation [24]. But, it was the \u201csoft-lithography\u201d techniques [25] introduced by Whitesides et al. that enabled the widespread use of PDMS and opened up the era of PDMS-based microfluidics in the late 1990s. Replica molding, which is the casting of prepolymer against a master and generating a replica of the master in PDMS, has become a standard fabrication technique available in almost every research laboratory. Detailed overviews of soft-lithography techniques and their applications can be found from the reviews by McDonald et al. [26] and Sia et al. [27]. Nowadays, many tools dedicated for this purpose are available and can be purchased as a complete set (e.g. SoftLithoBox\u00ae provided by Elveflow (USA) [28]). Moreover, companies, such as FlowJEM (Canada) [29], Microfluidic Innovations (USA) [30], and Scientific Device Laboratory (USA) [31] provide rapid prototyping service for PDMS-based LOC devices.\n", "keywords": "casting of prepolymer against a master and generating a replica of the master in PDMS\nfabricate\nfabricate microfluidic structures\nfabrication technique\nlaser ablation\nmicrofluidic\nmicrofluidics\nPDMS\nPDMS-based LOC devices\nPDMS-based microfluidics\nphotolithographic patterning\nPolydimethylsiloxane\nprepolymer\nrapid prototyping\nrapid prototyping service\nReplica molding\nreplication of molds\nSoftLithoBox\u00ae\n\u201csoft-lithography\u201d techniques\nsoft-lithography techniques\nwet and dry etching\n"}, {"id": "S0167931714004456", "text": "PDMS (Polydimethylsiloxane) has become by far the most popular material in the academic microfluidics community because it is inexpensive, easy to fabricate by replication of molds made using rapid prototyping or other techniques, flexible, optically transparent, biocompatible and its fabrication does not require high capital investment and cleanroom conditions. Various techniques have been adapted to fabricate microfluidic structures in PDMS, including wet and dry etching [20\u201322], photolithographic patterning of a photosensitive PDMS [23], and laser ablation [24]. But, it was the \u201csoft-lithography\u201d techniques [25] introduced by Whitesides et al. that enabled the widespread use of PDMS and opened up the era of PDMS-based microfluidics in the late 1990s. Replica molding, which is the casting of prepolymer against a master and generating a replica of the master in PDMS, has become a standard fabrication technique available in almost every research laboratory. Detailed overviews of soft-lithography techniques and their applications can be found from the reviews by McDonald et al. [26] and Sia et al. [27]. Nowadays, many tools dedicated for this purpose are available and can be purchased as a complete set (e.g. SoftLithoBox\u00ae provided by Elveflow (USA) [28]). Moreover, companies, such as FlowJEM (Canada) [29], Microfluidic Innovations (USA) [30], and Scientific Device Laboratory (USA) [31] provide rapid prototyping service for PDMS-based LOC devices.\n", "keywords": "casting of prepolymer against a master and generating a replica of the master in PDMS\nfabricate\nfabricate microfluidic structures\nfabrication technique\nlaser ablation\nmicrofluidic\nmicrofluidics\nPDMS\nPDMS-based LOC devices\nPDMS-based microfluidics\nphotolithographic patterning\nPolydimethylsiloxane\nprepolymer\nrapid prototyping\nrapid prototyping service\nReplica molding\nreplication of molds\nSoftLithoBox\u00ae\n\u201csoft-lithography\u201d techniques\nsoft-lithography techniques\nwet and dry etching\n"}, {"id": "S0167931714004456", "text": "PDMS (Polydimethylsiloxane) has become by far the most popular material in the academic microfluidics community because it is inexpensive, easy to fabricate by replication of molds made using rapid prototyping or other techniques, flexible, optically transparent, biocompatible and its fabrication does not require high capital investment and cleanroom conditions. Various techniques have been adapted to fabricate microfluidic structures in PDMS, including wet and dry etching [20\u201322], photolithographic patterning of a photosensitive PDMS [23], and laser ablation [24]. But, it was the \u201csoft-lithography\u201d techniques [25] introduced by Whitesides et al. that enabled the widespread use of PDMS and opened up the era of PDMS-based microfluidics in the late 1990s. Replica molding, which is the casting of prepolymer against a master and generating a replica of the master in PDMS, has become a standard fabrication technique available in almost every research laboratory. Detailed overviews of soft-lithography techniques and their applications can be found from the reviews by McDonald et al. [26] and Sia et al. [27]. Nowadays, many tools dedicated for this purpose are available and can be purchased as a complete set (e.g. SoftLithoBox\u00ae provided by Elveflow (USA) [28]). Moreover, companies, such as FlowJEM (Canada) [29], Microfluidic Innovations (USA) [30], and Scientific Device Laboratory (USA) [31] provide rapid prototyping service for PDMS-based LOC devices.\n", "keywords": "casting of prepolymer against a master and generating a replica of the master in PDMS\nfabricate\nfabricate microfluidic structures\nfabrication technique\nlaser ablation\nmicrofluidic\nmicrofluidics\nPDMS\nPDMS-based LOC devices\nPDMS-based microfluidics\nphotolithographic patterning\nPolydimethylsiloxane\nprepolymer\nrapid prototyping\nrapid prototyping service\nReplica molding\nreplication of molds\nSoftLithoBox\u00ae\n\u201csoft-lithography\u201d techniques\nsoft-lithography techniques\nwet and dry etching\n"}, {"id": "S0167931714004456", "text": "PDMS (Polydimethylsiloxane) has become by far the most popular material in the academic microfluidics community because it is inexpensive, easy to fabricate by replication of molds made using rapid prototyping or other techniques, flexible, optically transparent, biocompatible and its fabrication does not require high capital investment and cleanroom conditions. Various techniques have been adapted to fabricate microfluidic structures in PDMS, including wet and dry etching [20\u201322], photolithographic patterning of a photosensitive PDMS [23], and laser ablation [24]. But, it was the \u201csoft-lithography\u201d techniques [25] introduced by Whitesides et al. that enabled the widespread use of PDMS and opened up the era of PDMS-based microfluidics in the late 1990s. Replica molding, which is the casting of prepolymer against a master and generating a replica of the master in PDMS, has become a standard fabrication technique available in almost every research laboratory. Detailed overviews of soft-lithography techniques and their applications can be found from the reviews by McDonald et al. [26] and Sia et al. [27]. Nowadays, many tools dedicated for this purpose are available and can be purchased as a complete set (e.g. SoftLithoBox\u00ae provided by Elveflow (USA) [28]). Moreover, companies, such as FlowJEM (Canada) [29], Microfluidic Innovations (USA) [30], and Scientific Device Laboratory (USA) [31] provide rapid prototyping service for PDMS-based LOC devices.\n", "keywords": "casting of prepolymer against a master and generating a replica of the master in PDMS\nfabricate\nfabricate microfluidic structures\nfabrication technique\nlaser ablation\nmicrofluidic\nmicrofluidics\nPDMS\nPDMS-based LOC devices\nPDMS-based microfluidics\nphotolithographic patterning\nPolydimethylsiloxane\nprepolymer\nrapid prototyping\nrapid prototyping service\nReplica molding\nreplication of molds\nSoftLithoBox\u00ae\n\u201csoft-lithography\u201d techniques\nsoft-lithography techniques\nwet and dry etching\n"}, {"id": "S0029549314002970", "text": "The design, and the temperature reached in the sample holders, guarantees that the Na remains liquid during operation to improve the heating transfer and avoiding solid formation (too cold working temperature) or sodium boiling (too hot working temperature). The temperature above and just below the Na surface will be monitored by six dedicated thermocouples. In order to prevent oxidation of the Na, the plenum of the 1st containment is filled with high-purity He at 0.1MPa, sealed after final assembly and kept closed during in-pile operation (no gas circulation in the 1st containment). The heat generated by fission and gamma absorption in the materials will be radially dissipated through the Na bath, the structural materials and the gas gaps by conduction and radiation to the downstream primary coolant of the TRIO wet channel.\n", "keywords": "fission and gamma absorption\ngas\ngas circulation\nhigh-purity He\nimprove the heating transfer\nmonitored by six dedicated thermocouples\nNa\nNa bath\nNa surface\noxidation\nsample holders\nsodium\nsodium boiling\nsolid formation\nTRIO wet channel\n"}, {"id": "S0022311514006941", "text": "The formulation in Table 1 was derived by an empirical approach and led to a non-classical glass matrix. Carter et al. [3] and Zhang et al. [4] took a more systematic approach to such glass-ceramic wasteforms. These wasteforms were targeted at Hanford K-basin sludges and the immobilisation of the primary waste stream from production of molybdenum-99 at the Australian Nuclear Science and Technology Organisation site in Sydney respectively. In the work of Carter et al. and Zhang et al. the intended crystalline phase was the closely related titanate pyrochlore, CaUTi2O7. The glass matrix was formulated such that the trivalent species in the glass network, boron and aluminium, were charge compensated on a molar basis by sodium. The stoichiometric composition of the glass in this wasteform was Na2AlBSi6O16. This glass provides a method by which the glass composition can be varied systematically. Given that the initial observations inferred an important role played by alumina, it was decided to prepare a suite of zirconolite glass-ceramics in which the glass matrix was defined by Na2Al1+xB1\u2013xSi6O16 to investigate the role played by glass composition in controlling crystalline phase stability. The x=1 end member gives the mineral albite, NaAlSi3O8. The melting point of albite is 1120\u00b0C [5] and the composition cools to a glass at the cooling rates that occur during a HIP cycle. From the available phase diagrams, [6] no boron analogue for albite was shown, and the liquidus estimated from the relevant phase diagram is 1100\u20131200\u00b0C. No phase diagrams for the quaternary system Na2O\u2013Al2O3\u2013B2O3\u2013SiO2 could be found.\n", "keywords": "albite\nalumina\naluminium\nboron\nCaUTi2O7\ncrystalline\nglass\nglass-ceramic wasteforms\nglass composition\nglass matrix\nglass network\nHanford K-basin sludges\nimmobilisation of the primary waste stream from production of molybdenum-99\ninvestigate the role played by glass composition in controlling crystalline phase stability\nmineral albite\nmolybdenum-99\nNa2Al1+xB1\u2013xSi6O16\nNa2AlBSi6O16\nNa2O\u2013Al2O3\u2013B2O3\u2013SiO2\nNaAlSi3O8\nnon-classical glass matrix\nsodium\ntitanate pyrochlore\nzirconolite glass-ceramics\n"}, {"id": "S002199911500025X", "text": "A fully-coupled numerical framework for two-phase flows with an implicit implementation of surface tension has been introduced in this article. This fully-coupled framework has then been used to compare the influence of the surface tension treatment on the time-step restrictions resulting from capillary waves. The conducted study demonstrates that restrictions on the numerical time-step resulting from capillary waves are valid and unchanged regardless of the numerical treatment of surface tension. Since surface tension is not a function of pressure or velocity, the change in implementation does not affect the matrix coefficients of the primitive variables and, thus, numerical stability is independent of the treatment of surface tension. Further analysis shows that the capillary time-step constraint is a requirement imposed by the spatiotemporal sampling of capillary waves, which is independent of the applied numerical methodology.\n", "keywords": "applied numerical methodology\ncapillary waves\ncompare the influence\nfully-coupled framework\nfully-coupled numerical framework\nnumerical treatment of surface tension\nrestrictions on the numerical time-step resulting from capillary waves\nspatiotemporal sampling of capillary waves\nsurface tension\nsurface tension treatment\ntime-step restrictions resulting from capillary waves\ntreatment of surface tension\ntwo-phase flows\n"}, {"id": "S0029549313003439", "text": "An essential part of nuclear reactor analysis is the prediction of the three-dimensional space-time kinetics of neutrons in a relatively large, finite, heterogeneous, three-dimensional reactor core. In a majority of safety analyses the prediction of reactor physics responses is performed using neutron diffusion theory applied to three-dimensional systems, with inputs usually derived from deterministic neutron transport solutions of two-dimensional lattice geometries. There has been increased activity related to uncertainty and sensitivity in reactor physics calculations, and the Organization for Economic Cooperation and Development \u2013 Nuclear Energy Agency (OECD-NEA) has sponsored an ongoing benchmark entitled \u201cUncertainty Analysis in Modelling\u201d (UAM) related to these efforts. The goal of this work is to offer a strategy for computing lattice sensitivities using the DRAGON lattice code and WIMS-D4 multi-group library. Results are presented with comparison to those from TSUNAMI, developed by Oak Ridge National Laboratories.\n", "keywords": "deterministic neutron transport solutions\nDRAGON lattice code\nneutron\nneutron diffusion theory applied to three-dimensional systems\nneutrons\nnuclear reactor\nnuclear reactor analysis\nprediction of reactor physics responses\nprediction of the three-dimensional space-time kinetics of neutrons\nreactor\nreactor core\nreactor physics calculations\nsafety analyses\nstrategy for computing lattice sensitivities\nTSUNAMI\ntwo-dimensional lattice geometries\nUAM\nUncertainty Analysis in Modelling\nWIMS-D4 multi-group library\n"}, {"id": "S0029549313003439", "text": "An essential part of nuclear reactor analysis is the prediction of the three-dimensional space-time kinetics of neutrons in a relatively large, finite, heterogeneous, three-dimensional reactor core. In a majority of safety analyses the prediction of reactor physics responses is performed using neutron diffusion theory applied to three-dimensional systems, with inputs usually derived from deterministic neutron transport solutions of two-dimensional lattice geometries. There has been increased activity related to uncertainty and sensitivity in reactor physics calculations, and the Organization for Economic Cooperation and Development \u2013 Nuclear Energy Agency (OECD-NEA) has sponsored an ongoing benchmark entitled \u201cUncertainty Analysis in Modelling\u201d (UAM) related to these efforts. The goal of this work is to offer a strategy for computing lattice sensitivities using the DRAGON lattice code and WIMS-D4 multi-group library. Results are presented with comparison to those from TSUNAMI, developed by Oak Ridge National Laboratories.\n", "keywords": "deterministic neutron transport solutions\nDRAGON lattice code\nneutron\nneutron diffusion theory applied to three-dimensional systems\nneutrons\nnuclear reactor\nnuclear reactor analysis\nprediction of reactor physics responses\nprediction of the three-dimensional space-time kinetics of neutrons\nreactor\nreactor core\nreactor physics calculations\nsafety analyses\nstrategy for computing lattice sensitivities\nTSUNAMI\ntwo-dimensional lattice geometries\nUAM\nUncertainty Analysis in Modelling\nWIMS-D4 multi-group library\n"}, {"id": "S0029549313003439", "text": "An essential part of nuclear reactor analysis is the prediction of the three-dimensional space-time kinetics of neutrons in a relatively large, finite, heterogeneous, three-dimensional reactor core. In a majority of safety analyses the prediction of reactor physics responses is performed using neutron diffusion theory applied to three-dimensional systems, with inputs usually derived from deterministic neutron transport solutions of two-dimensional lattice geometries. There has been increased activity related to uncertainty and sensitivity in reactor physics calculations, and the Organization for Economic Cooperation and Development \u2013 Nuclear Energy Agency (OECD-NEA) has sponsored an ongoing benchmark entitled \u201cUncertainty Analysis in Modelling\u201d (UAM) related to these efforts. The goal of this work is to offer a strategy for computing lattice sensitivities using the DRAGON lattice code and WIMS-D4 multi-group library. Results are presented with comparison to those from TSUNAMI, developed by Oak Ridge National Laboratories.\n", "keywords": "deterministic neutron transport solutions\nDRAGON lattice code\nneutron\nneutron diffusion theory applied to three-dimensional systems\nneutrons\nnuclear reactor\nnuclear reactor analysis\nprediction of reactor physics responses\nprediction of the three-dimensional space-time kinetics of neutrons\nreactor\nreactor core\nreactor physics calculations\nsafety analyses\nstrategy for computing lattice sensitivities\nTSUNAMI\ntwo-dimensional lattice geometries\nUAM\nUncertainty Analysis in Modelling\nWIMS-D4 multi-group library\n"}, {"id": "S0254058415001212", "text": "Gamma titanium aluminides are a family of low density, high performance alloys with the potential to replace current Ni-base superalloys used in the production of aero-engine components. Investment casting is one of the most economical methods to produce titanium and titanium aluminide alloy products, increasing the components' integrity and mechanical properties, whilst reducing material waste and machining cost [1]. Titanium aluminides are difficult to process mainly due to the low fluidity of the TiAl alloy around its melting temperature [2]. Due to the high affinity of elements such as oxygen, nitrogen etc., titanium and its alloys can easily interact with mould materials during the investment casting process, resulting in an interaction hardened layer being generated at the metal surface [3,4]. This hardened layer contains a large amount of dissolved oxygen, and it is very brittle and susceptible to crack generation and propagation [5].\n", "keywords": "aero-engine components\nalloys\ncasting process\ncrack generation and propagation\ndissolved oxygen\nelements\nGamma titanium aluminides\nhardened layer\ninteraction hardened layer\nInvestment casting\nmetal surface\nmould materials\nNi-base superalloys\nnitrogen\noxygen\nproduce titanium and titanium aluminide alloy products\nproduction of aero-engine components\nthe components\nTiAl alloy\ntitanium\ntitanium aluminide alloy\nTitanium aluminides\n"}, {"id": "S2212667814000756", "text": "This study is focused on the water-gas shift reaction (WGSR), occurring in the chemical kinetics equipment, which is used to increase hydrogen recovery from industrial processes. The research deals with comparing hydrogen recovery with the use of three different catalysts. The amount of the produced hydrogen depends considerably on the reaction state and the catalyst composition. To improve the course of the reaction, natural catalysts \u2013 calcite, coal char (unburned residues from coal) and modified olivine \u2013 are added to the gasification process and heated to the process temperature of 800, 850 and 900oC.", "keywords": "calcite\ncatalyst\ncatalysts\nchemical kinetics equipment\ncoal char\ngas\ngasification process\nheated\nhydrogen\nhydrogen recovery\nindustrial processes\nmodified olivine\nnatural catalysts\nreaction\nunburned residues from coal\nwater\nwater-gas shift reaction\nWGSR\n"}, {"id": "S2212667814000756", "text": "This study is focused on the water-gas shift reaction (WGSR), occurring in the chemical kinetics equipment, which is used to increase hydrogen recovery from industrial processes. The research deals with comparing hydrogen recovery with the use of three different catalysts. The amount of the produced hydrogen depends considerably on the reaction state and the catalyst composition. To improve the course of the reaction, natural catalysts \u2013 calcite, coal char (unburned residues from coal) and modified olivine \u2013 are added to the gasification process and heated to the process temperature of 800, 850 and 900oC.", "keywords": "calcite\ncatalyst\ncatalysts\nchemical kinetics equipment\ncoal char\ngas\ngasification process\nheated\nhydrogen\nhydrogen recovery\nindustrial processes\nmodified olivine\nnatural catalysts\nreaction\nunburned residues from coal\nwater\nwater-gas shift reaction\nWGSR\n"}, {"id": "S1010603013001809", "text": "The other methods for enhancement of photocatalytic activity are grafting co-catalysts. There are two kinds of co-catalysts in terms of its function: one is for separation of electrons and the other is for separation of holes. The former representative co-catalysts are Pt, Fe3+, and Cu2+ [9\u201312]. It was reported that Fe3+ and Cu2+ were grafted as amorphous oxide cluster [9,10], and reduced into Fe2+ and Cu+ by receiving one electron, respectively [11,12]. The reduced metal oxide cluster with reduced ions could return into the original state by giving more than one electron to molecular oxygen. The latter ones are CoOx, CoPi (CoPOx), IrOx, and RuOx which are used for water oxidation, among which CoPi is reported to be the most effective co-catalyst for water oxidation [13]. However, there were few reports concerning co-grafting effects on photocatalytic activity especially in gaseous phase. We expected that by co-grafting of both co-catalysts for separations of electrons and holes, photocatalytic activity in gaseous phase would be further enhanced. Moreover, complex of BiVO4 with the other materials of p-type semiconductor is also effective for enhancing photocatalytic activity.\n", "keywords": "amorphous oxide cluster\nBiVO4\nco-catalyst\nco-catalysts\nco-grafting\nCoOx\nCoPi\nCoPOx\nCu+\nCu2+\nelectron\nelectrons\nenhancement of photocatalytic activity\nFe2+\nFe3+\ngaseous phase\nholes\nIrOx\noxygen\nphotocatalytic activity\nPt\np-type semiconductor\nreduced ions\nreduced metal oxide cluster\nRuOx\nseparation of electrons\nseparation of holes\nwater oxidation\n"}, {"id": "S1010603013001809", "text": "The other methods for enhancement of photocatalytic activity are grafting co-catalysts. There are two kinds of co-catalysts in terms of its function: one is for separation of electrons and the other is for separation of holes. The former representative co-catalysts are Pt, Fe3+, and Cu2+ [9\u201312]. It was reported that Fe3+ and Cu2+ were grafted as amorphous oxide cluster [9,10], and reduced into Fe2+ and Cu+ by receiving one electron, respectively [11,12]. The reduced metal oxide cluster with reduced ions could return into the original state by giving more than one electron to molecular oxygen. The latter ones are CoOx, CoPi (CoPOx), IrOx, and RuOx which are used for water oxidation, among which CoPi is reported to be the most effective co-catalyst for water oxidation [13]. However, there were few reports concerning co-grafting effects on photocatalytic activity especially in gaseous phase. We expected that by co-grafting of both co-catalysts for separations of electrons and holes, photocatalytic activity in gaseous phase would be further enhanced. Moreover, complex of BiVO4 with the other materials of p-type semiconductor is also effective for enhancing photocatalytic activity.\n", "keywords": "amorphous oxide cluster\nBiVO4\nco-catalyst\nco-catalysts\nco-grafting\nCoOx\nCoPi\nCoPOx\nCu+\nCu2+\nelectron\nelectrons\nenhancement of photocatalytic activity\nFe2+\nFe3+\ngaseous phase\nholes\nIrOx\noxygen\nphotocatalytic activity\nPt\np-type semiconductor\nreduced ions\nreduced metal oxide cluster\nRuOx\nseparation of electrons\nseparation of holes\nwater oxidation\n"}, {"id": "S0885230816300043", "text": "The final set of experiments involved an adaptive retraining of the GMM\u2013HMM parameters following the aNAT procedure. This new model only provided an improvement of 0.3%, similar to using the aCMLLR transforms on the baseline GMM\u2013HMM model. However, training show-based aCMLLR transforms on top of the adaptively trained model boosted the improvement to 0.8% absolute. This showed how adaptive training provided a better flexibility of the model to adapt to specific background conditions existing in each show. Finally, the factorisation approach using MLLR speaker transforms on top of the aNAT model and show-based aCMLLR transforms was tested. This only increased the improvement to 0.9% absolute (2.9% relative), which reflects the difficulty of performing accurate speaker clustering in this task and how this actually hampers speaker adaptation.\n", "keywords": "aCMLLR transforms\nadaptive retraining of the GMM\u2013HMM parameters\nadaptive training\naNAT model\naNAT procedure\nfactorisation approach using MLLR speaker transforms\nGMM\u2013HMM model\nspeaker adaptation\nspeaker clustering\ntraining show-based aCMLLR transforms\n"}, {"id": "S0021999116303291", "text": "DPD was first proposed in order to recover the properties of isotropy (and Galilean invariance) that were broken in the so-called lattice-gas automata method [5]. In DPD, each body is regarded as a coarse-grained particle. These particles interact in a soft (and short-ranged) potential, allowing larger integration timesteps than would be possible in MD, while simultaneously decreasing the number of degrees of freedom required. As in Langevin dynamics, a thermostat consisting of well-balanced damping and stochastic terms is applied to each particle. However, unlike in Langevin dynamics, both terms are pairwise and the damping term is based on relative velocities, leading to the conservation of both the angular momentum and the linear momentum. The property of Galilean invariance (i.e., the dependence on the relative velocity) makes DPD a profile-unbiased thermostat (PUT) [6,7] by construction and thus it is an ideal thermostat for nonequilibrium molecular dynamics (NEMD) [8]. The momentum is expected to propagate locally (while global momentum is conserved) and thus the correct hydrodynamics is expected in DPD [8], as demonstrated previously in [9]. Due to the aforementioned properties, DPD has been widely used to recover thermodynamic, dynamical, and rheological properties of complex fluids, with applications in polymer solutions [10], colloidal suspensions [11], multiphase flows [12], and biological systems [13]. DPD has been compared with Langevin dynamics for out-of-equilibrium simulations of polymeric systems in [14], where as expected the correct dynamic fluctuations of the polymers were obtained with the former but not with the latter.\n", "keywords": "angular momentum\nbiological systems\ncoarse-grained particle\ncolloidal suspensions\ncomplex fluids\ndependence on the relative velocity\nDPD\ndynamic fluctuations\nGalilean invariance\nisotropy\nLangevin dynamics\nlattice-gas automata\nlinear momentum\nMD\nmultiphase flows\nNEMD\nnonequilibrium molecular dynamics\nnonequilibrium molecular dynamics (NEMD)\nparticles\npolymeric systems\npolymers\npolymer solutions\nprofile-unbiased thermostat\nPUT\nrecover thermodynamic, dynamical, and rheological properties\nthermostat\n"}, {"id": "S0021999116303291", "text": "DPD was first proposed in order to recover the properties of isotropy (and Galilean invariance) that were broken in the so-called lattice-gas automata method [5]. In DPD, each body is regarded as a coarse-grained particle. These particles interact in a soft (and short-ranged) potential, allowing larger integration timesteps than would be possible in MD, while simultaneously decreasing the number of degrees of freedom required. As in Langevin dynamics, a thermostat consisting of well-balanced damping and stochastic terms is applied to each particle. However, unlike in Langevin dynamics, both terms are pairwise and the damping term is based on relative velocities, leading to the conservation of both the angular momentum and the linear momentum. The property of Galilean invariance (i.e., the dependence on the relative velocity) makes DPD a profile-unbiased thermostat (PUT) [6,7] by construction and thus it is an ideal thermostat for nonequilibrium molecular dynamics (NEMD) [8]. The momentum is expected to propagate locally (while global momentum is conserved) and thus the correct hydrodynamics is expected in DPD [8], as demonstrated previously in [9]. Due to the aforementioned properties, DPD has been widely used to recover thermodynamic, dynamical, and rheological properties of complex fluids, with applications in polymer solutions [10], colloidal suspensions [11], multiphase flows [12], and biological systems [13]. DPD has been compared with Langevin dynamics for out-of-equilibrium simulations of polymeric systems in [14], where as expected the correct dynamic fluctuations of the polymers were obtained with the former but not with the latter.\n", "keywords": "angular momentum\nbiological systems\ncoarse-grained particle\ncolloidal suspensions\ncomplex fluids\ndependence on the relative velocity\nDPD\ndynamic fluctuations\nGalilean invariance\nisotropy\nLangevin dynamics\nlattice-gas automata\nlinear momentum\nMD\nmultiphase flows\nNEMD\nnonequilibrium molecular dynamics\nnonequilibrium molecular dynamics (NEMD)\nparticles\npolymeric systems\npolymers\npolymer solutions\nprofile-unbiased thermostat\nPUT\nrecover thermodynamic, dynamical, and rheological properties\nthermostat\n"}, {"id": "S0957417416302561", "text": "\u2022More efforts should be directed towards advancing the methods of feature extraction to overcome the influence of dynamic factors that limit the performance. The use of advanced machine learning methods such as deep neural networks and muscles synergies extraction should also be investigated on problems under the influence of multiple dynamic factors as such methods may provide substantial improvements upon the utilized time-and-frequency EMG feature extraction methods (Diener, Janke, & Schultz, 2015; Ison, Vujaklija, Whitsell, Farina, & Artemiadis, 2016; Park & Lee, 2016). Meanwhile, we showed that the performance of the learning algorithms can be improved by using feature extraction methods that rely on the angular information of muscle activation patterns. Features such as the TD-PSD and the DFT proved more successful than others in reducing the impact of the two dynamic factors that we considered in this paper. Such features can be readily implemented into a prosthesis controller for real-time control, especially that the EMG pattern recognition systems are nowadays becoming available for clinical testing, e.g. the COAPT complete control system (Kuiken et\u00a0al., 2014)11https://www.coaptengineering.com/.\n", "keywords": "advanced machine learning methods\nclinical testing\nCOAPT complete control system\ndeep neural networks\nDFT\nEMG pattern recognition systems\nfeature extraction\nfeature extraction methods\nFeatures\nmuscles synergies extraction\nperformance\nprosthesis controller\nreal-time control\nreducing the impact of the two dynamic factors\nTD-PSD\nutilized time-and-frequency EMG feature extraction methods\n"}, {"id": "S0957417416302561", "text": "\u2022More efforts should be directed towards advancing the methods of feature extraction to overcome the influence of dynamic factors that limit the performance. The use of advanced machine learning methods such as deep neural networks and muscles synergies extraction should also be investigated on problems under the influence of multiple dynamic factors as such methods may provide substantial improvements upon the utilized time-and-frequency EMG feature extraction methods (Diener, Janke, & Schultz, 2015; Ison, Vujaklija, Whitsell, Farina, & Artemiadis, 2016; Park & Lee, 2016). Meanwhile, we showed that the performance of the learning algorithms can be improved by using feature extraction methods that rely on the angular information of muscle activation patterns. Features such as the TD-PSD and the DFT proved more successful than others in reducing the impact of the two dynamic factors that we considered in this paper. Such features can be readily implemented into a prosthesis controller for real-time control, especially that the EMG pattern recognition systems are nowadays becoming available for clinical testing, e.g. the COAPT complete control system (Kuiken et\u00a0al., 2014)11https://www.coaptengineering.com/.\n", "keywords": "advanced machine learning methods\nclinical testing\nCOAPT complete control system\ndeep neural networks\nDFT\nEMG pattern recognition systems\nfeature extraction\nfeature extraction methods\nFeatures\nmuscles synergies extraction\nperformance\nprosthesis controller\nreal-time control\nreducing the impact of the two dynamic factors\nTD-PSD\nutilized time-and-frequency EMG feature extraction methods\n"}, {"id": "S004578251300176X", "text": "Powder metallurgy is a versatile technology for the manufacturing of components to (near) net-shape with high product quality. For a hardmetal (such as WC-Co) cold compaction of the powder to a \u201cgreen body\u201d is followed by liquid-phase sintering from the subsequent heating. This means that the binder metal Co is heated to melt in order to obtain sufficient mobility via capillary action, i.e., via surface traction, stemming from stored surface energy. The resulting flow causes gradual filling of the pore space and brings about a macroscopic shrinkage of the particle compact until a completely dense state is obtained, at least ideally. To model and quantitatively simulate the sintering process is a challenging task. The goal is to (i) estimate the final resulting quality (i.e., in terms of porosity) and (ii) to predict the final net shape and size of the sintered component.\n", "keywords": "Co\nCo is heated\ndense state\nestimate the final resulting quality\ngradual filling of the pore\ngreen body\nhardmetal\nheating\nliquid-phase sintering\nmacroscopic shrinkage\nmodel and quantitatively simulate the sintering process\nobtain sufficient mobility via capillary action\nparticle\nporosity\npowder\nPowder metallurgy\nsintered component\nsintering process\nsurface traction\ntechnology for the manufacturing of components to (near) net-shape\nto predict the final net shape and size\nWC-Co\n"}, {"id": "S0021999113006955", "text": "The test cases confirm that the high-order discretisation retains exponential convergence properties with increasing geometric and expansion polynomial order if both the solution and true surface are smooth. Errors are found to saturate when the geometric errors, due to the parametrisation of the surface elements, begin to dominate the temporal and spatial discretisation errors. For the smooth solutions considered as test cases, the results show that this dominance of geometric errors quickly limits the effectiveness of further increases in the number of degrees of freedom, either through mesh refinement or higher solution polynomial orders. Increasing the order of the geometry parametrisation reduces the geometric error. The analytic test cases presented here use a coarse curvilinear mesh; for applications, meshes are typically more refined in order to capture features in the solution and so will better capture the geometry and consequently reduce this lower bound on the solution error. If the solution is not smooth, we do not expect to see rapid convergence. In the case that the solution is smooth, but the true surface is not, then exponential convergence with P and Pg can only be achieved if, and only if, the discontinuities are aligned with element boundaries. However, if discontinuities lie within an element, convergence will be limited by the geometric approximation, since the true surface cannot be captured. In the cardiac problem, we consider both the true surface and solution to be smooth.\n", "keywords": "analytic test cases\ncapture features in the solution\ncapture the geometry\ncardiac problem\ncoarse curvilinear mesh\ndiscontinuities are aligned with element boundaries\nelement\nelement boundaries\nexponential convergence\nexponential convergence properties\ngeometric approximation\ngeometric error\ngeometry parametrisation\nhigher solution polynomial orders\nhigh-order discretisation\nmeshes\nmesh refinement\nP\nparametrisation of the surface elements\nPg\nreduce this lower bound\nsmooth solutions\nsolution\nsurface elements\ntrue surface\n"}, {"id": "S0021999113006955", "text": "The test cases confirm that the high-order discretisation retains exponential convergence properties with increasing geometric and expansion polynomial order if both the solution and true surface are smooth. Errors are found to saturate when the geometric errors, due to the parametrisation of the surface elements, begin to dominate the temporal and spatial discretisation errors. For the smooth solutions considered as test cases, the results show that this dominance of geometric errors quickly limits the effectiveness of further increases in the number of degrees of freedom, either through mesh refinement or higher solution polynomial orders. Increasing the order of the geometry parametrisation reduces the geometric error. The analytic test cases presented here use a coarse curvilinear mesh; for applications, meshes are typically more refined in order to capture features in the solution and so will better capture the geometry and consequently reduce this lower bound on the solution error. If the solution is not smooth, we do not expect to see rapid convergence. In the case that the solution is smooth, but the true surface is not, then exponential convergence with P and Pg can only be achieved if, and only if, the discontinuities are aligned with element boundaries. However, if discontinuities lie within an element, convergence will be limited by the geometric approximation, since the true surface cannot be captured. In the cardiac problem, we consider both the true surface and solution to be smooth.\n", "keywords": "analytic test cases\ncapture features in the solution\ncapture the geometry\ncardiac problem\ncoarse curvilinear mesh\ndiscontinuities are aligned with element boundaries\nelement\nelement boundaries\nexponential convergence\nexponential convergence properties\ngeometric approximation\ngeometric error\ngeometry parametrisation\nhigher solution polynomial orders\nhigh-order discretisation\nmeshes\nmesh refinement\nP\nparametrisation of the surface elements\nPg\nreduce this lower bound\nsmooth solutions\nsolution\nsurface elements\ntrue surface\n"}, {"id": "S0021999113006955", "text": "The test cases confirm that the high-order discretisation retains exponential convergence properties with increasing geometric and expansion polynomial order if both the solution and true surface are smooth. Errors are found to saturate when the geometric errors, due to the parametrisation of the surface elements, begin to dominate the temporal and spatial discretisation errors. For the smooth solutions considered as test cases, the results show that this dominance of geometric errors quickly limits the effectiveness of further increases in the number of degrees of freedom, either through mesh refinement or higher solution polynomial orders. Increasing the order of the geometry parametrisation reduces the geometric error. The analytic test cases presented here use a coarse curvilinear mesh; for applications, meshes are typically more refined in order to capture features in the solution and so will better capture the geometry and consequently reduce this lower bound on the solution error. If the solution is not smooth, we do not expect to see rapid convergence. In the case that the solution is smooth, but the true surface is not, then exponential convergence with P and Pg can only be achieved if, and only if, the discontinuities are aligned with element boundaries. However, if discontinuities lie within an element, convergence will be limited by the geometric approximation, since the true surface cannot be captured. In the cardiac problem, we consider both the true surface and solution to be smooth.\n", "keywords": "analytic test cases\ncapture features in the solution\ncapture the geometry\ncardiac problem\ncoarse curvilinear mesh\ndiscontinuities are aligned with element boundaries\nelement\nelement boundaries\nexponential convergence\nexponential convergence properties\ngeometric approximation\ngeometric error\ngeometry parametrisation\nhigher solution polynomial orders\nhigh-order discretisation\nmeshes\nmesh refinement\nP\nparametrisation of the surface elements\nPg\nreduce this lower bound\nsmooth solutions\nsolution\nsurface elements\ntrue surface\n"}, {"id": "S0021999113006955", "text": "The test cases confirm that the high-order discretisation retains exponential convergence properties with increasing geometric and expansion polynomial order if both the solution and true surface are smooth. Errors are found to saturate when the geometric errors, due to the parametrisation of the surface elements, begin to dominate the temporal and spatial discretisation errors. For the smooth solutions considered as test cases, the results show that this dominance of geometric errors quickly limits the effectiveness of further increases in the number of degrees of freedom, either through mesh refinement or higher solution polynomial orders. Increasing the order of the geometry parametrisation reduces the geometric error. The analytic test cases presented here use a coarse curvilinear mesh; for applications, meshes are typically more refined in order to capture features in the solution and so will better capture the geometry and consequently reduce this lower bound on the solution error. If the solution is not smooth, we do not expect to see rapid convergence. In the case that the solution is smooth, but the true surface is not, then exponential convergence with P and Pg can only be achieved if, and only if, the discontinuities are aligned with element boundaries. However, if discontinuities lie within an element, convergence will be limited by the geometric approximation, since the true surface cannot be captured. In the cardiac problem, we consider both the true surface and solution to be smooth.\n", "keywords": "analytic test cases\ncapture features in the solution\ncapture the geometry\ncardiac problem\ncoarse curvilinear mesh\ndiscontinuities are aligned with element boundaries\nelement\nelement boundaries\nexponential convergence\nexponential convergence properties\ngeometric approximation\ngeometric error\ngeometry parametrisation\nhigher solution polynomial orders\nhigh-order discretisation\nmeshes\nmesh refinement\nP\nparametrisation of the surface elements\nPg\nreduce this lower bound\nsmooth solutions\nsolution\nsurface elements\ntrue surface\n"}, {"id": "S0032386109004996", "text": "SPM, and AFM in particular, has been widely applied to questions in polymer crystallization. The technique has several strengths that make it ideally suited for such studies. It is a high resolution technology, routinely resolving sub 10nm features [13,14], and hence allowing the fundamental length scale of the polymer lamellar crystal, its thickness, to be observed. AFM requires no staining or metal coating of the sample, so sample preparation is relatively straightforward. Also, it is non-destructive under many circumstances. This allows images to be obtained while a process such as crystal growth or melting is occurring, giving time-resolved data at lamellar or sub-lamellar resolution [15\u201318]. It is this final feature that provides many of the most exciting possibilities of AFM for studying polymer crystallization, as it is now possible to watch crystal growth, crystal melting, and re-organisations within crystals at the lamellar scale, seeing how structure evolves and local conditions influence kinetics. AFM has a wide range of different measuring modes, and, with the ever increasing number of functional semicrystalline polymers available (e.g. [19]), the breadth of experiments that can be carried out with a single machine is also one of the techniques attractions.\n", "keywords": "AFM\ncrystal\ncrystal growth\ncrystal melting\ncrystals\nhigh resolution technology\nmelting\nmetal coating\npolymer\npolymer crystallization\npolymer lamellar crystal\nquestions in polymer crystallization\nre-organisations within crystals at the lamellar scale\nresolving sub 10nm features\nsample preparation\nsemicrystalline polymers\nSPM\nstaining\nstudying polymer crystallization\ntime-resolved data\n"}, {"id": "S0379711213001653", "text": "With development of performance-based design, some studies have been conducted on fire risk analysis in buildings from different perspectives and levels. Models such as FiRECAM [11,12] and FiERAsystem [13] were used to calculate the expected life risk. In other studies probabilistic methods have been used to assess levels of people safety in buildings [14]. Quantitative risk analysis approaches have also been used to quantify the risk to occupants using stochastic factors [15]. However, studies to date have largely been concerned with various aspects of fire risk analysis and there has been little in the way of development of systematic theoretical methods for analyzing fire risk in buildings in terms of fire risk management. Existing fire risk management involves the identification of alternative fire safety design options [16,17], the ongoing inspection, maintenance of fire protection systems [18] and evacuation training and drills [19]. In this study, basic process of fire risk analysis in building is described, and a fire risk analysis model based on scenario clusters is established with consideration of the characteristics of fire dynamics and occupants' behavior. The number of deaths and directive property loss are selected as fire risk indices and the average fire risk of residential buildings is quantitatively analyzed, so that appropriate fire risk management measures can be adopted.\n", "keywords": "aspects of fire risk analysis\naverage fire risk\nbasic process of fire risk analysis\nevacuation training and drills\nFiERAsystem\nFiRECAM\nidentification of alternative fire safety design options\nmaintenance of fire protection systems\nModels\nnumber of deaths and directive property loss\nongoing inspection\nperformance-based design\nprobabilistic methods\nQuantitative risk analysis approaches\nstochastic factors\nsystematic theoretical methods\n"}, {"id": "S0379711213001653", "text": "With development of performance-based design, some studies have been conducted on fire risk analysis in buildings from different perspectives and levels. Models such as FiRECAM [11,12] and FiERAsystem [13] were used to calculate the expected life risk. In other studies probabilistic methods have been used to assess levels of people safety in buildings [14]. Quantitative risk analysis approaches have also been used to quantify the risk to occupants using stochastic factors [15]. However, studies to date have largely been concerned with various aspects of fire risk analysis and there has been little in the way of development of systematic theoretical methods for analyzing fire risk in buildings in terms of fire risk management. Existing fire risk management involves the identification of alternative fire safety design options [16,17], the ongoing inspection, maintenance of fire protection systems [18] and evacuation training and drills [19]. In this study, basic process of fire risk analysis in building is described, and a fire risk analysis model based on scenario clusters is established with consideration of the characteristics of fire dynamics and occupants' behavior. The number of deaths and directive property loss are selected as fire risk indices and the average fire risk of residential buildings is quantitatively analyzed, so that appropriate fire risk management measures can be adopted.\n", "keywords": "aspects of fire risk analysis\naverage fire risk\nbasic process of fire risk analysis\nevacuation training and drills\nFiERAsystem\nFiRECAM\nidentification of alternative fire safety design options\nmaintenance of fire protection systems\nModels\nnumber of deaths and directive property loss\nongoing inspection\nperformance-based design\nprobabilistic methods\nQuantitative risk analysis approaches\nstochastic factors\nsystematic theoretical methods\n"}, {"id": "S0379711213001653", "text": "With development of performance-based design, some studies have been conducted on fire risk analysis in buildings from different perspectives and levels. Models such as FiRECAM [11,12] and FiERAsystem [13] were used to calculate the expected life risk. In other studies probabilistic methods have been used to assess levels of people safety in buildings [14]. Quantitative risk analysis approaches have also been used to quantify the risk to occupants using stochastic factors [15]. However, studies to date have largely been concerned with various aspects of fire risk analysis and there has been little in the way of development of systematic theoretical methods for analyzing fire risk in buildings in terms of fire risk management. Existing fire risk management involves the identification of alternative fire safety design options [16,17], the ongoing inspection, maintenance of fire protection systems [18] and evacuation training and drills [19]. In this study, basic process of fire risk analysis in building is described, and a fire risk analysis model based on scenario clusters is established with consideration of the characteristics of fire dynamics and occupants' behavior. The number of deaths and directive property loss are selected as fire risk indices and the average fire risk of residential buildings is quantitatively analyzed, so that appropriate fire risk management measures can be adopted.\n", "keywords": "aspects of fire risk analysis\naverage fire risk\nbasic process of fire risk analysis\nevacuation training and drills\nFiERAsystem\nFiRECAM\nidentification of alternative fire safety design options\nmaintenance of fire protection systems\nModels\nnumber of deaths and directive property loss\nongoing inspection\nperformance-based design\nprobabilistic methods\nQuantitative risk analysis approaches\nstochastic factors\nsystematic theoretical methods\n"}, {"id": "S0032386109007290", "text": "ELRs are particularly attractive for the synthesis of block copolymers that self-assemble into polymer nanostructures such as\u00a0micelles. The first work in this area involved an elastin-mimetic di-block copolymer containing VPGEG\u2013(IPGAG)4 and VPGFG\u2013(IPGVG)4 as the hydrophilic and hydrophobic blocks, respectively [49]. The resulting micelles were studied by dynamic light scattering (DLS) and DSC was used to measure the enthalpy of self-assembly. A tri-block copolymer was subsequently synthesized and the TEM images of this polymer showed that it formed spherical aggregates [50]. Other multivalent spherical micelles have been obtained from linear elastin-like AB di-block copolymers in the temperature range 37\u201342\u00b0C with the aim of targeting cancer cells [51]. Bidwell et\u00a0al. have also exploited the ELRs for its ability to serve as macromolecular carriers for thermally targeted delivery of drugs. Attachment of doxorubicin to ELR-based system showed enhanced cytotoxicity in uterine sarcoma cells when aggregation was induced with hyperthermia [52].\n", "keywords": "aggregation\nblock copolymers\ncytotoxicity\nDLS\ndoxorubicin\nDSC\ndynamic light scattering\nelastin-mimetic di-block copolymer\nELR-based system\nELRs\nhydrophilic and hydrophobic blocks\nhyperthermia\nlinear elastin-like AB di-block copolymers\nmacromolecular carriers\nmicelles\nmultivalent spherical micelles\npolymer\npolymer nanostructures\nself-assembly\nspherical aggregates\nsynthesis of block copolymers\ntargeting cancer cells\nTEM images\nthermally targeted delivery of drugs\ntri-block copolymer\nuterine sarcoma cells\nVPGEG\u2013(IPGAG)4\nVPGFG\u2013(IPGVG)4\n"}, {"id": "S0032386109007290", "text": "ELRs are particularly attractive for the synthesis of block copolymers that self-assemble into polymer nanostructures such as\u00a0micelles. The first work in this area involved an elastin-mimetic di-block copolymer containing VPGEG\u2013(IPGAG)4 and VPGFG\u2013(IPGVG)4 as the hydrophilic and hydrophobic blocks, respectively [49]. The resulting micelles were studied by dynamic light scattering (DLS) and DSC was used to measure the enthalpy of self-assembly. A tri-block copolymer was subsequently synthesized and the TEM images of this polymer showed that it formed spherical aggregates [50]. Other multivalent spherical micelles have been obtained from linear elastin-like AB di-block copolymers in the temperature range 37\u201342\u00b0C with the aim of targeting cancer cells [51]. Bidwell et\u00a0al. have also exploited the ELRs for its ability to serve as macromolecular carriers for thermally targeted delivery of drugs. Attachment of doxorubicin to ELR-based system showed enhanced cytotoxicity in uterine sarcoma cells when aggregation was induced with hyperthermia [52].\n", "keywords": "aggregation\nblock copolymers\ncytotoxicity\nDLS\ndoxorubicin\nDSC\ndynamic light scattering\nelastin-mimetic di-block copolymer\nELR-based system\nELRs\nhydrophilic and hydrophobic blocks\nhyperthermia\nlinear elastin-like AB di-block copolymers\nmacromolecular carriers\nmicelles\nmultivalent spherical micelles\npolymer\npolymer nanostructures\nself-assembly\nspherical aggregates\nsynthesis of block copolymers\ntargeting cancer cells\nTEM images\nthermally targeted delivery of drugs\ntri-block copolymer\nuterine sarcoma cells\nVPGEG\u2013(IPGAG)4\nVPGFG\u2013(IPGVG)4\n"}, {"id": "S0885230816301759", "text": "This paper proposes a sentence stress feedback system in which sentence stress prediction, detection, and feedback provision models are combined. This system provides non-native learners with feedback on sentence stress errors so that they can improve their English rhythm and fluency in a self-study setting. The sentence stress feedback system was devised to predict and detect the sentence stress of any practice sentence. The accuracy of the prediction and detection models was 96.6% and 84.1%, respectively. The stress feedback provision model offers positive or negative stress feedback for each spoken word by comparing the probability of the predicted stress pattern with that of the detected stress pattern. In an experiment that evaluated the educational effect of the proposed system incorporated in our CALL system, significant improvements in accentedness and rhythm were seen with the students who trained with our system but not with those in the control group.\n", "keywords": "CALL system\npredict and detect the sentence stress\nprediction and detection models\nprovides non-native learners with feedback on sentence stress errors\nsentence stress feedback system\nstress feedback provision model\n"}, {"id": "S0885230816301759", "text": "This paper proposes a sentence stress feedback system in which sentence stress prediction, detection, and feedback provision models are combined. This system provides non-native learners with feedback on sentence stress errors so that they can improve their English rhythm and fluency in a self-study setting. The sentence stress feedback system was devised to predict and detect the sentence stress of any practice sentence. The accuracy of the prediction and detection models was 96.6% and 84.1%, respectively. The stress feedback provision model offers positive or negative stress feedback for each spoken word by comparing the probability of the predicted stress pattern with that of the detected stress pattern. In an experiment that evaluated the educational effect of the proposed system incorporated in our CALL system, significant improvements in accentedness and rhythm were seen with the students who trained with our system but not with those in the control group.\n", "keywords": "CALL system\npredict and detect the sentence stress\nprediction and detection models\nprovides non-native learners with feedback on sentence stress errors\nsentence stress feedback system\nstress feedback provision model\n"}, {"id": "S0885230816301759", "text": "This paper proposes a sentence stress feedback system in which sentence stress prediction, detection, and feedback provision models are combined. This system provides non-native learners with feedback on sentence stress errors so that they can improve their English rhythm and fluency in a self-study setting. The sentence stress feedback system was devised to predict and detect the sentence stress of any practice sentence. The accuracy of the prediction and detection models was 96.6% and 84.1%, respectively. The stress feedback provision model offers positive or negative stress feedback for each spoken word by comparing the probability of the predicted stress pattern with that of the detected stress pattern. In an experiment that evaluated the educational effect of the proposed system incorporated in our CALL system, significant improvements in accentedness and rhythm were seen with the students who trained with our system but not with those in the control group.\n", "keywords": "CALL system\npredict and detect the sentence stress\nprediction and detection models\nprovides non-native learners with feedback on sentence stress errors\nsentence stress feedback system\nstress feedback provision model\n"}, {"id": "S0885230816301759", "text": "This paper proposes a sentence stress feedback system in which sentence stress prediction, detection, and feedback provision models are combined. This system provides non-native learners with feedback on sentence stress errors so that they can improve their English rhythm and fluency in a self-study setting. The sentence stress feedback system was devised to predict and detect the sentence stress of any practice sentence. The accuracy of the prediction and detection models was 96.6% and 84.1%, respectively. The stress feedback provision model offers positive or negative stress feedback for each spoken word by comparing the probability of the predicted stress pattern with that of the detected stress pattern. In an experiment that evaluated the educational effect of the proposed system incorporated in our CALL system, significant improvements in accentedness and rhythm were seen with the students who trained with our system but not with those in the control group.\n", "keywords": "CALL system\npredict and detect the sentence stress\nprediction and detection models\nprovides non-native learners with feedback on sentence stress errors\nsentence stress feedback system\nstress feedback provision model\n"}, {"id": "S0301010414003516", "text": "Arrays of TFTs and circuits were fabricated on precleaned, 5cm\u00d75cm, 125\u03bcm thick polyethylene naphthalate (PEN) substrates (Dupont-Teijin). Full details of our vacuum-fabrication procedures have been given in previous publications [17\u201319,23]. Briefly, aluminium gate electrodes and associated tracks were vacuum evaporated onto the substrates through shadow masks. Subsequently, the substrates were attached to a cooled web-coater drum (Aerre Machines). With the drum rotating at a linear speed of 25m/min under vacuum, flash-evaporated TPGDA monomer vapour which condensed onto the substrates was cross-linked by exposure, in situ, to a plasma. The resulting smooth, pinhole-free films were typically 500nm to 1\u03bcm thick with a measured dielectric constant varying in the range 4\u20135. For circuit fabrication, the insulator was patterned using shadow masks to define rectangular areas separated by 1mm gaps to act as vias for inter-layer metallic connections. The substrates were then transferred into an evaporator (Minispectros, Kurt Lesker) integrated into a nitrogen glovebox for the vacuum-deposition (2.4nm/min) of DNTT onto the insulator. Without exposing the substrates to ambient air, the gold source/drain metallisation layer was deposited through a shadow mask in the same evaporator.\n", "keywords": "aluminium gate electrodes\nambient air\ncircuit fabrication\ncircuits\ncross-linked\ndrum\nevaporator\nexposure\nfilms\nflash-evaporated TPGDA monomer vapour\ngold source/drain metallisation layer\ninsulator\ninter-layer metallic connections\nnitrogen glovebox\nPEN\nplasma\npolyethylene naphthalate\npolyethylene naphthalate (PEN) substrates\nshadow mask\nshadow masks\nsubstrates\nTFTs\ntracks\nvacuum\nvacuum-deposition\nvacuum evaporated\nvacuum-fabrication procedures\nweb-coater drum\n"}, {"id": "S2214657115000155", "text": "MicroCT has been applied to AM parts in various forms. Some preliminary results demonstrating the visualization of defects including porosity in AM components were reported in [6]. In another study, the porosity structures in parts built with improper settings were investigated [7]. In this work, the average porosity ranged from 0.1\u20130.5%, and large pores were observed which followed the build direction and may be attributed to the electron beam raster and overlap pattern. This was followed by more recent reports of the porosity distribution as a function of build strategy for electron beam melted samples with average porosity < 0.2% [8]. In another study, similar porosity images from microCT were reported at levels above 0.2% average porosity [9,10]. Very recent work reports similar images and may indicate that the porosity structure depends on the build direction [11]. Other applications of the use of microCT to characterize AM parts include the comparison of the part to its design model [12] and the characterization of surface roughness of such parts [13]. In the present work, the aim is to demonstrate a specific type of defect present at very low average porosity levels below 0.01%, and which does not follow the build direction as in some other reported examples. We also demonstrate how this porosity structure changes after Hot Isostatic Pressing (HIP) treatment of the same sample.\n", "keywords": "AM parts\naverage porosity\nbuild direction\ncharacterization of surface roughness\ncharacterize AM parts\ncomparison of the part to its design model\ndemonstrate a specific type of defect\nelectron beam melted samples\nelectron beam raster and overlap pattern.\nHIP\nHot Isostatic Pressing\nmicroCT\nMicroCT\nnot follow the build direction\nporosity distribution\nporosity images\nporosity in AM components\nporosity structure changes\nporosity structures in parts\npreliminary results\nrecent reports\nvery low average porosity levels\nvisualization of defects\n"}, {"id": "S2214657115000155", "text": "MicroCT has been applied to AM parts in various forms. Some preliminary results demonstrating the visualization of defects including porosity in AM components were reported in [6]. In another study, the porosity structures in parts built with improper settings were investigated [7]. In this work, the average porosity ranged from 0.1\u20130.5%, and large pores were observed which followed the build direction and may be attributed to the electron beam raster and overlap pattern. This was followed by more recent reports of the porosity distribution as a function of build strategy for electron beam melted samples with average porosity < 0.2% [8]. In another study, similar porosity images from microCT were reported at levels above 0.2% average porosity [9,10]. Very recent work reports similar images and may indicate that the porosity structure depends on the build direction [11]. Other applications of the use of microCT to characterize AM parts include the comparison of the part to its design model [12] and the characterization of surface roughness of such parts [13]. In the present work, the aim is to demonstrate a specific type of defect present at very low average porosity levels below 0.01%, and which does not follow the build direction as in some other reported examples. We also demonstrate how this porosity structure changes after Hot Isostatic Pressing (HIP) treatment of the same sample.\n", "keywords": "AM parts\naverage porosity\nbuild direction\ncharacterization of surface roughness\ncharacterize AM parts\ncomparison of the part to its design model\ndemonstrate a specific type of defect\nelectron beam melted samples\nelectron beam raster and overlap pattern.\nHIP\nHot Isostatic Pressing\nmicroCT\nMicroCT\nnot follow the build direction\nporosity distribution\nporosity images\nporosity in AM components\nporosity structure changes\nporosity structures in parts\npreliminary results\nrecent reports\nvery low average porosity levels\nvisualization of defects\n"}, {"id": "S2214657115000155", "text": "MicroCT has been applied to AM parts in various forms. Some preliminary results demonstrating the visualization of defects including porosity in AM components were reported in [6]. In another study, the porosity structures in parts built with improper settings were investigated [7]. In this work, the average porosity ranged from 0.1\u20130.5%, and large pores were observed which followed the build direction and may be attributed to the electron beam raster and overlap pattern. This was followed by more recent reports of the porosity distribution as a function of build strategy for electron beam melted samples with average porosity < 0.2% [8]. In another study, similar porosity images from microCT were reported at levels above 0.2% average porosity [9,10]. Very recent work reports similar images and may indicate that the porosity structure depends on the build direction [11]. Other applications of the use of microCT to characterize AM parts include the comparison of the part to its design model [12] and the characterization of surface roughness of such parts [13]. In the present work, the aim is to demonstrate a specific type of defect present at very low average porosity levels below 0.01%, and which does not follow the build direction as in some other reported examples. We also demonstrate how this porosity structure changes after Hot Isostatic Pressing (HIP) treatment of the same sample.\n", "keywords": "AM parts\naverage porosity\nbuild direction\ncharacterization of surface roughness\ncharacterize AM parts\ncomparison of the part to its design model\ndemonstrate a specific type of defect\nelectron beam melted samples\nelectron beam raster and overlap pattern.\nHIP\nHot Isostatic Pressing\nmicroCT\nMicroCT\nnot follow the build direction\nporosity distribution\nporosity images\nporosity in AM components\nporosity structure changes\nporosity structures in parts\npreliminary results\nrecent reports\nvery low average porosity levels\nvisualization of defects\n"}, {"id": "S2214657115000155", "text": "MicroCT has been applied to AM parts in various forms. Some preliminary results demonstrating the visualization of defects including porosity in AM components were reported in [6]. In another study, the porosity structures in parts built with improper settings were investigated [7]. In this work, the average porosity ranged from 0.1\u20130.5%, and large pores were observed which followed the build direction and may be attributed to the electron beam raster and overlap pattern. This was followed by more recent reports of the porosity distribution as a function of build strategy for electron beam melted samples with average porosity < 0.2% [8]. In another study, similar porosity images from microCT were reported at levels above 0.2% average porosity [9,10]. Very recent work reports similar images and may indicate that the porosity structure depends on the build direction [11]. Other applications of the use of microCT to characterize AM parts include the comparison of the part to its design model [12] and the characterization of surface roughness of such parts [13]. In the present work, the aim is to demonstrate a specific type of defect present at very low average porosity levels below 0.01%, and which does not follow the build direction as in some other reported examples. We also demonstrate how this porosity structure changes after Hot Isostatic Pressing (HIP) treatment of the same sample.\n", "keywords": "AM parts\naverage porosity\nbuild direction\ncharacterization of surface roughness\ncharacterize AM parts\ncomparison of the part to its design model\ndemonstrate a specific type of defect\nelectron beam melted samples\nelectron beam raster and overlap pattern.\nHIP\nHot Isostatic Pressing\nmicroCT\nMicroCT\nnot follow the build direction\nporosity distribution\nporosity images\nporosity in AM components\nporosity structure changes\nporosity structures in parts\npreliminary results\nrecent reports\nvery low average porosity levels\nvisualization of defects\n"}, {"id": "S2214657115000155", "text": "MicroCT has been applied to AM parts in various forms. Some preliminary results demonstrating the visualization of defects including porosity in AM components were reported in [6]. In another study, the porosity structures in parts built with improper settings were investigated [7]. In this work, the average porosity ranged from 0.1\u20130.5%, and large pores were observed which followed the build direction and may be attributed to the electron beam raster and overlap pattern. This was followed by more recent reports of the porosity distribution as a function of build strategy for electron beam melted samples with average porosity < 0.2% [8]. In another study, similar porosity images from microCT were reported at levels above 0.2% average porosity [9,10]. Very recent work reports similar images and may indicate that the porosity structure depends on the build direction [11]. Other applications of the use of microCT to characterize AM parts include the comparison of the part to its design model [12] and the characterization of surface roughness of such parts [13]. In the present work, the aim is to demonstrate a specific type of defect present at very low average porosity levels below 0.01%, and which does not follow the build direction as in some other reported examples. We also demonstrate how this porosity structure changes after Hot Isostatic Pressing (HIP) treatment of the same sample.\n", "keywords": "AM parts\naverage porosity\nbuild direction\ncharacterization of surface roughness\ncharacterize AM parts\ncomparison of the part to its design model\ndemonstrate a specific type of defect\nelectron beam melted samples\nelectron beam raster and overlap pattern.\nHIP\nHot Isostatic Pressing\nmicroCT\nMicroCT\nnot follow the build direction\nporosity distribution\nporosity images\nporosity in AM components\nporosity structure changes\nporosity structures in parts\npreliminary results\nrecent reports\nvery low average porosity levels\nvisualization of defects\n"}]